<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.2.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Teerapong Panboonyuen" />

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="https://kaopanboonyuen.github.io/publication-type/2/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.08f2e04360a1c87f5ad39547c02bf219.css" />

  



  

  

  




  
  
  

  
    <link rel="alternate" href="/publication-type/2/index.xml" type="application/rss+xml" title="Teerapong Panboonyuen" />
  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://kaopanboonyuen.github.io/publication-type/2/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Teerapong Panboonyuen" />
  <meta property="og:url" content="https://kaopanboonyuen.github.io/publication-type/2/" />
  <meta property="og:title" content="2 | Teerapong Panboonyuen" />
  <meta property="og:description" content="" /><meta property="og:image" content="https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2025-07-28T00:00:00&#43;00:00" />
    
  

  



  

  





  <title>2 | Teerapong Panboonyuen</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   "  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.4be02a3b391999348b0c7478778a0e4b.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#awards"><span>Awards</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#press"><span>Press</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#tags"><span>Topics</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#communities"><span>Communities</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/blog/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
          
          <li class="nav-item d-none d-lg-inline-flex">
            <a class="nav-link" href="https://x.com/kaopanboonyuen" data-toggle="tooltip" data-placement="bottom" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
              <i class="fab fa-twitter" aria-hidden="true"></i>
            </a>
          </li>
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>2</h1>

  

  
</div>



<div class="universal-wrapper">
  

  
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/" >GuidedBox: A segmentation-guided box teacher-student approach for weakly supervised road segmentation</a>
      </div>

      
      <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/"  class="summary-link">
        <div class="article-style">
          Road segmentation in remote sensing is crucial for applications like urban planning, traffic monitoring, and autonomous driving. Labeling objects via pixel-wise segmentation is challenging compared to bounding boxes. Existing weakly supervised segmentation methods often rely on heuristic bounding box priors, but we propose that box-supervised techniques can yield better results. Introducing GuidedBox, an end-to-end framework for weakly supervised instance segmentation. GuidedBox uses a teacher model to generate high-quality pseudo-masks and employs a confidence scoring mechanism to filter out noisy masks. We also introduce a noise-aware pixel loss and affinity loss to optimize the student model with pseudo-masks. Our extensive experiments show that GuidedBox outperforms state-of-the-art methods like SOLOv2, CondInst, and Mask R-CNN on the Massachusetts Roads Dataset, achieving an AP50 score of 0.9231. It also shows strong performance on SpaceNet and DeepGlobe datasets, proving its versatility in remote sensing applications. Code has been made available at <a href="https://github.com/kaopanboonyuen/GuidedBox" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/GuidedBox</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>European Journal of Remote Sensing (Taylor &amp; Francis)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/22797254.2025.2540963" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/GuidedBox/" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/GuidedBox/" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/GuidedBox/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://pbylab.github.io/" target="_blank" rel="noopener">
    
    PBY.LAB
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/" >
        <img src="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/compact_hu9b8e82bb734e9d02b1fef6fdeacc1ca4_1093084_300x0_resize_lanczos_3.png" alt="GuidedBox: A segmentation-guided box teacher-student approach for weakly supervised road segmentation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/" >SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery</a>
      </div>

      
      <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/"  class="summary-link">
        <div class="article-style">
          Satellite image inpainting is a critical task in remote sensing, requiring accurate restoration of missing or occluded regions for reliable image analysis. In this paper, we present SatDiff, an advanced inpainting framework based on diffusion models, specifically designed to tackle the challenges posed by very high-resolution (VHR) satellite datasets such as DeepGlobe and the Massachusetts Roads Dataset. Building on insights from our previous work, SatInPaint, we enhance the approach to achieve even higher recall and overall performance. SatDiff introduces a novel Latent Space Conditioning technique that leverages a compact latent space for efficient and precise inpainting. Additionally, we integrate Explicit Propagation into the diffusion process, enabling forward-backward fusion for improved stability and accuracy. Inspired by encoder-decoder architectures like the Segment Anything Model (SAM), SatDiff is seamlessly adaptable to diverse satellite imagery scenarios. By balancing the efficiency of preconditioned models with the flexibility of postconditioned approaches, SatDiff establishes a new benchmark in VHR satellite datasets, offering a scalable and high-performance solution for satellite image restoration. The code for SatDiff is publicly available at <a href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/SatDiff</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/SatDiff" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
    
    ArXiv
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/" >
        <img src="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/compact_huaf5cbfabad837d74a6ff8a88a71377d4_1877760_300x0_resize_lanczos_3.png" alt="SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/" >Investigating the use of deep learning-derived weighted mean temperature for GPS-PWVs estimation</a>
      </div>

      
      <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/"  class="summary-link">
        <div class="article-style">
          GNSS data offers a reliable alternative for estimating Precipitable Water Vapor (PWV), but accurate GPS-PWV determination in tropical climates requires weighted mean temperature (Tm). With traditional measurement methods often unavailable in Thailand, and existing empirical models showing low accuracy, we propose a deep learning approach. Our Bidirectional Learning with Attention (BLA) model incorporates GRUs and an attention mechanism for Tm modeling. Trained on ERA5 data (2017-2021) and evaluated on 2022 data, BLA-Tm achieved 76% improvement over conventional models, reducing biases significantly. Validation with 280 GNSS stations confirmed BLA-Tm’s superior accuracy in GPS-PWV estimation.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >C. Charoenphon</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >B. Zhang</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Journal of Spatial Science (Taylor &amp; Francis)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  Source Document
</a>



      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/" >
        <img src="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/compact_hu02d2c55b833f3b38a2d2cd3499a0b5f4_944504_300x0_resize_lanczos_3.png" alt="Investigating the use of deep learning-derived weighted mean temperature for GPS-PWVs estimation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/dota-deformable-optimized-transformer-architecture/" >DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation</a>
      </div>

      
      <a href="/publication/dota-deformable-optimized-transformer-architecture/"  class="summary-link">
        <div class="article-style">
          In this paper, we present a novel end-to-end framework that integrates ResNet and Vision Transformer (ViT) backbones with cutting-edge techniques such as Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations work together to significantly improve feature representation and Optical Character Recognition (OCR) performance. By replacing the standard convolution layers in the third and fourth blocks with Deformable Convolutions, the framework adapts more flexibly to complex text layouts, while adaptive dropout helps prevent overfitting and enhance generalization. Moreover, incorporating CRFs refines the sequence modeling for more accurate text recognition. Extensive experiments on six benchmark datasets—IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80—demonstrate the framework’s exceptional performance. Our method represents a significant leap forward in OCR technology, addressing challenges in recognizing text with various distortions, fonts, and orientations. The framework has proven not only effective in controlled conditions but also adaptable to more complex, real-world scenarios. The code for this framework is available at <a href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/DOTA</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >N. Nithisopa</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In 17th International Conference on Knowledge and Smart Technology (KST2025)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11003289" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dota-deformable-optimized-transformer-architecture/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2505.04175" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-53.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/dota-deformable-optimized-transformer-architecture/" >
        <img src="/publication/dota-deformable-optimized-transformer-architecture/compact_hue28a23c2ef65ff884772a0df9b38ad0f_797010_300x0_resize_lanczos_3.png" alt="DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/mevit-a-medium-resolution-vision-transformer/" >MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</a>
      </div>

      
      <a href="/publication/mevit-a-medium-resolution-vision-transformer/"  class="summary-link">
        <div class="article-style">
          In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT&rsquo;s effectiveness in accurately segmenting Thai Landsat-8 data.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/15/21/5124" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mevit-a-medium-resolution-vision-transformer/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MeVit" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MeViT/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/GYSS/panboonyuen_MeViT_Poster_toGYSS2025.pdf" target="_blank" rel="noopener">
  Poster
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=tgcKR97Ea8I" target="_blank" rel="noopener">
  Video
</a>




      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/mevit-a-medium-resolution-vision-transformer/" >
        <img src="/publication/mevit-a-medium-resolution-vision-transformer/compact_hud19889f4bffd1304aea8878ea4a99c88_1404370_300x0_resize_lanczos_3.png" alt="MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/" >Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama</a>
      </div>

      
      <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/"  class="summary-link">
        <div class="article-style">
          Detecting objects of varying sizes, like kilometer stones, remains a significant challenge and directly affects the accuracy of object counts. Transformers have shown remarkable success in natural language processing (NLP) and image processing due to their ability to model long-range dependencies. This paper proposes an enhanced YOLO (You Only Look Once) series with two key contributions, (i) We employ a pre-training objective to obtain original visual tokens from image patches of road assets, using a pre-trained Vision Transformer (ViT) backbone, which is then fine-tuned on downstream tasks with additional task layers. (ii) We incorporate Feature Pyramid Network (FPN) decoder designs into our deep learning network to learn the significance of different input features, avoiding issues like feature mismatch and performance degradation that arise from simple summation or concatenation. Our proposed method, Transformer-Based YOLOX with FPN, effectively learns general representations of objects and significantly outperforms state-of-the-art detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. It achieves a 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP on the test-dev dataset.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Information</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2078-2489/13/1/5" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/object-detection-of-road-assets-using-transformer-based-yolox/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/" >
        <img src="/publication/object-detection-of-road-assets-using-transformer-based-yolox/compact_hu3b317f59a79e72de1b05205333c2d7f9_616141_300x0_resize_lanczos_3.png" alt="Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/rainfall-prediction-a-machine-learning-approach/" >A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province</a>
      </div>

      
      <a href="/publication/rainfall-prediction-a-machine-learning-approach/"  class="summary-link">
        <div class="article-style">
          Flooding poses a significant challenge in Thailand due to its complex geography, traditionally addressed through GIS methods like the Flood Risk Assessment Model (FRAM) combined with the Analytical Hierarchy Process (AHP). This study assesses the efficacy of Artificial Neural Networks (ANN) in flood susceptibility mapping, using data from Ayutthaya Province and incorporating 5-fold cross-validation and Stochastic Gradient Descent (SGD) for training. ANN achieved superior performance with precision of 79.90%, recall of 79.04%, F1-score of 79.08%, and accuracy of 79.31%, outperforming the traditional FRAM approach. Notably, ANN identified that only three factors—flow accumulation, elevation, and soil types—were crucial for predicting flood-prone areas. This highlights the potential for ANN to simplify and enhance flood risk assessments. Moreover, the integration of advanced machine learning techniques underscores the evolving capability of AI in addressing complex environmental challenges.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >T. Vajeethaveesin</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Trends in Sciences (Trends Sci. or TiS)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://tis.wu.ac.th/index.php/tis/article/view/2038" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rainfall-prediction-a-machine-learning-approach/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RainNet-ML" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RainNet-ML" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/rainfall-prediction-a-machine-learning-approach/" >
        <img src="/publication/rainfall-prediction-a-machine-learning-approach/compact_hub06b8241b47209da7c8f2d1d7857e458_1587300_300x0_resize_lanczos_3.png" alt="A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/enhanced-feature-pyramid-vision-transformert/" >Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus</a>
      </div>

      
      <a href="/publication/enhanced-feature-pyramid-vision-transformert/"  class="summary-link">
        <div class="article-style">
          Semantic segmentation on Landsat-8 data is crucial in the integration of diverse data, allowing researchers to achieve more productivity and lower expenses. This research aimed to improve the versatile backbone for dense prediction without convolutions—namely, using the pyramid vision transformer (PRM-VS-TM) to incorporate attention mechanisms across various feature maps. Furthermore, the PRM-VS-TM constructs an end-to-end object detection system without convolutions and uses handcrafted components, such as dense anchors and non-maximum suspension (NMS). The present study was conducted on a private dataset, i.e., the Thailand Landsat-8 challenge. There are three baselines, DeepLab, Swin Transformer (Swin TF), and PRM-VS-TM. Results indicate that the proposed model significantly outperforms all current baselines on the Thailand Landsat-8 corpus, providing F1-scores greater than 80% in almost all categories. Finally, we demonstrate that our model, without utilizing pre-trained settings or any further post-processing, can outperform current state-of-the-art (SOTA) methods for both agriculture and forest classes.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Rakwatin</span>, <span >K. Intarat</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Information</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2078-2489/13/5/259" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/enhanced-feature-pyramid-vision-transformert/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/GeoAI-Landslides" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/GeoAI-Landslides/" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/enhanced-feature-pyramid-vision-transformert/" >
        <img src="/publication/enhanced-feature-pyramid-vision-transformert/compact_hu6fc248b430e2b5795f1f28d8ca3f99ec_800538_300x0_resize_lanczos_3.png" alt="Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/the-bangkok-urbanscapes-dataset/" >The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks</a>
      </div>

      
      <a href="/publication/the-bangkok-urbanscapes-dataset/"  class="summary-link">
        <div class="article-style">
          This paper addresses semantic segmentation for autonomous driving systems, focusing on self-driving cars in Thailand. We introduce DeepLab-V3-A1 with Xception, an enhanced version of DeepLab-V3+, and present the Bangkok Urbanscapes dataset. Our method improves segmentation accuracy by refining the decoder and modifying the Xception backbone. Experiments on four datasets, including CamVid, Cityscapes, IDD, and our proposed dataset, show our approach performs comparably to baseline methods. Our dataset includes 701 annotated images of various Bangkok driving environments, covering eleven semantic classes. The architecture and dataset aim to aid developers in improving autonomous driving systems for diverse urban conditions. Implementation codes and dataset are available at <a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">https://kaopanboonyuen.github.io/bkkurbanscapes</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >K. Thitisiriwech</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Kantavat</span>, <span >Y. Iwahori</span>, <span >B. Kijsirikul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779212" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/the-bangkok-urbanscapes-dataset/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/bkkurbanscapes" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">
    
    GitHub Page
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/the-bangkok-urbanscapes-dataset/" >
        <img src="/publication/the-bangkok-urbanscapes-dataset/compact_hu979c8f0d7e1c7eb9d595e6800592be70_1249132_300x0_resize_lanczos_3.png" alt="The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/" >Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</a>
      </div>

      
      <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/"  class="summary-link">
        <div class="article-style">
          Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% 𝐹1 score), Thailand North Landsat-8 corpus (63.12% 𝐹1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sesning</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/13/24/5100" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/transformer-based-decoder-designs-for-semantic-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/" >
        <img src="/publication/transformer-based-decoder-designs-for-semantic-segmentation/compact_huff770b2ecf5c710cbddd5786ef8411d7_456055_300x0_resize_lanczos_3.png" alt="Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
    
  

  
<nav class="mt-1">
  <ul class="pagination justify-content-center">
    
    
    <li class="page-item"><a class="page-link" href="/publication-type/2/page/2/">&raquo;</a></li>
    
  </ul>
</nav>


</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  

  
  <p class="powered-by">
    ©2025 Kao Panboonyuen
  </p>
  

  
  






  <p class="powered-by">
    
    Built using <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> and the <a href="https://github.com/wowchemy/starter-hugo-academic" target="_blank" rel="noopener">Wowchemy academic template</a>. View <a href="https://github.com/kaopanboonyuen/kaopanboonyuen.github.io" target="_blank" rel="noopener">source</a>.
        
  </p>
</footer>
    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/golang.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.b36873e4e886c7b03b21e4eb97d9b6d7.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
