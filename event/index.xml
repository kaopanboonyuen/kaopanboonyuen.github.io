<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recent &amp; Upcoming Talks | Teerapong Panboonyuen</title>
    <link>https://kaopanboonyuen.github.io/event/</link>
      <atom:link href="https://kaopanboonyuen.github.io/event/index.xml" rel="self" type="application/rss+xml" />
    <description>Recent &amp; Upcoming Talks</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>¬©2025 Kao Panboonyuen</copyright><lastBuildDate>Thu, 17 Jul 2025 13:30:00 +0000</lastBuildDate>
    <image>
      <url>https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png</url>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://kaopanboonyuen.github.io/event/</link>
    </image>
    
    <item>
      <title>TSCCM2025 (The 14th Critical Care Conference)</title>
      <link>https://kaopanboonyuen.github.io/talk/tsccm2025-the-14th-critical-care-conference/</link>
      <pubDate>Thu, 17 Jul 2025 13:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/tsccm2025-the-14th-critical-care-conference/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;üìù Interested in the full technical walkthrough? Read the complete CU-ICU LLM blog post &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;üé§ You can download the full presentation slides from my CU-ICU oral talk at TSCCM 2025 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/Panboonyuen_CUICU_TSCCM2025_Slide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;üìÑ Curious to dive deeper into the research? You can also read the full CU-ICU paper on arXiv &lt;a href=&#34;http://arxiv.org/abs/2507.13655&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>UAMC2025 (Applied Mathematics Conference)</title>
      <link>https://kaopanboonyuen.github.io/talk/uamc2025-applied-mathematics-conference/</link>
      <pubDate>Thu, 27 Mar 2025 13:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/uamc2025-applied-mathematics-conference/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;üöóüí° This Sunday (March 30, 2025), if you&amp;rsquo;re around Ladkrabang, come join us for a math talk! üìöüí¨ We‚Äôll dive deep into the fascinating world of Vision Transformers (ViTs) and their applications in car insurance AI. It&amp;rsquo;s an exciting opportunity to explore how advanced mathematical techniques are driving the future of AI in the insurance industry. Don‚Äôt miss out‚Äîjoin us for this insightful session!&lt;/p&gt;
&lt;p&gt;Topic: Mathematical Foundations of Vision Transformers in Car Insurance AI&lt;br&gt;
üïê Time: 1:00 PM&lt;/p&gt;
&lt;p&gt;üìç Location: &lt;a href=&#34;https://www.facebook.com/photo?fbid=1192313742905954&amp;amp;set=a.490286653108670&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Event Location&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;uamc2025_images/UAMC2025xTVIxMARS_0001.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xTVIxMARS_0002.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Read more on Thaivivat&amp;rsquo;s blog:&lt;/strong&gt; &lt;a href=&#34;https://careers.thaivivat.co.th/newsandevents/6808628ebfad6e8912fd5c57&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    In this insightful session, Dr. Teerapong Panboonyuen (Dr. Kao) will explore the mathematical foundations behind Vision Transformers (ViTs) and their groundbreaking applications in car insurance AI.
  &lt;/div&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;As the insurance industry increasingly leverages artificial intelligence, understanding the complex mathematical principles that drive these innovations is key. This talk will shed light on how ViTs are revolutionizing the way car insurance companies analyze and predict risk, offering a glimpse into the future of InsurTech. Whether you&amp;rsquo;re a student or a professional, this is an exciting opportunity to dive into the intersection of math, AI, and insurance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;(Reference on UAMC 2025)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_05.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_02.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_04.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_03.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_06.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_01.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_07.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_08.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In recent years, Vision Transformers (ViTs) have emerged as one of the most powerful models in the field of deep learning, particularly in tasks involving image recognition. This blog post explores the mathematical foundations of ViTs and their application in car insurance AI, with a focus on improving accuracy in claims prediction. We will cover key concepts such as the self-attention mechanism, custom loss functions, and how large language models (LLMs) could further enhance these AI systems. By the end of this article, you‚Äôll have a deeper understanding of how cutting-edge AI technologies are reshaping the insurance industry.&lt;/p&gt;
&lt;h2 id=&#34;vision-transformers-a-mathematical-overview&#34;&gt;Vision Transformers: A Mathematical Overview&lt;/h2&gt;
&lt;h3 id=&#34;the-transformer-architecture&#34;&gt;The Transformer Architecture&lt;/h3&gt;
&lt;p&gt;At the heart of Vision Transformers lies the Transformer architecture, which was originally designed for natural language processing tasks. Unlike traditional convolutional neural networks (CNNs), which rely on convolutional layers to process spatial data, ViTs treat an image as a sequence of patches and apply self-attention to learn relationships between these patches.&lt;/p&gt;
&lt;p&gt;The image is divided into non-overlapping patches, each of which is flattened into a vector. These patch vectors are then embedded into a higher-dimensional space. The idea is that these embedded patches will be processed as a sequence, similar to how words in a sentence are processed in NLP tasks. This sequence is passed through layers of self-attention, where the model learns how to focus on different parts of the image depending on their relevance.&lt;/p&gt;
&lt;h3 id=&#34;self-attention-mechanism&#34;&gt;Self-Attention Mechanism&lt;/h3&gt;
&lt;p&gt;The self-attention mechanism is what allows Vision Transformers to capture complex relationships between distant regions of the image. In a standard convolutional network, filters are used to detect local patterns in the image. However, in ViTs, the self-attention mechanism dynamically determines the importance of each patch relative to others. This makes it possible for the model to capture long-range dependencies and subtle patterns across the entire image, rather than focusing only on local features.&lt;/p&gt;
&lt;p&gt;By applying this mechanism, Vision Transformers can give more weight to certain parts of the image that are more relevant for the task at hand, whether it‚Äôs identifying damage in a vehicle or classifying certain types of claims.&lt;/p&gt;
&lt;h2 id=&#34;custom-loss-functions-for-car-insurance-ai&#34;&gt;Custom Loss Functions for Car Insurance AI&lt;/h2&gt;
&lt;h3 id=&#34;the-need-for-custom-loss-functions&#34;&gt;The Need for Custom Loss Functions&lt;/h3&gt;
&lt;p&gt;In the context of car insurance, traditional loss functions like cross-entropy are not always ideal. For instance, car insurance claims often involve rare and highly specific damage types that are underrepresented in training datasets. This creates a class imbalance issue, where the model may not perform well on minority classes, such as rare types of damage or low-frequency events.&lt;/p&gt;
&lt;p&gt;To address this, custom loss functions are developed to weight certain classes higher than others, ensuring that the model focuses on less frequent but equally important classes. This type of custom loss function helps improve prediction accuracy, particularly in a domain like insurance, where predicting the likelihood of rare events is just as crucial as predicting common ones.&lt;/p&gt;
&lt;h3 id=&#34;mean-squared-error-for-regression-tasks&#34;&gt;Mean Squared Error for Regression Tasks&lt;/h3&gt;
&lt;p&gt;In some cases, car insurance AI needs to predict continuous values, such as the cost of repair. For such tasks, a loss function like Mean Squared Error (MSE) is commonly used. This loss function measures the difference between the predicted and true values, with the aim of minimizing this error. Using MSE ensures that the model‚Äôs predictions are as close as possible to the actual values, which is critical when estimating damage repair costs.&lt;/p&gt;
&lt;h2 id=&#34;integrating-large-language-models-llms-for-enhanced-ai&#34;&gt;Integrating Large Language Models (LLMs) for Enhanced AI&lt;/h2&gt;
&lt;p&gt;The integration of Large Language Models (LLMs) like GPT-4 with Vision Transformers can open up new possibilities for multi-modal AI systems. In car insurance, LLMs can process textual data‚Äîsuch as customer reports, claim descriptions, and policy documents‚Äîwhile ViTs handle visual data like images of the damaged vehicle.&lt;/p&gt;
&lt;p&gt;By combining these two modalities, we can create a system that not only understands visual information but also the context behind it. For example, an LLM could help interpret the description of an accident or read through a claim submission and make sense of the visual evidence provided by the ViT. This multi-modal approach enhances decision-making, enabling more accurate claim assessments and better customer experiences.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Vision Transformers are revolutionizing AI in the car insurance industry by enabling the accurate and efficient processing of complex visual data. By leveraging advanced mathematical concepts like self-attention and custom loss functions, we can build AI models that are more accurate and robust, even when dealing with rare and highly variable data. The potential for combining ViTs with LLMs represents the next frontier in AI, making it possible to create systems that understand both images and text, paving the way for more intelligent, automated solutions in car insurance.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., &amp;amp; Polosukhin, I. (2017). &lt;em&gt;Attention is All You Need&lt;/em&gt;. In Advances in Neural Information Processing Systems (NeurIPS 2017). &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dosovitskiy, A., Berman, M., &amp;amp; Hinton, G. E. (2020). &lt;em&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/em&gt;. In Proceedings of the International Conference on Machine Learning (ICML 2020). &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shinn, N., &amp;amp; others. (2020). &lt;em&gt;Language Models are Few-Shot Learners&lt;/em&gt;. In Proceedings of NeurIPS 2020. &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OpenAI. (2023). &lt;em&gt;GPT-4 Technical Report&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2303.08774&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>NAC2025 (NSTDA Annual Conference 2025)</title>
      <link>https://kaopanboonyuen.github.io/talk/nac2025-nstda-annual-conference-2025/</link>
      <pubDate>Wed, 26 Feb 2025 09:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/nac2025-nstda-annual-conference-2025/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;h2 id=&#34;join-the-fun-world-of-ai-train-a-model-to-find-wheres-waldo&#34;&gt;&lt;strong&gt;Join the Fun World of AI: Train a Model to Find ‚ÄúWhere‚Äôs Waldo‚Äù!&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Hey there, AI enthusiasts and curious minds! Are you ready to take a deep dive into the fascinating world of artificial intelligence? Get ready for a fun and hands-on experience where we‚Äôll unravel the mysteries of &lt;strong&gt;Deep Learning&lt;/strong&gt; üß†, &lt;strong&gt;Computer Vision&lt;/strong&gt; üëÄ, and &lt;strong&gt;Vision Transformers&lt;/strong&gt;‚Äîthe game-changing technology behind image recognition. But here‚Äôs the twist: we‚Äôre going to train an AI to solve the ultimate challenge‚Äîfinding &lt;strong&gt;Where‚Äôs Waldo&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;Yep, you heard it right. We‚Äôll be combining cutting-edge AI techniques with a classic puzzle, and by the end of the session, you‚Äôll be equipped to build and train an AI model that can spot Waldo in a sea of distractions. Ready to unlock the magic? Let‚Äôs go!&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/e37dSDJeLos&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;If this sounds exciting to you, don‚Äôt miss out on the chance to be part of this hands-on AI experience at &lt;strong&gt;&lt;a href=&#34;https://www.nstda.or.th/nac/2025/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAC2025&lt;/a&gt;&lt;/strong&gt;. Whether you‚Äôre just starting to explore the world of artificial intelligence or looking to enhance your skills, this session is your perfect opportunity to learn, build, and innovate.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Join me and other AI enthusiasts on &lt;strong&gt;March 26th&lt;/strong&gt;, and together we‚Äôll dive into this challenge while discovering the potential of AI in real-world applications. &lt;strong&gt;Click &lt;a href=&#34;https://www.nstda.or.th/nac/2025/youth-activities/youth-activity-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/strong&gt; to secure your spot and be part of the future of AI-driven innovation at NAC2025. Let‚Äôs train some AI and find Waldo‚Äîtogether!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;what-youll-learn&#34;&gt;&lt;strong&gt;What You‚Äôll Learn&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;If you‚Äôve ever wondered how self-driving cars ‚Äúsee‚Äù the world, or how facial recognition software works, you‚Äôre in for a treat. In this session, we‚Äôll break down the essentials of &lt;strong&gt;Deep Learning&lt;/strong&gt; and &lt;strong&gt;Computer Vision&lt;/strong&gt;‚Äîtwo critical components that enable machines to understand and interpret the visual world, just like humans do.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The Basics of Deep Learning&lt;/strong&gt;&lt;br&gt;
At the core of modern AI, &lt;strong&gt;Deep Learning&lt;/strong&gt; involves training algorithms to recognize patterns and make decisions based on data. We‚Äôll start by exploring how these algorithms learn from massive datasets, allowing them to &amp;ldquo;see&amp;rdquo; and make sense of images, sounds, and even text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Understanding Computer Vision&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Computer Vision&lt;/strong&gt; is a field within AI that teaches computers to interpret and analyze visual information from the world. It&amp;rsquo;s the backbone behind everything from image classification to object detection, and it&amp;rsquo;s what powers applications like Google Images, Snapchat filters, and even medical imaging. We‚Äôll cover how vision systems break down images into recognizable patterns, enabling the AI to understand what it‚Äôs looking at.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The Rise of Vision Transformers&lt;/strong&gt;&lt;br&gt;
Here‚Äôs where it gets really cool. &lt;strong&gt;Vision Transformers&lt;/strong&gt; (ViTs) have emerged as a breakthrough in image recognition. Unlike traditional Convolutional Neural Networks (CNNs), which process images in a grid-like manner, Vision Transformers work by treating images as sequences, similar to how we process language. These models capture global relationships within an image, allowing them to recognize complex patterns with remarkable accuracy. We‚Äôll explore how ViTs work, and why they‚Äôre the future of computer vision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building an AI Model to Find Waldo&lt;/strong&gt;&lt;br&gt;
This is where you roll up your sleeves! In this hands-on segment, we‚Äôll show you how to train an AI model to identify Waldo in various images. We‚Äôll walk through the entire process‚Äîpreparing the data, building the model, training it, and evaluating its performance. Along the way, we‚Äôll use the power of &lt;strong&gt;Vision Transformers&lt;/strong&gt; to detect Waldo, just like how cutting-edge AI models are trained to spot objects, faces, or animals in any image. It&amp;rsquo;s going to be a blast as you watch your AI model get better and better at solving the ‚ÄúWhere‚Äôs Waldo‚Äù puzzle!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;why-vision-transformers&#34;&gt;&lt;strong&gt;Why Vision Transformers?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;So, why are we focusing on &lt;strong&gt;Vision Transformers&lt;/strong&gt;? It all comes down to performance. Traditional CNNs have been the go-to for image recognition for years, but ViTs are changing the game. Here‚Äôs why they‚Äôre so exciting:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Global Context Understanding&lt;/strong&gt;&lt;br&gt;
Vision Transformers are great at understanding the entire image as a whole. Instead of focusing on small patches, like CNNs, ViTs look at the global context, allowing them to capture relationships between distant parts of an image. This makes them excellent for complex recognition tasks, like identifying Waldo in a crowded scene where he could be hiding behind multiple objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;br&gt;
Vision Transformers scale well with large datasets, which is essential for training models on more complex images. They can process thousands of images simultaneously, learning from a massive variety of examples to improve their performance. So, whether you‚Äôre training a model to find Waldo or doing more advanced tasks like facial recognition, ViTs can handle it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;&lt;br&gt;
ViTs are highly adaptable to various image types and problems. Whether you‚Äôre working with low-resolution images or high-quality satellite photos, Vision Transformers can be fine-tuned to deliver impressive results across different domains.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;hands-on-training-your-own-wheres-waldo-model&#34;&gt;&lt;strong&gt;Hands-on: Training Your Own ‚ÄúWhere‚Äôs Waldo‚Äù Model&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Now that we‚Äôve got the basics covered, it‚Äôs time to get your hands dirty! During this interactive session, you‚Äôll have the chance to train your very own AI model. Here‚Äôs a sneak peek at the process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preparing the Data&lt;/strong&gt;&lt;br&gt;
Before we can train our AI, we need to prepare a dataset of images featuring Waldo. You‚Äôll learn how to curate and preprocess images for training‚Äîan essential skill for any AI project. You‚Äôll also get hands-on experience labeling images to help your model recognize Waldo in the wild.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building the Model&lt;/strong&gt;&lt;br&gt;
Once we have the data, we‚Äôll build our &lt;strong&gt;Vision Transformer&lt;/strong&gt; model from scratch. You‚Äôll learn the key concepts behind Vision Transformers‚Äîhow they work, what makes them different from traditional CNNs, and why they‚Äôre so powerful for image recognition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training the Model&lt;/strong&gt;&lt;br&gt;
This is where the magic happens. We‚Äôll train our model using the images and labels you‚Äôve prepared. You‚Äôll see how the model starts to ‚Äúlearn‚Äù and get better at recognizing Waldo over time. We‚Äôll guide you through each step, from selecting the right optimizer to tuning the model‚Äôs hyperparameters for maximum performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Evaluating the Results&lt;/strong&gt;&lt;br&gt;
After training your model, it‚Äôs time to evaluate how well it does at finding Waldo in new, unseen images. We‚Äôll show you how to measure accuracy and fine-tune the model to improve its performance. You‚Äôll walk away with a trained model that can spot Waldo with impressive accuracy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;whats-the-big-deal-about-wheres-waldo&#34;&gt;&lt;strong&gt;What‚Äôs the Big Deal About ‚ÄúWhere‚Äôs Waldo‚Äù?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;You might be wondering, why use a ‚ÄúWhere‚Äôs Waldo‚Äù puzzle to teach AI? The answer is simple: it‚Äôs a fun, relatable challenge that‚Äôs perfect for learning the basics of image recognition. But it‚Äôs also a great example of real-world AI applications, where the task is to identify a specific object (Waldo) in a noisy environment (a crowded scene). By building this model, you‚Äôre learning the same techniques that power cutting-edge AI applications used by companies like Google, Facebook, and even self-driving car manufacturers!&lt;/p&gt;
&lt;h3 id=&#34;why-you-should-join-us&#34;&gt;&lt;strong&gt;Why You Should Join Us&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This isn‚Äôt just another dry lecture on AI. This is your chance to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learn in a fun, hands-on environment&lt;/strong&gt; with practical examples you can apply to your own projects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Master the basics of deep learning and computer vision&lt;/strong&gt; while exploring one of the most exciting advancements in AI.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Get a taste of the future&lt;/strong&gt; by learning about Vision Transformers, a breakthrough technology that‚Äôs revolutionizing image recognition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Walk away with a trained AI model&lt;/strong&gt; you can proudly say you built yourself!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Whether you&amp;rsquo;re a beginner eager to learn about AI or someone looking to deepen your understanding of computer vision, this session is for you. So, come join the fun, unleash your creativity, and let‚Äôs train a model that finds Waldo in no time! üöÄ&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Looking forward to seeing you at NAC2025, where AI meets fun! Let‚Äôs make the world of AI an adventure, one Waldo at a time! üéØ&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;meet-me-at-nac2025-wheres-waldo-ai-challenge-on-march-26th&#34;&gt;&lt;strong&gt;Meet Me at NAC2025: Where&amp;rsquo;s Waldo AI Challenge on March 26th!&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Get ready for a one-of-a-kind experience at &lt;strong&gt;NAC2025&lt;/strong&gt;‚Äîthe 20th NSTDA Annual Conference! While the conference runs from &lt;strong&gt;March 26-28, 2025&lt;/strong&gt;, &lt;strong&gt;I‚Äôm inviting you to join me on March 26th&lt;/strong&gt; for an exclusive, hands-on session where we‚Äôll dive deep into the world of &lt;strong&gt;AI and computer vision&lt;/strong&gt; with the ultimate challenge: &lt;strong&gt;Finding Where‚Äôs Waldo!&lt;/strong&gt; üéØ&lt;/p&gt;
&lt;p&gt;At &lt;strong&gt;NAC2025&lt;/strong&gt;, we‚Äôre exploring how AI is transforming industries and driving a more sustainable future for Thailand. But on &lt;strong&gt;March 26th&lt;/strong&gt;, I‚Äôm turning the spotlight on a super fun and interactive topic‚Äîteaching AI to spot &lt;strong&gt;Waldo&lt;/strong&gt; in a crowd! With &lt;strong&gt;Vision Transformers&lt;/strong&gt; and &lt;strong&gt;Deep Learning&lt;/strong&gt;, we‚Äôll break down how AI can ‚Äúsee‚Äù the world just like humans and even find that elusive red-and-white-striped figure hiding among distractions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;NAC2025_Kao_01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;NAC2025_Kao_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;So, if you‚Äôre ready to learn the basics of AI, dive into &lt;strong&gt;computer vision&lt;/strong&gt;, and have a blast solving the ‚ÄúWhere‚Äôs Waldo?‚Äù puzzle, don‚Äôt miss this exclusive event on March 26th! The rest of the &lt;strong&gt;NAC2025&lt;/strong&gt; conference is packed with innovation, but this hands-on workshop is your chance to get involved, ask questions, and &lt;strong&gt;train your very own AI model&lt;/strong&gt;. Trust me, it‚Äôs going to be a lot of fun!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;NAC2025_Kao_03.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;See you there on March 26th‚Äîlet&amp;rsquo;s crack the code and find Waldo together! üîç&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Where&amp;rsquo;s Waldo Dataset&lt;/strong&gt;: &lt;a href=&#34;https://www.kaggle.com/datasets/residentmario/wheres-waldo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle - Where&amp;rsquo;s Waldo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finding Waldo - A Primer&lt;/strong&gt;: &lt;a href=&#34;https://www.kaggle.com/code/residentmario/finding-waldo-a-primer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle - Finding Waldo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;image-credits&#34;&gt;&lt;strong&gt;Image Credits&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.imdb.com/title/tt0213376/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IMDB - Where&amp;rsquo;s Waldo (1992 TV series)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.in/Wheres-Waldo-Martin-Handford/dp/153621065X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Where&amp;rsquo;s Waldo - Book on Amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Young Scientists Quickfire Pitch</title>
      <link>https://kaopanboonyuen.github.io/talk/young-scientists-quickfire-pitch/</link>
      <pubDate>Thu, 24 Oct 2024 18:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/young-scientists-quickfire-pitch/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/tgcKR97Ea8I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Exploring Careers as an AI Research Scientist</title>
      <link>https://kaopanboonyuen.github.io/talk/exploring-careers-as-an-ai-research-scientist/</link>
      <pubDate>Mon, 02 Sep 2024 09:05:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/exploring-careers-as-an-ai-research-scientist/</guid>
      <description>&lt;h3 id=&#34;exploring-career-paths-in-ai-research&#34;&gt;Exploring Career Paths in AI Research&lt;/h3&gt;
&lt;p&gt;Hi guys! Welcome to my posts‚ÄîI‚Äôm stoked to have you here. I‚Äôm currently rocking the roles of Senior Research Scientist at MARS (Motor AI Recognition Solution) and Postdoctoral Fellow at Chulalongkorn University.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Teerapong Panboonyuen (‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;but you can call me Kao (‡πÄ‡∏Å‡πâ‡∏≤).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this space, I‚Äôm excited to share the highs and lows of my AI journey, how I juggle between academic and industry work, and the coolest trends shaking up the AI world. Stick around and dive into the world of AI with me!&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Wrote about generative AI trends and practical applications. &lt;a href=&#34;https://t.co/SphjkqXjNk&#34;&gt;https://t.co/SphjkqXjNk&lt;/a&gt;&lt;br&gt;&lt;br&gt;Here is what ChatGPT suggested as a fun tweet for the blog:&lt;br&gt;&lt;br&gt;üöÄ Explore the future of Generative AI!  &lt;br&gt;ü§ñ Uncover the latest trends and see how AI is revolutionizing various industries.&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1819576227579212096?ref_src=twsrc%5Etfw&#34;&gt;August 3, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h2 id=&#34;my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/h2&gt;
&lt;p&gt;I got into AI back when I was doing my Master‚Äôs at Chulalongkorn University. The challenges and possibilities in AI were just too exciting to ignore. By 24, I had my Master‚Äôs under my belt, and by 27, I was rocking a Ph.D. Since then, I‚Äôve been diving deep into AI research, especially in areas like Remote Sensing and Computer Vision. I‚Äôm all about the hardcore math behind AI‚Äîlike optimization and statistical learning. My big goal? Using AI to solve real-world problems and make the world a better place. If you want to see what I‚Äôm working on, check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exploring the Life of an AI Research Scientist&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the world of AI research, every day is a blend of cutting-edge exploration and meticulous analysis. As an AI research scientist, your life revolves around decoding complex algorithms, fine-tuning models, and pushing the boundaries of what artificial intelligence can achieve. The journey typically involves diving into vast datasets, developing and experimenting with sophisticated neural networks, and translating theoretical concepts into practical, real-world applications. The thrill of seeing a new model perform exceptionally well or uncovering a novel insight drives the passion in this field. Collaboration with peers and staying abreast of the latest advancements is crucial, making continuous learning an integral part of the job.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/A3TZSadhC9I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Transforming Research with Gemini and Modern LLMs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The landscape of AI research is undergoing a significant transformation with the advent of advanced large language models (LLMs) like Gemini. These cutting-edge tools are revolutionizing how researchers approach their work, enabling more efficient data processing and deeper insights. Gemini‚Äôs innovative architecture offers enhanced capabilities in understanding and generating human-like text, which streamlines the development of sophisticated AI systems. By leveraging LLMs, researchers can automate complex tasks, accelerate experimentation, and uncover patterns that were previously challenging to detect. This paradigm shift not only boosts productivity but also opens new avenues for exploration, setting the stage for groundbreaking advancements in artificial intelligence.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sPiOP_CB54A&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;Right now, I‚Äôm diving deep as a Postdoctoral Fellow in AI research, a role I‚Äôve embraced from the age of 27 to now, at 31. My journey involves crafting next-gen algorithms in Pattern Recognition, Optimization Theory, and Statistical Learning. At MARS, I‚Äôm on the front lines, applying AI to tackle real-world challenges, especially in the auto insurance sector.&lt;/p&gt;
&lt;p&gt;Curious to know more about my work and adventures? Check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;!-- ![](featured_vertical.png) --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/h2&gt;
&lt;p&gt;Why do I juggle both academic and industrial roles? The answer lies in the different kinds of excitement each provides. In academia, I&amp;rsquo;m drawn to the elegance and complexity of theoretical work‚Äîunderstanding AI at its core and pushing its boundaries. On the other hand, the industrial side offers the thrill of seeing AI solutions deployed in real-world applications, making a tangible impact.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    I firmly believe that combining both worlds enriches my research. It&amp;rsquo;s incredibly fulfilling to publish groundbreaking work and even more exhilarating when that research translates into practical solutions that benefit society. This dual approach keeps me grounded in the realities of implementation while allowing me to explore theoretical possibilities.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/h2&gt;
&lt;p&gt;The ideal characteristics (Fig. 2) envisioned for AI agents are numerous, each presenting its own significant research challenge before even considering the automatic acquisition of these traits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn&lt;/strong&gt;: The ability to enhance its learning process over time [2]‚Äì[8].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lifelong learning&lt;/strong&gt;: Engaging in continual and incremental learning throughout its existence [9]‚Äì[13].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradual knowledge and skill accumulation&lt;/strong&gt;: Building up knowledge and abilities progressively, layer by layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reuse of learned knowledge&lt;/strong&gt;: Applying previously acquired skills to discover and learn new ones, incorporating both forward and backward knowledge transfer [10].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-ended exploration&lt;/strong&gt;: The capability to explore without predefined boundaries [14], [15] and to set its own self-invented goals for learning [16]‚Äì[20].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Out-of-distribution generalization&lt;/strong&gt;: Extending its learning capabilities to new and previously unseen problems [21]‚Äì[24] and making logical extrapolations beyond its initial training data [25], [26].&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_01.png&#34; alt=&#34;TA Badger &#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. TA Badger agent is trained with bi-level optimization, involving two loops: the outer loop, which focuses on lifelong learning and other requirements, and the inner loop, where the agent undergoes extensive training on various curricula to develop skills approaching human-level proficiency. &lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34;&gt;Goodai-Research-Roadmap&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;featured_03.png&#34; alt=&#34;Kao&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. I had the chance to dive into &#34;Career Paths for AI Research Scientists: My Journey and Insights&#34; during a talk at Sirindhorn Science Home (SSH). It was a great opportunity to share my experiences and offer some tips on navigating the exciting world of AI research. &lt;a href=&#34;https://www.nstda.or.th/ssh/&#34; target=&#34;_blank&#34;&gt;Sirindhorn Science Home (SSH)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are various strategies to develop agents with these properties. At GoodAI, they have converged on foundational principles such as the modularity of agents, a shared policy across modules with varying internal states, and a blend of meta-learning in the outer loop followed by open-ended learning in the inner loop. These principles are central to their Badger architectures and will be discussed further in the section &amp;ldquo;Towards Implementation.&amp;rdquo; It is essential to highlight that these desired properties should manifest during the agent&amp;rsquo;s operational phase, specifically in the inner loop (the agent‚Äôs lifetime). They often utilize a meta-learning approach, which involves a bi-level optimization process where optimization occurs at two levels [4], [27], [28]. This meta-learning framework is considered the default setting throughout this discussion unless otherwise noted.&lt;/p&gt;
&lt;h2 id=&#34;the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/h2&gt;
&lt;p&gt;One of the key motivators for any researcher is the &amp;ldquo;cool factor&amp;rdquo;‚Äîthat sense of excitement when working on something groundbreaking. For me, that thrill comes from applying AI to satellite imagery for Land Use and Land Cover (LULC) analysis in agriculture. The very idea of using AI to derive insights from images captured from space is inherently fascinating.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Imagine using AI to assist in medical diagnostics. For instance, developing an AI model that can detect polyps or tumors during a colonoscopy more accurately than current state-of-the-art methods. Not only is this research cool, but it also has a profound impact‚Äîit can save lives. AI might not yet match human experts in every scenario, but as an early detection tool, its potential is undeniable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;For those pursuing a career as AI research scientists, it&amp;rsquo;s essential to understand the different categories of AI based on their capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Narrow AI (Weak AI or ANI):&lt;/strong&gt; Narrow AI is specialized in performing specific tasks. It is designed with a narrow focus and cannot operate outside its pre-defined capabilities. Research in this area involves developing and fine-tuning algorithms to perform specialized tasks efficiently, such as facial recognition, language translation, and recommendation systems. Career opportunities here include roles like AI specialist, data scientist, and machine learning engineer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;General AI (Strong AI or AGI):&lt;/strong&gt; General AI aims to mirror human cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. Working in this field requires a deep understanding of various AI and machine learning techniques, and researchers often focus on creating systems that can think and reason like humans. Careers in this area might involve research positions in advanced AI labs, academia, or tech companies that are pioneering AGI development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Artificial Superintelligence (ASI):&lt;/strong&gt; ASI represents the pinnacle of AI development, where machines would surpass human intelligence across all domains. Research here is still theoretical but involves exploring concepts that could eventually lead to machines with superior cognitive abilities. Professionals focusing on ASI are usually involved in speculative research, ethical considerations, and futuristic technology development. Career paths might include roles as AI ethicists, theoretical AI researchers, or innovators at cutting-edge research institutions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Understanding these AI types (Fig. 2) can guide aspiring AI researchers in choosing the right focus area for their careers, whether it&amp;rsquo;s enhancing specialized AI applications or contributing to the quest for creating truly intelligent machines.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_02.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. Types of Artificial Intelligence (Image source: viso.ai, &lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34;&gt;viso.ai/artificial-intelligence-types&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/h2&gt;
&lt;p&gt;Embark on a structured journey to master Artificial Intelligence with this comprehensive roadmap. Begin with foundational mathematics, including linear algebra, calculus, and statistics, essential for understanding AI concepts. Gain proficiency in tools like Python and PyTorch, and dive into machine learning by writing algorithms from scratch, competing in challenges, and deploying models. Expand your skills in deep learning through practical applications and competitive projects, and explore advanced topics like large language models. Stay updated with the latest trends and resources to ensure continuous learning and growth in the field of AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: Learn the fundamentals of linear algebra, crucial for understanding data manipulation and algorithmic operations. For a comprehensive introduction, refer to &lt;a href=&#34;https://www.3blue1brown.com/lessons/linear-algebra&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown‚Äôs Essence of Linear Algebra&lt;/a&gt; and &lt;em&gt;Introduction to Linear Algebra for Applied Machine Learning with Python&lt;/em&gt;. Dive deeper with &lt;a href=&#34;https://www.imperial.ac.uk/computing/prospective-students/courses/undergraduate/courses/linear-algebra-and-multivariate-calculus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imperial College London‚Äôs lectures on Linear Algebra&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Explore how calculus enables optimization in machine learning, crucial for learning algorithms and adjusting models. Key resources include &lt;a href=&#34;https://www.3blue1brown.com/lessons/calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown‚Äôs Essence of Calculus&lt;/a&gt; and &lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT OpenCourseWare‚Äôs Calculus Courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: Understand the role of probability and statistics in making predictions and decisions under uncertainty. Useful resources are &lt;a href=&#34;https://www.youtube.com/channel/UCtK1v8qWJghuX-GEw5A9kQQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatQuest‚Äôs Statistics Fundamentals&lt;/a&gt; and the book &lt;em&gt;Mathematics for Machine Learning&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: Begin with practical Python programming using &lt;a href=&#34;https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Practical Python Programming&lt;/a&gt; and advance to &lt;a href=&#34;https://www.udemy.com/course/advanced-python-mastery/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced Python Mastery&lt;/a&gt;. For deeper insights, explore &lt;a href=&#34;https://www.dabeaz.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Beazley‚Äôs courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: Learn PyTorch with &lt;a href=&#34;https://www.youtube.com/playlist?list=PLG2GkXjGgAr0UgfllZ3btzdkqT9lKjyRt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch Tutorials by Aladdin Persson&lt;/a&gt; and use resources like the &lt;a href=&#34;https://pytorch.org/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official PyTorch tutorials&lt;/a&gt; and &lt;a href=&#34;https://www.oreilly.com/library/view/programming-pytorch/9781492045518/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Programming PyTorch for Deep Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Write from Scratch&lt;/strong&gt;: Practice building algorithms from scratch with repositories such as &lt;a href=&#34;https://github.com/eriklindernoren/ML-From-Scratch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML-From-Scratch&lt;/a&gt; and &lt;a href=&#34;https://github.com/trekhleb/homemade-machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;homemade-machine-learning&lt;/a&gt;. For a more in-depth challenge, try &lt;a href=&#34;https://www.youtube.com/playlist?list=PLZ9ACV_z1Zq_5jlBLuRTmExbQj-RD4O9D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniTorch: A DIY Course on Machine Learning Engineering&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compete&lt;/strong&gt;: Apply your skills in machine learning competitions on platforms like &lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; and &lt;a href=&#34;https://bitgrit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bitgrit&lt;/a&gt;. Study past winning solutions to enhance your learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do Side Projects&lt;/strong&gt;: Start side projects using datasets from sources like &lt;a href=&#34;https://earthdata.nasa.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA Earth data&lt;/a&gt; and create user interfaces with &lt;a href=&#34;https://streamlit.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit&lt;/a&gt;. Refer to &lt;a href=&#34;https://vickiboykis.com/2020/07/22/getting-machine-learning-to-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting Machine Learning to Production&lt;/a&gt; for practical insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy Them&lt;/strong&gt;: Gain experience in deploying models and managing their lifecycle with resources like &lt;a href=&#34;https://madewithml.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Made With ML&lt;/a&gt; and &lt;a href=&#34;https://evidentlyai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evidently AI&lt;/a&gt;. Learn about tracking experiments and monitoring model performance with &lt;a href=&#34;https://datatalks.club/mlops-zoomcamp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DataTalksClub‚Äôs MLOps Zoomcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supplementary&lt;/strong&gt;: Explore additional materials such as &lt;em&gt;Machine Learning with PyTorch and Scikit-Learn&lt;/em&gt; and &lt;a href=&#34;https://arxiv.org/abs/1811.12808&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.ai&lt;/strong&gt;: Engage with &lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast.ai‚Äôs courses&lt;/a&gt; for a top-down approach to deep learning. Explore further with &lt;a href=&#34;https://fullstackdeeplearning.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning&lt;/a&gt; for a comprehensive view.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do More Competitions&lt;/strong&gt;: Participate in advanced competitions like &lt;a href=&#34;https://www.kaggle.com/c/plant-traits-2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PlantTraits2024&lt;/a&gt; to apply deep learning techniques.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implement Papers&lt;/strong&gt;: Study and implement research from resources like &lt;a href=&#34;https://labml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;labml.ai&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: Delve into &lt;a href=&#34;http://cs231n.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n: Deep Learning for Computer Vision&lt;/a&gt; for an in-depth understanding of computer vision applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt;: Learn from Stanford&amp;rsquo;s &lt;a href=&#34;https://web.stanford.edu/class/cs224n/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 224N: Natural Language Processing with Deep Learning&lt;/a&gt; and Hugging Face‚Äôs &lt;a href=&#34;https://huggingface.co/learn/nlp-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP Course&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Watch Neural Networks: Zero to Hero&lt;/strong&gt;: Get a comprehensive overview of large language models with &lt;a href=&#34;https://www.youtube.com/watch?v=O5xeyo8wFfQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy‚Äôs Neural Networks: Zero to Hero&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Free LLM Boot Camp&lt;/strong&gt;: Explore free boot camps on LLMs, such as &lt;a href=&#34;https://fullstackdeeplearning.com/llm-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning‚Äôs LLM Bootcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build with LLMs&lt;/strong&gt;: Develop LLM applications using &lt;a href=&#34;https://huyenchip.com/2023/02/23/building-llm-applications-for-production.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building LLM Applications for Production&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/openai-cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participate in Hackathons&lt;/strong&gt;: Join AI hackathons on &lt;a href=&#34;https://lablab.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lablab.ai&lt;/a&gt; and connect with other participants.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Papers&lt;/strong&gt;: Stay updated with LLM research from &lt;a href=&#34;https://sebastianraschka.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sebastian Raschka‚Äôs articles&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write Transformers from Scratch&lt;/strong&gt;: Follow guides to build transformers from scratch, such as &lt;a href=&#34;https://lil-log.com/transformer-family-v2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Transformer Family Version 2.0 | Lil‚ÄôLog&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Some Good Blogs&lt;/strong&gt;: Read insightful blogs like &lt;a href=&#34;https://lil-log.com/gradient-descent-into-madness/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradient Descent into Madness&lt;/a&gt; and &lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Watch Umar Jamil&lt;/strong&gt;: View detailed explanations and coding tutorials by &lt;a href=&#34;https://www.youtube.com/c/UmarJamil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Umar Jamil&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn How to Run Open-Source Models&lt;/strong&gt;: Get practical experience with open-source LLMs using &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ollama&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Study techniques for effective prompt engineering with resources like &lt;a href=&#34;https://lil-log.com/prompt-engineering/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prompt Engineering | Lil‚ÄôLog&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Tuning LLMs&lt;/strong&gt;: Explore guides on fine-tuning models with &lt;a href=&#34;https://huggingface.co/docs/transformers/training&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face‚Äôs fine-tuning guide&lt;/a&gt; and &lt;a href=&#34;https://genai.ai/fine-tuning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fine-Tuning ‚Äî The GenAI Guidebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAG&lt;/strong&gt;: Learn about Retrieval-Augmented Generation with articles such as &lt;a href=&#34;https://anyscale.com/blog/building-rag-based-llm-applications-for-production&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building RAG-based LLM Applications for Production&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;How to Stay Updated&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularly engage with leading blogs, research papers, and online courses to remain current with the latest advancements in AI and machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other Curriculums/Listicles You May Find Useful&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore additional curriculums and listicles for a broader understanding of AI topics, available through various educational and professional resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;highlighted-publications&#34;&gt;Highlighted Publications&lt;/h2&gt;
&lt;p&gt;Throughout my career, I&amp;rsquo;ve had the privilege to contribute to several exciting research projects. Below are some of my notable publications, each representing a unique challenge and innovative solution:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-86725-1_21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MARS Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: ICIAP 2023 Workshops, Lecture Notes in Computer Science, Springer, Cham&lt;/em&gt;&lt;br&gt;
This paper introduces a novel approach for car damage detection using Mask Attention Refinement with sequential quadtree nodes, specifically designed to enhance accuracy in the segmentation of damaged areas on vehicles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/15/21/5124&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2023&lt;/em&gt;&lt;br&gt;
MeViT is a Vision Transformer-based model that processes medium-resolution satellite images to classify different types of land cover in agricultural areas. This research has significant implications for monitoring and managing agricultural resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2078-2489/13/1/5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Information, 2022&lt;/em&gt;&lt;br&gt;
This paper explores an innovative method for detecting road assets, such as traffic signs and barriers, using a Transformer-based YOLOX model. The approach significantly improves the accuracy and reliability of object detection in complex environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/13/24/5100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2021&lt;/em&gt;&lt;br&gt;
Here, we investigate the use of Transformer-based architectures for segmenting high-resolution remote sensing images. This work pushes the boundaries of traditional convolutional neural networks by leveraging the power of self-attention mechanisms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/12/8/1233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2020&lt;/em&gt;&lt;br&gt;
This publication introduces a feature fusion approach for semantic labeling tasks, combining multiple feature maps to improve the accuracy of land cover classification in remote sensing imagery.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/h2&gt;
&lt;p&gt;The field of AI is constantly evolving, with several exciting trends emerging. Here&amp;rsquo;s a look at some of the most promising areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative AI&lt;/strong&gt;: With models like GANs and diffusion models, generative AI is revolutionizing how we create content, from art and music to realistic simulations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;: This approach is gaining traction as it reduces the need for labeled data, making it easier to train AI models on vast datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI for Social Good&lt;/strong&gt;: Applications of AI in healthcare, environmental monitoring, and disaster response highlight the technology&amp;rsquo;s potential to solve some of humanity&amp;rsquo;s biggest challenges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explainable AI (XAI)&lt;/strong&gt;: As AI systems become more complex, the need for transparency and interpretability is critical. XAI focuses on making AI decisions understandable to humans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI Security and Ethics&lt;/strong&gt;: With the growing deployment of AI, addressing ethical considerations and ensuring AI security are more important than ever.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/h2&gt;
&lt;p&gt;For those considering a career in AI research, my advice is simple: find a topic that excites you. Choose projects that you find inherently cool. This passion will sustain you through the challenges of research. Start by exploring current literature to understand what has already been done and identify gaps. Decide whether to build on existing models or innovate from scratch. Focus on how you can improve accuracy, speed, or applicability of AI solutions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remember, research is a journey, not a destination. Be curious, be patient, and never stop learning. The most rewarding part of research is not just the recognition that comes from publishing a paper but seeing your work make a real-world impact. Whether it&amp;rsquo;s through advancing technology or improving lives, your contribution as a researcher can make a difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Here‚Äôs Some Exciting News!&lt;/h2&gt;
&lt;p&gt;I‚Äôm thrilled to announce that I‚Äôve been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) (Fig. 3) in Singapore from January 6-10, 2025. This recognition is a major boost for my passion and drive to push the envelope in innovation!&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;panboonyuen_GYSS2025.jpg&#34; alt=&#34;Kao_GYSS2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. I am excited to announce that I have been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) in Singapore from January 6-10, 2025. This esteemed recognition greatly fuels my passion and determination to drive forward innovation! &lt;a href=&#34;https://www.facebook.com/photo.php?fbid=1061339665992254&amp;set=pb.100063486913512.-2207520000&amp;type=3&#34; target=&#34;_blank&#34;&gt;(Facebook) Global Young Scientists Summit&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;th&#34; dir=&#34;ltr&#34;&gt;‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏Å‡∏ô‡∏¥‡∏©‡∏ê‡∏≤‡∏ò‡∏¥‡∏£‡∏≤‡∏ä‡πÄ‡∏à‡πâ‡∏≤ ‡∏Å‡∏£‡∏°‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡πÄ‡∏ó‡∏û‡∏£‡∏±‡∏ï‡∏ô‡∏£‡∏≤‡∏ä‡∏™‡∏∏‡∏î‡∏≤ ‡∏Ø ‡∏™‡∏¢‡∏≤‡∏°‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏Å‡∏∏‡∏°‡∏≤‡∏£‡∏µ ‡∏ó‡∏£‡∏á‡∏°‡∏µ‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏π‡πâ‡πÅ‡∏ó‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° Global Young Scientists Summit (GYSS) ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ 2568&lt;a href=&#34;https://t.co/APrbWBQynK&#34;&gt;https://t.co/APrbWBQynK&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/ChulaEngineering?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ChulaEngineering&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/%E0%B8%A7%E0%B8%B4%E0%B8%A8%E0%B8%A7%E0%B8%88%E0%B8%B8%E0%B8%AC%E0%B8%B2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#‡∏ß‡∏¥‡∏®‡∏ß‡∏à‡∏∏‡∏¨‡∏≤&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Chula?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Chula&lt;/a&gt; &lt;a href=&#34;https://t.co/UpVqWCvHBo&#34;&gt;pic.twitter.com/UpVqWCvHBo&lt;/a&gt;&lt;/p&gt;&amp;mdash; ChulaEngineering_Official (@cueng_official) &lt;a href=&#34;https://twitter.com/cueng_official/status/1829356709363798177?ref_src=twsrc%5Etfw&#34;&gt;August 30, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;strong&gt;Global Young Scientists Summit (GYSS)&lt;/strong&gt; is a dynamic annual event that brings together exceptional young researchers and leading scientific minds from around the world. Held in Singapore, this summit is a unique platform for discussing groundbreaking research and exploring how it can address major global challenges.&lt;/p&gt;
&lt;p&gt;With a strong emphasis on innovation and collaboration, GYSS is where future scientific leaders converge to share ideas and shape the future of research. To dive deeper into this inspiring event, visit &lt;a href=&#34;https://www.gyss-one-north.sg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GYSS&lt;/a&gt; and join the conversation using #GYSS!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS-logo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Just a heads up‚Äîonce I wrap up at GYSS, I&amp;rsquo;ll be crafting a new blog to share all the awesome experiences with you. Stay tuned!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Being part of the AI revolution is a unique privilege. It&amp;rsquo;s a field where theoretical elegance meets real-world impact, offering endless opportunities for those willing to explore. Whether you are inclined toward academia or industry, or like me, both, there is a place for you in AI research. Let&amp;rsquo;s continue to push the boundaries and contribute to a future where AI plays a positive and transformative role in our lives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you for reading! I look forward to hearing your thoughts and engaging in discussions about AI research and career paths.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Career Paths for AI Research Scientists: My Journey and Insights&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024careerpaths,
  title   = &amp;quot;Career Paths for AI Research Scientists: My Journey and Insights.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it üôå
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.goodai.com/goodai-research-roadmap-2021-2022/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://viso.ai/deep-learning/artificial-intelligence-types/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Inspiring the Future of AI Innovations</title>
      <link>https://kaopanboonyuen.github.io/talk/inspiring-the-future-of-ai-innovations/</link>
      <pubDate>Thu, 01 Feb 2024 09:09:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/inspiring-the-future-of-ai-innovations/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Geospatial Big Data Analytics</title>
      <link>https://kaopanboonyuen.github.io/talk/geospatial-big-data-analytics/</link>
      <pubDate>Fri, 01 Dec 2023 08:15:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/geospatial-big-data-analytics/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Invited to Italy for ICIAP2023</title>
      <link>https://kaopanboonyuen.github.io/talk/invited-to-italy-for-iciap2023/</link>
      <pubDate>Fri, 11 Aug 2023 10:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/invited-to-italy-for-iciap2023/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;mars_italy_iciap0001.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0002.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0003.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0004.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0005.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0006.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0007.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;&lt;/p&gt;
&lt;p&gt;I am excited to share that my research, &lt;strong&gt;&amp;ldquo;MARS: Mask Attention Refinement with Sequential Quadtree Nodes,&amp;rdquo;&lt;/strong&gt; was accepted for presentation at the &lt;strong&gt;ICIAP 2023 Workshop&lt;/strong&gt; in Italy. This prestigious conference, held biennially by the CVPL under the International Association for Pattern Recognition (IAPR), brought together leading experts from around the world to discuss the latest advancements in car insurance and computer vision technologies.&lt;/p&gt;
&lt;p&gt;My research addressed the critical challenge of evaluating car damages with greater accuracy. Current deep learning networks struggle with this task, producing coarse segmented masks that are not suitable for real-world applications. To tackle this issue, I developed MARS, which employs self-attention mechanisms and quadtree transformers to refine instance segmentation accuracy. MARS represents a significant advancement in the field by drawing global dependencies between sequential quadtree nodes and recalibrating channel weights to predict highly accurate instance masks.&lt;/p&gt;
&lt;p&gt;The extensive experiments conducted as part of my research demonstrated that MARS outperforms several state-of-the-art instance segmentation methods, including Mask R-CNN, PointRend, and Mask Transfiner. Specifically, MARS achieved a substantial improvement in maskAP scores, with a +1.3 increase using the R50-FPN backbone and a +2.3 increase with the R101-FPN backbone on the Thai car-damage dataset. These results highlight the potential of MARS to significantly enhance the accuracy of car damage evaluations, offering promising applications for the car insurance industry.&lt;/p&gt;
&lt;!-- ![Teerapong Panboonyuen at Italy](mars_italy_iciap0008.jpg) --&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;mars_italy_iciap0008.jpg&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. I have published an article with Techsauce. (Image source: techsauce.co, &lt;a href=&#34;https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai&#34; target=&#34;_blank&#34;&gt;mars-deep-tech-startup-thaivivat-ai&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you to Techsauce, the Thai tech news agency, for showcasing my research on the application of AI in the auto insurance and garage industry. You can read the full article here: &lt;a href=&#34;https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Techsauce&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Presenting my findings at ICIAP 2023 was a fantastic opportunity to engage with fellow researchers and industry professionals, exploring the opportunities, challenges, and future directions in this rapidly evolving field.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distributed ML for Geospatial Data</title>
      <link>https://kaopanboonyuen.github.io/talk/distributed-ml-for-geospatial-data/</link>
      <pubDate>Thu, 01 Dec 2022 08:45:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/distributed-ml-for-geospatial-data/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Achieve Data Science First Meet</title>
      <link>https://kaopanboonyuen.github.io/talk/achieve-data-science-first-meet/</link>
      <pubDate>Tue, 01 Dec 2020 09:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/achieve-data-science-first-meet/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Ph.D. Thesis Defense</title>
      <link>https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/</link>
      <pubDate>Fri, 31 Jul 2020 14:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/</guid>
      <description>&lt;h2 id=&#34;-phd-dissertation-defense--triumphant&#34;&gt;üéì Ph.D. Dissertation Defense ‚Äì Triumphant&lt;/h2&gt;
&lt;p&gt;On &lt;strong&gt;July 9, 2020&lt;/strong&gt;, I successfully defended my Ph.D. dissertation titled &lt;em&gt;&amp;ldquo;Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network&amp;rdquo;&lt;/em&gt; before a distinguished committee of Thai professors, each of whom earned their doctoral degrees from world-renowned international universities.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_phd_defense_day_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The list below shows the names of those who submitted their final Ph.D. dissertations; considering I‚Äôm from the class of 2017 (student ID: 6071467821) and graduated in 2019, it took me just two years to complete this journey‚Äîan achievement that reflects my dedication and perseverance. I‚Äôm deeply proud and grateful to myself for the commitment I gave to this work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_phd_defense_day_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;üß† The research focused on applying deep learning techniques‚Äîspecifically convolutional encoder-decoder architectures‚Äîfor high-accuracy semantic segmentation in remotely sensed imagery, pushing the boundaries of geospatial AI.&lt;/p&gt;
&lt;p&gt;üîó &lt;strong&gt;Explore the full dissertation, source code, and project resources here:&lt;/strong&gt;&lt;br&gt;
üëâ &lt;a href=&#34;https://kaopanboonyuen.github.io/FusionNetGeoLabel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/FusionNetGeoLabel/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;üîó &lt;strong&gt;Interested in the full presentation slides? You can view them here:&lt;/strong&gt;&lt;br&gt;
üëâ &lt;a href=&#34;https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ph.D. Defense Slides&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;One word to define the journey:&lt;/strong&gt; &lt;code&gt;Triumphant&lt;/code&gt; üöÄ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;phd-journey-a-milestone-achieved&#34;&gt;PhD Journey: A Milestone Achieved&lt;/h3&gt;
&lt;p&gt;On May 19, 2022, I proudly completed my PhD at Chulalongkorn University, closing a remarkable chapter in my academic journey. This milestone was not just a moment of personal triumph, but also a time of reflection and deep gratitude. Graduating with a doctoral degree was a dream realized, filled with emotions that I will carry with me forever.&lt;/p&gt;
&lt;p&gt;Throughout this journey, I was fortunate to have the unwavering support of incredible mentors, advisors, colleagues, and friends. Their guidance and encouragement were instrumental in my success, and having them by my side on this special day was a poignant reminder of the profound impact they&amp;rsquo;ve had on both my academic and personal development.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;kaophd_002.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;
&lt;img src=&#34;kaophd_001.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;
&lt;img src=&#34;kaophd_003.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;
&lt;img src=&#34;kaophd_004.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;
&lt;img src=&#34;kaophd_005.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;&lt;/p&gt;
&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/a-oWa2CS8jg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;p&gt;Completing a PhD is more than just an academic achievement; it&amp;rsquo;s a journey of personal growth. It demands perseverance, resilience, and the ability to navigate and overcome numerous challenges. My passion for machine learning and my commitment to research were the driving forces that kept me moving forward, enabling me to make meaningful contributions to the field.&lt;/p&gt;
&lt;h3 id=&#34;phd-thesis-highlights&#34;&gt;PhD Thesis Highlights&lt;/h3&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Semantic segmentation in remote sensing images plays a crucial role in applications such as land use classification, urban planning, and environmental monitoring. Remote sensing images often come from diverse sources, ranging from medium-resolution satellite imagery (such as Landsat-8) to high-resolution aerial images (such as those from the ISPRS Vaihingen Challenge). However, the task of semantic segmentation remains challenging due to the variety of image scales, the scarcity of labeled data, and the need for models capable of extracting both high-level and low-level features effectively.&lt;/p&gt;
&lt;p&gt;In my PhD research, I propose a series of advancements in convolutional neural network (CNN)-based approaches to enhance the accuracy of semantic segmentation on remotely sensed data. Building upon the state-of-the-art methods, I introduce several innovations, including an enhanced &lt;strong&gt;Global Convolutional Network (GCN)&lt;/strong&gt; with channel attention, &lt;strong&gt;domain-specific transfer learning&lt;/strong&gt;, and the integration of &lt;strong&gt;feature fusion&lt;/strong&gt; and &lt;strong&gt;depthwise atrous convolutions&lt;/strong&gt;. These innovations aim to address the unique challenges of remote sensing datasets and push the boundaries of semantic segmentation performance.&lt;/p&gt;
&lt;h2 id=&#34;the-challenge-limitations-of-traditional-approaches&#34;&gt;The Challenge: Limitations of Traditional Approaches&lt;/h2&gt;
&lt;p&gt;Semantic segmentation models, particularly Deep Convolutional Encoder-Decoder (DCED) networks, have shown promise in image segmentation tasks. However, when applied to remote sensing imagery, these models face key limitations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resolution Differences&lt;/strong&gt;: Remote sensing images span a wide range of resolutions, from very high-resolution (VHR) aerial images to medium-resolution satellite images. Traditional models, designed for single-resolution tasks, struggle to generalize across these diverse scales.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Scarcity&lt;/strong&gt;: Annotated datasets for training deep models are scarce, particularly for high-resolution satellite or aerial imagery. This leads to overfitting and poor generalization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inability to Capture Global Context&lt;/strong&gt;: Traditional CNN models focus on local features, which are insufficient for understanding global context in satellite images, such as large rivers, forests, or urban areas.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;h3 id=&#34;global-convolutional-network-gcn-with-enhanced-backbone&#34;&gt;Global Convolutional Network (GCN) with Enhanced Backbone&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Global Convolutional Network (GCN)&lt;/strong&gt; is a modern CNN architecture that addresses the limitations of traditional models. GCN overcomes the challenge of capturing both local and global features by using a multi-level architecture. Each level in the GCN extracts features at different resolutions, ensuring that both fine-grained and broad contextual information are captured.&lt;/p&gt;
&lt;p&gt;This can be written as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{F}_{\text{GCN}} = \sum \mathbf{W}_l \cdot \mathbf{X}_l
$$&lt;/p&gt;
&lt;p&gt;Building on this architecture, I proposed an enhancement to the GCN backbone by modifying its structure and increasing the number of layers, making it more suitable for medium-resolution remote sensing imagery. Specifically, I employed the &lt;strong&gt;ResNet&lt;/strong&gt; architecture with varying depths‚ÄîResNet50, ResNet101, and ResNet152‚Äîto adapt the model to different datasets and resolutions.&lt;/p&gt;
&lt;h4 id=&#34;key-contributions&#34;&gt;Key Contributions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi-resolution Feature Extraction&lt;/strong&gt;: By stacking multiple convolutional layers at different stages, the GCN captures features across multiple resolutions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Boundary Refinement&lt;/strong&gt;: A boundary refinement module is introduced to improve the precision of segmentation boundaries, crucial for tasks like building or road detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;channel-attention-mechanism&#34;&gt;Channel Attention Mechanism&lt;/h3&gt;
&lt;p&gt;One of the most significant advancements in my work is the introduction of the &lt;strong&gt;Channel Attention Block&lt;/strong&gt;. Attention mechanisms, inspired by the human visual system, allow the model to focus on the most important features in the image. In the case of remote sensing images, this means highlighting key features such as roads, rivers, or vegetation, while suppressing irrelevant background information.&lt;/p&gt;
&lt;p&gt;The attention mechanism is mathematically modeled as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{z}_c = \sigma\left( W_c \cdot \text{AvgPool}(\mathbf{x}_c) + b_c \right)
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;channel attention block&lt;/strong&gt; modifies the network&amp;rsquo;s weights to prioritize the most discriminative channels during feature extraction, improving the network‚Äôs ability to differentiate between different land cover types. This is crucial in remote sensing, where the subtle difference between similar features (e.g., different vegetation types) can significantly impact segmentation accuracy.&lt;/p&gt;
&lt;h4 id=&#34;key-contributions-1&#34;&gt;Key Contributions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Adaptive Feature Selection&lt;/strong&gt;: The network dynamically adjusts the importance of features, focusing on those most relevant to the task at hand.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improved Discriminative Power&lt;/strong&gt;: By emphasizing discriminative features, the model is able to achieve higher classification accuracy, particularly for challenging classes in remote sensing datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;domain-specific-transfer-learning&#34;&gt;Domain-Specific Transfer Learning&lt;/h3&gt;
&lt;p&gt;One of the challenges in training deep learning models for remote sensing is the scarcity of annotated data, particularly for high-resolution images. To address this, I introduced &lt;strong&gt;Domain-Specific Transfer Learning&lt;/strong&gt;. This technique involves leveraging pre-trained models from different but related datasets to transfer knowledge across domains.&lt;/p&gt;
&lt;p&gt;By utilizing pre-trained models from one dataset (e.g., ISPRS Vaihingen) and applying them to another (e.g., Landsat-8), I was able to mitigate the data scarcity issue and improve the performance of my models. This approach ensures that knowledge gained from one domain can benefit another, allowing the model to generalize better with limited annotated data.&lt;/p&gt;
&lt;h4 id=&#34;key-contributions-2&#34;&gt;Key Contributions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-Domain Knowledge Transfer&lt;/strong&gt;: Knowledge learned from high-resolution datasets is transferred to medium-resolution tasks, significantly improving segmentation accuracy with minimal labeled data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pre-Trained Weights Utilization&lt;/strong&gt;: The use of pre-trained weights from different datasets enables the model to learn features that are generalizable across various remote sensing tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-fusion-and-depthwise-atrous-convolution&#34;&gt;Feature Fusion and Depthwise Atrous Convolution&lt;/h3&gt;
&lt;p&gt;To further refine feature extraction and improve segmentation performance, I introduced &lt;strong&gt;Feature Fusion&lt;/strong&gt; and &lt;strong&gt;Depthwise Atrous Convolution (DA)&lt;/strong&gt;. These techniques work synergistically to enhance the model&amp;rsquo;s ability to capture multi-scale information while maintaining high resolution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feature Fusion&lt;/strong&gt;: This technique fuses low-level features, such as edges and textures, from the backbone network with high-level features from the segmentation model. This fusion ensures that fine-grained details are preserved while providing the model with a richer set of features for segmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mathbf{F}_{\text{fused}} = \mathbf{F}_L + \alpha \cdot \mathbf{F}_H
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Depthwise Atrous Convolution&lt;/strong&gt;: The DA module applies dilated convolutions at multiple scales, enabling the network to capture contextual information over larger areas without losing spatial resolution. This is particularly important for remote sensing tasks where object boundaries (e.g., between forest and water) need to be sharply delineated.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Depthwise Atrous Convolution is mathematically expressed as:&lt;/p&gt;
&lt;p&gt;$$
y_d = \sum_{k} w_k \cdot x_{i + r \cdot k}
$$&lt;/p&gt;
&lt;h4 id=&#34;key-contributions-3&#34;&gt;Key Contributions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Improved Feature Representation&lt;/strong&gt;: By integrating low-level features with deep model representations, the model gains a more comprehensive understanding of the image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced Contextual Understanding&lt;/strong&gt;: The DA module allows the network to consider broader context in each layer, improving segmentation accuracy, especially for large objects and distant features.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments-and-results&#34;&gt;Experiments and Results&lt;/h2&gt;
&lt;h3 id=&#34;datasets&#34;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;I conducted experiments on three benchmark datasets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ISPRS Vaihingen Challenge Dataset&lt;/strong&gt; (Very High Resolution)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Landsat-8 Satellite Imagery&lt;/strong&gt; (Medium Resolution)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Private Datasets&lt;/strong&gt; from GISTDA (Geo-Informatics and Space Technology Development Agency)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These datasets represent a mix of very high-resolution and medium-resolution imagery, which provided a comprehensive testbed for evaluating the effectiveness of my proposed methods.&lt;/p&gt;
&lt;h3 id=&#34;performance-metrics&#34;&gt;Performance Metrics&lt;/h3&gt;
&lt;p&gt;The model was evaluated using standard segmentation metrics:&lt;/p&gt;
&lt;h4 id=&#34;1-f1-score&#34;&gt;1. &lt;strong&gt;F1 Score&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;The F1 Score measures the balance between &lt;strong&gt;Precision&lt;/strong&gt; and &lt;strong&gt;Recall&lt;/strong&gt;, and is calculated as:&lt;/p&gt;
&lt;p&gt;$$
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt; is the fraction of relevant instances among the retrieved instances:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\text{Precision} = \frac{TP}{TP + FP}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt; is the fraction of relevant instances that have been retrieved over the total amount of relevant instances:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\text{Recall} = \frac{TP}{TP + FN}
$$&lt;/p&gt;
&lt;h5 id=&#34;example-pineapple-class&#34;&gt;Example: &lt;strong&gt;Pineapple Class&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;For the &lt;strong&gt;pineapple&lt;/strong&gt; class:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;( TP = 50 )&lt;/li&gt;
&lt;li&gt;( FP = 10 )&lt;/li&gt;
&lt;li&gt;( FN = 15 )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We calculate &lt;strong&gt;Precision&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\text{Precision} = \frac{50}{50 + 10} = 0.8333
$$&lt;/p&gt;
&lt;p&gt;And &lt;strong&gt;Recall&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\text{Recall} = \frac{50}{50 + 15} = 0.7692
$$&lt;/p&gt;
&lt;p&gt;Now calculate the &lt;strong&gt;F1 Score&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
F1 = 2 \cdot \frac{0.8333 \cdot 0.7692}{0.8333 + 0.7692} = 0.799
$$&lt;/p&gt;
&lt;p&gt;Thus, the &lt;strong&gt;F1 Score&lt;/strong&gt; for the pineapple class is &lt;strong&gt;0.799&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-mean-iou-intersection-over-union&#34;&gt;2. &lt;strong&gt;Mean IoU (Intersection over Union)&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;IoU&lt;/strong&gt; quantifies the overlap between the predicted and true segmentation maps:&lt;/p&gt;
&lt;p&gt;$$
IoU = \frac{TP}{TP + FP + FN}
$$&lt;/p&gt;
&lt;h5 id=&#34;example-corn-class&#34;&gt;Example: &lt;strong&gt;Corn Class&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;For the &lt;strong&gt;corn&lt;/strong&gt; class:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;( TP = 80 )&lt;/li&gt;
&lt;li&gt;( FP = 5 )&lt;/li&gt;
&lt;li&gt;( FN = 20 )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We calculate the &lt;strong&gt;IoU&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
IoU = \frac{80}{80 + 5 + 20} = 0.7619
$$&lt;/p&gt;
&lt;p&gt;Thus, the &lt;strong&gt;IoU&lt;/strong&gt; for the corn class is &lt;strong&gt;0.7619&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-mean-iou-for-multiple-classes&#34;&gt;3. &lt;strong&gt;Mean IoU for Multiple Classes&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;For evaluating the &lt;strong&gt;Mean IoU&lt;/strong&gt; over multiple classes (e.g., pineapple, corn, pararubber), we compute the average IoU:&lt;/p&gt;
&lt;p&gt;$$
\text{Mean IoU} = \frac{IoU_{\text{pineapple}} + IoU_{\text{corn}} + IoU_{\text{pararubber}}}{3}
$$&lt;/p&gt;
&lt;p&gt;Using the IoUs for each class:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IoU for pineapple = 0.799&lt;/li&gt;
&lt;li&gt;IoU for corn = 0.7619&lt;/li&gt;
&lt;li&gt;IoU for pararubber = 0.85&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We get the &lt;strong&gt;Mean IoU&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\text{Mean IoU} = \frac{0.799 + 0.7619 + 0.85}{3} = 0.8036
$$&lt;/p&gt;
&lt;p&gt;Thus, the &lt;strong&gt;Mean IoU&lt;/strong&gt; across all classes is &lt;strong&gt;0.8036&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The proposed model significantly outperformed baseline models, including traditional Deep Convolutional Encoder-Decoder (DCED) networks, across all datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ISPRS Vaihingen Dataset&lt;/strong&gt;: F1 Score of &lt;strong&gt;0.9362&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Landsat-8 Dataset&lt;/strong&gt;: F1 Score of &lt;strong&gt;0.9114&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The proposed model consistently exceeded the &lt;strong&gt;90% F1 score&lt;/strong&gt; threshold across all classes, demonstrating its robustness and adaptability to different image resolutions and domains.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This research introduces several key advancements in deep learning for remote sensing semantic segmentation. By incorporating multi-resolution feature extraction, channel attention, domain-specific transfer learning, feature fusion, and depthwise atrous convolutions, my approach addresses the unique challenges posed by remote sensing data. The experimental results validate the effectiveness of these techniques, providing a solid foundation for further improvements in remote sensing applications.&lt;/p&gt;
&lt;p&gt;With the successful application of these methods, I am confident that these innovations will contribute significantly to the field of remote sensing and provide new avenues for improving the accuracy and generalization of deep learning models in this domain.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Explore more about my PhD story &lt;a href=&#34;https://kaopanboonyuen.wordpress.com/2022/05/23/the-phd-journey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kao Panboonyuen&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Science Pathway Special Clinic Session</title>
      <link>https://kaopanboonyuen.github.io/talk/data-science-pathway-special-clinic-session/</link>
      <pubDate>Sun, 01 Dec 2019 08:45:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/data-science-pathway-special-clinic-session/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>GET TO KNOW ME BETTER</title>
      <link>https://kaopanboonyuen.github.io/talk/get-to-know-me-better/</link>
      <pubDate>Sat, 01 Dec 2018 04:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/get-to-know-me-better/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;kao_japan_2020/KAO_PANBOONYUEN_JP20_02.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_img01.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_img02.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;p&gt;Alright, let‚Äôs break the ice! Beyond the research papers and tech jargon, here‚Äôs the real scoop about me:&lt;/p&gt;
&lt;p&gt;I‚Äôm a tech enthusiast with a serious passion for using technology to make the world a better place. When I‚Äôm not buried in code or geeking out over the latest tech, you might find me lacing up my running shoes or gearing up for a triathlon. Yep, marathons and triathlons are my thing‚Äîthey keep me fit and remind me that anything‚Äôs possible with a little grit and a lot of sweat.&lt;/p&gt;
&lt;p&gt;I‚Äôm a lifelong learner at heart. Whether it‚Äôs catching the latest tech trends, diving into new research, or just chatting with fellow nerds, I‚Äôm always on the lookout for the next big thing in technology.&lt;/p&gt;
&lt;p&gt;But it‚Äôs not all work and no play. I love giving back to the community and am always up for volunteering at events that spark my interest. Connecting with people and making a difference is what keeps me motivated.&lt;/p&gt;
&lt;p&gt;In my daily life, I love sharing insights and experiences, so feel free to follow my journey at &lt;a href=&#34;https://kaopanboonyuen.wordpress.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My WordPress Blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, if you want to swap stories about the newest gadgets, discuss the latest research, or just chat about anything under the sun, hit me up at &lt;a href=&#34;mailto:panboonyuen.kao@gmail.com&#34;&gt;panboonyuen.kao@gmail.com&lt;/a&gt;!&lt;/p&gt;
&lt;!-- ![Teerapong Panboonyuen](kao/panboonyuen_kao_01.jpg)
![Teerapong Panboonyuen](kao/panboonyuen_kao_02.jpg)
![Teerapong Panboonyuen](kao/panboonyuen_kao_03.jpg)
![Teerapong Panboonyuen](kao/panboonyuen_kao_04.jpg) --&gt;
&lt;!-- ![Teerapong Panboonyuen](featured_ITALY.jpg) --&gt;
&lt;p&gt;&lt;img src=&#34;kao_japan_2020/KAO_PANBOONYUEN_JP20_01.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;kao_japan_2024/panboonyuen_kao_japan_01.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;kao_japan_2024/panboonyuen_kao_japan_02.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;!-- ![Teerapong Panboonyuen](kao_japan_2024/panboonyuen_kao_japan_03.jpg)
![Teerapong Panboonyuen](kao_japan_2024/panboonyuen_kao_japan_04.jpg) --&gt;
&lt;!-- ![Teerapong Panboonyuen](kao/panboonyuen_kao_05.jpg)
![Teerapong Panboonyuen](kao/panboonyuen_kao_06.jpg)
 --&gt;
&lt;p&gt;&lt;strong&gt;Kao Panboonyuen&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
