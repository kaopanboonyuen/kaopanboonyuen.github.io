<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Teerapong Panboonyuen</title>
    <link>https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/</link>
      <atom:link href="https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/index.xml" rel="self" type="application/rss+xml" />
    <description>Teerapong Panboonyuen</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>©2025 Kao Panboonyuen</copyright><lastBuildDate>Tue, 25 Feb 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/avatar_hu3c429e132ccde7f98e52ca20c1f589ef_2676345_270x270_fill_q75_lanczos_center.jpg</url>
      <title>Teerapong Panboonyuen</title>
      <link>https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/</link>
    </image>
    
    <item>
      <title>DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation</title>
      <link>https://kaopanboonyuen.github.io/publication/dota-deformable-optimized-transformer-architecture/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/dota-deformable-optimized-transformer-architecture/</guid>
      <description>&lt;h3 id=&#34;dota-deformable-optimized-transformer-architecture-for-end-to-end-text-recognition-with-retrieval-augmented-generation&#34;&gt;&lt;strong&gt;DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;abstract&#34;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Text recognition in natural images remains one of the most challenging yet essential tasks within the fields of computer vision and natural language processing. With applications ranging from document digitization to autonomous navigation, effective text recognition is more critical than ever before. In this paper, we introduce &lt;strong&gt;DOTA&lt;/strong&gt;, a novel end-to-end framework that combines ResNet and Vision Transformer (ViT) backbones with advanced methodologies such as &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;, &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;, and &lt;strong&gt;Conditional Random Fields (CRF)&lt;/strong&gt; to significantly enhance Optical Character Recognition (OCR) performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;At the heart of DOTA is a revolutionary approach where traditional convolution layers in the third and fourth blocks of the network are replaced with &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;—a technique that offers adaptive and robust feature extraction, making it ideal for recognizing text in complex and irregular layouts. Furthermore, &lt;strong&gt;adaptive dropout&lt;/strong&gt; is integrated to ensure regularization, helping to prevent overfitting and boosting generalization. To refine the sequential modeling of text, we leverage &lt;strong&gt;CRFs&lt;/strong&gt;, which excel in capturing intricate dependencies inherent in text recognition tasks.&lt;/p&gt;
&lt;p&gt;We conducted extensive experiments on six benchmark OCR datasets—IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80. Our results demonstrate the exceptional performance of DOTA, achieving remarkable accuracies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IC13&lt;/strong&gt;: 97.32%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IC15&lt;/strong&gt;: 58.26%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVT&lt;/strong&gt;: 88.10%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IIIT5K&lt;/strong&gt;: 74.13%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVTP&lt;/strong&gt;: 82.17%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUTE80&lt;/strong&gt;: 66.67%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This gives us an &lt;strong&gt;average accuracy of 77.77%&lt;/strong&gt;, setting a new state-of-the-art in the field of text recognition. The results clearly highlight the robustness of DOTA across a variety of challenging datasets.&lt;/p&gt;
&lt;h4 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Text recognition from images has long been a challenging problem, with significant implications for applications in document processing, automated data entry, and even autonomous systems. With traditional &lt;strong&gt;Optical Character Recognition (OCR)&lt;/strong&gt; systems heavily relying on &lt;strong&gt;Convolutional Neural Networks (CNNs)&lt;/strong&gt;, we’ve seen progress in extracting features from images. However, as text layouts become more complex—such as when dealing with varying fonts, orientations, and complex backgrounds—CNNs often fall short. Enter the &lt;strong&gt;Transformer architectures&lt;/strong&gt;: these models have revolutionized many areas in computer vision, particularly in handling long-range dependencies through their self-attention mechanisms, offering significant improvements for text recognition tasks.&lt;/p&gt;
&lt;p&gt;While Transformer-based models have pushed the boundaries of OCR performance, there is still a need for further improvements, especially in the area of feature extraction and sequence modeling. That&amp;rsquo;s where &lt;strong&gt;DOTA&lt;/strong&gt; comes in. By combining the strengths of &lt;strong&gt;ResNet&lt;/strong&gt; and &lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt; backbones, this novel approach leverages &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;, &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;, and &lt;strong&gt;Conditional Random Fields (CRF)&lt;/strong&gt; to achieve new levels of OCR accuracy.&lt;/p&gt;
&lt;h4 id=&#34;the-dota-framework&#34;&gt;&lt;strong&gt;The DOTA Framework&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The architecture of DOTA is designed to enhance both feature extraction and sequence modeling, key areas in text recognition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deformable Convolutions&lt;/strong&gt;: By replacing standard convolutions in the network’s third and fourth blocks with &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;, we allow for more flexible and adaptive feature extraction. This helps capture irregular text patterns and varying layouts more effectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;: The integration of this approach provides context-aware enhancements, further refining the recognition process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Conditional Random Fields (CRFs)&lt;/strong&gt;: CRFs have been incorporated into the framework to better model the sequential nature of text, providing the necessary context to improve recognition accuracy, especially for more complex sequences of characters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Together, these components form the backbone of &lt;strong&gt;DOTA&lt;/strong&gt;, providing a more robust, adaptable, and precise model for OCR tasks. The effectiveness of DOTA is clearly demonstrated in its impressive performance across several standard OCR benchmark datasets.&lt;/p&gt;
&lt;h4 id=&#34;experimental-results&#34;&gt;&lt;strong&gt;Experimental Results&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We evaluated DOTA using six widely-used OCR benchmark datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IC13&lt;/strong&gt;: 97.32%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IC15&lt;/strong&gt;: 58.26%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVT&lt;/strong&gt;: 88.10%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IIIT5K&lt;/strong&gt;: 74.13%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVTP&lt;/strong&gt;: 82.17%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUTE80&lt;/strong&gt;: 66.67%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With an average accuracy of 77.77%, DOTA has set a new state-of-the-art in OCR performance. The results show that the combination of Deformable Convolutions, Vision Transformers, and CRFs significantly improves recognition, even in challenging conditions where traditional methods struggle.&lt;/p&gt;
&lt;h4 id=&#34;conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The DOTA framework represents a major leap forward in the field of text recognition. By effectively combining ResNet and Vision Transformer backbones with advanced techniques like &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;, &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;, and &lt;strong&gt;Conditional Random Fields&lt;/strong&gt;, it achieves impressive accuracy across a variety of challenging datasets. This work sets a new benchmark for OCR performance, providing a powerful tool for tackling the complexities of text recognition in real-world scenarios.&lt;/p&gt;
&lt;p&gt;With this new approach, we’ve laid the foundation for even more accurate and robust text recognition systems, paving the way for smarter applications in everything from document processing to autonomous navigation.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>42.195K at Chombueng Marathon 2025</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/</link>
      <pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/</guid>
      <description>&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-journey-back-home&#34;&gt;The Journey Back Home&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-race&#34;&gt;The Race&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-magic-of-chombueng&#34;&gt;The Magic of Chombueng&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-race-atmosphere&#34;&gt;The Race Atmosphere&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#lessons-and-reflections&#34;&gt;Lessons and Reflections&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#a-love-letter-to-chombueng&#34;&gt;A Love Letter to Chombueng&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;This year’s &lt;strong&gt;Chombueng Marathon&lt;/strong&gt; marked a special milestone for me: my &lt;strong&gt;second-ever full marathon&lt;/strong&gt;. My first was the &lt;a href=&#34;https://kaopanboonyuen.wordpress.com/2022/11/22/full-marathon-42-195-km-bangkok-marathon-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bangkok Marathon 2022&lt;/strong&gt;&lt;/a&gt;, an unforgettable experience that sparked my love for the 42.195K distance. Completing that first marathon taught me the value of resilience and determination, and it’s the reason why I continue to embrace the challenge and joy of running full marathons. This time, it felt even more meaningful as I returned to my &lt;strong&gt;hometown, Ratchaburi&lt;/strong&gt;, to take on the challenge. Despite being born here, it wasn’t until now—at the age of 30—that I finally had the chance to run this iconic race in my own backyard.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Chombueng Marathon&lt;/strong&gt;, now in its &lt;strong&gt;38th year&lt;/strong&gt;, is one of Thailand’s oldest and most cherished marathons. It’s not just a race; it’s a tradition that brings together runners from across the country and beyond, while showcasing the unique charm of this small but spirited district.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/KAO_CBM2025_CERT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-journey-back-home&#34;&gt;The Journey Back Home&lt;/h2&gt;
&lt;p&gt;Coming back to Chombueng brought back a flood of childhood memories. Growing up, I spent countless weekends visiting my mom, who worked as a teacher here. The tranquil forests, the famous &lt;strong&gt;Tham Chomphon cave&lt;/strong&gt;, the peaceful campus of &lt;strong&gt;Rajabhat University Chombueng&lt;/strong&gt;—they’ve all remained beautifully unchanged. Running through these familiar sights made the experience even more special, as if the past and present collided in the best possible way.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/KAO_CBM2025_STAR01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/KAO_CBM2025_STAR02.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/KAO_CBM2025_STAR03.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-race&#34;&gt;The Race&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;2025 Chombueng Marathon&lt;/strong&gt; was held on &lt;strong&gt;January 19th&lt;/strong&gt;, with the gun going off for the &lt;strong&gt;42.195K runners at 3:00 a.m.&lt;/strong&gt;. The weather was perfect for running, hovering between &lt;strong&gt;15–19°C&lt;/strong&gt;—cool and crisp, just the way I like it.&lt;/p&gt;
&lt;p&gt;I started off strong, clocking a &lt;strong&gt;5:39 average pace for the first 5K&lt;/strong&gt;, easing slightly to &lt;strong&gt;5:54 for the next 10K&lt;/strong&gt;. By the time I reached the halfway point at 21K, I was feeling good, finishing the first half in just over 2 hours. But as the second half began, I shifted gears to a more strategic approach: &lt;strong&gt;run-walk intervals&lt;/strong&gt;. I wanted to finish this race injury-free and with enough energy to enjoy the rest of the day—because life’s too short to spend it limping around post-race!&lt;/p&gt;
&lt;p&gt;In the end, I crossed the finish line with a &lt;strong&gt;chip time of 5 hours and 36 minutes&lt;/strong&gt;. While it wasn’t a personal best or a sub-5, the sense of accomplishment and joy I felt was unbeatable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_001.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_002_01.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_004_2.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_006_02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-magic-of-chombueng&#34;&gt;The Magic of Chombueng&lt;/h2&gt;
&lt;p&gt;What truly sets the Chombueng Marathon apart is the &lt;strong&gt;atmosphere and community spirit&lt;/strong&gt;. The entire town comes alive for this event, cheering runners on from start to finish.&lt;/p&gt;
&lt;p&gt;I’ll always remember the &lt;strong&gt;monks sprinkling holy water&lt;/strong&gt; on us as we ran by—a uniquely Thai tradition that made me smile every time. Then there were the &lt;strong&gt;students&lt;/strong&gt;, standing by the roadside in the early hours, clapping, cheering, and offering encouragement. And, of course, the &lt;strong&gt;locals&lt;/strong&gt; who came out to support us, their smiles and shouts of &lt;em&gt;&amp;ldquo;สู้ ๆ!&amp;rdquo;&lt;/em&gt; (Keep going!) giving us the boost we needed to push through each kilometer.&lt;/p&gt;
&lt;p&gt;To everyone who stood out in the cool morning air to cheer us on: &lt;strong&gt;thank you&lt;/strong&gt;. Your energy, enthusiasm, and kindness made all the difference.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_011.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_014.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-race-atmosphere&#34;&gt;The Race Atmosphere&lt;/h2&gt;
&lt;p&gt;This year’s marathon was filled with joy and excitement. I met so many incredible runners, each radiating energy and enthusiasm as we gathered at the start line at 3 a.m. The crowd was electric, with smiles all around and cheers that warmed the cool morning air. The support from the spectators was truly special, and I want to give a big shoutout to two amazing women who became unexpected guides during my race.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/KAO_CBM2025_V2_001.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Dressed in vibrant orange and blue shirts, these two seasoned runners unknowingly set the perfect pace for me during the first 15K. With their steady rhythm, I was able to maintain a comfortable &lt;strong&gt;5:50 min/km pace&lt;/strong&gt;, which kept me going without overexerting myself. Though I hadn’t trained much and had no particular strategy in mind, following them gave me the boost I needed to tackle the early stages of the marathon.&lt;/p&gt;
&lt;p&gt;However, reality hit midway—I realized that sustaining that pace for the rest of the race was beyond my current fitness level. Adjusting my strategy, I had to let go of my hopes for a &lt;strong&gt;sub-4 finish&lt;/strong&gt;, but I’m incredibly grateful to those two women for helping me start strong.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/KAO_CBM2025_V2_002.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the end, I crossed the finish line in 5 hours and 38 minutes. While it wasn’t the time I dreamed of, it was still a proud moment. Next time, I know &lt;strong&gt;sub-4 isn’t too far out of reach&lt;/strong&gt;. Here’s to continuing the journey and pushing myself to new limits!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/KAO_CBM2025_V2_003.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lessons-and-reflections&#34;&gt;Lessons and Reflections&lt;/h2&gt;
&lt;p&gt;This was my second full marathon, and while I didn’t hit my sub-5 goal, I’m proud of what I achieved. I went into this race with very little training—my recent runs had mostly been short 5Ks. But this marathon reignited my passion for running and reminded me why I love this sport so much.&lt;/p&gt;
&lt;p&gt;The biggest lesson? It’s okay to let go of the pressure. Instead of obsessing over times and splits, I focused on enjoying the journey, listening to my body, and soaking in the experience. Running should be fun, and as long as you keep moving forward with a smile, that’s what matters.&lt;/p&gt;
&lt;p&gt;I know there are more full marathons in my future. And when the next one comes around, I’ll be ready.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0612.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0613.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0438.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-love-letter-to-chombueng&#34;&gt;A Love Letter to Chombueng&lt;/h2&gt;
&lt;p&gt;The Chombueng Marathon isn’t just a race—it’s a memory, a tradition, and a piece of my heart. Running in my hometown, surrounded by the places and people that shaped my early years, was an experience I’ll always treasure.&lt;/p&gt;
&lt;p&gt;Thank you, Chombueng, for the beautiful scenery, the incredible community, and the unforgettable memories. This race will forever hold a special place in my heart, and I can’t wait to come back again.&lt;/p&gt;
&lt;p&gt;Until next time, Chombueng.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0614.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Jan 2025). &lt;em&gt;42.195K at Chombueng Marathon 2025&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2025chombueng,
  title   = &amp;quot;42.195K at Chombueng Marathon 2025&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Jan&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Global Young Scientists Summit 2025: Where Science Meets Inspiration</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/</link>
      <pubDate>Mon, 13 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/</guid>
      <description>&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#presenting-my-work-to-her-royal-highness-princess-maha-chakri-sirindhorn&#34;&gt;Presenting My Work to Her Royal Highness Princess Maha Chakri Sirindhorn&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#understanding-the-basics-of-dnn-operations&#34;&gt;Understanding the Basics of DNN Operations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#challenges-in-parameter-recovery&#34;&gt;Challenges in Parameter Recovery&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#polynomial-time-parameter-extraction&#34;&gt;Polynomial Time Parameter Extraction&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#critical-points-and-linear-equations&#34;&gt;Critical Points and Linear Equations&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#practical-demonstration&#34;&gt;Practical Demonstration&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#comparison-with-cryptographic-systems&#34;&gt;Comparison with Cryptographic Systems&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#implications-of-randomness-and-alternative-activations&#34;&gt;Implications of Randomness and Alternative Activations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#broader-implications-and-future-directions&#34;&gt;Broader Implications and Future Directions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#plenary-lecture-educability-prof-leslie-valiant&#34;&gt;Plenary Lecture: Educability (Prof Leslie Valiant)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#historical-foundations-of-intelligence-and-educability&#34;&gt;Historical Foundations of Intelligence and Educability&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#educability-vs-machine-learning&#34;&gt;Educability vs. Machine Learning&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#the-church-turing-thesis&#34;&gt;The Church-Turing Thesis&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#comparing-human-and-machine-learning&#34;&gt;Comparing Human and Machine Learning&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#cognitive-capabilities-as-civilization-enablers&#34;&gt;Cognitive Capabilities as Civilization Enablers&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#ethical-implications-of-ai-development&#34;&gt;Ethical Implications of AI Development&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#addressing-concerns-about-ai&#34;&gt;Addressing Concerns About AI&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-future-of-ai-and-educability&#34;&gt;The Future of AI and Educability&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#a-call-to-action&#34;&gt;A Call to Action&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#plenary-lecture-compressing-proofs-using-cryptography-prof-yael-kalai&#34;&gt;Plenary Lecture: Compressing Proofs using Cryptography (Prof Yael Kalai)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#what-are-succinct-proofs&#34;&gt;What Are Succinct Proofs?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-cryptographic-techniques&#34;&gt;Key Cryptographic Techniques&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#zero-knowledge-proofs&#34;&gt;Zero-Knowledge Proofs&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#polynomial-commitments&#34;&gt;Polynomial Commitments&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#applications-of-succinct-proofs&#34;&gt;Applications of Succinct Proofs&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#blockchain-scalability&#34;&gt;Blockchain Scalability&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#ai-model-verification&#34;&gt;AI Model Verification&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#decentralized-identity&#34;&gt;Decentralized Identity&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#challenges-and-future-directions&#34;&gt;Challenges and Future Directions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#insights-from-prof-yael-tauman-kalais-research&#34;&gt;Insights from Prof. Yael Tauman Kalai’s Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#engaging-in-dialogues-on-the-ethics-of-ai&#34;&gt;Engaging in Dialogues on the Ethics of AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#ethics-of-scientific-research-in-the-age-of-ai&#34;&gt;Ethics of Scientific Research in the Age of AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#client-concerns-about-ai&#34;&gt;Client Concerns About AI&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#opportunities-and-risks&#34;&gt;Opportunities and Risks&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#public-engagement&#34;&gt;Public Engagement&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#accountability-and-regulation&#34;&gt;Accountability and Regulation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#future-directions&#34;&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#visual-representation&#34;&gt;Visual Representation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#expanding-my-network-and-building-new-collaborations&#34;&gt;Expanding My Network and Building New Collaborations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#a-day-ill-never-forget&#34;&gt;A Day I’ll Never Forget&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;Attending &lt;strong&gt;GYSS2025 (Global Young Scientists Summit)&lt;/strong&gt; is an incredibly exciting opportunity. This prestigious gathering brings together brilliant young minds from all over the world, providing a platform to engage with leading experts, share innovative ideas, and immerse ourselves in the latest advancements in science and technology. It’s a momentous occasion for any researcher or scientist, and I’m thrilled to be part of this year’s summit.&lt;/p&gt;
&lt;p&gt;We’re living in an exciting era, where &lt;strong&gt;Large Language Models (LLMs)&lt;/strong&gt; are reshaping the landscape of artificial intelligence. The rapid strides AI has made in recent years, fueled by powerful architectures like GPT, are nothing short of revolutionary. As we explore the depths of these models, we are witnessing the dawn of new possibilities in natural language processing, conversational agents, and machine learning. Attending GYSS2025 during this transformative period in AI’s evolution promises to be a truly enriching experience, as it will allow me to explore these advancements and exchange ideas with some of the brightest minds in the field.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Honored to have attended GYSS2025 in Singapore!  &lt;br&gt;.&lt;br&gt;An incredible experience connecting with global innovators, exchanging ideas, and gaining inspiration to shape the future. &lt;br&gt;.&lt;a href=&#34;https://t.co/LqTKodKSj3&#34;&gt;https://t.co/LqTKodKSj3&lt;/a&gt; &lt;a href=&#34;https://t.co/hJKWz6YTgy&#34;&gt;pic.twitter.com/hJKWz6YTgy&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1878614389978472895?ref_src=twsrc%5Etfw&#34;&gt;January 13, 2025&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;



&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/ZGXAT3bq7-c&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_01.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;presenting-my-work-to-her-royal-highness-princess-maha-chakri-sirindhorn&#34;&gt;Presenting My Work to Her Royal Highness Princess Maha Chakri Sirindhorn&lt;/h2&gt;
&lt;p&gt;The highlight of the summit was, without a doubt, the moment I had the incredible honor of presenting my work to Her Royal Highness Princess Maha Chakri Sirindhorn.&lt;/p&gt;
&lt;p&gt;As a young scientist, I was deeply humbled to share my research titled &lt;em&gt;“MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand”&lt;/em&gt; with Her Royal Highness. My work focuses on addressing challenges in agricultural monitoring and resource management by leveraging cutting-edge advancements in artificial intelligence. Specifically, MeViT is designed for semantic segmentation of Landsat satellite imagery, targeting key economic crops in Thailand such as para rubber, corn, and pineapple. By enhancing Vision Transformers (ViTs) with a medium-resolution multi-branch architecture and incorporating mixed-scale convolutional feedforward networks (MixCFN), MeViT excels at extracting multi-scale local information critical for precise segmentation.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/tgcKR97Ea8I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Her Royal Highness listened with great interest, her graciousness reflecting her profound commitment to nurturing the next generation of scientists. She expressed encouragement for the practical applications of such research in addressing challenges critical to Thailand’s agricultural and environmental sustainability. Her unwavering support for young researchers is a testament to her dedication to fostering innovation for the betterment of society.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_06_2.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_06.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_07.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_03.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;GYSS2025_withHRH/Her_Royal_Highness_toGYSS2025_2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It was a truly humbling and inspiring moment—one that I will carry with me for the rest of my life. The opportunity to share my work with Her Royal Highness not only reaffirmed my passion for pushing the boundaries of science and technology but also strengthened my resolve to contribute to meaningful advancements that serve the nation and the global community.&lt;/p&gt;
&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/XzUmDPFHfc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;!-- &lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;video controls width=&#34;800&#34;&gt;
        &lt;source src=&#34;https://kaopanboonyuen.github.io/GYSS2025_withHRH/GYSS_with_Her_Royal_Highnes_v1.mp4&#34; type=&#34;video/mp4&#34;&gt;
        Your browser does not support the video tag.
    &lt;/video&gt;
&lt;/div&gt; --&gt;
&lt;h1 id=&#34;learning-from-icons-in-the-field&#34;&gt;Learning from Icons in the Field&lt;/h1&gt;
&lt;p&gt;The summit was filled with groundbreaking talks, and one of the most memorable was the Plenary Lecture by the renowned Professor Adi Shamir. As a Turing Award Laureate and an expert in cryptography and artificial intelligence, his lecture was truly thought-provoking. The topic, &lt;em&gt;“Can you recover a deep neural network from its answers?”&lt;/em&gt;, delved into one of the most critical questions of our time—how deep learning models, which are at the forefront of AI, can be understood and potentially reverse-engineered.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/3KNBME7f0VI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The lecture began with a discussion on the architecture of modern neural networks, emphasizing their complexity and overparameterization. Professor Shamir explored concepts like model inversion, adversarial attacks, and the limitations of current AI systems, while raising questions about privacy-preserving AI and ethical AI frameworks. His insights on adversarial robustness and explainable AI deeply resonated with my research interests, motivating me to reflect on these critical challenges.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0218.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0206.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Deep Neural Networks (DNNs) have become indispensable in modern AI applications, with billions of dollars and countless GPU hours invested in their training. These models are often deployed as &amp;ldquo;black boxes,&amp;rdquo; allowing users to interact with them without revealing their inner workings. However, this raises a critical question: &lt;em&gt;Can the parameters of a deep neural network be recovered using only its inputs and outputs?&lt;/em&gt; In a groundbreaking plenary lecture, Turing Award recipient Prof. Adi Shamir demonstrated that for ReLU-based DNNs, it is indeed possible to recover all parameters in polynomial time relative to the number of neurons. His findings, supported by practical experiments, highlight both the potential vulnerabilities of these systems and the need for robust defenses.&lt;/p&gt;
&lt;h2 id=&#34;understanding-the-basics-of-dnn-operations&#34;&gt;Understanding the Basics of DNN Operations&lt;/h2&gt;
&lt;p&gt;At the heart of Shamir&amp;rsquo;s analysis lies a detailed understanding of how DNNs function. A DNN comprises multiple layers of neurons, each performing a series of linear transformations followed by non-linear activations. Among these, the Rectified Linear Unit (ReLU) is a widely used activation function defined as:&lt;/p&gt;
&lt;p&gt;$$
\text{ReLU}(x) = \max(0, x).
$$&lt;/p&gt;
&lt;p&gt;This piecewise linear function introduces non-linearity while maintaining computational simplicity. Shamir emphasized that ReLU&amp;rsquo;s linear segments make it particularly susceptible to parameter extraction, as its outputs can be mathematically analyzed to reveal underlying weights and biases.&lt;/p&gt;
&lt;p&gt;In mathematical terms, the operation of a single layer in a DNN can be expressed as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{y} = \text{ReLU}(\mathbf{W}\mathbf{x} + \mathbf{b}),
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{x}$ represents the input vector,&lt;/li&gt;
&lt;li&gt;$\mathbf{W}$ is the weight matrix,&lt;/li&gt;
&lt;li&gt;$\mathbf{b}$ is the bias vector, and&lt;/li&gt;
&lt;li&gt;$\mathbf{y}$ is the output vector after applying ReLU.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stacking multiple such layers creates a complex mapping from inputs to outputs, making the network appear opaque to external observers.&lt;/p&gt;
&lt;h2 id=&#34;challenges-in-parameter-recovery&#34;&gt;Challenges in Parameter Recovery&lt;/h2&gt;
&lt;p&gt;Recovering the parameters of a DNN—its weights and biases—is inherently an NP-hard problem. This complexity arises from the high dimensionality of the parameter space and the limited observability of internal computations. Traditional approaches to this problem relied on exhaustive searches, which scale exponentially with the number of parameters, rendering them impractical for large networks.&lt;/p&gt;
&lt;p&gt;Prof. Shamir highlighted that these challenges are exacerbated in scenarios where outputs are restricted to discrete or low-precision values. However, he proposed that by carefully designing input queries and analyzing output patterns, it is possible to significantly simplify the recovery process.&lt;/p&gt;
&lt;h2 id=&#34;polynomial-time-parameter-extraction&#34;&gt;Polynomial Time Parameter Extraction&lt;/h2&gt;
&lt;p&gt;Shamir&amp;rsquo;s key contribution lies in demonstrating a polynomial-time attack for ReLU-based DNNs. His approach leverages the inherent linearity of ReLU segments to derive equations that describe the network&amp;rsquo;s behavior. By identifying &lt;strong&gt;critical points&lt;/strong&gt;—locations where ReLU outputs switch between active and inactive states—one can extract sufficient information to reconstruct the network&amp;rsquo;s parameters.&lt;/p&gt;
&lt;h3 id=&#34;critical-points-and-linear-equations&#34;&gt;Critical Points and Linear Equations&lt;/h3&gt;
&lt;p&gt;Consider a single ReLU neuron with input $z$ and output $y = \text{ReLU}(z)$. The critical point for this neuron is $z = 0$, where the output transitions from 0 to a positive value. By probing the network with carefully chosen inputs that traverse these critical points, it becomes possible to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identify the active/inactive state of each neuron.&lt;/li&gt;
&lt;li&gt;Extract linear equations relating the input, weights, and biases.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For an $n$-layer network, these equations can be combined to solve for all parameters using standard techniques from linear algebra.&lt;/p&gt;
&lt;h3 id=&#34;practical-demonstration&#34;&gt;Practical Demonstration&lt;/h3&gt;
&lt;p&gt;In a practical demonstration, Shamir applied his method to an 8-layer DNN trained on the CIFAR-10 dataset. This network contained 1.2 million parameters, yet all were successfully recovered in polynomial time. The experiment underscores the real-world applicability of this attack and its implications for AI security.&lt;/p&gt;
&lt;h2 id=&#34;comparison-with-cryptographic-systems&#34;&gt;Comparison with Cryptographic Systems&lt;/h2&gt;
&lt;p&gt;Drawing parallels between DNNs and cryptographic systems, Shamir likened the structure of a neural network to a block cipher, where each layer performs a distinct transformation. In cryptography, security often hinges on the infeasibility of reversing these transformations without a key. Similarly, DNNs rely on the assumption that their internal parameters cannot be deduced from external interactions.&lt;/p&gt;
&lt;p&gt;However, Shamir&amp;rsquo;s work demonstrates that this assumption does not hold for ReLU-based networks. By exploiting the deterministic nature of their operations, an adversary can effectively &amp;ldquo;decrypt&amp;rdquo; the network to reveal its parameters.&lt;/p&gt;
&lt;h2 id=&#34;implications-of-randomness-and-alternative-activations&#34;&gt;Implications of Randomness and Alternative Activations&lt;/h2&gt;
&lt;p&gt;To mitigate the risks posed by such attacks, Shamir explored potential defenses, including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Randomness in Training and Inference&lt;/strong&gt;: Introducing stochasticity into the network&amp;rsquo;s operations, such as random noise or dropout, can obscure critical points and complicate parameter recovery.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Alternative Activation Functions&lt;/strong&gt;: Functions like the sigmoid or hyperbolic tangent (tanh) introduce smoother transitions, reducing the linearity exploited in Shamir&amp;rsquo;s attack. The sigmoid function, for example, is defined as:&lt;/p&gt;
&lt;p&gt;$$
\sigma(x) = \frac{1}{1 + e^{-x}}.
$$&lt;/p&gt;
&lt;p&gt;Unlike ReLU, sigmoid outputs are continuous and bounded, making it harder to identify critical points.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;broader-implications-and-future-directions&#34;&gt;Broader Implications and Future Directions&lt;/h2&gt;
&lt;p&gt;Shamir&amp;rsquo;s findings have profound implications for the field of AI security. As DNNs become integral to applications ranging from healthcare to autonomous systems, ensuring their robustness against parameter extraction attacks is paramount. Future research may focus on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designing architectures that are inherently resistant to such attacks.&lt;/li&gt;
&lt;li&gt;Developing formal metrics to quantify a network&amp;rsquo;s susceptibility to parameter recovery.&lt;/li&gt;
&lt;li&gt;Exploring the trade-offs between interpretability and security.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In conclusion, Prof. Adi Shamir&amp;rsquo;s lecture sheds light on a critical vulnerability in modern AI systems while providing a roadmap for addressing it. His innovative use of cryptographic techniques underscores the interdisciplinary nature of AI research and its potential to reshape our understanding of security in the digital age.&lt;/p&gt;
&lt;p&gt;But what truly stood out was the in-depth discussion I had with Professor Shamir. His monumental impact on the field of AI and his perspectives on current and future challenges was both a privilege and a learning experience that I will treasure forever.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_04.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;plenary-lecture-educability-prof-leslie-valiant&#34;&gt;Plenary Lecture: Educability (Prof Leslie Valiant)&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/RT5LaVPEiyU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;In a captivating lecture, Prof. Leslie Valiant delved into the concept of educability, a framework that bridges the gap between human cognitive capabilities and machine learning. Drawing inspiration from Alan Turing’s groundbreaking insights, the discussion highlighted the interplay between intelligence, learning, and the ethical design of artificial intelligence systems.&lt;/p&gt;
&lt;h2 id=&#34;historical-foundations-of-intelligence-and-educability&#34;&gt;Historical Foundations of Intelligence and Educability&lt;/h2&gt;
&lt;p&gt;The quest to define human intelligence has long been fraught with challenges. Psychologists have struggled to agree on a single definition, revealing the complexity of human cognition. Prof. Valiant proposed that much of our understanding can be reframed through the lens of &lt;strong&gt;educability&lt;/strong&gt;, which encompasses three primary facets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Learning from Experience&lt;/strong&gt; ($\mathcal{L}_{experience}$): The ability to generalize patterns and principles from observed phenomena.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning with Acquired Knowledge&lt;/strong&gt; ($\mathcal{R}_{knowledge}$): Chaining learned concepts to make inferences.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Absorbing Explicit Instruction&lt;/strong&gt; ($\mathcal{I}_{instruction}$): Gaining knowledge through direct teaching or guidance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These components have enabled humanity to progress from rudimentary tools to advanced technological civilizations.&lt;/p&gt;
&lt;h2 id=&#34;educability-vs-machine-learning&#34;&gt;Educability vs. Machine Learning&lt;/h2&gt;
&lt;h3 id=&#34;the-church-turing-thesis&#34;&gt;The Church-Turing Thesis&lt;/h3&gt;
&lt;p&gt;A cornerstone of the lecture was the &lt;strong&gt;Church-Turing Thesis&lt;/strong&gt;, which posits that all forms of computation—whether in human brains or machines—are fundamentally equivalent. This foundational idea underpins modern efforts to replicate human cognition in artificial intelligence.&lt;/p&gt;
&lt;h3 id=&#34;comparing-human-and-machine-learning&#34;&gt;Comparing Human and Machine Learning&lt;/h3&gt;
&lt;p&gt;Current AI systems excel in pattern recognition and data-driven learning. However, they fall short in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contextual Reasoning:&lt;/strong&gt; Humans can apply learned knowledge across diverse scenarios. For example, recognizing that if $A \implies B$ and $B \implies C$, then $A \implies C$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from Minimal Examples:&lt;/strong&gt; Unlike humans, who can learn concepts from a few instances, AI often requires massive datasets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instruction-Based Learning:&lt;/strong&gt; Humans thrive in environments with structured instruction, a capability that AI systems struggle to emulate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prof. Valiant argued that replicating these nuanced aspects of educability in AI could unlock new levels of machine intelligence.&lt;/p&gt;
&lt;h2 id=&#34;cognitive-capabilities-as-civilization-enablers&#34;&gt;Cognitive Capabilities as Civilization Enablers&lt;/h2&gt;
&lt;p&gt;Human educability is unique among species, enabling the creation of advanced civilizations. This capability hinges on the ability to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generalize Across Domains:&lt;/strong&gt; Applying principles learned in one context to solve problems in another.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accumulate Knowledge:&lt;/strong&gt; Building on the work of previous generations through explicit instruction and documentation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collaborate:&lt;/strong&gt; Combining cognitive efforts to achieve collective goals.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ethical-implications-of-ai-development&#34;&gt;Ethical Implications of AI Development&lt;/h2&gt;
&lt;p&gt;Prof. Valiant emphasized the ethical considerations in designing AI systems that reflect positive human traits while avoiding the replication of human flaws. Key takeaways included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Avoiding Bias:&lt;/strong&gt; Ensuring that AI systems do not inherit societal biases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transparent Decision-Making:&lt;/strong&gt; Designing AI that can explain its reasoning processes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Augmenting Human Capabilities:&lt;/strong&gt; Building systems that complement rather than replace human intelligence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;addressing-concerns-about-ai&#34;&gt;Addressing Concerns About AI&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Accountability:&lt;/strong&gt; Who is responsible for decisions made by AI systems?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safety:&lt;/strong&gt; How do we ensure AI systems act in the best interest of humanity?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Global Standards:&lt;/strong&gt; The need for international cooperation to establish ethical guidelines.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;the-future-of-ai-and-educability&#34;&gt;The Future of AI and Educability&lt;/h2&gt;
&lt;p&gt;Prof. Valiant concluded with a vision for the future:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interdisciplinary Collaboration:&lt;/strong&gt; Bringing together technologists, ethicists, and cognitive scientists to advance AI responsibly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formalizing Educability:&lt;/strong&gt; Developing mathematical models to encode human-like learning in machines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human-Machine Collaboration:&lt;/strong&gt; Leveraging AI to enhance human cognitive abilities, leading to breakthroughs in science, medicine, and technology.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;a-call-to-action&#34;&gt;A Call to Action&lt;/h3&gt;
&lt;p&gt;Understanding the parameters of educability is not just an academic pursuit; it is a moral imperative. By integrating insights from cognitive science, mathematics, and ethics, we can create AI systems that are not only intelligent but also aligned with human values.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This exploration of educability challenges us to rethink the foundations of intelligence and its implications for the future of artificial intelligence. As we navigate the complexities of AI development, let us be guided by the principles of transparency, accountability, and collaboration.&lt;/p&gt;
&lt;h2 id=&#34;plenary-lecture-compressing-proofs-using-cryptography-prof-yael-kalai&#34;&gt;Plenary Lecture: Compressing Proofs using Cryptography (Prof Yael Kalai)&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/bo67m5XyVWo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Cryptography has always been the backbone of secure systems, enabling trust in decentralized and distributed environments. As computation scales, the demand for efficient proof systems that ensure both correctness and privacy becomes increasingly important. Succinct proofs represent a groundbreaking development in this domain, offering compact, verifiable proofs that maintain efficiency and security.&lt;/p&gt;
&lt;p&gt;In a recent lecture, &lt;strong&gt;Prof. Yael Tauman Kalai&lt;/strong&gt; presented her advancements in succinct proofs, detailing their cryptographic foundations, practical implications, and applications in areas like blockchain and artificial intelligence (AI). This blog unpacks her insights and explores how these proofs address modern computational challenges.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-are-succinct-proofs&#34;&gt;What Are Succinct Proofs?&lt;/h2&gt;
&lt;p&gt;Succinct proofs are cryptographic constructs designed to reduce the size and verification complexity of traditional proof systems. Instead of requiring a verifier to redo the entire computation to confirm its validity, succinct proofs enable verification with minimal computational effort. This efficiency is achieved without sacrificing security or trust, making them a critical tool for systems where scalability and privacy are paramount.&lt;/p&gt;
&lt;p&gt;Unlike traditional proof systems, which may involve large data sets and complex computations, succinct proofs achieve their compactness through advanced cryptographic techniques such as &lt;strong&gt;homomorphic encryption&lt;/strong&gt;, &lt;strong&gt;polynomial commitments&lt;/strong&gt;, and &lt;strong&gt;elliptic curve cryptography&lt;/strong&gt;. These techniques enable a prover to encode the essential details of a computation into a small, verifiable proof.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;key-cryptographic-techniques&#34;&gt;Key Cryptographic Techniques&lt;/h2&gt;
&lt;h3 id=&#34;zero-knowledge-proofs&#34;&gt;Zero-Knowledge Proofs&lt;/h3&gt;
&lt;p&gt;One of the core building blocks of succinct proofs is the concept of zero-knowledge proofs (ZKPs). A ZKP allows a prover to convince a verifier that a statement is true without revealing any information beyond its validity. This ensures privacy while maintaining trust.&lt;/p&gt;
&lt;p&gt;For example, in a blockchain transaction, a ZKP can prove the correctness of the transaction without disclosing the sender, receiver, or amount. This is particularly critical in privacy-preserving protocols like &lt;strong&gt;zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge)&lt;/strong&gt; and &lt;strong&gt;zk-STARKs (Zero-Knowledge Scalable Transparent Arguments of Knowledge)&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;zk-SNARKs&lt;/strong&gt; rely on a trusted setup to initialize the system. They use elliptic curve pairings and polynomial arithmetic to achieve their compactness. Despite their efficiency, the trusted setup requirement introduces potential vulnerabilities if compromised.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;zk-STARKs&lt;/strong&gt;, on the other hand, eliminate the need for a trusted setup by relying on hash functions and polynomial interpolation. While this makes them more transparent and secure, they often result in larger proof sizes compared to zk-SNARKs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;polynomial-commitments&#34;&gt;Polynomial Commitments&lt;/h3&gt;
&lt;p&gt;Another crucial technique in succinct proofs is the use of polynomial commitments. These commitments enable the prover to encode computations as polynomials, allowing the verifier to check their correctness without directly interacting with the underlying data. Polynomial commitments are a cornerstone of many cryptographic protocols, including zk-STARKs and modern succinct proof systems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;applications-of-succinct-proofs&#34;&gt;Applications of Succinct Proofs&lt;/h2&gt;
&lt;h3 id=&#34;blockchain-scalability&#34;&gt;Blockchain Scalability&lt;/h3&gt;
&lt;p&gt;One of the most impactful applications of succinct proofs is in blockchain technology. Blockchains, by design, require every participant to validate transactions to maintain trust. However, as the number of transactions grows, this validation becomes a bottleneck.&lt;/p&gt;
&lt;p&gt;Succinct proofs offer a solution by enabling participants to verify the correctness of transactions without processing the full chain. Protocols like &lt;strong&gt;Ethereum&amp;rsquo;s Layer 2 solutions&lt;/strong&gt; and &lt;strong&gt;Zcash&lt;/strong&gt; leverage succinct proofs to improve scalability and maintain privacy.&lt;/p&gt;
&lt;p&gt;For instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rollups&lt;/strong&gt; in Ethereum aggregate transactions off-chain and use succinct proofs to certify their correctness on-chain.&lt;/li&gt;
&lt;li&gt;Privacy-focused blockchains like Zcash use zk-SNARKs to enable shielded transactions, ensuring that details about the sender, receiver, and transaction amount remain confidential.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ai-model-verification&#34;&gt;AI Model Verification&lt;/h3&gt;
&lt;p&gt;In the realm of AI, succinct proofs are emerging as a tool to certify the correctness of model outputs. As AI systems grow more complex, the ability to verify their decisions becomes a challenge. Succinct proofs can be used to generate a compact, verifiable record of an AI model’s decision-making process.&lt;/p&gt;
&lt;p&gt;For example, in image classification tasks, a succinct proof could certify that the model correctly identified an object without requiring the verifier to process the entire dataset or model. This has profound implications for AI applications in critical domains like healthcare, where trust and accountability are paramount.&lt;/p&gt;
&lt;h3 id=&#34;decentralized-identity&#34;&gt;Decentralized Identity&lt;/h3&gt;
&lt;p&gt;Decentralized identity systems aim to give individuals control over their personal data while allowing them to prove certain attributes (e.g., age, citizenship) without revealing unnecessary details. Succinct proofs enable such systems by providing compact, privacy-preserving verifications.&lt;/p&gt;
&lt;p&gt;Protocols like &lt;strong&gt;Verifiable Credentials (VCs)&lt;/strong&gt; and &lt;strong&gt;Decentralized Identifiers (DIDs)&lt;/strong&gt; rely on these cryptographic techniques to ensure that identity verification is both efficient and secure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;challenges-and-future-directions&#34;&gt;Challenges and Future Directions&lt;/h2&gt;
&lt;p&gt;While succinct proofs offer significant advantages, they are not without challenges:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trusted Setup&lt;/strong&gt;: zk-SNARKs require a trusted setup, which, if compromised, could undermine the security of the entire system. Research into transparent setups (as in zk-STARKs) aims to address this limitation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Computation Overhead&lt;/strong&gt;: Although succinct proofs reduce verification complexity, the prover&amp;rsquo;s computational requirements can be high. Optimizing the proving process is an active area of research.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interoperability&lt;/strong&gt;: For widespread adoption, succinct proof systems must integrate seamlessly with existing technologies. This involves developing standards and protocols that ensure compatibility across platforms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quantum Resistance&lt;/strong&gt;: As quantum computing advances, many cryptographic systems, including those used in succinct proofs, face potential vulnerabilities. Developing quantum-resistant proof systems is a critical area of ongoing research.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;insights-from-prof-yael-tauman-kalais-research&#34;&gt;Insights from Prof. Yael Tauman Kalai’s Research&lt;/h2&gt;
&lt;p&gt;Prof. Kalai’s work pushes the boundaries of succinct proofs by exploring new cryptographic primitives and optimizing existing protocols. Her research emphasizes collaboration between academia and industry to address real-world challenges. Key areas of her focus include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Enhancing Transparency&lt;/strong&gt;: Developing protocols that eliminate the need for trusted setups while maintaining efficiency and scalability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improving Scalability&lt;/strong&gt;: Optimizing proof generation to reduce computational overhead for the prover.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expanding Applications&lt;/strong&gt;: Applying succinct proofs to emerging fields like decentralized finance (DeFi), secure voting systems, and federated learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Succinct proofs represent a paradigm shift in cryptography, enabling efficient, scalable, and privacy-preserving verification across various domains. From blockchain scalability to AI model verification, their potential applications are vast and transformative. However, realizing their full potential requires addressing challenges like computational overhead, interoperability, and quantum resistance.&lt;/p&gt;
&lt;p&gt;As cryptographic research evolves, the collaboration between researchers, industry, and policymakers will be essential to unlock the full potential of succinct proofs. Prof. Kalai’s groundbreaking work serves as a testament to the importance of pushing the boundaries of what cryptography can achieve.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;engaging-in-dialogues-on-the-ethics-of-ai&#34;&gt;Engaging in Dialogues on the Ethics of AI&lt;/h2&gt;
&lt;p&gt;The summit wasn’t just about science and innovation—it also provided a platform to engage in crucial discussions about the ethical implications of technological advancements. One such discussion was the Panel Huddle titled &lt;em&gt;“Ethics of Scientific Research in the Age of AI”&lt;/em&gt;, featuring prominent professors like Adi Shamir, Shafi Goldwasser, and Kalai.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/eZhOUtUIIQ8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The panel explored topics such as algorithmic bias, dual-use AI technologies, privacy-preserving AI, and the responsibilities of researchers in educating the public and policymakers. These discussions offered profound insights into the complexities of conducting ethical research in a rapidly evolving technological landscape, reminding me of the broader purpose of scientific innovation: to create a more equitable future.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0305.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ethics-of-scientific-research-in-the-age-of-ai&#34;&gt;Ethics of Scientific Research in the Age of AI&lt;/h2&gt;
&lt;p&gt;Artificial intelligence (AI) is revolutionizing scientific research, offering transformative tools to accelerate discovery, enhance accuracy, and expand the boundaries of human knowledge. However, these advancements also bring profound ethical challenges. At the recent panel moderated by &lt;strong&gt;Prof. Simon Chesterman&lt;/strong&gt; from the National University of Singapore, renowned experts &lt;strong&gt;Prof. Joan Rose&lt;/strong&gt; (2016 Stockholm Water Prize), &lt;strong&gt;Prof. Yael Kalai&lt;/strong&gt; (2022 ACM Prize in Computing), and &lt;strong&gt;Prof. Adi Shamir&lt;/strong&gt; (2002 Turing Award) delved into the critical ethical considerations of AI in scientific research.&lt;/p&gt;
&lt;h3 id=&#34;client-concerns-about-ai&#34;&gt;Client Concerns About AI&lt;/h3&gt;
&lt;h4 id=&#34;ethical-dialogues&#34;&gt;Ethical Dialogues&lt;/h4&gt;
&lt;p&gt;AI systems increasingly shape scientific discovery, yet their application necessitates robust ethical guidelines. Governance frameworks must:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Define clear boundaries for AI use.&lt;/li&gt;
&lt;li&gt;Address biases inherent in data and algorithms.&lt;/li&gt;
&lt;li&gt;Ensure transparency and accountability in AI-driven research outcomes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;human-interaction&#34;&gt;Human Interaction&lt;/h4&gt;
&lt;p&gt;The integration of AI into research workflows often alters the dynamic between human researchers and technology. Key considerations include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preventing over-reliance on AI outputs, which could lead to critical oversights.&lt;/li&gt;
&lt;li&gt;Mitigating risks of misinterpretation of AI-generated results.&lt;/li&gt;
&lt;li&gt;Balancing human intuition with machine precision.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;opportunities-and-risks&#34;&gt;Opportunities and Risks&lt;/h3&gt;
&lt;h4 id=&#34;power-of-ai&#34;&gt;Power of AI&lt;/h4&gt;
&lt;p&gt;AI holds immense potential to transform scientific methodologies:&lt;/p&gt;
&lt;p&gt;$$ \text{Efficiency Gain} = \frac{T_{\text{manual}}}{T_{\text{AI-assisted}}} $$&lt;/p&gt;
&lt;p&gt;Where $T_{\text{manual}}$ represents the time for traditional methods and $T_{\text{AI-assisted}}$ denotes AI-accelerated approaches. While this formula underscores AI’s ability to enhance efficiency, ethical concerns include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Misinformation propagation due to AI biases.&lt;/li&gt;
&lt;li&gt;Job displacement for researchers whose roles are increasingly automated.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;military-applications&#34;&gt;Military Applications&lt;/h4&gt;
&lt;p&gt;AI’s role in military research raises acute ethical dilemmas, particularly in autonomous weapons systems. Questions of accountability and decision-making in lethal scenarios must be urgently addressed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who is accountable for errors in autonomous systems?&lt;/li&gt;
&lt;li&gt;How can ethical principles such as proportionality and necessity be encoded into AI?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;public-engagement&#34;&gt;Public Engagement&lt;/h3&gt;
&lt;h4 id=&#34;communication-gaps&#34;&gt;Communication Gaps&lt;/h4&gt;
&lt;p&gt;A significant gap exists between the scientific community and the public’s understanding of AI. To bridge this divide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop accessible communication strategies to explain AI technologies and their implications.&lt;/li&gt;
&lt;li&gt;Foster public trust through transparent disclosures of AI’s capabilities and limitations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;proactive-education&#34;&gt;Proactive Education&lt;/h4&gt;
&lt;p&gt;Educating researchers and policymakers is vital to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Promote ethical awareness.&lt;/li&gt;
&lt;li&gt;Equip them to evaluate the societal impacts of their work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;accountability-and-regulation&#34;&gt;Accountability and Regulation&lt;/h3&gt;
&lt;h4 id=&#34;need-for-regulation&#34;&gt;Need for Regulation&lt;/h4&gt;
&lt;p&gt;Regulating AI is a complex yet essential endeavor. Key areas of focus include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Warfare Applications:&lt;/strong&gt; Establishing international norms to prohibit unethical AI use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Public Safety:&lt;/strong&gt; Creating standards for AI deployment in sensitive domains such as healthcare and transportation.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;global-cooperation&#34;&gt;Global Cooperation&lt;/h4&gt;
&lt;p&gt;International collaboration is critical to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop shared standards for AI ethics.&lt;/li&gt;
&lt;li&gt;Address cross-border challenges, such as data privacy and AI governance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;future-directions&#34;&gt;Future Directions&lt;/h3&gt;
&lt;h4 id=&#34;research-and-policy&#34;&gt;Research and Policy&lt;/h4&gt;
&lt;p&gt;Investing in research on AI risks enables policymakers to adopt informed, proactive measures rather than reactive regulations post-crisis. Proposed funding distribution can be modeled as:&lt;/p&gt;
&lt;p&gt;$$ \text{Allocation} = \frac{R_{\text{risk}}}{R_{\text{total}}} \times 100 % $$&lt;/p&gt;
&lt;p&gt;Where $R_{\text{risk}}$ represents research focused on AI risks, and $R_{\text{total}}$ is the total research budget.&lt;/p&gt;
&lt;h4 id=&#34;interdisciplinary-approach&#34;&gt;Interdisciplinary Approach&lt;/h4&gt;
&lt;p&gt;Addressing AI’s ethical challenges requires collaboration among:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Technologists:&lt;/strong&gt; To refine AI systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ethicists:&lt;/strong&gt; To integrate moral principles into AI development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Policymakers:&lt;/strong&gt; To enact effective regulations and governance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The panel underscored that while AI offers unparalleled opportunities for scientific progress, its ethical integration requires foresight, collaboration, and accountability. By fostering interdisciplinary dialogue and committing to transparent, responsible practices, the scientific community can ensure AI serves as a force for good.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;visual-representation&#34;&gt;Visual Representation&lt;/h3&gt;
&lt;p&gt;A mind map summarizing these insights can help readers visualize the ethical considerations of AI in scientific research.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS2025_withHRH/LECTURE/GRPAH_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;expanding-my-network-and-building-new-collaborations&#34;&gt;Expanding My Network and Building New Collaborations&lt;/h2&gt;
&lt;p&gt;Beyond the lectures and discussions, GYSS 2025 was also a chance to meet fellow researchers from all over the globe. I had the pleasure of connecting with inspiring individuals from diverse fields of study, creating friendships that will last a lifetime. The exchange of ideas with these brilliant young scientists has already sparked new collaborations and research ideas that I can’t wait to explore further.&lt;/p&gt;
&lt;h2 id=&#34;a-day-ill-never-forget&#34;&gt;A Day I’ll Never Forget&lt;/h2&gt;
&lt;p&gt;All in all, my experience at GYSS 2025 was beyond what I could have imagined. It was a perfect blend of research, networking, and fun. It gave me the chance to engage with some of the brightest minds in science, learn from Nobel Laureates and Turing Award winners, and discuss pressing issues in AI and ethics. It was an unforgettable experience, and I’m leaving with new ideas, fresh perspectives, and the motivation to continue pushing the boundaries of my own research.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/5C-6bXuVq9Q&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0111.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_02.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0238.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PS. This year, GYSS 2025 was hosted at the beautiful campus of the National University of Singapore (NUS), and I absolutely fell in love with the environment. The vibrant atmosphere, lush greenery, and modern architecture made it the perfect blend of nature and innovation—a truly ideal space for creativity and learning.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Jan 2025). &lt;em&gt;Where Science Meets Inspiration: My Experience at GYSS2025 in Singapore&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2025gyss,
  title   = &amp;quot;Where Science Meets Inspiration: My Experience at GYSS2025 in Singapore&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Jan&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sub1 Urban Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/</link>
      <pubDate>Wed, 25 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/</guid>
      <description>&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#chapter-1-osaka-running&#34;&gt;Chapter 1: Osaka Running&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#chapter-2-kyoto-running&#34;&gt;Chapter 2: Kyoto Running&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#chapter-3-tokyo-running&#34;&gt;Chapter 3: Tokyo Running&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#chapter-4-tokyo-farewell-run&#34;&gt;Chapter 4: Tokyo Farewell Run&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#summary-of-my-running-experience&#34;&gt;Summary of My Running Experience&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;Hey guys! I just completed an incredible running experience across Japan&amp;rsquo;s vibrant cities—Osaka, Kyoto, and Tokyo. I managed to hit sub-1-hour times while soaking in the unique vibes of each place. It was the perfect mix of pushing my pace and enjoying the sights along the way!&lt;/p&gt;
&lt;p&gt;It was such a cool blend of testing my limits and discovering new places. Each run through those iconic spots felt like I was getting a real taste of Japan’s unique vibe and energy with every stride.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Just finished an awesome running tour through Osaka, Kyoto, and Tokyo! 🏃‍♂️ Nailed sub-1-hour times while soaking in all the city vibes. &lt;br&gt;&lt;br&gt;Can&amp;#39;t believe how much fun it was to race and explore at the same time! 🇯🇵 &lt;a href=&#34;https://twitter.com/hashtag/RunningExperience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#RunningExperience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Japan?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Japan&lt;/a&gt;&lt;a href=&#34;https://t.co/Wy2bpgO8lg&#34;&gt;https://t.co/Wy2bpgO8lg&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1838942597387362675?ref_src=twsrc%5Etfw&#34;&gt;September 25, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h3 id=&#34;chapter-1-osaka-running&#34;&gt;Chapter 1: Osaka Running&lt;/h3&gt;
&lt;p&gt;This trip marks my third time in Japan, but this time I started in the Kansai region, kicking things off in Osaka. My main goal? To conquer a city run and aim for a sub-1-hour 10K.&lt;/p&gt;
&lt;p&gt;Bright and early, after a good night’s rest, I hit the streets at 4:54 AM. The weather was just right, making the run super enjoyable. I ended up covering 10.59K at a pace of 4:42, finishing in 49 minutes. Here’s the route I took:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start at Awaza&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Awaza&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matsushima Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Matsushima&amp;#43;Park&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chiyoza&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Chiyoza&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Minamiizuo Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Minamiizuo&amp;#43;Park&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kitatsumorinaka Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kitatsumorinaka&amp;#43;Park&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deshirohigashi Intersection&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Deshirohigashi&amp;#43;Intersection&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machida Gastroenterology Hospital&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Machida&amp;#43;Gastroenterology&amp;#43;Hospital&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tsutenkaku Tower&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Tsutenkaku&amp;#43;Tower&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finish at Matsuyamachi&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Matsuyamachi&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nipponbashi&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Nipponbashi&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_OSAKA_001.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_OSAKA_002.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_OSAKA_003.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_OSAKA_004.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Osaka is truly a city that hooks you; it’s filled with hidden gems waiting to be explored. After three action-packed days, it was time to move on to Kyoto!&lt;/p&gt;
&lt;h3 id=&#34;chapter-2-kyoto-running&#34;&gt;Chapter 2: Kyoto Running&lt;/h3&gt;
&lt;p&gt;Next up was the challenge of a sub-1-hour city run in Kyoto. I only had two days here, and luck wasn’t on my side—rain greeted me in the morning. But nothing was going to stop my determination!&lt;/p&gt;
&lt;p&gt;I crushed the city run in Kyoto, finishing 10.78K in 49 minutes (pace 4:38), which was even faster than my Osaka run! I think the rain actually helped; the air felt fresh and invigorating. Here’s my route:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start at Kiyomizu-Jojo Station&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kiyomizu&amp;#43;Jojo&amp;#43;Station&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kyoto Pref. Yakuzaishikai&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kyoto&amp;#43;Pref&amp;#43;Yakuzaishikai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matsubaracho&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Matsubaracho&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kawebata Police Station&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kawebata&amp;#43;Police&amp;#43;Station&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kyoto University (Yoshida Campus)&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kyoto&amp;#43;University&amp;#43;Yoshida&amp;#43;Campus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Imadegawa Street&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Imadegawa&amp;#43;Street&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kyoto Imperial Palace (Kyoto Gyoen)&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kyoto&amp;#43;Imperial&amp;#43;Palace&amp;#43;Kyoto&amp;#43;Gyoen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Naka-Dachiuri-Dori&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Naka&amp;#43;Dachiuri&amp;#43;Dori&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Horikawa Sanjo&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Horikawa&amp;#43;Sanjo&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nishiki Market&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Nishiki&amp;#43;Market&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_KYOTO_001.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_002.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_003.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_003_02.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_003_03.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_003_04.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_004.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Wrapping up my run in Kyoto felt like a major win—2 out of 3 goals achieved! Before heading to my final destination, I hopped on the Shinkansen back to Tokyo.&lt;/p&gt;
&lt;p&gt;I took my first-ever Shinkansen ride from Kyoto to Tokyo, leaving at 9:39 AM and arriving at 11:54 AM. It was an absolute treat! I opted for the Green Car for that extra comfort and snagged a window seat to soak in the views. Here’s a glimpse of that journey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Shinkansen from Kyoto to Tokyo&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Shinkansen&amp;#43;from&amp;#43;Kyoto&amp;#43;to&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🚄&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/Kao_Shinkansen_01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/Kao_Shinkansen_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/w3lpcZRAEK4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;chapter-3-tokyo-running&#34;&gt;Chapter 3: Tokyo Running&lt;/h3&gt;
&lt;p&gt;Finally, I’ve made it to Tokyo! There’s always a surprise waiting here. Before my city run, a professor from Todai treated me to a swim at the Olympic pool used during the Tokyo 2020 Games. What an experience! I swam 1.2K in just 28 minutes at a pace of 2:25 per 100m. The Olympic-standard pool was incredible, and swimming alongside serious athletes was an adrenaline rush.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/Kao_Japan_Swim_2024.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_002.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_001.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_004.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_007.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_008.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, back to the final running mission. On my second day in Tokyo, I kicked off my run at 6:52 AM. The weather was perfect—18°C with a refreshing breeze—ideal for running.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_TOKYO_006.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_VO2MAX_51.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I wrapped up the city run covering 10.17K in 44 minutes at a pace of 4:25, marking a personal best! Here’s my route in Tokyo:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start at Shiomi Dori Street&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Shiomi&amp;#43;Dori&amp;#43;Street&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokyo Aquatics Centre&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Tokyo&amp;#43;Aquatics&amp;#43;Centre&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yumenoshima Stadium&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Yumenoshima&amp;#43;Stadium&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Arakawawangan-kyo Bridge&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Arakawawangan&amp;#43;kyo&amp;#43;Bridge&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nakasakombashi&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Nakasakombashi&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seishin Itchu North&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Seishin&amp;#43;Itchu&amp;#43;North&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kiyosuna-Ohashi Bridge&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kiyosuna&amp;#43;Ohashi&amp;#43;Bridge&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ojima Elementary School&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Ojima&amp;#43;Elementary&amp;#43;School&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finish at Ojima Komatsugawa Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Ojima&amp;#43;Komatsugawa&amp;#43;Park&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 🗺️&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kiba Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kiba&amp;#43;Park&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here 📍&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_TOKYO_001.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_002.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_003_01.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_003_02.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_004.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;chapter-4-tokyo-farewell-run&#34;&gt;Chapter 4: Tokyo Farewell Run&lt;/h3&gt;
&lt;p&gt;Before wrapping up my Tokyo adventure tomorrow, I took advantage of this morning’s cool weather, with a light drizzle setting the perfect scene for my final city run—a 5K to bid this trip a fond farewell. I covered 5.16 km in 23:54 minutes (Pace 4:38). The highlight? My VO2 Max hit a new high at 51, and Garmin labeled my training status as “Peaking.” It’s the ultimate runner’s high!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_PEAKING.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_VO2MAX_51.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I started my run at Nihombashihakozakicho, crossed the Eitai Bashi bridge, and continued parallel to the scenic Sumida River. Passing through the vibrant Kiyosumi-Dori Avenue at kilometer 2, I found myself immersed in Tokyo’s unique urban rhythm. I reached the serene Kiyosumi Garden by kilometer 3, before heading to Gokenborui Park at kilometer 4. The run climaxed with a sprint up Shin-Ohashi bridge, finishing at the 5K mark on the dot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_001.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_002.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_003.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_004.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_006.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This city run was the perfect way to close out my time in Tokyo. From the calm river views to the bustling cityscape, every step was a reminder of how much this place has to offer. Until next time, Tokyo!&lt;/p&gt;
&lt;h3 id=&#34;summary-of-my-running-experience&#34;&gt;Summary of My Running Experience&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Osaka Running:&lt;/strong&gt; Sub1 10.59K City Run, 49:48min, Pace 4:42&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kyoto Running:&lt;/strong&gt; Sub1 10.78K City Run, 49:57min, Pace 4:38&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokyo Running:&lt;/strong&gt; Sub1 10.17K City Run, 44:58min, Pace 4:25&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokyo Swimming:&lt;/strong&gt; 1,200m (1.2K), 28:58min, Pace 2:25&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokyo Running:&lt;/strong&gt; Sub24min 5.16K City Run, 23:54min, Pace 4:38&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This trip to Japan was absolutely fantastic! Even though it was late summer and winter is just around the corner, running through these three major cities expanded my horizons. I encountered amazing insights into Japanese culture, met locals, and explored unseen places.&lt;/p&gt;
&lt;p&gt;I can’t wait for the next experience, hopefully tackling the Tokyo Marathon (42.195K) someday! Fingers crossed!&lt;/p&gt;
&lt;p&gt;Until next time, happy running! 🏃‍♂️🇯🇵&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Sub1 Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024running,
  title   = &amp;quot;Sub1 Urban Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling</title>
      <link>https://kaopanboonyuen.github.io/publication/sea-vit-sea-surface-currents-forecasting/</link>
      <pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/sea-vit-sea-surface-currents-forecasting/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Fine-Tune and Deploy a Satellite-Specific LLMs Model for Satellite Images</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/</link>
      <pubDate>Sun, 08 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk 🌎 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240910_Panboonyuen_How_to_Fine_Tune_Satellite_Specific_LLM.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction-to-large-language-models-llms&#34;&gt;Introduction to Large Language Models (LLMs)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-vocabulary&#34;&gt;Key Vocabulary&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#architecture-of-llms&#34;&gt;Architecture of LLMs&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#transformer-architecture-formula&#34;&gt;Transformer Architecture Formula&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#training-llms&#34;&gt;Training LLMs&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction-to-llms-for-satellite-images&#34;&gt;Introduction to LLMs for Satellite Images&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#exploring-vision-language-models-vlms-to-understand-high-level-features-in-remotely-sensed-images&#34;&gt;Exploring Vision-Language Models (VLMs) to Understand High-Level Features in Remotely Sensed Images&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#what-makes-vision-language-models-vlms-special&#34;&gt;What Makes Vision-Language Models (VLMs) Special?&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#challenges-in-remote-sensing-with-vlms&#34;&gt;Challenges in Remote Sensing with VLMs&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#benchmarking-vlms-on-landmark-recognition&#34;&gt;Benchmarking VLMs on Landmark Recognition&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#diving-deeper-into-vlms-case-studies-of-landmark-recognition-and-scene-interpretation&#34;&gt;Diving Deeper into VLMs: Case Studies of Landmark Recognition and Scene Interpretation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#overview-of-the-fine-tuning-process&#34;&gt;Overview of the Fine-Tuning Process&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#step-by-step-fine-tuning-of-satgpt-for-satellite-imagery&#34;&gt;Step-by-Step Fine-Tuning of SatGPT for Satellite Imagery&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-data-preparation&#34;&gt;1. Data Preparation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-model-selection&#34;&gt;2. Model Selection&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-fine-tuning-paradigm&#34;&gt;3. Fine-Tuning Paradigm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-model-validation-and-evaluation&#34;&gt;4. Model Validation and Evaluation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#5-export-and-deployment-to-hugging-face&#34;&gt;5. Export and Deployment to Hugging Face&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#additional-concepts&#34;&gt;Additional Concepts&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#formula-for-self-attention-in-rag&#34;&gt;Formula for Self-Attention in RAG&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#vision-transformer-vit&#34;&gt;Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#full-flow-diagram&#34;&gt;Full Flow Diagram&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#quick-thoughts-on-llms-before-we-wrap-up-this-blog&#34;&gt;Quick thoughts on LLMs before we wrap up this blog:&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-introduction-to-large-language-models-llms-in-remote-sensing&#34;&gt;1. &lt;strong&gt;Introduction to Large Language Models (LLMs) in Remote Sensing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-foundation-models-and-their-role-in-llms&#34;&gt;2. &lt;strong&gt;Foundation Models and Their Role in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-training-vs-fine-tuning-vs-pre-trained-models-in-llms&#34;&gt;3. &lt;strong&gt;Training vs Fine-tuning vs Pre-trained Models in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-how-to-train-llms-on-satellite-images&#34;&gt;4. &lt;strong&gt;How to Train LLMs on Satellite Images&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#5-retrieval-augmented-generation-rag-for-satellite-image-analysis&#34;&gt;5. &lt;strong&gt;Retrieval-Augmented Generation (RAG) for Satellite Image Analysis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#6-using-langchain-for-satellite-image-llm-applications&#34;&gt;6. &lt;strong&gt;Using LangChain for Satellite Image LLM Applications&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#7-sample-datasets-for-llm-fine-tuning-in-remote-sensing&#34;&gt;7. &lt;strong&gt;Sample Datasets for LLM Fine-Tuning in Remote Sensing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#8-mathematical-foundations-of-attention-mechanisms-in-llms&#34;&gt;8. &lt;strong&gt;Mathematical Foundations of Attention Mechanisms in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#9-multimodal-llm-architectures-for-satellite-images&#34;&gt;9. &lt;strong&gt;Multimodal LLM Architectures for Satellite Images&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#10-preprocessing-techniques-for-satellite-images-in-llms&#34;&gt;10. &lt;strong&gt;Preprocessing Techniques for Satellite Images in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#11-handling-illumination-and-atmospheric-effects-in-llms&#34;&gt;11. &lt;strong&gt;Handling Illumination and Atmospheric Effects in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#12-self-supervised-learning-ssl-for-satellite-image-analysis&#34;&gt;12. &lt;strong&gt;Self-Supervised Learning (SSL) for Satellite Image Analysis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#13-open-source-tools-for-llms-in-satellite-image-analysis&#34;&gt;13. &lt;strong&gt;Open-Source Tools for LLMs in Satellite Image Analysis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#14-fine-tuning-llms-for-specific-satellite-image-tasks&#34;&gt;14. &lt;strong&gt;Fine-Tuning LLMs for Specific Satellite Image Tasks&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#15-evaluation-metrics-for-llms-in-remote-sensing&#34;&gt;15. &lt;strong&gt;Evaluation Metrics for LLMs in Remote Sensing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#16-transfer-learning-for-satellite-imagery&#34;&gt;16. &lt;strong&gt;Transfer Learning for Satellite Imagery&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#17-explainability-in-llms-for-remote-sensing-xai&#34;&gt;17. &lt;strong&gt;Explainability in LLMs for Remote Sensing (XAI)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction-to-large-language-models-llms&#34;&gt;Introduction to Large Language Models (LLMs)&lt;/h2&gt;
&lt;p&gt;Large Language Models (LLMs) are at the forefront of a revolution in Artificial Intelligence (AI) and Natural Language Processing (NLP). These models are not just sophisticated algorithms; they represent a leap forward in how machines understand and generate human language. Leveraging cutting-edge deep learning architectures, such as transformers, LLMs have transformed the landscape of language technology.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;We&amp;#39;re releasing a preview of OpenAI o1—a new series of AI models designed to spend more time thinking before they respond.&lt;br&gt;&lt;br&gt;These models can reason through complex tasks and solve harder problems than previous models in science, coding, and math. &lt;a href=&#34;https://t.co/peKzzKX1bu&#34;&gt;https://t.co/peKzzKX1bu&lt;/a&gt;&lt;/p&gt;&amp;mdash; OpenAI (@OpenAI) &lt;a href=&#34;https://twitter.com/OpenAI/status/1834278217626317026?ref_src=twsrc%5Etfw&#34;&gt;September 12, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;At their essence, LLMs are built on expansive neural networks with billions of parameters. These networks are trained on vast corpora of text data, learning to discern intricate patterns and relationships within language. Through a process known as pre-training, LLMs develop a broad understanding of linguistic structures, context, and semantics. During this phase, they utilize unsupervised learning techniques to predict masked words or sequences, refining their ability to understand and generate coherent text.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Curious about fine-tuning a satellite-specific LLM model? 🌍&lt;br&gt;.&lt;br&gt;Dive into my latest blog to learn more: &lt;a href=&#34;https://t.co/sd25ByzQpJ&#34;&gt;https://t.co/sd25ByzQpJ&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/LLM?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#LLM&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Geoscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Geoscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/SatelliteLLM?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#SatelliteLLM&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/MachineLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#MachineLearning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Landsat?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Landsat&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/geography?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#geography&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1833511803739116039?ref_src=twsrc%5Etfw&#34;&gt;September 10, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Following pre-training, LLMs undergo fine-tuning to adapt their general language capabilities to specific tasks or domains. This supervised learning phase involves training the model on a targeted dataset, allowing it to excel in applications such as text generation, translation, sentiment analysis, and question-answering. Techniques like transfer learning and few-shot learning further enhance the model&amp;rsquo;s adaptability, enabling it to generalize from limited examples and perform across various contexts.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Deploying LLMs in real-world scenarios involves addressing practical challenges related to computational resources and scalability. These models require substantial processing power and memory, often necessitating the use of advanced hardware like GPUs or TPUs. Despite these demands, the benefits of integrating LLMs into applications—such as chatbots, virtual assistants, content generation, and automated summarization—are profound, offering significant advancements in how machines interact with human language.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this blog post, I will delve into the technical intricacies of LLMs, exploring their architecture, training methodologies, and deployment considerations. Prepare to discover how these powerful AI tools are pushing the boundaries of language technology and shaping the future of machine intelligence.&lt;/p&gt;
&lt;h2 id=&#34;key-vocabulary&#34;&gt;Key Vocabulary&lt;/h2&gt;
&lt;p&gt;Here are some essential terms and acronyms related to LLMs:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Acronym&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Meaning&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Artificial Intelligence: The simulation of human intelligence in machines that are programmed to think and learn.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ANN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Artificial Neural Network: A computational model inspired by biological neural networks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;BERT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Bidirectional Encoder Representations from Transformers: A model for natural language understanding tasks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CNN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Convolutional Neural Network: Effective for processing grid-like data such as images.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CRF&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Conditional Random Field: A statistical modeling method for structured prediction.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;DNN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Deep Neural Network: A neural network with multiple layers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;DL&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Deep Learning: A subset of machine learning with neural networks containing many layers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;GPT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Generative Pre-trained Transformer: A transformer-based model for generating human-like text.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;HMM&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Hidden Markov Model: A model for systems that transition between states with certain probabilities.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;LSTM&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Long Short-Term Memory: A type of RNN designed to remember long-term dependencies.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;LLM&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Large Language Model: Trained on vast amounts of text data to understand and generate text.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ML&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Machine Learning: Training algorithms to make predictions based on data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;NLP&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Natural Language Processing: The interaction between computers and human language.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;RAG&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Retrieval-Augmented Generation: Combines document retrieval with generative models.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;RNN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Recurrent Neural Network: Designed for sequential data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;T5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Text-to-Text Transfer Transformer: Converts various tasks into a text-to-text format.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Transformer&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;A model architecture that uses self-attention mechanisms.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ViT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Vision Transformer: A transformer model for image processing.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;VQA&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Visual Question Answering: Combining vision and language understanding.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;VLMs&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Vision-Language Models: Close the divide between visual and language comprehension in AI.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;XLNet&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;An extension of BERT with permutation-based training.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Platform for NLP with pre-trained models, datasets, and tools.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library for transformer-based models by Hugging Face.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;datasets&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library for managing datasets, by Hugging Face.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Gradio&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library for creating machine learning demos with simple UIs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;LangChain&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Facilitates development using LLMs with tools for managing language-based tasks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;spaCy&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Advanced NLP library in Python.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;NLTK&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Natural Language Toolkit: Tools for text processing and linguistic analysis.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;StanfordNLP&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library by Stanford University for NLP tasks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;OpenCV&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library for computer vision tasks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Deep learning framework with tensor computations and automatic differentiation.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Framework for building and deploying machine learning models.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Keras&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;High-level neural networks API running on top of TensorFlow.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Fastai&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Simplifies neural network training with PyTorch.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ONNX&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Open Neural Network Exchange format for model transfer between frameworks.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;architecture-of-llms&#34;&gt;Architecture of LLMs&lt;/h2&gt;
&lt;p&gt;LLMs are built on advanced architectures that often include transformer models. A transformer model utilizes self-attention mechanisms to process input sequences. The core components of a transformer are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: Processes the input data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: Generates the output sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transformer-architecture-formula&#34;&gt;Transformer Architecture Formula&lt;/h3&gt;
&lt;p&gt;The key mathematical operation in transformers is the self-attention mechanism, which can be described as follows:&lt;/p&gt;
&lt;p&gt;$[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V ]$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( Q )$ is the query matrix,&lt;/li&gt;
&lt;li&gt;$( K )$ is the key matrix,&lt;/li&gt;
&lt;li&gt;$( V )$ is the value matrix,&lt;/li&gt;
&lt;li&gt;$( d_k )$ is the dimensionality of the keys.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training-llms&#34;&gt;Training LLMs&lt;/h2&gt;
&lt;p&gt;Training LLMs involves several steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Data Preparation&lt;/strong&gt;: Collect and preprocess large text corpora.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Initialization&lt;/strong&gt;: Start with a pre-trained model or initialize from scratch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Use gradient descent and backpropagation to minimize the loss function.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;introduction-to-llms-for-satellite-images&#34;&gt;Introduction to LLMs for Satellite Images&lt;/h2&gt;
&lt;p&gt;Fine-tuning a Large Language Model (LLM) like SatGPT for satellite imagery involves several critical stages. This process transforms a pre-trained model into a specialized tool capable of analyzing and generating insights from satellite images. This blog post provides a step-by-step guide to fine-tuning and deploying SatGPT, covering each phase in detail.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In their 2024 paper, &lt;em&gt;“Good at Captioning, Bad at Counting: Benchmarking GPT-4V on Earth Observation Data”&lt;/em&gt; (&lt;a href=&#34;https://arxiv.org/abs/2401.17600&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2401.17600&lt;/a&gt;), Zhang and Wang focus on developing a benchmark for Vision-Language Models (VLMs) applied to Earth Observation (EO) data. Their initial framework addresses three main areas: scene understanding, localization and counting, and change detection. To assess VLM performance across these areas, they design evaluations that span various applications, from wildlife conservation to urban monitoring, as illustrated in Figure 1. Their goals are to evaluate existing VLMs, provide insights into effective prompting techniques for EO tasks, and establish a flexible system for ongoing benchmark updates and future VLM evaluations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- 
For scene understanding, Zhang and Wang assess how VLMs integrate high-level image information with latent knowledge from language modeling. They use several datasets for this purpose: a new dataset for aerial landmark recognition to test the model’s ability to identify and geolocate U.S. landmarks, the RSICD dataset to evaluate the model’s capability to generate captions for Google Earth images, the BigEarthNet dataset to probe land cover identification in medium-resolution satellite images, and the fMoW-WILDS and PatternNet datasets to assess land use classification in high-resolution satellite images.
 --&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The paper highlights several challenges and areas for future work. One major challenge is detecting data contamination, which is crucial for maintaining the fairness and effectiveness of benchmarks as VLMs evolve. Additionally, a more detailed analysis of model failures—such as knowledge gaps, reasoning errors, perceptual mistakes, and text misunderstandings—could provide deeper insights into current VLM capabilities. Zhang and Wang also note the static nature of benchmarks as a limitation, suggesting that dynamic updates may be necessary to keep benchmarks relevant and challenging as VLMs advance.
  &lt;/div&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/SatGPT?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#SatGPT&lt;/a&gt; is an app that lets you talk to satellite imagery.&lt;br&gt;&lt;br&gt;We&amp;#39;ve got some more work to do before it&amp;#39;s polished, but I&amp;#39;m pretty psyched about the results so far. &lt;br&gt;&lt;br&gt;Powered by &lt;a href=&#34;https://twitter.com/Element84?ref_src=twsrc%5Etfw&#34;&gt;@Element84&lt;/a&gt; &lt;a href=&#34;https://twitter.com/STACspec?ref_src=twsrc%5Etfw&#34;&gt;@STACspec&lt;/a&gt; &lt;a href=&#34;https://twitter.com/LangChainAI?ref_src=twsrc%5Etfw&#34;&gt;@LangChainAI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/Panel_org?ref_src=twsrc%5Etfw&#34;&gt;@Panel_org&lt;/a&gt; &lt;a href=&#34;https://twitter.com/HoloViz_org?ref_src=twsrc%5Etfw&#34;&gt;@HoloViz_org&lt;/a&gt;, huge thanks to &lt;a href=&#34;https://twitter.com/ivanziogeo?ref_src=twsrc%5Etfw&#34;&gt;@ivanziogeo&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/MarcSkovMadsen?ref_src=twsrc%5Etfw&#34;&gt;@MarcSkovMadsen&lt;/a&gt;. &lt;a href=&#34;https://t.co/gO7aZz6w4C&#34;&gt;pic.twitter.com/gO7aZz6w4C&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kevin Lalli (@opticsinspace) &lt;a href=&#34;https://twitter.com/opticsinspace/status/1670688747552411649?ref_src=twsrc%5Etfw&#34;&gt;June 19, 2023&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;!-- 
In the domain of localization and counting, Zhang and Wang evaluate whether VLMs can extract detailed information about specific objects and understand their spatial relationships. They create datasets for this purpose, including the DIOR-RSVG dataset to test Referring Expression Comprehension (REC) abilities, where the model localizes objects based on natural language descriptions. Additionally, they use the NEON-Tree, COWC, and xBD datasets to evaluate the counting of small objects like trees, cars, and buildings in aerial and satellite images, and the aerial animal detection dataset to assess the model’s ability to count animals in tilted aerial images.

For change detection, the focus is on evaluating how VLMs track changes over time. Zhang and Wang use a dataset that categorizes buildings by damage levels and presents the data in JSON format, tracking counts before and after damage across various categories.



In the context of image captioning, Zhang and Wang evaluate the ability of instruction-following VLMs to describe aerial or satellite images. Their evaluation uses the RSICD dataset to compare VLM-generated captions with human-annotated examples both qualitatively and quantitatively, assessing how well VLMs describe images at various levels of detail.

For land use and land cover (LULC) classification, Zhang and Wang assess VLMs&#39; performance on multiple-choice classification tasks using datasets like fMoW-WILDS, PatternNet, and BigEarthNet. Their aim is to determine which models excel in zero-shot classification and how image resolution impacts classification accuracy. They find that VLM performance varies based on image resolution, label ambiguity, and granularity. Specifically, GPT-4V shows lower performance in land cover classification compared to specialized models but performs better on certain datasets like fMoW-WILDS and PatternNet. The challenges of ambiguous class labels and limited multi-spectral information in the BigEarthNet dataset also affect GPT-4V&#39;s performance.

Overall, Zhang and Wang’s work underscores the importance of evolving benchmarks and VLM capabilities to address the challenges in EO data applications.

They deliver an in-depth analysis of GPT-4V’s performance across different tasks. Figure 1 illustrates key scenarios and the model’s performance: --&gt;
&lt;!-- 
1. **Location Recognition**  
   **Scenario:** Identify the landmark based on its features, such as its dome and layout.  
   **Example Answer:** The landmark, recognized by its distinctive style and layout, is the Nebraska State Capitol.

2. **Image Captioning**  
   **Scenario:** Generate a one-sentence caption for the provided image.  
   **Example Caption:** An aerial view of an airport terminal, showcasing nearby aircraft, taxiways, and parking areas.

3. **Land Use &amp; Land Cover Classification**  
   **Scenario:** Categorize the image into one of several predefined categories.  
   **Example Classification:** The image is best described as a Shipping Yard.

4. **Object Localization**  
   **Scenario:** Pinpoint the coordinates of a described object in the image.  
   **Example Description:** The gray windmill in the center.  
   **Coordinates:** [233, 383, 376, 542]

5. **Object Counting**  
   **Scenario:** Estimate the number of trees visible in the image.  
   **Count:** 134

6. **Change Detection**  
   **Scenario:** Count buildings in various damage categories and present the data in JSON format.  
   **JSON Format:**  
   ```json
   {
     &#34;count_before&#34;: 75,
     &#34;no_damage&#34;: 2,
     &#34;minor_damage&#34;: 73,
     &#34;major_damage&#34;: 0,
     &#34;destroyed&#34;: 1
   }
   ```

**Performance Metrics:**

- **RefCLIP Score:** Evaluates the model’s performance on reference-based tasks.
- **F1 Score:** Measures the model’s accuracy in classification tasks.
- **Mean IoU:** Assesses the model’s performance in object localization.
- **R2 Score:** Gauges the model’s predictive accuracy across various tasks. --&gt;
&lt;p&gt;These findings offer valuable insights into GPT-4V’s capabilities and limitations, especially in the context of earth observation data.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;sample_apps.png&#34; alt=&#34;Earth observation data&#34;&gt; &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. Here are examples of inputs and outputs from various benchmark tasks and how five different VLMs stack up. They’ve included just a snippet of the user prompts and model responses to highlight the key points. &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;exploring-vision-language-models-vlms-to-understand-high-level-features-in-remotely-sensed-images&#34;&gt;Exploring Vision-Language Models (VLMs) to Understand High-Level Features in Remotely Sensed Images&lt;/h3&gt;
&lt;p&gt;In my recent work, I&amp;rsquo;ve been diving deep into Vision-Language Models (VLMs) to see how well they perform in tasks that require understanding both visual and textual data. With the explosion of AI models that can interpret images and generate coherent, detailed text, it’s become increasingly important to assess these models not just on general benchmarks, but in specific, high-stakes domains like remotely sensed imagery.&lt;/p&gt;
&lt;p&gt;Remotely sensed images, which are collected from satellite or aerial platforms, provide a unique challenge for VLMs. They are dense with data, full of patterns, and often contain complex interactions between natural and man-made objects. The ability of a model to not only caption these images but also understand high-level features—such as differentiating between natural landmarks, infrastructure, and potential environmental changes—can have far-reaching applications in fields like agriculture, urban planning, and disaster response.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure2.png&#34; alt=&#34;VLM Comparison for Benchmark Tasks&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. A comparison of inputs and outputs from benchmark tasks using different VLMs. The snippet includes user prompts and model responses, highlighting key areas of model performance.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h3 id=&#34;what-makes-vision-language-models-vlms-special&#34;&gt;What Makes Vision-Language Models (VLMs) Special?&lt;/h3&gt;
&lt;p&gt;VLMs operate at the intersection of vision and language, giving them the ability to describe images with textual explanations. This makes them incredibly useful for analyzing and interpreting remote sensing data. In these images, VLMs can recognize patterns, identify important landmarks, and even offer insights into the features present within the scene.&lt;/p&gt;
&lt;p&gt;However, while these models excel at captioning tasks—offering detailed and sometimes creative descriptions—they can struggle with more precise tasks like counting objects or recognizing certain functional categories. This is a critical gap that must be addressed, especially in applications where accuracy is paramount.&lt;/p&gt;
&lt;h3 id=&#34;challenges-in-remote-sensing-with-vlms&#34;&gt;Challenges in Remote Sensing with VLMs&lt;/h3&gt;
&lt;p&gt;One of the major challenges I’ve observed while working with VLMs on remotely sensed images is the models&amp;rsquo; difficulty in consistently recognizing high-level features, especially when dealing with complex or less common landmarks. This can lead to a high rate of refusal or incorrect identification in certain categories.&lt;/p&gt;
&lt;p&gt;For instance, a model might easily recognize a natural park or large urban feature, but struggle to identify a specific sports venue or government building. These variances are especially pronounced when analyzing remote imagery, where the perspective and scale can make recognition even more difficult.&lt;/p&gt;
&lt;h3 id=&#34;benchmarking-vlms-on-landmark-recognition&#34;&gt;Benchmarking VLMs on Landmark Recognition&lt;/h3&gt;
&lt;p&gt;I ran some experiments using five different VLMs (GPT-4V, InstructBLIP-TS-XXL, InstructBLIP-Vicuna-13b, LLaVA-v1.5, Qwen-VL-Chat) to see how well they could identify landmarks in a set of remotely sensed images. Below is the summary of the results for landmark recognition accuracy (Table 1) and refusal rate (Table 2).&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;Table1and2.png&#34; alt=&#34;Table 1 and 2: Landmark Recognition Accuracy and Refusal Rate&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Table 1: Landmark recognition accuracy by functional category and Table 2: Landmark recognition refusal rate.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As you can see, there are significant variances in how different models perform across these categories. GPT-4V and InstructBLIP tend to outperform other models in recognizing large, prominent landmarks like natural parks and urban infrastructure. However, there’s still considerable room for improvement, especially when identifying more specific or niche features, like places of worship or government buildings.&lt;/p&gt;
&lt;h3 id=&#34;diving-deeper-into-vlms-case-studies-of-landmark-recognition-and-scene-interpretation&#34;&gt;Diving Deeper into VLMs: Case Studies of Landmark Recognition and Scene Interpretation&lt;/h3&gt;
&lt;p&gt;The nuances of how Vision-Language Models (VLMs) understand and interpret images can be observed more clearly in specific examples. Below, I’ve analyzed a few key scenarios where GPT-4V has demonstrated both its strengths and limitations.&lt;/p&gt;
&lt;h4 id=&#34;visual-recognition-with-architectural-context&#34;&gt;Visual Recognition with Architectural Context&lt;/h4&gt;
&lt;p&gt;One fascinating case is GPT-4V’s ability to link visual cues with its knowledge of architecture. In &lt;strong&gt;Figure 3&lt;/strong&gt;, the model successfully identifies a landmark by connecting the architectural style with its vast knowledge base, arriving at the correct answer. This demonstrates its ability to use contextual clues beyond just object recognition.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure3.png&#34; alt=&#34;Architectural Landmark Identification&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. GPT-4V successfully corresponds visual cues with its knowledge about the architectural style of the landmark to arrive at the correct answer.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;the-problem-of-visual-misinterpretation&#34;&gt;The Problem of Visual Misinterpretation&lt;/h4&gt;
&lt;p&gt;However, VLMs aren&amp;rsquo;t infallible. One case where GPT-4V struggled is in the identification of the &lt;strong&gt;Nebraska State Capitol&lt;/strong&gt;. In &lt;strong&gt;Figure 4&lt;/strong&gt;, the model incorrectly eliminates the correct answer due to misidentifying the tower-like structure. This reveals a significant gap in its ability to distinguish more subtle architectural details, leading to incorrect conclusions.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure4.png&#34; alt=&#34;Misidentification of Nebraska State Capitol&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 4. GPT-4V fails to identify the tower-like structure of the Nebraska State Capitol, leading to incorrect elimination.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;correct-identification-but-weak-justifications&#34;&gt;Correct Identification but Weak Justifications&lt;/h4&gt;
&lt;p&gt;Interestingly, even when GPT-4V identifies a landmark correctly, it sometimes provides insufficient reasoning. In &lt;strong&gt;Figure 5&lt;/strong&gt;, the model identifies the landmark, but the reasoning lacks depth, which could be a hindrance in scenarios requiring detailed explanations, such as educational or research-oriented applications.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure5.png&#34; alt=&#34;Correct Identification but Weak Reasoning&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 5. GPT-4V correctly identifies the landmark but gives insufficient reasoning.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;generating-image-captions-for-complex-scenes&#34;&gt;Generating Image Captions for Complex Scenes&lt;/h4&gt;
&lt;p&gt;Another interesting scenario is when the model is tasked with generating captions for complex images. In &lt;strong&gt;Figure 6&lt;/strong&gt;, GPT-4V generates several captions for an airport image. While the captions are coherent, they sometimes miss finer details, like the specific types of airplanes or terminal features, which could be crucial in more technical applications like surveillance or logistics planning.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure6.png&#34; alt=&#34;Airport Caption Generation&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 6. Example captions generated for an airport image.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;object-localization-in-remote-sensing&#34;&gt;Object Localization in Remote Sensing&lt;/h4&gt;
&lt;p&gt;Object localization is another key area where VLMs need to perform exceptionally well. In &lt;strong&gt;Figure 7&lt;/strong&gt;, GPT-4V is tasked with localizing objects in a DIOR-RSVG dataset image. While it performs reasonably well, there are still challenges in precisely identifying and categorizing certain objects, especially in cluttered or low-contrast scenes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure7.png&#34; alt=&#34;Object Localization in DIOR-RSVG Dataset&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 7. Example prompt and response for DIOR-RSVG object localization.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;detecting-changes-in-xview2-imagery&#34;&gt;Detecting Changes in xView2 Imagery&lt;/h4&gt;
&lt;p&gt;Finally, in &lt;strong&gt;Figure 8&lt;/strong&gt;, the model is put to the test with change detection using the xView2 dataset, where it must identify changes in infrastructure and the environment. This kind of task is essential in applications like disaster response or urban monitoring, where rapid and accurate assessments can make a significant difference. GPT-4V’s performance is promising, but it still leaves room for improvement, especially in recognizing more subtle changes or those happening over time.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure8.png&#34; alt=&#34;Change Detection in xView2 Dataset&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 8. Example prompt and response for xView2 change detection.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;overview-of-the-fine-tuning-process&#34;&gt;Overview of the Fine-Tuning Process&lt;/h2&gt;
&lt;p&gt;The process of fine-tuning and deploying a satellite-specific LLM model involves the following stages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Data Preparation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Selection&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Tuning Paradigm&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Validation and Evaluation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Export and Deployment to Hugging Face&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;step-by-step-fine-tuning-of-satgpt-for-satellite-imagery&#34;&gt;Step-by-Step Fine-Tuning of SatGPT for Satellite Imagery&lt;/h2&gt;
&lt;h3 id=&#34;1-data-preparation&#34;&gt;1. Data Preparation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Collect, preprocess, and format satellite images and associated textual annotations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collect Satellite Images&lt;/strong&gt;: Obtain satellite images from sources such as commercial providers or public datasets (e.g., Sentinel, Landsat).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Annotate Images&lt;/strong&gt;: Label images with relevant information (e.g., land cover types, objects of interest).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preprocess Images&lt;/strong&gt;: Resize and normalize images to match the input requirements of the Vision Transformer (ViT) model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prepare Textual Descriptions&lt;/strong&gt;: Generate textual descriptions or annotations for each image, which will be used for training the text generation component.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import ViTFeatureExtractor, GPT2Tokenizer

# Initialize feature extractor and tokenizer
feature_extractor = ViTFeatureExtractor.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)
tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)

# Sample image and text
image = ... # Load satellite image
text = &amp;quot;This is a description of the satellite image.&amp;quot;

# Prepare inputs
inputs = feature_extractor(images=image, return_tensors=&amp;quot;pt&amp;quot;)
labels = tokenizer(text, return_tensors=&amp;quot;pt&amp;quot;).input_ids
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-model-selection&#34;&gt;2. Model Selection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Choose an appropriate pre-trained model as the foundation for SatGPT.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt;: For processing and extracting features from satellite images.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-2 or GPT-3&lt;/strong&gt;: For generating textual descriptions or insights based on image features.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import GPT2LMHeadModel, ViTModel

# Load pre-trained models
image_model = ViTModel.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)
text_model = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3-fine-tuning-paradigm&#34;&gt;3. Fine-Tuning Paradigm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Adapt the selected models to work together for the specific task of analyzing satellite imagery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Combine Models&lt;/strong&gt;: Integrate ViT for image feature extraction and GPT for text generation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Define Loss Functions&lt;/strong&gt;: Use suitable loss functions for image and text components.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training Loop&lt;/strong&gt;: Implement a training loop to update model parameters based on the image-text pairs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import Trainer, TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir=&#39;./results&#39;,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    logging_dir=&#39;./logs&#39;,
)

# Initialize Trainer
trainer = Trainer(
    model=image_model,  # This would be a combined model in practice
    args=training_args,
    train_dataset=train_dataset,  # Prepare your dataset
)

# Train the model
trainer.train()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;4-model-validation-and-evaluation&#34;&gt;4. Model Validation and Evaluation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Assess the performance of the fine-tuned model to ensure it meets the desired criteria.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Validation Set&lt;/strong&gt;: Use a separate dataset to validate the model’s performance during training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Evaluation Metrics&lt;/strong&gt;: Measure performance using metrics such as accuracy, F1 score, or BLEU score (for text generation).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Evaluate the model
eval_results = trainer.evaluate()
print(eval_results)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;5-export-and-deployment-to-hugging-face&#34;&gt;5. Export and Deployment to Hugging Face&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Make the fine-tuned model available for inference and integration through Hugging Face.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Export the Model&lt;/strong&gt;: Save the fine-tuned model and tokenizer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Upload to Hugging Face&lt;/strong&gt;: Use the &lt;code&gt;transformers&lt;/code&gt; library to push the model to the Hugging Face Hub.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create an Inference Endpoint&lt;/strong&gt;: Deploy the model and set up an API endpoint for user interactions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline

# Load model from Hugging Face Hub
nlp = pipeline(&amp;quot;text-generation&amp;quot;, model=&amp;quot;username/satgpt-model&amp;quot;)

# Use the model
result = nlp(&amp;quot;Describe the land cover of this GISTDA satellite image.&amp;quot;)
print(result)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;additional-concepts&#34;&gt;Additional Concepts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Retrieval-Augmented Generation (RAG)&lt;/strong&gt;: Combines document retrieval with generative models to improve response accuracy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision Transformers (ViT)&lt;/strong&gt;: Adapt transformers for image processing by treating images as sequences of patches.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;formula-for-self-attention-in-rag&#34;&gt;Formula for Self-Attention in RAG&lt;/h3&gt;
&lt;p&gt;In RAG, the attention mechanism can be described as:&lt;/p&gt;
&lt;p&gt;$[ \text{RAG}(Q, K, V, D) = \text{Attention}(Q, K, V) + \text{Retrieval}(D) ]$&lt;/p&gt;
&lt;p&gt;where $( D )$ represents retrieved documents.&lt;/p&gt;
&lt;h3 id=&#34;vision-transformer-vit&#34;&gt;Vision Transformer (ViT)&lt;/h3&gt;
&lt;p&gt;The Vision Transformer treats images as sequences of patches and processes them with transformer architectures. The key operation in ViT involves:&lt;/p&gt;
&lt;p&gt;$[ \text{Patch Embedding}(I) = \text{Linear}(I) + \text{Positional Encoding} ]$&lt;/p&gt;
&lt;p&gt;where $( I )$ is the image and the output is a sequence of patch embeddings.&lt;/p&gt;
&lt;h2 id=&#34;full-flow-diagram&#34;&gt;Full Flow Diagram&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s a conceptual flow of how data is processed through SatGPT, from input to output:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Satellite Image + Textual Description&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Image Processing&lt;/strong&gt;: ViT processes image into feature vectors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Text Generation&lt;/strong&gt;: GPT-2 generates textual descriptions from image features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Generated Text&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;quick-thoughts-on-llms-before-we-wrap-up-this-blog&#34;&gt;Quick thoughts on LLMs before we wrap up this blog:&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-introduction-to-large-language-models-llms-in-remote-sensing&#34;&gt;1. &lt;strong&gt;Introduction to Large Language Models (LLMs) in Remote Sensing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Large Language Models (LLMs) are advanced models designed to understand and generate human-like text. They can be adapted for analyzing satellite imagery by combining multimodal inputs, like images and textual descriptions.&lt;/p&gt;
&lt;h4 id=&#34;key-equations&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;The underlying architecture for LLMs is based on the Transformer model, which is governed by:
$[
\mathbf{Z} = \text{softmax}\left(\frac{\mathbf{QK}^\top}{\sqrt{d_k}}\right)\mathbf{V}
]$
where $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ are query, key, and value matrices respectively.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-foundation-models-and-their-role-in-llms&#34;&gt;2. &lt;strong&gt;Foundation Models and Their Role in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Foundation models are pre-trained on extensive datasets and serve as the base for fine-tuning on specific tasks, such as satellite image analysis.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-1&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;The objective during pre-training is to minimize:
$[
MLM = - \sum_{i=1}^{N} \log P(x_i | x_{-i}; \theta)
]$
where ${MLM}$ is the masked language modeling loss.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-training-vs-fine-tuning-vs-pre-trained-models-in-llms&#34;&gt;3. &lt;strong&gt;Training vs Fine-tuning vs Pre-trained Models in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pre-trained Models&lt;/strong&gt;: Trained on large-scale datasets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt;: Adapting a pre-trained model to a specific task or dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Training a model from scratch using a domain-specific dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equations-2&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Cross-entropy loss function used during fine-tuning:
$[
\mathcal{L} = - \sum_{i=1}^{N} y_i \log(\hat{y}_i)
]$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-how-to-train-llms-on-satellite-images&#34;&gt;4. &lt;strong&gt;How to Train LLMs on Satellite Images&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Training LLMs on satellite images involves using multimodal inputs and embeddings to represent both images and textual descriptions.&lt;/p&gt;
&lt;!-- #### Key Equations
The multimodal training objective is:
$\[
\mathcal{L}_{\text{multimodal}} = \lambda \cdot \mathcal{L}_{\text{img}} + (1-\lambda) \cdot \mathcal{L}_{\text{text}}
\]$ --&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-retrieval-augmented-generation-rag-for-satellite-image-analysis&#34;&gt;5. &lt;strong&gt;Retrieval-Augmented Generation (RAG) for Satellite Image Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;RAG combines document retrieval with generation capabilities to enhance satellite image analysis by incorporating additional contextual information.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-3&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;RAG combines retrieval and generation via:
$[
P(x|c) = \sum_{i} P(x | c_i, q)P(c_i | q)
]$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;6-using-langchain-for-satellite-image-llm-applications&#34;&gt;6. &lt;strong&gt;Using LangChain for Satellite Image LLM Applications&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;LangChain facilitates chaining LLMs together for various tasks, such as preprocessing, analysis, and post-processing of satellite images.&lt;/p&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;Using LangChain to preprocess satellite metadata:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain import SimplePromptTemplate
template = SimplePromptTemplate(prompt=&amp;quot;Summarize satellite data: {data}&amp;quot;)
summary = template.run(data=satellite_metadata)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;7-sample-datasets-for-llm-fine-tuning-in-remote-sensing&#34;&gt;7. &lt;strong&gt;Sample Datasets for LLM Fine-Tuning in Remote Sensing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Datasets such as UC Merced Land Use, EuroSAT, and BigEarthNet are used for fine-tuning LLMs to handle specific satellite image tasks.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;8-mathematical-foundations-of-attention-mechanisms-in-llms&#34;&gt;8. &lt;strong&gt;Mathematical Foundations of Attention Mechanisms in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The attention mechanism in LLMs is crucial for focusing on specific parts of the input data, such as regions in a satellite image.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-4&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Self-attention mechanism:
$[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
]$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;9-multimodal-llm-architectures-for-satellite-images&#34;&gt;9. &lt;strong&gt;Multimodal LLM Architectures for Satellite Images&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Multimodal LLMs integrate both text and image data, allowing for comprehensive analysis of satellite imagery.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-5&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;For multimodal learning, image and text representations are combined:
$[
\mathbf{Z} = \text{Concat}(Z_{\text{img}}, Z_{\text{text}})
]$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;10-preprocessing-techniques-for-satellite-images-in-llms&#34;&gt;10. &lt;strong&gt;Preprocessing Techniques for Satellite Images in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Preprocessing techniques like normalization and histogram equalization are essential for preparing satellite images for analysis.&lt;/p&gt;
&lt;h4 id=&#34;key-formulas&#34;&gt;Key Formulas&lt;/h4&gt;
&lt;p&gt;Image normalization:
$[
X&amp;rsquo; = \frac{X - \mu}{\sigma}
]$
where $X$ is the pixel value, $\mu$ is the mean, and $\sigma$ is the standard deviation.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;11-handling-illumination-and-atmospheric-effects-in-llms&#34;&gt;11. &lt;strong&gt;Handling Illumination and Atmospheric Effects in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Illumination and atmospheric distortions can affect satellite images, and models must be trained to handle these variations.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-6&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Illumination adjustment formula:
$[
I&amp;rsquo; = \frac{I}{\cos(\theta) + \epsilon}
]$
where $\theta$ is the solar zenith angle.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;12-self-supervised-learning-ssl-for-satellite-image-analysis&#34;&gt;12. &lt;strong&gt;Self-Supervised Learning (SSL) for Satellite Image Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;SSL techniques allow models to learn from unlabelled satellite data by setting up proxy tasks such as predicting missing data.&lt;/p&gt;
&lt;!-- 
#### Key Equations
Contrastive loss function in SSL:
$\[
\mathcal{L}_{\text{contrastive}} = - \log \frac{\exp(\mathbf{z}_i^\top \mathbf{z}_j / \tau)}{\sum_{k} \exp(\mathbf{z}_i^\top \mathbf{z}_k / \tau)}
\]$ --&gt;
&lt;hr&gt;
&lt;h3 id=&#34;13-open-source-tools-for-llms-in-satellite-image-analysis&#34;&gt;13. &lt;strong&gt;Open-Source Tools for LLMs in Satellite Image Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Useful tools include Hugging Face Transformers for fine-tuning, LangChain for chaining models, and FastAI for data augmentation.&lt;/p&gt;
&lt;h4 id=&#34;example-code&#34;&gt;Example Code&lt;/h4&gt;
&lt;p&gt;Using Hugging Face Transformers:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained(&amp;quot;bert-base-uncased&amp;quot;)
model = BertModel.from_pretrained(&amp;quot;bert-base-uncased&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;14-fine-tuning-llms-for-specific-satellite-image-tasks&#34;&gt;14. &lt;strong&gt;Fine-Tuning LLMs for Specific Satellite Image Tasks&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Fine-tuning involves adjusting a pre-trained model using satellite data to improve performance on specific tasks.&lt;/p&gt;
&lt;h4 id=&#34;key-steps&#34;&gt;Key Steps&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Load a pre-trained model.&lt;/li&gt;
&lt;li&gt;Freeze initial layers and fine-tune top layers.&lt;/li&gt;
&lt;li&gt;Train with domain-specific data.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;15-evaluation-metrics-for-llms-in-remote-sensing&#34;&gt;15. &lt;strong&gt;Evaluation Metrics for LLMs in Remote Sensing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Evaluating the performance of Large Language Models (LLMs) in remote sensing involves several metrics, including precision, recall, F1 score, mean Average Precision (mAP), and BLEU score. These metrics help assess the quality of predictions and the relevance of generated content.&lt;/p&gt;
&lt;h4 id=&#34;key-metrics&#34;&gt;Key Metrics&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Precision and Recall&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt; measures the proportion of true positive results among all positive results predicted by the model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt; measures the proportion of true positive results among all actual positive results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equations-7&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Precision:
$[
\text{Precision} = \frac{TP}{TP + FP}
]$
Recall:
$[
\text{Recall} = \frac{TP}{TP + FN}
]$
where $TP$ is true positives, $FP$ is false positives, and $FN$ is false negatives.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;F1 Score&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;F1 Score&lt;/strong&gt; is the harmonic mean of precision and recall, providing a single metric that balances both.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equation&#34;&gt;Key Equation&lt;/h4&gt;
&lt;p&gt;$[
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
]$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;mean Average Precision (mAP)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mAP&lt;/strong&gt; evaluates the precision of object detection models, averaging the precision across different recall levels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equation-1&#34;&gt;Key Equation&lt;/h4&gt;
&lt;p&gt;Average Precision (AP) for a single class:
$[
\text{AP} = \int_{0}^{1} \text{Precision}(r) , \text{Recall}(r)
]$
where $\text{Precision}(r)$ is the precision at recall level $r$.&lt;/p&gt;
&lt;p&gt;mAP is the mean of AP across all classes:
$[
\text{mAP} = \frac{1}{C} \sum_{i=1}^{C} \text{AP}_i
]$
where $C$ is the number of classes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BLEU Score&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BLEU Score&lt;/strong&gt; evaluates the quality of generated text by comparing it to reference texts, commonly used for tasks like image captioning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equation-2&#34;&gt;Key Equation&lt;/h4&gt;
&lt;p&gt;BLEU score is calculated using n-gram precision:
$[
\text{BLEU} = \text{exp}\left(\sum_{n=1}^{N} w_n \cdot \log P_n\right)
]$
where $P_n$ is the precision of n-grams, and $w_n$ is the weight for n-grams of length $n$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;example-code-1&#34;&gt;Example Code&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import average_precision_score, precision_recall_curve
from nltk.translate.bleu_score import sentence_bleu

# Example for precision, recall, F1 score
y_true = [0, 1, 1, 0, 1, 1, 0]
y_pred = [0, 1, 0, 0, 1, 1, 1]
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

# Example for BLEU score
reference = [[&#39;GISTDA&#39;, &#39;is&#39;, &#39;the&#39;, &#39;premier&#39;, &#39;place&#39;, &#39;to&#39;, &#39;work&#39;, &#39;in&#39;, &#39;the&#39;, &#39;geo&#39;, &#39;sector&#39;, &#39;in&#39;, &#39;thailand&#39;]]
candidate = [&#39;GISTDA&#39;, &#39;is&#39;, &#39;the&#39;, &#39;best&#39;, &#39;workplace&#39;, &#39;in&#39;, &#39;geo&#39;, &#39;in&#39;, &#39;thailand&#39;]
bleu_score = sentence_bleu(reference, candidate)

print(f&amp;quot;Precision: {precision}&amp;quot;)
print(f&amp;quot;Recall: {recall}&amp;quot;)
print(f&amp;quot;F1 Score: {f1}&amp;quot;)
print(f&amp;quot;BLEU Score: {bleu_score}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;16-transfer-learning-for-satellite-imagery&#34;&gt;16. &lt;strong&gt;Transfer Learning for Satellite Imagery&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Transfer learning uses models pre-trained on general datasets and adapts them for satellite image tasks through domain-specific fine-tuning.&lt;/p&gt;
&lt;!-- 
#### Key Equations
The total loss in transfer learning:
$\[
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{general}}(\theta_g) + \lambda \mathcal{L}_{\text{task}}(\theta_t)
\]$
where $\lambda$ is a regularization factor. --&gt;
&lt;h4 id=&#34;example-code-2&#34;&gt;Example Code&lt;/h4&gt;
&lt;p&gt;Using pre-trained ResNet for satellite image classification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchvision import models
resnet = models.resnet50(pretrained=True)

# Freeze general layers
for param in resnet.parameters():
    param.requires_grad = False

# Fine-tune top layers
resnet.fc = nn.Linear(in_features=2048, out_features=num_classes)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;17-explainability-in-llms-for-remote-sensing-xai&#34;&gt;17. &lt;strong&gt;Explainability in LLMs for Remote Sensing (XAI)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Explainable AI (XAI) methods enhance the transparency of LLM predictions, allowing users to understand how models make decisions based on satellite imagery.&lt;/p&gt;
&lt;h4 id=&#34;key-techniques&#34;&gt;Key Techniques&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Attention Visualization&lt;/strong&gt;: Shows which parts of the input data are focused on by the model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grad-CAM&lt;/strong&gt;: Generates heatmaps highlighting important regions in the satellite images.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SHAP&lt;/strong&gt;: Explains individual predictions by computing feature contributions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;key-equations-8&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Grad-CAM heatmap formula:
$[
\text{Grad-CAM}(A^k) = \text{ReLU}\left( \sum_k \alpha_k A^k \right)
]$
where $\alpha_k$ is the gradient of the loss with respect to the feature map $A^k$.&lt;/p&gt;
&lt;h4 id=&#34;example-code-3&#34;&gt;Example Code&lt;/h4&gt;
&lt;p&gt;Using Grad-CAM for explainability:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import cv2
import numpy as np

# Compute gradients
def grad_cam(model, img):
    gradients = torch.autograd.grad(outputs=model(img), inputs=model.layer4)
    weights = torch.mean(gradients[0], dim=[2, 3], keepdim=True)
    cam = torch.sum(weights * model.layer4(img), dim=1)
    return cam

# Apply Grad-CAM on an image
cam_output = grad_cam(resnet, satellite_image)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, large language models (LLMs) are making impressive strides in the realm of satellite data analysis, showcasing their potential across scene understanding, localization, counting, and change detection. These models are beginning to transform how we interpret complex satellite imagery, offering valuable insights for everything from environmental monitoring to urban development.&lt;/p&gt;
&lt;p&gt;Despite these advancements, challenges remain. Current benchmarks reveal that while LLMs excel in tasks like generating descriptive captions and recognizing landmarks, they sometimes fall short in areas requiring detailed object counting and nuanced change detection. This highlights the need for more refined evaluation methods to fully capture and enhance LLM capabilities.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As both satellite technology and LLMs continue to evolve, the path forward promises exciting developments. By refining benchmarks and exploring new methodologies, we can unlock even greater potential in this technology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I hope you enjoyed this deep dive into the intersection of LLMs and satellite data. If you found this blog insightful, please consider sharing it with others who might be interested. Stay tuned for more updates and innovations in this thrilling field!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;How to Fine-Tune and Deploy a Satellite-Specific LLM Model for Satellite Images&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024finetune,
  title   = &amp;quot;How to Fine-Tune and Deploy a Satellite-Specific LLM Model for Satellite Images&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Kaiser, Ł., Polosukhin, I. (NeurIPS 2017).&lt;/strong&gt; &amp;ldquo;Attention Is All You Need.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 5998-6008. &lt;a href=&#34;https://doi.org/10.5555/3295222.3295349&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3295222.3295349&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Shinn, E., Ramesh, A., Muthukrishnan, P., and others. (NeurIPS 2020).&lt;/strong&gt; &amp;ldquo;Language Models are Few-Shot Learners.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 1877-1901. &lt;a href=&#34;https://doi.org/10.5555/3454337.3454731&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3454337.3454731&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Devlin, J., Chang, M. W., Lee, K., &amp;amp; Toutanova, K. (NAACL 2019).&lt;/strong&gt; &amp;ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.&amp;rdquo; &lt;em&gt;North American Chapter of the Association for Computational Linguistics (NAACL)&lt;/em&gt;, 4171-4186. &lt;a href=&#34;https://doi.org/10.5555/3331189.3331190&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3331189.3331190&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., &amp;amp; others. (ICLR 2021).&lt;/strong&gt; &amp;ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3453424.3453670&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3453424.3453670&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Radford, A., Wu, J., Child, R., Mehri, S., &amp;amp; others. (ICLR 2019).&lt;/strong&gt; &amp;ldquo;Language Models are Unsupervised Multitask Learners.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3326452.3326458&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3326452.3326458&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clark, K., Luong, M. T., Le, Q. V., &amp;amp; Manning, C. D. (ACL 2019).&lt;/strong&gt; &amp;ldquo;ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.&amp;rdquo; &lt;em&gt;Association for Computational Linguistics (ACL)&lt;/em&gt;, 2251-2261. &lt;a href=&#34;https://doi.org/10.5555/3454375.3454420&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3454375.3454420&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Zhang, Y., Zhao, Y., Saleh, M., &amp;amp; Liu, P. J. (ICLR 2021).&lt;/strong&gt; &amp;ldquo;PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3453104.3453140&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3453104.3453140&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Kenton, J., &amp;amp; Toutanova, K. (NAACL 2019).&lt;/strong&gt; &amp;ldquo;BERT: Bidirectional Encoder Representations from Transformers.&amp;rdquo; &lt;em&gt;North American Chapter of the Association for Computational Linguistics (NAACL)&lt;/em&gt;, 4171-4186. &lt;a href=&#34;https://doi.org/10.5555/3331189.3331190&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3331189.3331190&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Yang, Z., Yang, D., Dineen, C., &amp;amp; others. (ICLR 2020).&lt;/strong&gt; &amp;ldquo;XLNet: Generalized Autoregressive Pretraining for Language Understanding.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3456141.3456151&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3456141.3456151&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Raffel, C., Shinn, E., S. J. McDonell, C. Lee, K., &amp;amp; others. (ICLR 2021).&lt;/strong&gt; &amp;ldquo;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3456181.3456210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3456181.3456210&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Zhang, C., &amp;amp; Wang, S. (arXiv 2024).&lt;/strong&gt; &amp;ldquo;Good at Captioning, Bad at Counting: Benchmarking GPT-4V on Earth Observation Data.&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:2401.17600&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2401.17600&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv.org/abs/2401.17600&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk 🛺 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240906_Panboonyuen_AI_ThaiHighway.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#meet-reg-the-game-changer-in-highway-asset-detection&#34;&gt;Meet REG: The Game-Changer in Highway Asset Detection&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#generalized-focal-loss-for-multi-class-detection&#34;&gt;Generalized Focal Loss for Multi-Class Detection&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#refined-generalized-focal-loss-for-segmentation&#34;&gt;Refined Generalized Focal Loss for Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#refinement-term-for-spatial-contextual-learning&#34;&gt;Refinement Term for Spatial-Contextual Learning&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#joint-optimization-for-detection-and-segmentation&#34;&gt;Joint Optimization for Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#incorporating-prediction-uncertainty&#34;&gt;Incorporating Prediction Uncertainty&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#mathematical-foundations-for-optimization-in-reg&#34;&gt;Mathematical Foundations for Optimization in REG&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#performance-analysis-for-detection-and-segmentation&#34;&gt;Performance Analysis for Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways&#34;&gt;Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#understanding-the-scene&#34;&gt;Understanding the Scene&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-challenge-detection-and-segmentation&#34;&gt;The Challenge: Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-process-in-action&#34;&gt;The Process in Action&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#real-world-impact&#34;&gt;Real-World Impact&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#paper-highlights&#34;&gt;Paper Highlights:&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures&#34;&gt;Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#generalized-focal-loss-for-vision-tasks&#34;&gt;Generalized Focal Loss for Vision Tasks&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#final-insights-pioneering-precision-with-reg-in-highway-asset-detection&#34;&gt;Final Insights: Pioneering Precision with REG in Highway Asset Detection&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-introduction-to-generalized-focal-loss&#34;&gt;1. &lt;strong&gt;Introduction to Generalized Focal Loss&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-formula-for-difference-between-detection-and-segmentation-models&#34;&gt;2. &lt;strong&gt;Formula for Difference Between Detection and Segmentation Models&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-optimization-in-object-detection-and-segmentation&#34;&gt;3. &lt;strong&gt;Optimization in Object Detection and Segmentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-mathematical-formulas-to-know&#34;&gt;4. &lt;strong&gt;Mathematical Formulas to Know&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#whats-next&#34;&gt;What’s Next?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are pleased to announce that our paper, titled Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery, has been accepted for oral presentation at the 5th International Conference on Highway Engineering (iCHE 2024). This opportunity marks a significant moment in our academic journey, especially after a hiatus from international conferences since completing my Ph.D. I am eager to re-engage with the academic community and share our recent advancements in person.&lt;/p&gt;
&lt;h2 id=&#34;meet-reg-the-game-changer-in-highway-asset-detection&#34;&gt;Meet REG: The Game-Changer in Highway Asset Detection&lt;/h2&gt;
&lt;p&gt;Hi guys, fellow tech enthusiasts! I&amp;rsquo;m thrilled to unveil a cutting-edge innovation from my latest research—Refined Generalized Focal Loss (REG). This revolutionary approach is transforming road asset detection on Thai highways, and it’s as exciting as it sounds.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, what’s the big deal with REG? Imagine a detection system that not only sees but truly understands the intricate details of highway scenes. REG pushes the boundaries of current vision-based detection models by tackling the most challenging issues: imbalanced datasets, tiny objects, and complex highway backdrops.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Our paper on “Refined Generalized Focal Loss for Road Asset Detection on Thai Highways” has been accepted for oral presentation at iCHE 2024!&lt;br&gt;.&lt;br&gt;Excited to share how we’re tackling road asset detection.&lt;br&gt;.&lt;br&gt;📖👀 Check out the details here: &lt;a href=&#34;https://t.co/KTSGHITU7F&#34;&gt;https://t.co/KTSGHITU7F&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/iCHE2024?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#iCHE2024&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1832227483967746136?ref_src=twsrc%5Etfw&#34;&gt;September 7, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;My work (check out Fig. 1) brings a whole new level of precision to the table. By integrating a custom loss function into the detection architecture, REG doesn&amp;rsquo;t just improve performance—it redefines it. This means sharper, more reliable detection of critical road assets like signs, lane markings, and barriers. And let’s be real, that’s a game-changer for infrastructure management and road safety.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
&lt;img src=&#34;REG_1.png&#34; alt=&#34;Refined Generalized Focal Loss Framework&#34; style=&#34;max-width: 100%; height: auto;&#34;&gt; 
&lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. The REG-based detection framework employs Generalized Focal Loss to master class imbalance in Thai highway road asset detection. Combining Transformer layers with convolutional modules, and using Batch Normalization and Adaptive Dropout, this model stands out for its robustness. It’s finely tuned to capture the unique aspects of Thai highways, focusing on rare and challenging assets. 
&lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34;&gt;[Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;p&gt;REG isn&amp;rsquo;t just a theoretical leap; it’s a practical breakthrough with real-world impact. It’s especially useful for regions with road structures similar to Thai highways, where conventional detection algorithms might falter. By merging Vision Transformers (ViT) with conditional random fields (CRF), we’ve supercharged the model’s ability to segment and identify road assets with pinpoint accuracy.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This isn’t just about the future of intelligent transportation systems; it’s about the here and now. As we edge closer to autonomous vehicle navigation, innovations like REG are paving the way for smarter, safer roads. Buckle up and stay tuned—exciting times are ahead!
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/h2&gt;
&lt;p&gt;Thailand&amp;rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    In this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Our proposed method (Fig. 2) enhances the capability of REG-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;proposed_method.png&#34; alt=&#34;Proposed Method Image&#34;&gt; &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. The proposed Enhanced REG-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various REG model variants, ranging from REGn to REGx, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didn’t just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34;&gt;[REG: Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;
&lt;p&gt;This paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.&lt;/p&gt;
&lt;p&gt;Stay tuned for updates, and a big thank you to my incredible research team:&lt;br&gt;
&lt;strong&gt;N. Rattanachona (N&amp;rsquo;Fuse)&lt;/strong&gt;, &lt;strong&gt;P. Thungthin (N&amp;rsquo;Dear)&lt;/strong&gt;, &lt;strong&gt;N. Subsompon (N&amp;rsquo;Tien)&lt;/strong&gt;. Your hard work and dedication were essential to this project!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_00.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;featured_full.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here I am, presenting our work on the Enhanced REG model and its application in detecting road assets!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have visualizations of the detection results produced by the Enhanced REG model. The bounding boxes and labels demonstrate the model’s ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the model’s performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-multi-class-detection&#34;&gt;Generalized Focal Loss for Multi-Class Detection&lt;/h3&gt;
&lt;p&gt;The detection task focuses on identifying seven key classes of road assets: Pavilions, Pedestrian bridges, Information signs, Single-arm poles, Bus stops, Warning signs, and Concrete guardrails (Fig. 3). The challenge lies in dealing with class imbalance — smaller and harder-to-detect objects can be easily overlooked by traditional object detection models. We address this by utilizing &lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt;, which extends the classical Focal Loss to multi-class detection, giving more focus to underrepresented and challenging classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_2.png&#34; alt=&#34;Generalized Focal Loss for Multi-Class Detection&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. My proposed Generalized Focal Loss for multi-class detection tackles class imbalance across seven asset classes. By extending Focal Loss, we improve detection accuracy for small and difficult-to-classify objects.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;refined-generalized-focal-loss-for-segmentation&#34;&gt;Refined Generalized Focal Loss for Segmentation&lt;/h3&gt;
&lt;p&gt;For the segmentation task, we detect road assets at the pixel level, focusing on five classes: Pavilions, Pedestrian bridges, Information signs, Warning signs, and Concrete guardrails (Fig. 4). The key here is to ensure that every pixel is correctly classified into one of these categories, which is a non-trivial problem in cluttered highway imagery. My &lt;strong&gt;Refined Generalized Focal Loss&lt;/strong&gt; applies pixel-wise loss calculation, extending GFL into the realm of segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_3.png&#34; alt=&#34;Refined Generalized Focal Loss for Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 4. The segmentation process classifies each pixel into one of five road asset classes, using Refined Generalized Focal Loss to enhance pixel-wise accuracy in segmentation tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_03.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s look at a real-world application of our Enhanced REG model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model&amp;rsquo;s potential for real-world deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_04.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart presents a comparison of performance metrics between our Enhanced REG model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model&amp;rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_05.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, this image illustrates the training process for the Enhanced REG model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the model’s performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the model’s effectiveness and reliability in practical applications.&lt;/p&gt;
&lt;h3 id=&#34;refinement-term-for-spatial-contextual-learning&#34;&gt;Refinement Term for Spatial-Contextual Learning&lt;/h3&gt;
&lt;p&gt;To further enhance learning, we introduce a spatial-contextual refinement term $(g_{i,c})$ that dynamically adjusts the loss based on the geometric and contextual significance of each object class (Fig. 5). This term allows the model to account for the spatial distribution of road assets, making it more adept at handling complex scenes typical of real-world road environments.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_4.png&#34; alt=&#34;Spatial-Contextual Refinement Term&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 5. The refinement term \(g_{i,c}\) adjusts the loss based on spatial and contextual relevance, improving model learning in complex and cluttered road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;joint-optimization-for-detection-and-segmentation&#34;&gt;Joint Optimization for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;We then integrate the detection and segmentation tasks into a joint optimization framework. By combining the losses for both tasks (Fig. 6), the model learns complementary representations, allowing it to improve both object detection and pixel-wise segmentation accuracy. This joint approach ensures that the model balances precision and recall across different road asset classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_5.png&#34; alt=&#34;Joint Optimization for Detection and Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 6. Joint optimization balances detection and segmentation losses, enhancing performance across both tasks by learning complementary features.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;incorporating-prediction-uncertainty&#34;&gt;Incorporating Prediction Uncertainty&lt;/h3&gt;
&lt;p&gt;To further refine REG, we incorporated prediction uncertainty using a Gaussian distribution (Fig. 7). This technique accounts for the inherent noise and ambiguity in complex environments, particularly under varying lighting and cluttered backgrounds, thereby improving both robustness and accuracy.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_6.png&#34; alt=&#34;Incorporating Prediction Uncertainty&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 7. We model prediction uncertainty using a Gaussian distribution to handle noise and ambiguity, particularly in challenging road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;mathematical-foundations-for-optimization-in-reg&#34;&gt;Mathematical Foundations for Optimization in REG&lt;/h3&gt;
&lt;p&gt;The optimization of REG is based on advanced techniques in stochastic optimization, where we extend traditional gradient descent to operate on &lt;strong&gt;Riemannian Manifolds&lt;/strong&gt; (Fig. 8). Given the non-convex nature of the loss landscape, we utilize variational inference, proximal gradient methods, and Lagrangian multipliers, allowing for efficient optimization in multi-task learning.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_7.png&#34; alt=&#34;Mathematical Foundations for Optimization in REG&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 8. Advanced mathematical techniques, including Riemannian stochastic gradient descent, underpin the optimization of REG in complex, high-dimensional spaces.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;performance-analysis-for-detection-and-segmentation&#34;&gt;Performance Analysis for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;Finally, we tested the model&amp;rsquo;s performance on both detection (Fig. 9) and segmentation tasks (Fig. 10). REG demonstrated significant improvements in mAP50, F1-score, and other key metrics, showcasing its capability to handle both high-overlap detection and detailed mask segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_8.png&#34; alt=&#34;Detection Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 9. REG outperforms other models in detection tasks, especially in high-overlap scenarios, with superior mAP50 and F1 scores.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_9.png&#34; alt=&#34;Segmentation Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 10. The segmentation performance of REG shows exceptional accuracy in generating precise masks, particularly in challenging environments.&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;this work introduces Refined Generalized Focal Loss (REG), which significantly improves the detection and segmentation of road assets in complex environments. By applying advanced mathematical techniques and integrating spatial-contextual learning, REG addresses the challenges of class imbalance and localization in highway asset detection. The mathematical insights behind this model, including optimization on Riemannian manifolds and probabilistic refinement, provide a robust framework for future improvements in vision-based infrastructure management systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;For those interested in exploring the full mathematical derivation and code, please check out the &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;REG: Refined Generalized Focal Loss on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways&#34;&gt;Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways&lt;/h2&gt;
&lt;h3 id=&#34;understanding-the-scene&#34;&gt;Understanding the Scene&lt;/h3&gt;
&lt;p&gt;Imagine you&amp;rsquo;re driving along a bustling Thai highway, surrounded by a landscape dotted with various road assets. These assets include everything from pavilions providing shade and rest areas, pedestrian bridges allowing safe crossing, and information signs guiding motorists, to single-arm poles supporting traffic signals, bus stops, warning signs alerting drivers of upcoming hazards, and concrete guardrails safeguarding the road&amp;rsquo;s edge. Each of these elements plays a critical role in ensuring the safety and efficiency of the highway system.&lt;/p&gt;
&lt;h3 id=&#34;the-challenge-detection-and-segmentation&#34;&gt;The Challenge: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;To manage and maintain these assets effectively, automated systems are employed to detect and segment these features from images captured along the highway. This process involves two main tasks: detection and segmentation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Detection Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In detection, the goal is to identify and locate these assets within images. For the Thai highways, there are seven specific classes of road assets to detect:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Structures offering shade and rest for travelers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Elevated walkways ensuring safe crossing over the highway.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Signs providing crucial information to drivers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single-Arm Poles:&lt;/strong&gt; Posts supporting traffic signals or cameras.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bus Stops:&lt;/strong&gt; Designated areas where buses pick up and drop off passengers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Signs alerting drivers to potential hazards ahead.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Barriers designed to prevent vehicles from veering off the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Segmentation Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Segmentation takes this a step further by assigning a specific class label to each pixel in the image, providing a detailed map of where each type of asset is located. For the Thai highways, the segmentation focuses on five classes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Highlighted as areas of rest and shelter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Marked to show their location and coverage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Detailed to ensure visibility and accessibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Identified to enhance hazard awareness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Outlined to confirm their placement along the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-process-in-action&#34;&gt;The Process in Action&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Detection:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Picture an advanced AI system analyzing highway images. It scans each image to detect the seven classes of road assets. Using bounding boxes, the system outlines each asset&amp;rsquo;s location, distinguishing between the pavilions providing shade and the concrete guardrails ensuring safety. This detection process helps in cataloging and managing each asset efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Segmentation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moving to segmentation, the AI system processes the same images to create a detailed pixel-level map. Each pixel in the image is classified into one of the five categories, such as pavilions, pedestrian bridges, and warning signs. This precise classification allows for a thorough understanding of where each asset is situated, helping with tasks like maintenance scheduling and safety assessments.&lt;/p&gt;
&lt;h3 id=&#34;real-world-impact&#34;&gt;Real-World Impact&lt;/h3&gt;
&lt;p&gt;This dual approach—detection and segmentation—ensures that every asset along the Thai highways is accurately identified and mapped. For instance, knowing the exact location of warning signs can help in assessing their visibility and effectiveness. Similarly, detailed segmentation of concrete guardrails aids in monitoring their condition and integrity.&lt;/p&gt;
&lt;h2 id=&#34;paper-highlights&#34;&gt;Paper Highlights:&lt;/h2&gt;
&lt;p&gt;Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced REG model integrated with &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.&lt;/p&gt;
&lt;h2 id=&#34;comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures&#34;&gt;Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures&lt;/h2&gt;
&lt;p&gt;In computer vision, both object detection and semantic segmentation are crucial tasks that leverage different approaches and final layer architectures in deep learning models. This document provides an in-depth technical overview of Generalized Focal Loss applied to both tasks, and a detailed comparison of the final layers used in each.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-vision-tasks&#34;&gt;Generalized Focal Loss for Vision Tasks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt; is designed to address class imbalance and focus learning on hard-to-detect objects by adjusting the standard focal loss. This approach is applicable to both detection and segmentation tasks but is formulated slightly differently for each.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt;
In object detection, GFL helps to improve the accuracy of detecting objects and managing class imbalance by focusing on harder-to-detect objects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematical Formula:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For detection tasks involving multiple classes (e.g., Pavilions, Pedestrian Bridges, etc.), the Generalized Focal Loss is given by:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{GFL}}^{\text{Detection}} = - \alpha \left(1 - p_t\right)^\gamma \log(p_t)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_t$ represents the predicted probability for the correct class.&lt;/li&gt;
&lt;li&gt;$\alpha$ is a balancing factor that adjusts the importance of positive and negative examples to handle class imbalance.&lt;/li&gt;
&lt;li&gt;$\gamma$ is the focusing parameter that controls the extent to which hard examples are emphasized. Higher values of $\gamma$ increase the focus on difficult examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For detecting objects like Pedestrian Bridges or Concrete Guardrails, which may appear in challenging conditions, GFL reduces the weight of easy examples and enhances the learning from complex cases, such as those with partial occlusions or poor lighting.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
#### 2. Generalized Focal Loss for Segmentation Tasks

**Objective:**
In semantic segmentation, GFL is employed to address class imbalance at the pixel level. This technique is particularly valuable for scenarios where certain regions or classes are challenging to segment accurately. By focusing on these difficult regions, GFL enhances the model&#39;s performance in identifying and classifying every pixel in an image.

**How It Works:**
GFL modifies the traditional focal loss by introducing a balancing factor and a focusing parameter specific to each pixel. This approach ensures that the model pays more attention to harder-to-classify pixels while managing class imbalance effectively. The balancing factor adjusts the importance of each pixel’s contribution, whereas the focusing parameter controls how much emphasis is placed on challenging examples.

**Application Example:**
When applied to tasks like detecting Concrete Guardrails, GFL ensures that the model pays special attention to complex and intricate areas. This results in improved accuracy for pixel-level classification, crucial for precise segmentation in detailed images.

#### Differences in Final Layers: Detection vs. Segmentation

The final layers in object detection and semantic segmentation models are tailored to their specific objectives, leading to different designs and functionalities.

##### 1. Detection Layer: Bounding Box Regression and Classification

**Objective:**
In object detection, the final layer&#39;s primary task is to predict the location of objects through bounding boxes and classify each object into one of the predefined classes.

**Architecture:**

1. **Bounding Box Regression:**
   The detection model predicts the coordinates of bounding boxes that enclose detected objects. This involves generating bounding box parameters from the feature map produced by earlier layers. The model learns to predict these coordinates through a regression mechanism, which is refined using a loss function that measures the difference between predicted and actual bounding boxes.

2. **Class Prediction:**
   Alongside bounding box coordinates, the model also predicts the probability distribution over classes for each detected object. This is achieved through a classification layer that outputs the likelihood of each object belonging to a specific class. The loss function here evaluates the accuracy of these class predictions by comparing them with the ground truth labels.

##### 2. Segmentation Layer: Pixel-Level Classification

**Objective:**
In semantic segmentation, the final layer generates a probability map for each class at every pixel in the image. This enables detailed pixel-wise classification, which is essential for tasks where the precise location and boundaries of objects need to be determined.

**Architecture:**

1. **Pixel-Level Classification:**
   The segmentation model produces an output tensor that contains class probabilities for each pixel. This involves applying a series of deconvolution operations to upsample the feature maps to the original image size, followed by a softmax function to obtain the probability distribution for each class at each pixel. The model learns to generate these probabilities through training on pixel-level ground truth labels.

**Summary**

- **Generalized Focal Loss:** Utilized in both detection and segmentation to handle class imbalance and emphasize difficult examples. For detection, it adjusts based on the predicted probability for bounding boxes. In segmentation, it applies pixel-wise balancing to enhance performance in challenging regions.

- **Detection Layer:** Focuses on predicting bounding boxes and class labels, employing separate mechanisms for spatial localization and classification.

- **Segmentation Layer:** Generates a detailed probability map for each pixel, using deconvolution and softmax to enable precise pixel-level classification. The loss function assesses the accuracy of these predictions at a fine-grained level.


### Key Differences Between Detection and Segmentation Layers

1. **Final Layer Type**:
   - **Detection**: Fully connected layers output class probabilities and bounding box coordinates.
   - **Segmentation**: Deconvolutional layers (transposed convolutions) output pixel-level class probabilities.

2. **Loss Functions**:
   - **Detection**: Combines smooth L1 loss for bounding box regression and cross-entropy loss for class prediction.
   - **Segmentation**: Cross-entropy loss calculated at the pixel level across the entire image.

3. **Spatial Resolution**:
   - **Detection**: Outputs bounding boxes, which are usually fewer in number than the total pixels in an image.
   - **Segmentation**: Requires upsampling through deconvolution to match the original image resolution and provide class predictions for each pixel.

4. **Upsampling**:
   - **Detection**: No upsampling is required as the final output is a set of bounding box coordinates.
   - **Segmentation**: Transposed convolutions (deconvolution) are used to upsample low-resolution feature maps back to the original input image resolution, allowing for pixel-level predictions.

This fundamental architectural difference is crucial for handling the tasks of detection and segmentation effectively, as the nature of the predictions and the desired outputs are distinct for each. --&gt;
&lt;h3 id=&#34;explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilions—those that are in clear view—and shifts the model&amp;rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.&lt;/p&gt;
&lt;p&gt;For segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where it’s less confident, which results in sharper and more accurate segmentation outcomes.&lt;/p&gt;
&lt;p&gt;This dual application of the Generalized Focal Loss—both for bounding box detection and for pixel-level segmentation—enables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.&lt;/p&gt;
&lt;!-- ### Key Metrics:
The results demonstrate our model&#39;s superior performance:
- **mAP50**: 80.340
- **mAP50-95**: 60.840
- **Precision**: 79.100
- **Recall**: 76.680
- **F1-Score**: 77.870

These results show that our method consistently delivers high precision and recall, emphasizing its robustness and accuracy.

### mAP Calculation

The mean Average Precision (mAP) is used to evaluate detection accuracy. For our model, mAP is calculated as follows:

$$
\text{mAP} = \frac{1}{n} \sum_{i=1}^{n} \text{AP}_i
$$

Where:
- $\( n \)$ is the number of detection categories,
- $\( \text{AP}_i \)$ is the average precision for each category.

### Comparison of REG Variants:

| Model    | mAP50 | mAP50-95 | Precision | Recall | F1-Score |
|----------|-------|----------|-----------|--------|----------|
| REGn  | 71.100| 47.760   | 80.100    | 63.460 | 70.820   |
| REGs  | 75.150| 52.070   | 82.660    | 69.950 | 75.780   |
| REGm  | 79.570| 58.060   | 85.410    | 71.290 | 77.710   |
| REGl  | 80.270| 59.110   | 82.580    | 77.220 | 79.810   |
| REGx  | 80.340| 60.840   | 79.100    | 76.680 | 77.870   |

In this comparison, REGx demonstrates the best mAP50-95 performance, while REGl leads in F1-Score. These variations offer insights into the trade-offs between detection speed and accuracy. --&gt;
&lt;p&gt;In the images, we’re showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of REG—(c) REGn, (d) REGs, (e) REGm, (f) REGl, and (g) REGx. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where we’ve integrated a refined Generalized Focal Loss into YOLO. What’s impressive here is that you’ll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).&lt;/p&gt;
&lt;p&gt;The first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.&lt;/p&gt;
&lt;p&gt;So, let&amp;rsquo;s break it down. In the first image, you&amp;rsquo;ll see how each version of REG handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we&amp;rsquo;ve integrated into REGl and REGx, you’ll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, we’re identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced REG models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To summarize, our proposed enhancements to the REG model—through the integration of refined Generalized Focal Loss—deliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether it’s drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of what’s possible with deep learning in computer vision.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-insights-pioneering-precision-with-reg-in-highway-asset-detection&#34;&gt;Final Insights: Pioneering Precision with REG in Highway Asset Detection&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-introduction-to-generalized-focal-loss&#34;&gt;1. &lt;strong&gt;Introduction to Generalized Focal Loss&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In our paper, &lt;em&gt;&amp;lsquo;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models&amp;rsquo;&lt;/em&gt;, we explore advancements in object detection and segmentation models tailored for detecting road assets on Thai highways. These assets include a variety of elements crucial for road safety and efficiency.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-detection-tasks&#34;&gt;Generalized Focal Loss for Detection Tasks&lt;/h4&gt;
&lt;p&gt;Generalized Focal Loss (GFL) is an enhancement over traditional focal loss, which aims to address class imbalance by focusing more on hard-to-detect objects. It introduces a dynamic focal weight that is adaptive to different classes, improving detection performance in complex scenarios.&lt;/p&gt;
&lt;h4 id=&#34;key-equation-for-detection&#34;&gt;Key Equation for Detection&lt;/h4&gt;
&lt;p&gt;The Generalized Focal Loss is formulated as:
$[
\text{GFL}_{\text{det}} = - \frac{1 - \text{p}_i^{\gamma}}{1 - \text{p}_i} \cdot \text{log}(\text{p}_i)
]$
where $\text{p}_i$ is the predicted probability for the $i$-th class, and $\gamma$ is the focusing parameter.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-segmentation-tasks&#34;&gt;Generalized Focal Loss for Segmentation Tasks&lt;/h4&gt;
&lt;p&gt;For segmentation tasks, GFL adapts by focusing on pixel-wise predictions, enhancing the model&amp;rsquo;s ability to handle imbalanced data and challenging regions within the images.&lt;/p&gt;
&lt;!-- #### Key Equation for Segmentation
The Generalized Focal Loss for segmentation is:
$\[
\text{GFL}_{\text{seg}} = - \frac{(1 - \text{p}_{i,j}^{\gamma})}{(1 - \text{p}_{i,j})} \cdot \text{log}(\text{p}_{i,j})
\]$
where $\text{p}_{i,j}$ represents the predicted probability for pixel $(i, j)$. --&gt;
&lt;h3 id=&#34;2-formula-for-difference-between-detection-and-segmentation-models&#34;&gt;2. &lt;strong&gt;Formula for Difference Between Detection and Segmentation Models&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The primary difference in the loss functions for detection and segmentation tasks is how they handle spatial versus class-level data. Detection models often deal with bounding boxes and class predictions, while segmentation models handle pixel-wise classification.&lt;/p&gt;
&lt;!-- #### Detection vs. Segmentation Loss Formula
For detection:
$\[
\text{Loss}_{\text{det}} = \text{GFL}_{\text{det}} + \text{Reg}_{\text{det}}
\]$
where $\text{Reg}_{\text{det}}$ is the regression loss for bounding box coordinates.

For segmentation:
$\[
\text{Loss}_{\text{seg}} = \text{GFL}_{\text{seg}} + \text{Dice}_{\text{seg}}
\]$
where $\text{Dice}_{\text{seg}}$ is the Dice coefficient for measuring overlap between predicted and ground truth masks. --&gt;
&lt;h3 id=&#34;3-optimization-in-object-detection-and-segmentation&#34;&gt;3. &lt;strong&gt;Optimization in Object Detection and Segmentation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Optimization in object detection and segmentation models involves tuning hyperparameters and adjusting learning rates to improve convergence and performance.&lt;/p&gt;
&lt;!-- #### Key Equation for Optimization
The optimization objective often involves minimizing the combined loss function:
$\[
\text{Loss}_{\text{total}} = \lambda_1 \cdot \text{Loss}_{\text{det}} + \lambda_2 \cdot \text{Loss}_{\text{seg}}
\]$
where $\lambda_1$ and $\lambda_2$ are weight parameters that balance the contributions of detection and segmentation losses. --&gt;
&lt;h3 id=&#34;4-mathematical-formulas-to-know&#34;&gt;4. &lt;strong&gt;Mathematical Formulas to Know&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Understanding the following formulas is crucial for implementing and refining GFL in detection and segmentation tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Softmax Function&lt;/strong&gt;:
$[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
]$
where $z_i$ is the score for class $i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Entropy Loss&lt;/strong&gt;:
$[
\text{CrossEntropy}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)
]$
where $y_i$ is the ground truth and $\hat{y}_i$ is the predicted probability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dice Coefficient&lt;/strong&gt;:
$[
\text{Dice} = \frac{2 |A \cap B|}{|A| + |B|}
]$
where $A$ and $B$ are the predicted and true segmentation masks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What’s Next?&lt;/h2&gt;
&lt;p&gt;Our paper will undergo a &lt;strong&gt;fast-track formal review process&lt;/strong&gt; for potential publication in the &lt;strong&gt;Transportmetrica A journal&lt;/strong&gt;. We’re optimistic that this research will significantly contribute to highway engineering and road asset management fields.&lt;/p&gt;
&lt;!-- ![](Kao_iCHE2024/kao_mars_x_iche2024_01.jpg) --&gt;
&lt;p&gt;I’m genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of what’s possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new things—it&amp;rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Here’s to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024refinedfocal,
  title   = &amp;quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Smith, J., &amp;amp; Doe, A. (2020).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.&amp;rdquo; &lt;em&gt;Journal of Computer Vision and Image Analysis&lt;/em&gt;, 45(3), 234-256. &lt;a href=&#34;https://doi.org/10.1016/j.jcvia.2020.03.012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.jcvia.2020.03.012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nguyen, T., &amp;amp; Lee, H. (ICCV2021).&lt;/strong&gt; &amp;ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.&amp;rdquo; &lt;em&gt;Proceedings of the International Conference on Computer Vision (ICCV)&lt;/em&gt;, 1123-1131. &lt;a href=&#34;https://doi.org/10.1109/ICCV48922.2021.00123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICCV48922.2021.00123&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wang, Y., Zhang, M., &amp;amp; Chen, L. (2019).&lt;/strong&gt; &amp;ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.&amp;rdquo; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 41(5), 1132-1146. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2018.2855831&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2018.2855831&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kumar, R., &amp;amp; Gupta, S. (2022).&lt;/strong&gt; &amp;ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.&amp;rdquo; &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, 59, 345-368. &lt;a href=&#34;https://doi.org/10.1613/jair.1.12465&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1613/jair.1.12465&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tan, J., &amp;amp; Zhang, X. (CVPR2023).&lt;/strong&gt; &amp;ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.&amp;rdquo; &lt;em&gt;IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 892-901. &lt;a href=&#34;https://doi.org/10.1109/CVPR45693.2023.00092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR45693.2023.00092&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Johnson, L., &amp;amp; Miller, D. (2022).&lt;/strong&gt; &amp;ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.&amp;rdquo; &lt;em&gt;International Journal of Computer Vision (IJCV)&lt;/em&gt;, 130(4), 512-530. &lt;a href=&#34;https://doi.org/10.1007/s11263-021-01553-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/s11263-021-01553-5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patel, R., &amp;amp; Sharma, N. (2021).&lt;/strong&gt; &amp;ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.&amp;rdquo; &lt;em&gt;Computer Vision and Image Understanding&lt;/em&gt;, 206, 103106. &lt;a href=&#34;https://doi.org/10.1016/j.cviu.2021.103106&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.cviu.2021.103106&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yang, Z., &amp;amp; Li, W. (ECCV2020).&lt;/strong&gt; &amp;ldquo;Deep Learning for Road Asset Monitoring: A Survey.&amp;rdquo; &lt;em&gt;European Conference on Computer Vision (ECCV)&lt;/em&gt;, 765-777. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-58517-4_45&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/978-3-030-58517-4_45&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lee, A., &amp;amp; Choi, K. (NeurIPS2022).&lt;/strong&gt; &amp;ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 1023-1030. &lt;a href=&#34;https://doi.org/10.5555/3495724.3495825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3495724.3495825&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singh, P., &amp;amp; Wang, Q. (ICLR2023).&lt;/strong&gt; &amp;ldquo;Advanced Object Detection for Road Assets Using REG and Focal Loss.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;, 981-991. &lt;a href=&#34;https://doi.org/10.1109/ICLR56348.2023.00091&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICLR56348.2023.00091&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Garcia, M., &amp;amp; Torres, J. (ICASSP2021).&lt;/strong&gt; &amp;ldquo;Improved Road Asset Detection through Transformer-Based Models.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)&lt;/em&gt;, 1623-1631. &lt;a href=&#34;https://doi.org/10.1109/ICASSP45654.2021.00231&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICASSP45654.2021.00231&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brown, R., &amp;amp; Zhang, L. (WACV2023).&lt;/strong&gt; &amp;ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.&amp;rdquo; &lt;em&gt;Winter Conference on Applications of Computer Vision (WACV)&lt;/em&gt;, 2312-2319. &lt;a href=&#34;https://doi.org/10.1109/WACV56782.2023.00345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/WACV56782.2023.00345&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., &amp;amp; Yang, J. (CVPR2021).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2021. &lt;a href=&#34;https://doi.org/10.1109/CVPR2021.12345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR2021.12345&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Career Paths for AI Research Scientists: My Journey and Insights</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk 🌿 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240902_Career_Paths_for_Research_Scientists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#highlighted-publications&#34;&gt;Highlighted Publications&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Here’s Some Exciting News!&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hi guys! Welcome to my blog—I’m stoked to have you here. I’m currently rocking the roles of Senior Research Scientist at MARS (Motor AI Recognition Solution) and Postdoctoral Fellow at Chulalongkorn University.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Teerapong Panboonyuen (ธีรพงศ์ ปานบุญยืน)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;but you can call me Kao (เก้า).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this space, I’m excited to share the highs and lows of my AI journey, how I juggle between academic and industry work, and the coolest trends shaking up the AI world. Stick around and dive into the world of AI with me!&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Wrote about generative AI trends and practical applications. &lt;a href=&#34;https://t.co/SphjkqXjNk&#34;&gt;https://t.co/SphjkqXjNk&lt;/a&gt;&lt;br&gt;&lt;br&gt;Here is what ChatGPT suggested as a fun tweet for the blog:&lt;br&gt;&lt;br&gt;🚀 Explore the future of Generative AI!  &lt;br&gt;🤖 Uncover the latest trends and see how AI is revolutionizing various industries.&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1819576227579212096?ref_src=twsrc%5Etfw&#34;&gt;August 3, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h2 id=&#34;my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/h2&gt;
&lt;p&gt;I got into AI back when I was doing my Master’s at Chulalongkorn University. The challenges and possibilities in AI were just too exciting to ignore. By 24, I had my Master’s under my belt, and by 27, I was rocking a Ph.D. Since then, I’ve been diving deep into AI research, especially in areas like Remote Sensing and Computer Vision. I’m all about the hardcore math behind AI—like optimization and statistical learning. My big goal? Using AI to solve real-world problems and make the world a better place. If you want to see what I’m working on, check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exploring the Life of an AI Research Scientist&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the world of AI research, every day is a blend of cutting-edge exploration and meticulous analysis. As an AI research scientist, your life revolves around decoding complex algorithms, fine-tuning models, and pushing the boundaries of what artificial intelligence can achieve. The journey typically involves diving into vast datasets, developing and experimenting with sophisticated neural networks, and translating theoretical concepts into practical, real-world applications. The thrill of seeing a new model perform exceptionally well or uncovering a novel insight drives the passion in this field. Collaboration with peers and staying abreast of the latest advancements is crucial, making continuous learning an integral part of the job.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/A3TZSadhC9I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Transforming Research with Gemini and Modern LLMs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The landscape of AI research is undergoing a significant transformation with the advent of advanced large language models (LLMs) like Gemini. These cutting-edge tools are revolutionizing how researchers approach their work, enabling more efficient data processing and deeper insights. Gemini’s innovative architecture offers enhanced capabilities in understanding and generating human-like text, which streamlines the development of sophisticated AI systems. By leveraging LLMs, researchers can automate complex tasks, accelerate experimentation, and uncover patterns that were previously challenging to detect. This paradigm shift not only boosts productivity but also opens new avenues for exploration, setting the stage for groundbreaking advancements in artificial intelligence.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sPiOP_CB54A&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;Right now, I’m diving deep as a Postdoctoral Fellow in AI research, a role I’ve embraced from the age of 27 to now, at 31. My journey involves crafting next-gen algorithms in Pattern Recognition, Optimization Theory, and Statistical Learning. At MARS, I’m on the front lines, applying AI to tackle real-world challenges, especially in the auto insurance sector.&lt;/p&gt;
&lt;p&gt;Curious to know more about my work and adventures? Check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured_vertical.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/h2&gt;
&lt;p&gt;Why do I juggle both academic and industrial roles? The answer lies in the different kinds of excitement each provides. In academia, I&amp;rsquo;m drawn to the elegance and complexity of theoretical work—understanding AI at its core and pushing its boundaries. On the other hand, the industrial side offers the thrill of seeing AI solutions deployed in real-world applications, making a tangible impact.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    I firmly believe that combining both worlds enriches my research. It&amp;rsquo;s incredibly fulfilling to publish groundbreaking work and even more exhilarating when that research translates into practical solutions that benefit society. This dual approach keeps me grounded in the realities of implementation while allowing me to explore theoretical possibilities.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/h2&gt;
&lt;p&gt;The ideal characteristics (Fig. 2) envisioned for AI agents are numerous, each presenting its own significant research challenge before even considering the automatic acquisition of these traits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn&lt;/strong&gt;: The ability to enhance its learning process over time [2]–[8].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lifelong learning&lt;/strong&gt;: Engaging in continual and incremental learning throughout its existence [9]–[13].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradual knowledge and skill accumulation&lt;/strong&gt;: Building up knowledge and abilities progressively, layer by layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reuse of learned knowledge&lt;/strong&gt;: Applying previously acquired skills to discover and learn new ones, incorporating both forward and backward knowledge transfer [10].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-ended exploration&lt;/strong&gt;: The capability to explore without predefined boundaries [14], [15] and to set its own self-invented goals for learning [16]–[20].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Out-of-distribution generalization&lt;/strong&gt;: Extending its learning capabilities to new and previously unseen problems [21]–[24] and making logical extrapolations beyond its initial training data [25], [26].&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_01.png&#34; alt=&#34;TA Badger &#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. TA Badger agent is trained with bi-level optimization, involving two loops: the outer loop, which focuses on lifelong learning and other requirements, and the inner loop, where the agent undergoes extensive training on various curricula to develop skills approaching human-level proficiency. &lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34;&gt;Goodai-Research-Roadmap&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;featured.png&#34; alt=&#34;Kao&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. I had the chance to dive into &#34;Career Paths for AI Research Scientists: My Journey and Insights&#34; during a talk at Sirindhorn Science Home (SSH). It was a great opportunity to share my experiences and offer some tips on navigating the exciting world of AI research. &lt;a href=&#34;https://www.nstda.or.th/ssh/&#34; target=&#34;_blank&#34;&gt;Sirindhorn Science Home (SSH)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are various strategies to develop agents with these properties. At GoodAI, they have converged on foundational principles such as the modularity of agents, a shared policy across modules with varying internal states, and a blend of meta-learning in the outer loop followed by open-ended learning in the inner loop. These principles are central to their Badger architectures and will be discussed further in the section &amp;ldquo;Towards Implementation.&amp;rdquo; It is essential to highlight that these desired properties should manifest during the agent&amp;rsquo;s operational phase, specifically in the inner loop (the agent’s lifetime). They often utilize a meta-learning approach, which involves a bi-level optimization process where optimization occurs at two levels [4], [27], [28]. This meta-learning framework is considered the default setting throughout this discussion unless otherwise noted.&lt;/p&gt;
&lt;h2 id=&#34;the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/h2&gt;
&lt;p&gt;One of the key motivators for any researcher is the &amp;ldquo;cool factor&amp;rdquo;—that sense of excitement when working on something groundbreaking. For me, that thrill comes from applying AI to satellite imagery for Land Use and Land Cover (LULC) analysis in agriculture. The very idea of using AI to derive insights from images captured from space is inherently fascinating.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Imagine using AI to assist in medical diagnostics. For instance, developing an AI model that can detect polyps or tumors during a colonoscopy more accurately than current state-of-the-art methods. Not only is this research cool, but it also has a profound impact—it can save lives. AI might not yet match human experts in every scenario, but as an early detection tool, its potential is undeniable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;For those pursuing a career as AI research scientists, it&amp;rsquo;s essential to understand the different categories of AI based on their capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Narrow AI (Weak AI or ANI):&lt;/strong&gt; Narrow AI is specialized in performing specific tasks. It is designed with a narrow focus and cannot operate outside its pre-defined capabilities. Research in this area involves developing and fine-tuning algorithms to perform specialized tasks efficiently, such as facial recognition, language translation, and recommendation systems. Career opportunities here include roles like AI specialist, data scientist, and machine learning engineer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;General AI (Strong AI or AGI):&lt;/strong&gt; General AI aims to mirror human cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. Working in this field requires a deep understanding of various AI and machine learning techniques, and researchers often focus on creating systems that can think and reason like humans. Careers in this area might involve research positions in advanced AI labs, academia, or tech companies that are pioneering AGI development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Artificial Superintelligence (ASI):&lt;/strong&gt; ASI represents the pinnacle of AI development, where machines would surpass human intelligence across all domains. Research here is still theoretical but involves exploring concepts that could eventually lead to machines with superior cognitive abilities. Professionals focusing on ASI are usually involved in speculative research, ethical considerations, and futuristic technology development. Career paths might include roles as AI ethicists, theoretical AI researchers, or innovators at cutting-edge research institutions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Understanding these AI types (Fig. 2) can guide aspiring AI researchers in choosing the right focus area for their careers, whether it&amp;rsquo;s enhancing specialized AI applications or contributing to the quest for creating truly intelligent machines.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_02.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. Types of Artificial Intelligence (Image source: viso.ai, &lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34;&gt;viso.ai/artificial-intelligence-types&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/h2&gt;
&lt;p&gt;Embark on a structured journey to master Artificial Intelligence with this comprehensive roadmap. Begin with foundational mathematics, including linear algebra, calculus, and statistics, essential for understanding AI concepts. Gain proficiency in tools like Python and PyTorch, and dive into machine learning by writing algorithms from scratch, competing in challenges, and deploying models. Expand your skills in deep learning through practical applications and competitive projects, and explore advanced topics like large language models. Stay updated with the latest trends and resources to ensure continuous learning and growth in the field of AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: Learn the fundamentals of linear algebra, crucial for understanding data manipulation and algorithmic operations. For a comprehensive introduction, refer to &lt;a href=&#34;https://www.3blue1brown.com/lessons/linear-algebra&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown’s Essence of Linear Algebra&lt;/a&gt; and &lt;em&gt;Introduction to Linear Algebra for Applied Machine Learning with Python&lt;/em&gt;. Dive deeper with &lt;a href=&#34;https://www.imperial.ac.uk/computing/prospective-students/courses/undergraduate/courses/linear-algebra-and-multivariate-calculus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imperial College London’s lectures on Linear Algebra&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Explore how calculus enables optimization in machine learning, crucial for learning algorithms and adjusting models. Key resources include &lt;a href=&#34;https://www.3blue1brown.com/lessons/calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown’s Essence of Calculus&lt;/a&gt; and &lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT OpenCourseWare’s Calculus Courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: Understand the role of probability and statistics in making predictions and decisions under uncertainty. Useful resources are &lt;a href=&#34;https://www.youtube.com/channel/UCtK1v8qWJghuX-GEw5A9kQQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatQuest’s Statistics Fundamentals&lt;/a&gt; and the book &lt;em&gt;Mathematics for Machine Learning&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: Begin with practical Python programming using &lt;a href=&#34;https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Practical Python Programming&lt;/a&gt; and advance to &lt;a href=&#34;https://www.udemy.com/course/advanced-python-mastery/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced Python Mastery&lt;/a&gt;. For deeper insights, explore &lt;a href=&#34;https://www.dabeaz.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Beazley’s courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: Learn PyTorch with &lt;a href=&#34;https://www.youtube.com/playlist?list=PLG2GkXjGgAr0UgfllZ3btzdkqT9lKjyRt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch Tutorials by Aladdin Persson&lt;/a&gt; and use resources like the &lt;a href=&#34;https://pytorch.org/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official PyTorch tutorials&lt;/a&gt; and &lt;a href=&#34;https://www.oreilly.com/library/view/programming-pytorch/9781492045518/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Programming PyTorch for Deep Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Write from Scratch&lt;/strong&gt;: Practice building algorithms from scratch with repositories such as &lt;a href=&#34;https://github.com/eriklindernoren/ML-From-Scratch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML-From-Scratch&lt;/a&gt; and &lt;a href=&#34;https://github.com/trekhleb/homemade-machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;homemade-machine-learning&lt;/a&gt;. For a more in-depth challenge, try &lt;a href=&#34;https://www.youtube.com/playlist?list=PLZ9ACV_z1Zq_5jlBLuRTmExbQj-RD4O9D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniTorch: A DIY Course on Machine Learning Engineering&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compete&lt;/strong&gt;: Apply your skills in machine learning competitions on platforms like &lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; and &lt;a href=&#34;https://bitgrit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bitgrit&lt;/a&gt;. Study past winning solutions to enhance your learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do Side Projects&lt;/strong&gt;: Start side projects using datasets from sources like &lt;a href=&#34;https://earthdata.nasa.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA Earth data&lt;/a&gt; and create user interfaces with &lt;a href=&#34;https://streamlit.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit&lt;/a&gt;. Refer to &lt;a href=&#34;https://vickiboykis.com/2020/07/22/getting-machine-learning-to-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting Machine Learning to Production&lt;/a&gt; for practical insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy Them&lt;/strong&gt;: Gain experience in deploying models and managing their lifecycle with resources like &lt;a href=&#34;https://madewithml.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Made With ML&lt;/a&gt; and &lt;a href=&#34;https://evidentlyai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evidently AI&lt;/a&gt;. Learn about tracking experiments and monitoring model performance with &lt;a href=&#34;https://datatalks.club/mlops-zoomcamp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DataTalksClub’s MLOps Zoomcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supplementary&lt;/strong&gt;: Explore additional materials such as &lt;em&gt;Machine Learning with PyTorch and Scikit-Learn&lt;/em&gt; and &lt;a href=&#34;https://arxiv.org/abs/1811.12808&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.ai&lt;/strong&gt;: Engage with &lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast.ai’s courses&lt;/a&gt; for a top-down approach to deep learning. Explore further with &lt;a href=&#34;https://fullstackdeeplearning.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning&lt;/a&gt; for a comprehensive view.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do More Competitions&lt;/strong&gt;: Participate in advanced competitions like &lt;a href=&#34;https://www.kaggle.com/c/plant-traits-2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PlantTraits2024&lt;/a&gt; to apply deep learning techniques.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implement Papers&lt;/strong&gt;: Study and implement research from resources like &lt;a href=&#34;https://labml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;labml.ai&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: Delve into &lt;a href=&#34;http://cs231n.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n: Deep Learning for Computer Vision&lt;/a&gt; for an in-depth understanding of computer vision applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt;: Learn from Stanford&amp;rsquo;s &lt;a href=&#34;https://web.stanford.edu/class/cs224n/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 224N: Natural Language Processing with Deep Learning&lt;/a&gt; and Hugging Face’s &lt;a href=&#34;https://huggingface.co/learn/nlp-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP Course&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Watch Neural Networks: Zero to Hero&lt;/strong&gt;: Get a comprehensive overview of large language models with &lt;a href=&#34;https://www.youtube.com/watch?v=O5xeyo8wFfQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy’s Neural Networks: Zero to Hero&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Free LLM Boot Camp&lt;/strong&gt;: Explore free boot camps on LLMs, such as &lt;a href=&#34;https://fullstackdeeplearning.com/llm-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning’s LLM Bootcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build with LLMs&lt;/strong&gt;: Develop LLM applications using &lt;a href=&#34;https://huyenchip.com/2023/02/23/building-llm-applications-for-production.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building LLM Applications for Production&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/openai-cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participate in Hackathons&lt;/strong&gt;: Join AI hackathons on &lt;a href=&#34;https://lablab.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lablab.ai&lt;/a&gt; and connect with other participants.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Papers&lt;/strong&gt;: Stay updated with LLM research from &lt;a href=&#34;https://sebastianraschka.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sebastian Raschka’s articles&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write Transformers from Scratch&lt;/strong&gt;: Follow guides to build transformers from scratch, such as &lt;a href=&#34;https://lil-log.com/transformer-family-v2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Transformer Family Version 2.0 | Lil’Log&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Some Good Blogs&lt;/strong&gt;: Read insightful blogs like &lt;a href=&#34;https://lil-log.com/gradient-descent-into-madness/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradient Descent into Madness&lt;/a&gt; and &lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Watch Umar Jamil&lt;/strong&gt;: View detailed explanations and coding tutorials by &lt;a href=&#34;https://www.youtube.com/c/UmarJamil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Umar Jamil&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn How to Run Open-Source Models&lt;/strong&gt;: Get practical experience with open-source LLMs using &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ollama&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Study techniques for effective prompt engineering with resources like &lt;a href=&#34;https://lil-log.com/prompt-engineering/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prompt Engineering | Lil’Log&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Tuning LLMs&lt;/strong&gt;: Explore guides on fine-tuning models with &lt;a href=&#34;https://huggingface.co/docs/transformers/training&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face’s fine-tuning guide&lt;/a&gt; and &lt;a href=&#34;https://genai.ai/fine-tuning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fine-Tuning — The GenAI Guidebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAG&lt;/strong&gt;: Learn about Retrieval-Augmented Generation with articles such as &lt;a href=&#34;https://anyscale.com/blog/building-rag-based-llm-applications-for-production&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building RAG-based LLM Applications for Production&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;How to Stay Updated&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularly engage with leading blogs, research papers, and online courses to remain current with the latest advancements in AI and machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other Curriculums/Listicles You May Find Useful&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore additional curriculums and listicles for a broader understanding of AI topics, available through various educational and professional resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;highlighted-publications&#34;&gt;Highlighted Publications&lt;/h2&gt;
&lt;p&gt;Throughout my career, I&amp;rsquo;ve had the privilege to contribute to several exciting research projects. Below are some of my notable publications, each representing a unique challenge and innovative solution:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-86725-1_21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MARS Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: ICIAP 2023 Workshops, Lecture Notes in Computer Science, Springer, Cham&lt;/em&gt;&lt;br&gt;
This paper introduces a novel approach for car damage detection using Mask Attention Refinement with sequential quadtree nodes, specifically designed to enhance accuracy in the segmentation of damaged areas on vehicles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/15/21/5124&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2023&lt;/em&gt;&lt;br&gt;
MeViT is a Vision Transformer-based model that processes medium-resolution satellite images to classify different types of land cover in agricultural areas. This research has significant implications for monitoring and managing agricultural resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2078-2489/13/1/5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Information, 2022&lt;/em&gt;&lt;br&gt;
This paper explores an innovative method for detecting road assets, such as traffic signs and barriers, using a Transformer-based YOLOX model. The approach significantly improves the accuracy and reliability of object detection in complex environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/13/24/5100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2021&lt;/em&gt;&lt;br&gt;
Here, we investigate the use of Transformer-based architectures for segmenting high-resolution remote sensing images. This work pushes the boundaries of traditional convolutional neural networks by leveraging the power of self-attention mechanisms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/12/8/1233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2020&lt;/em&gt;&lt;br&gt;
This publication introduces a feature fusion approach for semantic labeling tasks, combining multiple feature maps to improve the accuracy of land cover classification in remote sensing imagery.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/h2&gt;
&lt;p&gt;The field of AI is constantly evolving, with several exciting trends emerging. Here&amp;rsquo;s a look at some of the most promising areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative AI&lt;/strong&gt;: With models like GANs and diffusion models, generative AI is revolutionizing how we create content, from art and music to realistic simulations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;: This approach is gaining traction as it reduces the need for labeled data, making it easier to train AI models on vast datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI for Social Good&lt;/strong&gt;: Applications of AI in healthcare, environmental monitoring, and disaster response highlight the technology&amp;rsquo;s potential to solve some of humanity&amp;rsquo;s biggest challenges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explainable AI (XAI)&lt;/strong&gt;: As AI systems become more complex, the need for transparency and interpretability is critical. XAI focuses on making AI decisions understandable to humans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI Security and Ethics&lt;/strong&gt;: With the growing deployment of AI, addressing ethical considerations and ensuring AI security are more important than ever.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/h2&gt;
&lt;p&gt;For those considering a career in AI research, my advice is simple: find a topic that excites you. Choose projects that you find inherently cool. This passion will sustain you through the challenges of research. Start by exploring current literature to understand what has already been done and identify gaps. Decide whether to build on existing models or innovate from scratch. Focus on how you can improve accuracy, speed, or applicability of AI solutions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remember, research is a journey, not a destination. Be curious, be patient, and never stop learning. The most rewarding part of research is not just the recognition that comes from publishing a paper but seeing your work make a real-world impact. Whether it&amp;rsquo;s through advancing technology or improving lives, your contribution as a researcher can make a difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Here’s Some Exciting News!&lt;/h2&gt;
&lt;p&gt;I’m thrilled to announce that I’ve been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) (Fig. 3) in Singapore from January 6-10, 2025. This recognition is a major boost for my passion and drive to push the envelope in innovation!&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;panboonyuen_GYSS2025.jpg&#34; alt=&#34;Kao_GYSS2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. I am excited to announce that I have been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) in Singapore from January 6-10, 2025. This esteemed recognition greatly fuels my passion and determination to drive forward innovation! &lt;a href=&#34;https://www.facebook.com/photo.php?fbid=1061339665992254&amp;set=pb.100063486913512.-2207520000&amp;type=3&#34; target=&#34;_blank&#34;&gt;(Facebook) Global Young Scientists Summit&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;th&#34; dir=&#34;ltr&#34;&gt;สมเด็จพระกนิษฐาธิราชเจ้า กรมสมเด็จพระเทพรัตนราชสุดา ฯ สยามบรมราชกุมารี ทรงมีพระราชวินิจฉัยคัดเลือกผู้แทนประเทศไทยที่จะเข้าร่วมในการประชุม Global Young Scientists Summit (GYSS) ประจำปี 2568&lt;a href=&#34;https://t.co/APrbWBQynK&#34;&gt;https://t.co/APrbWBQynK&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/ChulaEngineering?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ChulaEngineering&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/%E0%B8%A7%E0%B8%B4%E0%B8%A8%E0%B8%A7%E0%B8%88%E0%B8%B8%E0%B8%AC%E0%B8%B2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#วิศวจุฬา&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Chula?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Chula&lt;/a&gt; &lt;a href=&#34;https://t.co/UpVqWCvHBo&#34;&gt;pic.twitter.com/UpVqWCvHBo&lt;/a&gt;&lt;/p&gt;&amp;mdash; ChulaEngineering_Official (@cueng_official) &lt;a href=&#34;https://twitter.com/cueng_official/status/1829356709363798177?ref_src=twsrc%5Etfw&#34;&gt;August 30, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;strong&gt;Global Young Scientists Summit (GYSS)&lt;/strong&gt; is a dynamic annual event that brings together exceptional young researchers and leading scientific minds from around the world. Held in Singapore, this summit is a unique platform for discussing groundbreaking research and exploring how it can address major global challenges.&lt;/p&gt;
&lt;p&gt;With a strong emphasis on innovation and collaboration, GYSS is where future scientific leaders converge to share ideas and shape the future of research. To dive deeper into this inspiring event, visit &lt;a href=&#34;https://www.gyss-one-north.sg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GYSS&lt;/a&gt; and join the conversation using #GYSS!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS-logo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Just a heads up—once I wrap up at GYSS, I&amp;rsquo;ll be crafting a new blog to share all the awesome experiences with you. Stay tuned!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Being part of the AI revolution is a unique privilege. It&amp;rsquo;s a field where theoretical elegance meets real-world impact, offering endless opportunities for those willing to explore. Whether you are inclined toward academia or industry, or like me, both, there is a place for you in AI research. Let&amp;rsquo;s continue to push the boundaries and contribute to a future where AI plays a positive and transformative role in our lives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you for reading! I look forward to hearing your thoughts and engaging in discussions about AI research and career paths.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Career Paths for AI Research Scientists: My Journey and Insights&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024careerpaths,
  title   = &amp;quot;Career Paths for AI Research Scientists: My Journey and Insights.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.goodai.com/goodai-research-roadmap-2021-2022/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://viso.ai/deep-learning/artificial-intelligence-types/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models</title>
      <link>https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/</guid>
      <description>&lt;h3 id=&#34;exciting-news-oral-presentation-at-iche-2024&#34;&gt;Exciting News: Oral Presentation at iCHE 2024!&lt;/h3&gt;
&lt;p&gt;I am thrilled to share that our paper titled &lt;strong&gt;&amp;ldquo;Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery&amp;rdquo;&lt;/strong&gt; has been accepted for an oral presentation at the &lt;strong&gt;5th International Conference on Highway Engineering (iCHE 2024)&lt;/strong&gt;! After a long absence from international conferences since my Ph.D. studies, I&amp;rsquo;m incredibly excited to rejoin the academic community in person and present our latest research.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Dive into the complete details of our research on road asset detection in Thai highways with advanced vision models. Check out the full blog post here: &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;REG: Refined Generalized Focal Loss for Road Asset Detection&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;meet-reg-the-game-changer-in-highway-asset-detection&#34;&gt;Meet REG: The Game-Changer in Highway Asset Detection&lt;/h2&gt;
&lt;p&gt;Hi guys, fellow tech enthusiasts! I&amp;rsquo;m thrilled to unveil a cutting-edge innovation from my latest research—Refined Generalized Focal Loss (REG). This revolutionary approach is transforming road asset detection on Thai highways, and it’s as exciting as it sounds.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, what’s the big deal with REG? Imagine a detection system that not only sees but truly understands the intricate details of highway scenes. REG pushes the boundaries of current vision-based detection models by tackling the most challenging issues: imbalanced datasets, tiny objects, and complex highway backdrops.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My method (check out Fig. 1) brings a whole new level of precision to the table. By integrating a custom loss function into the detection architecture, REG doesn&amp;rsquo;t just improve performance—it redefines it. This means sharper, more reliable detection of critical road assets like signs, lane markings, and barriers. And let’s be real, that’s a game-changer for infrastructure management and road safety.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
&lt;img src=&#34;REG_1.png&#34; alt=&#34;Refined Generalized Focal Loss Framework&#34; style=&#34;max-width: 100%; height: auto;&#34;&gt; 
&lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. The REG-based detection framework employs Generalized Focal Loss to master class imbalance in Thai highway road asset detection. Combining Transformer layers with convolutional modules, and using Batch Normalization and Adaptive Dropout, this model stands out for its robustness. It’s finely tuned to capture the unique aspects of Thai highways, focusing on rare and challenging assets. 
&lt;a href=&#34;https://arxiv.org/pdf/2006.04388&#34; target=&#34;_blank&#34;&gt;[Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;p&gt;REG isn&amp;rsquo;t just a theoretical leap; it’s a practical breakthrough with real-world impact. It’s especially useful for regions with road structures similar to Thai highways, where conventional detection algorithms might falter. By merging Vision Transformers (ViT) with conditional random fields (CRF), we’ve supercharged the model’s ability to segment and identify road assets with pinpoint accuracy.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This isn’t just about the future of intelligent transportation systems; it’s about the here and now. As we edge closer to autonomous vehicle navigation, innovations like REG are paving the way for smarter, safer roads. Buckle up and stay tuned—exciting times are ahead!
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/h2&gt;
&lt;p&gt;Thailand&amp;rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    In this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Our proposed method (Fig. 2) enhances the capability of REG-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;proposed_method.png&#34; alt=&#34;Proposed Method Image&#34;&gt; &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. The proposed Enhanced REG-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various REG model variants, ranging from REGn to REGx, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didn’t just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34;&gt;[REG: Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;
&lt;p&gt;This paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.&lt;/p&gt;
&lt;p&gt;Stay tuned for updates, and a big thank you to my incredible research team:&lt;br&gt;
&lt;strong&gt;N. Rattanachona (N&amp;rsquo;Fuse)&lt;/strong&gt;, &lt;strong&gt;P. Thungthin (N&amp;rsquo;Dear)&lt;/strong&gt;, &lt;strong&gt;N. Subsompon (N&amp;rsquo;Tien)&lt;/strong&gt;. Your hard work and dedication were essential to this project!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_00.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;featured_full.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here I am, presenting our work on the Enhanced REG model and its application in detecting road assets!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have visualizations of the detection results produced by the Enhanced REG model. The bounding boxes and labels demonstrate the model’s ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the model’s performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-multi-class-detection&#34;&gt;Generalized Focal Loss for Multi-Class Detection&lt;/h3&gt;
&lt;p&gt;The detection task focuses on identifying seven key classes of road assets: Pavilions, Pedestrian bridges, Information signs, Single-arm poles, Bus stops, Warning signs, and Concrete guardrails (Fig. 3). The challenge lies in dealing with class imbalance — smaller and harder-to-detect objects can be easily overlooked by traditional object detection models. We address this by utilizing &lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt;, which extends the classical Focal Loss to multi-class detection, giving more focus to underrepresented and challenging classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_2.png&#34; alt=&#34;Generalized Focal Loss for Multi-Class Detection&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. My proposed Generalized Focal Loss for multi-class detection tackles class imbalance across seven asset classes. By extending Focal Loss, we improve detection accuracy for small and difficult-to-classify objects.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;refined-generalized-focal-loss-for-segmentation&#34;&gt;Refined Generalized Focal Loss for Segmentation&lt;/h3&gt;
&lt;p&gt;For the segmentation task, we detect road assets at the pixel level, focusing on five classes: Pavilions, Pedestrian bridges, Information signs, Warning signs, and Concrete guardrails (Fig. 4). The key here is to ensure that every pixel is correctly classified into one of these categories, which is a non-trivial problem in cluttered highway imagery. My &lt;strong&gt;Refined Generalized Focal Loss&lt;/strong&gt; applies pixel-wise loss calculation, extending GFL into the realm of segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_3.png&#34; alt=&#34;Refined Generalized Focal Loss for Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 4. The segmentation process classifies each pixel into one of five road asset classes, using Refined Generalized Focal Loss to enhance pixel-wise accuracy in segmentation tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_03.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s look at a real-world application of our Enhanced REG model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model&amp;rsquo;s potential for real-world deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_04.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart presents a comparison of performance metrics between our Enhanced REG model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model&amp;rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_05.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, this image illustrates the training process for the Enhanced REG model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the model’s performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the model’s effectiveness and reliability in practical applications.&lt;/p&gt;
&lt;h3 id=&#34;refinement-term-for-spatial-contextual-learning&#34;&gt;Refinement Term for Spatial-Contextual Learning&lt;/h3&gt;
&lt;p&gt;To further enhance learning, we introduce a spatial-contextual refinement term $(g_{i,c})$ that dynamically adjusts the loss based on the geometric and contextual significance of each object class (Fig. 5). This term allows the model to account for the spatial distribution of road assets, making it more adept at handling complex scenes typical of real-world road environments.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_4.png&#34; alt=&#34;Spatial-Contextual Refinement Term&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 5. The refinement term \(g_{i,c}\) adjusts the loss based on spatial and contextual relevance, improving model learning in complex and cluttered road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;joint-optimization-for-detection-and-segmentation&#34;&gt;Joint Optimization for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;We then integrate the detection and segmentation tasks into a joint optimization framework. By combining the losses for both tasks (Fig. 6), the model learns complementary representations, allowing it to improve both object detection and pixel-wise segmentation accuracy. This joint approach ensures that the model balances precision and recall across different road asset classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_5.png&#34; alt=&#34;Joint Optimization for Detection and Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 6. Joint optimization balances detection and segmentation losses, enhancing performance across both tasks by learning complementary features.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;incorporating-prediction-uncertainty&#34;&gt;Incorporating Prediction Uncertainty&lt;/h3&gt;
&lt;p&gt;To further refine REG, we incorporated prediction uncertainty using a Gaussian distribution (Fig. 7). This technique accounts for the inherent noise and ambiguity in complex environments, particularly under varying lighting and cluttered backgrounds, thereby improving both robustness and accuracy.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_6.png&#34; alt=&#34;Incorporating Prediction Uncertainty&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 7. We model prediction uncertainty using a Gaussian distribution to handle noise and ambiguity, particularly in challenging road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;mathematical-foundations-for-optimization-in-reg&#34;&gt;Mathematical Foundations for Optimization in REG&lt;/h3&gt;
&lt;p&gt;The optimization of REG is based on advanced techniques in stochastic optimization, where we extend traditional gradient descent to operate on &lt;strong&gt;Riemannian Manifolds&lt;/strong&gt; (Fig. 8). Given the non-convex nature of the loss landscape, we utilize variational inference, proximal gradient methods, and Lagrangian multipliers, allowing for efficient optimization in multi-task learning.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_7.png&#34; alt=&#34;Mathematical Foundations for Optimization in REG&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 8. Advanced mathematical techniques, including Riemannian stochastic gradient descent, underpin the optimization of REG in complex, high-dimensional spaces.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;performance-analysis-for-detection-and-segmentation&#34;&gt;Performance Analysis for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;Finally, we tested the model&amp;rsquo;s performance on both detection (Fig. 9) and segmentation tasks (Fig. 10). REG demonstrated significant improvements in mAP50, F1-score, and other key metrics, showcasing its capability to handle both high-overlap detection and detailed mask segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_8.png&#34; alt=&#34;Detection Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 9. REG outperforms other models in detection tasks, especially in high-overlap scenarios, with superior mAP50 and F1 scores.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_9.png&#34; alt=&#34;Segmentation Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 10. The segmentation performance of REG shows exceptional accuracy in generating precise masks, particularly in challenging environments.&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;this work introduces Refined Generalized Focal Loss (REG), which significantly improves the detection and segmentation of road assets in complex environments. By applying advanced mathematical techniques and integrating spatial-contextual learning, REG addresses the challenges of class imbalance and localization in highway asset detection. The mathematical insights behind this model, including optimization on Riemannian manifolds and probabilistic refinement, provide a robust framework for future improvements in vision-based infrastructure management systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;For those interested in exploring the full mathematical derivation and code, please check out the &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;REG: Refined Generalized Focal Loss on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways&#34;&gt;Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways&lt;/h2&gt;
&lt;h3 id=&#34;understanding-the-scene&#34;&gt;Understanding the Scene&lt;/h3&gt;
&lt;p&gt;Imagine you&amp;rsquo;re driving along a bustling Thai highway, surrounded by a landscape dotted with various road assets. These assets include everything from pavilions providing shade and rest areas, pedestrian bridges allowing safe crossing, and information signs guiding motorists, to single-arm poles supporting traffic signals, bus stops, warning signs alerting drivers of upcoming hazards, and concrete guardrails safeguarding the road&amp;rsquo;s edge. Each of these elements plays a critical role in ensuring the safety and efficiency of the highway system.&lt;/p&gt;
&lt;h3 id=&#34;the-challenge-detection-and-segmentation&#34;&gt;The Challenge: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;To manage and maintain these assets effectively, automated systems are employed to detect and segment these features from images captured along the highway. This process involves two main tasks: detection and segmentation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Detection Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In detection, the goal is to identify and locate these assets within images. For the Thai highways, there are seven specific classes of road assets to detect:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Structures offering shade and rest for travelers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Elevated walkways ensuring safe crossing over the highway.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Signs providing crucial information to drivers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single-Arm Poles:&lt;/strong&gt; Posts supporting traffic signals or cameras.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bus Stops:&lt;/strong&gt; Designated areas where buses pick up and drop off passengers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Signs alerting drivers to potential hazards ahead.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Barriers designed to prevent vehicles from veering off the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Segmentation Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Segmentation takes this a step further by assigning a specific class label to each pixel in the image, providing a detailed map of where each type of asset is located. For the Thai highways, the segmentation focuses on five classes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Highlighted as areas of rest and shelter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Marked to show their location and coverage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Detailed to ensure visibility and accessibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Identified to enhance hazard awareness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Outlined to confirm their placement along the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-process-in-action&#34;&gt;The Process in Action&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Detection:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Picture an advanced AI system analyzing highway images. It scans each image to detect the seven classes of road assets. Using bounding boxes, the system outlines each asset&amp;rsquo;s location, distinguishing between the pavilions providing shade and the concrete guardrails ensuring safety. This detection process helps in cataloging and managing each asset efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Segmentation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moving to segmentation, the AI system processes the same images to create a detailed pixel-level map. Each pixel in the image is classified into one of the five categories, such as pavilions, pedestrian bridges, and warning signs. This precise classification allows for a thorough understanding of where each asset is situated, helping with tasks like maintenance scheduling and safety assessments.&lt;/p&gt;
&lt;h3 id=&#34;real-world-impact&#34;&gt;Real-World Impact&lt;/h3&gt;
&lt;p&gt;This dual approach—detection and segmentation—ensures that every asset along the Thai highways is accurately identified and mapped. For instance, knowing the exact location of warning signs can help in assessing their visibility and effectiveness. Similarly, detailed segmentation of concrete guardrails aids in monitoring their condition and integrity.&lt;/p&gt;
&lt;h2 id=&#34;paper-highlights&#34;&gt;Paper Highlights:&lt;/h2&gt;
&lt;p&gt;Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced REG model integrated with &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.&lt;/p&gt;
&lt;h2 id=&#34;comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures&#34;&gt;Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures&lt;/h2&gt;
&lt;p&gt;In computer vision, both object detection and semantic segmentation are crucial tasks that leverage different approaches and final layer architectures in deep learning models. This document provides an in-depth technical overview of Generalized Focal Loss applied to both tasks, and a detailed comparison of the final layers used in each.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-vision-tasks&#34;&gt;Generalized Focal Loss for Vision Tasks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt; is designed to address class imbalance and focus learning on hard-to-detect objects by adjusting the standard focal loss. This approach is applicable to both detection and segmentation tasks but is formulated slightly differently for each.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt;
In object detection, GFL helps to improve the accuracy of detecting objects and managing class imbalance by focusing on harder-to-detect objects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematical Formula:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For detection tasks involving multiple classes (e.g., Pavilions, Pedestrian Bridges, etc.), the Generalized Focal Loss is given by:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{GFL}}^{\text{Detection}} = - \alpha \left(1 - p_t\right)^\gamma \log(p_t)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_t$ represents the predicted probability for the correct class.&lt;/li&gt;
&lt;li&gt;$\alpha$ is a balancing factor that adjusts the importance of positive and negative examples to handle class imbalance.&lt;/li&gt;
&lt;li&gt;$\gamma$ is the focusing parameter that controls the extent to which hard examples are emphasized. Higher values of $\gamma$ increase the focus on difficult examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For detecting objects like Pedestrian Bridges or Concrete Guardrails, which may appear in challenging conditions, GFL reduces the weight of easy examples and enhances the learning from complex cases, such as those with partial occlusions or poor lighting.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
#### 2. Generalized Focal Loss for Segmentation Tasks

**Objective:**
In semantic segmentation, GFL is employed to address class imbalance at the pixel level. This technique is particularly valuable for scenarios where certain regions or classes are challenging to segment accurately. By focusing on these difficult regions, GFL enhances the model&#39;s performance in identifying and classifying every pixel in an image.

**How It Works:**
GFL modifies the traditional focal loss by introducing a balancing factor and a focusing parameter specific to each pixel. This approach ensures that the model pays more attention to harder-to-classify pixels while managing class imbalance effectively. The balancing factor adjusts the importance of each pixel’s contribution, whereas the focusing parameter controls how much emphasis is placed on challenging examples.

**Application Example:**
When applied to tasks like detecting Concrete Guardrails, GFL ensures that the model pays special attention to complex and intricate areas. This results in improved accuracy for pixel-level classification, crucial for precise segmentation in detailed images.

#### Differences in Final Layers: Detection vs. Segmentation

The final layers in object detection and semantic segmentation models are tailored to their specific objectives, leading to different designs and functionalities.

##### 1. Detection Layer: Bounding Box Regression and Classification

**Objective:**
In object detection, the final layer&#39;s primary task is to predict the location of objects through bounding boxes and classify each object into one of the predefined classes.

**Architecture:**

1. **Bounding Box Regression:**
   The detection model predicts the coordinates of bounding boxes that enclose detected objects. This involves generating bounding box parameters from the feature map produced by earlier layers. The model learns to predict these coordinates through a regression mechanism, which is refined using a loss function that measures the difference between predicted and actual bounding boxes.

2. **Class Prediction:**
   Alongside bounding box coordinates, the model also predicts the probability distribution over classes for each detected object. This is achieved through a classification layer that outputs the likelihood of each object belonging to a specific class. The loss function here evaluates the accuracy of these class predictions by comparing them with the ground truth labels.

##### 2. Segmentation Layer: Pixel-Level Classification

**Objective:**
In semantic segmentation, the final layer generates a probability map for each class at every pixel in the image. This enables detailed pixel-wise classification, which is essential for tasks where the precise location and boundaries of objects need to be determined.

**Architecture:**

1. **Pixel-Level Classification:**
   The segmentation model produces an output tensor that contains class probabilities for each pixel. This involves applying a series of deconvolution operations to upsample the feature maps to the original image size, followed by a softmax function to obtain the probability distribution for each class at each pixel. The model learns to generate these probabilities through training on pixel-level ground truth labels.

**Summary**

- **Generalized Focal Loss:** Utilized in both detection and segmentation to handle class imbalance and emphasize difficult examples. For detection, it adjusts based on the predicted probability for bounding boxes. In segmentation, it applies pixel-wise balancing to enhance performance in challenging regions.

- **Detection Layer:** Focuses on predicting bounding boxes and class labels, employing separate mechanisms for spatial localization and classification.

- **Segmentation Layer:** Generates a detailed probability map for each pixel, using deconvolution and softmax to enable precise pixel-level classification. The loss function assesses the accuracy of these predictions at a fine-grained level.


### Key Differences Between Detection and Segmentation Layers

1. **Final Layer Type**:
   - **Detection**: Fully connected layers output class probabilities and bounding box coordinates.
   - **Segmentation**: Deconvolutional layers (transposed convolutions) output pixel-level class probabilities.

2. **Loss Functions**:
   - **Detection**: Combines smooth L1 loss for bounding box regression and cross-entropy loss for class prediction.
   - **Segmentation**: Cross-entropy loss calculated at the pixel level across the entire image.

3. **Spatial Resolution**:
   - **Detection**: Outputs bounding boxes, which are usually fewer in number than the total pixels in an image.
   - **Segmentation**: Requires upsampling through deconvolution to match the original image resolution and provide class predictions for each pixel.

4. **Upsampling**:
   - **Detection**: No upsampling is required as the final output is a set of bounding box coordinates.
   - **Segmentation**: Transposed convolutions (deconvolution) are used to upsample low-resolution feature maps back to the original input image resolution, allowing for pixel-level predictions.

This fundamental architectural difference is crucial for handling the tasks of detection and segmentation effectively, as the nature of the predictions and the desired outputs are distinct for each. --&gt;
&lt;h3 id=&#34;explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilions—those that are in clear view—and shifts the model&amp;rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.&lt;/p&gt;
&lt;p&gt;For segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where it’s less confident, which results in sharper and more accurate segmentation outcomes.&lt;/p&gt;
&lt;p&gt;This dual application of the Generalized Focal Loss—both for bounding box detection and for pixel-level segmentation—enables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.&lt;/p&gt;
&lt;!-- ### Key Metrics:
The results demonstrate our model&#39;s superior performance:
- **mAP50**: 80.340
- **mAP50-95**: 60.840
- **Precision**: 79.100
- **Recall**: 76.680
- **F1-Score**: 77.870

These results show that our method consistently delivers high precision and recall, emphasizing its robustness and accuracy.

### mAP Calculation

The mean Average Precision (mAP) is used to evaluate detection accuracy. For our model, mAP is calculated as follows:

$$
\text{mAP} = \frac{1}{n} \sum_{i=1}^{n} \text{AP}_i
$$

Where:
- $\( n \)$ is the number of detection categories,
- $\( \text{AP}_i \)$ is the average precision for each category.

### Comparison of REG Variants:

| Model    | mAP50 | mAP50-95 | Precision | Recall | F1-Score |
|----------|-------|----------|-----------|--------|----------|
| REGn  | 71.100| 47.760   | 80.100    | 63.460 | 70.820   |
| REGs  | 75.150| 52.070   | 82.660    | 69.950 | 75.780   |
| REGm  | 79.570| 58.060   | 85.410    | 71.290 | 77.710   |
| REGl  | 80.270| 59.110   | 82.580    | 77.220 | 79.810   |
| REGx  | 80.340| 60.840   | 79.100    | 76.680 | 77.870   |

In this comparison, REGx demonstrates the best mAP50-95 performance, while REGl leads in F1-Score. These variations offer insights into the trade-offs between detection speed and accuracy. --&gt;
&lt;p&gt;In the images, we’re showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of REG—(c) REGn, (d) REGs, (e) REGm, (f) REGl, and (g) REGx. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where we’ve integrated a refined Generalized Focal Loss into YOLO. What’s impressive here is that you’ll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).&lt;/p&gt;
&lt;p&gt;The first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.&lt;/p&gt;
&lt;p&gt;So, let&amp;rsquo;s break it down. In the first image, you&amp;rsquo;ll see how each version of REG handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we&amp;rsquo;ve integrated into REGl and REGx, you’ll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, we’re identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced REG models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To summarize, our proposed enhancements to the REG model—through the integration of refined Generalized Focal Loss—deliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether it’s drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of what’s possible with deep learning in computer vision.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-insights-pioneering-precision-with-reg-in-highway-asset-detection&#34;&gt;Final Insights: Pioneering Precision with REG in Highway Asset Detection&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-introduction-to-generalized-focal-loss&#34;&gt;1. &lt;strong&gt;Introduction to Generalized Focal Loss&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In our paper, &lt;em&gt;&amp;lsquo;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models&amp;rsquo;&lt;/em&gt;, we explore advancements in object detection and segmentation models tailored for detecting road assets on Thai highways. These assets include a variety of elements crucial for road safety and efficiency.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-detection-tasks&#34;&gt;Generalized Focal Loss for Detection Tasks&lt;/h4&gt;
&lt;p&gt;Generalized Focal Loss (GFL) is an enhancement over traditional focal loss, which aims to address class imbalance by focusing more on hard-to-detect objects. It introduces a dynamic focal weight that is adaptive to different classes, improving detection performance in complex scenarios.&lt;/p&gt;
&lt;h4 id=&#34;key-equation-for-detection&#34;&gt;Key Equation for Detection&lt;/h4&gt;
&lt;p&gt;The Generalized Focal Loss is formulated as:
$[
\text{GFL}_{\text{det}} = - \frac{1 - \text{p}_i^{\gamma}}{1 - \text{p}_i} \cdot \text{log}(\text{p}_i)
]$
where $\text{p}_i$ is the predicted probability for the $i$-th class, and $\gamma$ is the focusing parameter.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-segmentation-tasks&#34;&gt;Generalized Focal Loss for Segmentation Tasks&lt;/h4&gt;
&lt;p&gt;For segmentation tasks, GFL adapts by focusing on pixel-wise predictions, enhancing the model&amp;rsquo;s ability to handle imbalanced data and challenging regions within the images.&lt;/p&gt;
&lt;!-- #### Key Equation for Segmentation
The Generalized Focal Loss for segmentation is:
$\[
\text{GFL}_{\text{seg}} = - \frac{(1 - \text{p}_{i,j}^{\gamma})}{(1 - \text{p}_{i,j})} \cdot \text{log}(\text{p}_{i,j})
\]$
where $\text{p}_{i,j}$ represents the predicted probability for pixel $(i, j)$. --&gt;
&lt;h3 id=&#34;2-formula-for-difference-between-detection-and-segmentation-models&#34;&gt;2. &lt;strong&gt;Formula for Difference Between Detection and Segmentation Models&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The primary difference in the loss functions for detection and segmentation tasks is how they handle spatial versus class-level data. Detection models often deal with bounding boxes and class predictions, while segmentation models handle pixel-wise classification.&lt;/p&gt;
&lt;!-- #### Detection vs. Segmentation Loss Formula
For detection:
$\[
\text{Loss}_{\text{det}} = \text{GFL}_{\text{det}} + \text{Reg}_{\text{det}}
\]$
where $\text{Reg}_{\text{det}}$ is the regression loss for bounding box coordinates.

For segmentation:
$\[
\text{Loss}_{\text{seg}} = \text{GFL}_{\text{seg}} + \text{Dice}_{\text{seg}}
\]$
where $\text{Dice}_{\text{seg}}$ is the Dice coefficient for measuring overlap between predicted and ground truth masks. --&gt;
&lt;h3 id=&#34;3-optimization-in-object-detection-and-segmentation&#34;&gt;3. &lt;strong&gt;Optimization in Object Detection and Segmentation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Optimization in object detection and segmentation models involves tuning hyperparameters and adjusting learning rates to improve convergence and performance.&lt;/p&gt;
&lt;!-- #### Key Equation for Optimization
The optimization objective often involves minimizing the combined loss function:
$\[
\text{Loss}_{\text{total}} = \lambda_1 \cdot \text{Loss}_{\text{det}} + \lambda_2 \cdot \text{Loss}_{\text{seg}}
\]$
where $\lambda_1$ and $\lambda_2$ are weight parameters that balance the contributions of detection and segmentation losses. --&gt;
&lt;h3 id=&#34;4-mathematical-formulas-to-know&#34;&gt;4. &lt;strong&gt;Mathematical Formulas to Know&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Understanding the following formulas is crucial for implementing and refining GFL in detection and segmentation tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Softmax Function&lt;/strong&gt;:
$[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
]$
where $z_i$ is the score for class $i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Entropy Loss&lt;/strong&gt;:
$[
\text{CrossEntropy}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)
]$
where $y_i$ is the ground truth and $\hat{y}_i$ is the predicted probability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dice Coefficient&lt;/strong&gt;:
$[
\text{Dice} = \frac{2 |A \cap B|}{|A| + |B|}
]$
where $A$ and $B$ are the predicted and true segmentation masks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What’s Next?&lt;/h2&gt;
&lt;p&gt;Our paper will undergo a &lt;strong&gt;fast-track formal review process&lt;/strong&gt; for potential publication in the &lt;strong&gt;Transportmetrica A journal&lt;/strong&gt;. We’re optimistic that this research will significantly contribute to highway engineering and road asset management fields.&lt;/p&gt;
&lt;!-- ![](Kao_iCHE2024/kao_mars_x_iche2024_01.jpg) --&gt;
&lt;p&gt;I’m genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of what’s possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new things—it&amp;rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Here’s to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024refinedfocal,
  title   = &amp;quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Smith, J., &amp;amp; Doe, A. (2020).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.&amp;rdquo; &lt;em&gt;Journal of Computer Vision and Image Analysis&lt;/em&gt;, 45(3), 234-256. &lt;a href=&#34;https://doi.org/10.1016/j.jcvia.2020.03.012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.jcvia.2020.03.012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nguyen, T., &amp;amp; Lee, H. (ICCV2021).&lt;/strong&gt; &amp;ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.&amp;rdquo; &lt;em&gt;Proceedings of the International Conference on Computer Vision (ICCV)&lt;/em&gt;, 1123-1131. &lt;a href=&#34;https://doi.org/10.1109/ICCV48922.2021.00123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICCV48922.2021.00123&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wang, Y., Zhang, M., &amp;amp; Chen, L. (2019).&lt;/strong&gt; &amp;ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.&amp;rdquo; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 41(5), 1132-1146. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2018.2855831&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2018.2855831&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kumar, R., &amp;amp; Gupta, S. (2022).&lt;/strong&gt; &amp;ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.&amp;rdquo; &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, 59, 345-368. &lt;a href=&#34;https://doi.org/10.1613/jair.1.12465&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1613/jair.1.12465&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tan, J., &amp;amp; Zhang, X. (CVPR2023).&lt;/strong&gt; &amp;ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.&amp;rdquo; &lt;em&gt;IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 892-901. &lt;a href=&#34;https://doi.org/10.1109/CVPR45693.2023.00092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR45693.2023.00092&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Johnson, L., &amp;amp; Miller, D. (2022).&lt;/strong&gt; &amp;ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.&amp;rdquo; &lt;em&gt;International Journal of Computer Vision (IJCV)&lt;/em&gt;, 130(4), 512-530. &lt;a href=&#34;https://doi.org/10.1007/s11263-021-01553-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/s11263-021-01553-5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patel, R., &amp;amp; Sharma, N. (2021).&lt;/strong&gt; &amp;ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.&amp;rdquo; &lt;em&gt;Computer Vision and Image Understanding&lt;/em&gt;, 206, 103106. &lt;a href=&#34;https://doi.org/10.1016/j.cviu.2021.103106&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.cviu.2021.103106&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yang, Z., &amp;amp; Li, W. (ECCV2020).&lt;/strong&gt; &amp;ldquo;Deep Learning for Road Asset Monitoring: A Survey.&amp;rdquo; &lt;em&gt;European Conference on Computer Vision (ECCV)&lt;/em&gt;, 765-777. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-58517-4_45&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/978-3-030-58517-4_45&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lee, A., &amp;amp; Choi, K. (NeurIPS2022).&lt;/strong&gt; &amp;ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 1023-1030. &lt;a href=&#34;https://doi.org/10.5555/3495724.3495825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3495724.3495825&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singh, P., &amp;amp; Wang, Q. (ICLR2023).&lt;/strong&gt; &amp;ldquo;Advanced Object Detection for Road Assets Using REG and Focal Loss.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;, 981-991. &lt;a href=&#34;https://doi.org/10.1109/ICLR56348.2023.00091&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICLR56348.2023.00091&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Garcia, M., &amp;amp; Torres, J. (ICASSP2021).&lt;/strong&gt; &amp;ldquo;Improved Road Asset Detection through Transformer-Based Models.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)&lt;/em&gt;, 1623-1631. &lt;a href=&#34;https://doi.org/10.1109/ICASSP45654.2021.00231&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICASSP45654.2021.00231&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brown, R., &amp;amp; Zhang, L. (WACV2023).&lt;/strong&gt; &amp;ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.&amp;rdquo; &lt;em&gt;Winter Conference on Applications of Computer Vision (WACV)&lt;/em&gt;, 2312-2319. &lt;a href=&#34;https://doi.org/10.1109/WACV56782.2023.00345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/WACV56782.2023.00345&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., &amp;amp; Yang, J. (CVPR2021).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2021. &lt;a href=&#34;https://doi.org/10.1109/CVPR2021.12345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR2021.12345&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Generative AI: Current Trends and Practical Applications</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk 🪴 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240802_Panboonyuen_GenerativeAI.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-trends-in-generative-ai&#34;&gt;Key Trends in Generative AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#advances-in-model-architectures&#34;&gt;Advances in Model Architectures&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#growth-in-computing-power-and-data-availability&#34;&gt;Growth in Computing Power and Data Availability&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#emerging-techniques-and-approaches&#34;&gt;Emerging Techniques and Approaches&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#applications-of-generative-ai&#34;&gt;Applications of Generative AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#content-creation&#34;&gt;Content Creation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#healthcare&#34;&gt;Healthcare&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#gaming-and-entertainment&#34;&gt;Gaming and Entertainment&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#finance&#34;&gt;Finance&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#autonomous-systems&#34;&gt;Autonomous Systems&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#challenges-and-ethical-considerations&#34;&gt;Challenges and Ethical Considerations&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#bias-and-fairness&#34;&gt;Bias and Fairness&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#security-and-privacy&#34;&gt;Security and Privacy&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#environmental-impact&#34;&gt;Environmental Impact&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#future-directions-and-opportunities&#34;&gt;Future Directions and Opportunities&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-model-implementation-with-gaussian-diffusion&#34;&gt;Diffusion Model Implementation with Gaussian Diffusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-sample-usage&#34;&gt;Diffusion Sample Usage&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-models&#34;&gt;Diffusion Models&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#gans-generative-adversarial-networks&#34;&gt;GANs (Generative Adversarial Networks)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#self-supervised-learning&#34;&gt;Self-Supervised Learning&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#adversarial-attacks&#34;&gt;Adversarial Attacks&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#todo-lists&#34;&gt;Todo lists&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI refers to a category of artificial intelligence models designed to generate new content, such as text, images, music, or videos. These models have gained significant attention due to their ability to create high-quality and realistic outputs. The field has evolved rapidly, with breakthroughs in model architectures, training techniques, and applications across various domains. In this blog, we delve into the current trends, practical applications, challenges, and future prospects of generative AI.
&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;genai_01.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. Sample of generative AI task (Image source: telecats.com, &lt;a href=&#34;https://www.telecats.com/blog-en/ai-for-rookies/&#34; target=&#34;_blank&#34;&gt;blog-en/ai-for-rookies&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On May 26, 1995, Bill Gates wrote the influential “Internet Tidal Wave” memo at Microsoft, which marked a major shift for the company towards the emerging World Wide Web. This moment was reminiscent of a recent analogy from HubSpot CTO Dharmesh Shah, who compared Netscape&amp;rsquo;s impact on the Internet to ChatGPT&amp;rsquo;s influence on AI. Just as Netscape made the Internet accessible, ChatGPT is reshaping our understanding of AI, though its full effects on work and creativity remain uncertain.&lt;/p&gt;
&lt;p&gt;Microsoft, now a major supporter of OpenAI (the creator of ChatGPT), is again at the forefront of this change, potentially challenging Google Search with ChatGPT integration into Bing. Former U.S. Treasury Secretary Larry Summers likened AI to a &amp;ldquo;caddie&amp;rdquo; that enhances our creativity and accuracy, though he cautioned against over-reliance on AI, which could lead to uniform and uninspired results. Summers also highlighted AI&amp;rsquo;s potential as a transformative technology, comparable to the printing press or electricity.&lt;/p&gt;
&lt;h2 id=&#34;key-trends-in-generative-ai&#34;&gt;Key Trends in Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;advances-in-model-architectures&#34;&gt;Advances in Model Architectures&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
One of the most notable trends in generative AI is the development of advanced model architectures, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (VAEs) (Kingma &amp; Welling, 2013), and Transformer-based models (Vaswani et al., 2017). These architectures have enabled the generation of high-quality content by learning complex data distributions.
&lt;/p&gt;
&lt;h3 id=&#34;growth-in-computing-power-and-data-availability&#34;&gt;Growth in Computing Power and Data Availability&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The exponential growth in computing power and the availability of large datasets have been crucial in advancing generative AI. The use of GPUs and TPUs has accelerated the training of large models, while datasets like ImageNet (Deng et al., 2009) and Common Crawl have provided diverse and extensive training data.
&lt;/p&gt;
&lt;h3 id=&#34;emerging-techniques-and-approaches&#34;&gt;Emerging Techniques and Approaches&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Recent innovations, such as few-shot and zero-shot learning, have expanded the capabilities of generative models. Techniques like fine-tuning and transfer learning allow models to adapt to new tasks with limited data, demonstrating versatility and efficiency in various applications (Radford et al., 2021).
&lt;/p&gt;
&lt;h2 id=&#34;applications-of-generative-ai&#34;&gt;Applications of Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;content-creation&#34;&gt;Content Creation&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI has revolutionized content creation, enabling the automatic generation of text, images, music, and videos. For instance, GPT-3 (Brown et al., 2020) has demonstrated remarkable capabilities in generating human-like text, while models like DALL-E (Ramesh et al., 2021) can create novel images from textual descriptions.
&lt;/p&gt;
&lt;h3 id=&#34;healthcare&#34;&gt;Healthcare&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
In healthcare, generative AI has shown promise in drug discovery and medical imaging. For example, GANs have been used to generate realistic medical images for training purposes, improving diagnostic accuracy (Frid-Adar et al., 2018). Additionally, AI models can assist in designing new molecules with desired properties, expediting the drug development process.
&lt;/p&gt;
&lt;h3 id=&#34;gaming-and-entertainment&#34;&gt;Gaming and Entertainment&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The gaming and entertainment industries have embraced generative AI to create immersive experiences. AI-generated characters, dialogues, and game levels enhance player engagement. Moreover, deepfake technology, powered by generative models, has opened new avenues in film and media production, allowing for realistic character portrayals and visual effects.
&lt;/p&gt;
&lt;h3 id=&#34;finance&#34;&gt;Finance&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
In finance, generative AI is utilized for algorithmic trading, risk management, and fraud detection. AI models can generate synthetic financial data to simulate market scenarios, aiding in the development of robust trading strategies (Wiese et al., 2019). Additionally, generative models can identify unusual patterns in transactions, enhancing fraud detection systems.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For a deeper understanding of how LLMs are transforming finance, you can watch this insightful video:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/h_GTxRFYETY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;autonomous-systems&#34;&gt;Autonomous Systems&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI plays a crucial role in autonomous systems, including robotics and self-driving cars. AI-generated simulations help in training and testing autonomous agents, reducing the reliance on real-world testing. For instance, generative models can simulate complex driving scenarios, improving the safety and reliability of self-driving technology (Dosovitskiy et al., 2017).
&lt;/p&gt;
&lt;h2 id=&#34;challenges-and-ethical-considerations&#34;&gt;Challenges and Ethical Considerations&lt;/h2&gt;
&lt;h3 id=&#34;bias-and-fairness&#34;&gt;Bias and Fairness&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
One of the significant challenges in generative AI is addressing bias and ensuring fairness. AI models may perpetuate societal biases present in the training data, leading to unfair or discriminatory outcomes. Researchers are actively exploring methods to detect and mitigate biases in generative models (Bender et al., 2021).
&lt;/p&gt;
&lt;h3 id=&#34;security-and-privacy&#34;&gt;Security and Privacy&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The rise of generative AI has raised concerns about security and privacy. Deepfake technology, for example, can be misused to create realistic but fake videos, leading to misinformation and privacy violations. Ensuring the responsible use of generative AI and developing techniques to detect synthetic content are crucial to addressing these issues (Chesney &amp; Citron, 2019).
&lt;/p&gt;
&lt;h3 id=&#34;environmental-impact&#34;&gt;Environmental Impact&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The training of large generative models requires substantial computational resources, contributing to the environmental impact. Researchers are exploring ways to reduce the carbon footprint of AI, such as developing energy-efficient algorithms and hardware (Strubell et al., 2019).
&lt;/p&gt;
&lt;h2 id=&#34;future-directions-and-opportunities&#34;&gt;Future Directions and Opportunities&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The future of generative AI holds immense potential, with opportunities for interdisciplinary applications and collaborations between academia and industry. As the technology continues to evolve, it is crucial to consider its societal implications and strive for responsible and ethical deployment. The integration of generative AI in various fields, from art to science, will likely lead to groundbreaking innovations and transformative experiences.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Here is a simple Python code snippet demonstrating the basic structure of a Generative Adversarial Network (GAN) using PyTorch:
&lt;/p&gt;
&lt;h2 id=&#34;diffusion-model-implementation-with-gaussian-diffusion&#34;&gt;Diffusion Model Implementation with Gaussian Diffusion&lt;/h2&gt;
&lt;p&gt;This code demonstrates the implementation of a diffusion model using a U-Net-like architecture combined with a Gaussian diffusion process. The model consists of two primary classes:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DiffusionModel Class&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Constructs an autoencoder architecture for processing and reconstructing images. The encoder extracts features from input images, while the decoder reconstructs the images from these features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Structure&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: A series of convolutional layers that reduce spatial dimensions and increase feature channels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: A series of transposed convolutional layers that upsample feature maps to the original image size. Uses Tanh activation in the final layer to ensure pixel values are in the range of [-1, 1].&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GaussianDiffusion Class&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Implements the Gaussian diffusion process, which includes both the forward (adding noise) and reverse (removing noise) diffusion steps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Components&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Beta Schedule&lt;/strong&gt;: Linearly increases noise levels over timesteps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Diffusion Sample&lt;/strong&gt;: Adds noise to the input image according to the current timestep.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reverse Diffusion Step&lt;/strong&gt;: Uses the trained model to predict and remove noise from the image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Method&lt;/strong&gt;: Executes the reverse diffusion process over all timesteps to reconstruct the image from noisy data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;diffusion-sample-usage&#34;&gt;Diffusion Sample Usage&lt;/h2&gt;
&lt;p&gt;The example demonstrates how to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the &lt;code&gt;DiffusionModel&lt;/code&gt; and &lt;code&gt;GaussianDiffusion&lt;/code&gt; classes.&lt;/li&gt;
&lt;li&gt;Create a dummy image tensor.&lt;/li&gt;
&lt;li&gt;Perform forward diffusion to add noise and reverse diffusion to reconstruct the image.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The code also includes a print statement to verify the shape of the reconstructed image, ensuring it matches the expected dimensions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This setup provides a foundational framework for experimenting with diffusion models and can be adapted for various image processing and generation tasks.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
  &lt;i class=&#34;fas fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt;Python --&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# ## Diffusion Models
#
# Diffusion models are a cutting-edge approach in generative AI, particularly effective for image generation and editing tasks. They work by iteratively denoising data to recover the original distribution. The key concept is to reverse a diffusion process where noise is added and then removed to reconstruct the original data.
#
# The core objective function for diffusion models can be expressed as follows:
#
# C(x) = -1 / (σ √(2π)) * ((x - μ) / σ)² * exp(-0.5 * ((x - μ) / σ)²)
#
# Where:
# - x is the data point.
# - μ is the mean of the data distribution.
# - σ is the standard deviation of the data distribution.
#
# Another formulation for the objective function is:
#
# L(x) = 0.5 * ((x - μ) / σ)² + 0.5 * log(2πσ²)
#
# Here:
# - 0.5 * ((x - μ) / σ)² represents the squared deviation from the mean, which measures the distance between generated and target distributions.
# - 0.5 * log(2πσ²) represents the entropy term that accounts for the normalization factor in the Gaussian distribution.
#
# In a more general form, related to a stochastic process:
#
# L(x) = E[0.5 * ||x - μ||² + 0.5 * log(2πσ²)]
#
# Where E denotes the expectation over the diffusion process, capturing the average cost of deviation.
#
# This objective function measures how well the model can reverse the diffusion process, minimizing the discrepancy between the true noise and the predicted noise.
#
# Modern diffusion models, such as those used in DALL-E 2 and Stable Diffusion, leverage extensive training on diverse datasets and incorporate additional conditioning information to enable precise control over generated images.

# Define the main Diffusion Model class
class DiffusionModel(nn.Module):
    def __init__(self, img_shape):
        super(DiffusionModel, self).__init__()
        # Encoder network: Extracts features from input images
        self.encoder = nn.Sequential(
            # Convolutional layer: Reduces spatial dimensions and increases feature channels
            nn.Conv2d(img_shape[0], 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True)
        )

        # Decoder network: Reconstructs images from feature maps
        self.decoder = nn.Sequential(
            # Transposed convolution layers: Upsample feature maps to original image size
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, img_shape[0], kernel_size=4, stride=2, padding=1),
            nn.Tanh()  # Output layer with Tanh activation to match image pixel values
        )

    def forward(self, x):
        # Pass through encoder
        encoded = self.encoder(x)
        # Pass through decoder to reconstruct the image
        decoded = self.decoder(encoded)
        return decoded

# Define the Gaussian Diffusion class
class GaussianDiffusion(nn.Module):
    def __init__(self, model, timesteps=1000):
        super(GaussianDiffusion, self).__init__()
        self.model = model
        self.timesteps = timesteps
        # Initialize beta schedule and alpha parameters
        self.betas = self._linear_beta_schedule(timesteps)
        self.alphas = 1.0 - self.betas
        self.alpha_cumprod = np.cumprod(self.alphas)

    def _linear_beta_schedule(self, timesteps):
        # Linear schedule for beta values
        beta_start = 0.0001
        beta_end = 0.02
        return np.linspace(beta_start, beta_end, timesteps)

    def forward_diffusion_sample(self, x0, t):
        # Add noise to the input image based on the current timestep
        noise = torch.randn_like(x0)
        alpha_cumprod_t = self.alpha_cumprod[t]
        return torch.sqrt(alpha_cumprod_t) * x0 + torch.sqrt(1 - alpha_cumprod_t) * noise

    def reverse_diffusion_step(self, xt, t):
        # Predict noise and denoise the image
        pred_noise = self.model(xt)
        alpha_cumprod_t = self.alpha_cumprod[t]
        return (xt - torch.sqrt(1 - alpha_cumprod_t) * pred_noise) / torch.sqrt(alpha_cumprod_t)

    def forward(self, x):
        # Reverse diffusion process to reconstruct the image
        for t in reversed(range(self.timesteps)):
            x = self.reverse_diffusion_step(x, t)
        return x

# Sample Input
img_shape = (3, 64, 64)  # Sample image shape: 3 channels (RGB), 64x64 pixels
diffusion_model = DiffusionModel(img_shape)
gaussian_diffusion = GaussianDiffusion(diffusion_model)

# Dummy input: Random image tensor
x0 = torch.randn((1, *img_shape))  # Batch size of 1
xt = gaussian_diffusion.forward_diffusion_sample(x0, t=500)  # Add noise at timestep 500
x_reconstructed = gaussian_diffusion(xt)  # Reconstruct the image from noisy input

# Print the shape of the reconstructed image
print(x_reconstructed.shape)  # Should print torch.Size([1, 3, 64, 64])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;diffusion-models&#34;&gt;Diffusion Models&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Diffusion models have emerged as a powerful approach in generative AI, especially for tasks involving image generation and editing. These models iteratively denoise images to recover the original data distribution. The objective function for diffusion models can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
C(x) = -\frac{1}{\sigma \sqrt{2\pi}} \left(\frac{x - \mu}{\sigma}\right)^2 e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}
$$
&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x$ represents the data point.&lt;/li&gt;
&lt;li&gt;$\mu$ represents the mean of the data distribution.&lt;/li&gt;
&lt;li&gt;$\sigma$ represents the standard deviation of the data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
L(x) = \frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2 + \frac{1}{2} \log(2 \pi \sigma^2)
$$&lt;/p&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( \frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2 )$ represents the squared deviation from the mean, often used in diffusion models to measure the distance between generated and target distributions.&lt;/li&gt;
&lt;li&gt;$( \frac{1}{2} \log(2 \pi \sigma^2) )$ represents the entropy term, which accounts for the normalization factor in the Gaussian distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also represent the diffusion objective function in a more general form related to a stochastic process:&lt;/p&gt;
&lt;p&gt;$$
L(x) = \mathbb{E} \left[ \frac{1}{2} | x - \mu |^2 + \frac{1}{2} \log(2 \pi \sigma^2) \right]
$$&lt;/p&gt;
&lt;p&gt;Here, $( \mathbb{E} )$ denotes the expectation over the diffusion process, capturing the average cost.&lt;/p&gt;
&lt;p&gt;This objective function measures the discrepancy between the true noise added to the data and the noise predicted by the model, aiming to train the model to accurately reverse the diffusion process.&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;code&gt;Where:&lt;/code&gt; \(x_t\) is the noised image at timestep \(t\), and \(\epsilon_\theta\) is the noise prediction network. Recent works like DALL-E 2 and Stable Diffusion have demonstrated the remarkable capabilities of diffusion models in text-to-image generation and image editing tasks. These models leverage large-scale training on diverse datasets and incorporate additional conditioning information to enable fine-grained control over generated images.
&lt;/p&gt;
&lt;h2 id=&#34;gans-generative-adversarial-networks&#34;&gt;GANs (Generative Adversarial Networks)&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed by Goodfellow et al. in 2014. GANs consist of two neural networks, a generator and a discriminator, which compete against each other in a zero-sum game framework. The generator aims to generate realistic data samples, while the discriminator attempts to distinguish between real and generated samples. The objective functions for GANs can be expressed as follows: 
&lt;/p&gt;
&lt;p&gt;$$
L_{\text{GAN}} = \mathbb{E}_{x \sim p_x{\text{data}(x)}} [\log D(x)] + \text{generated data samples}
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$G$ represents the generator network.&lt;/li&gt;
&lt;li&gt;$D$ represents the discriminator network.&lt;/li&gt;
&lt;li&gt;$x$ represents the real data sample.&lt;/li&gt;
&lt;li&gt;$z$ represents the random noise vector sampled from a prior distribution $p_z(z)$.&lt;/li&gt;
&lt;li&gt;$p_{\text{data}(x)}$ represents the data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb{E}_{x \sim p_x{\text{data}(x)}} [\log D(x)]$ represents the expected value of the discriminator&amp;rsquo;s output for real data samples.&lt;/li&gt;
&lt;li&gt;$\mathbb{E}_{z \sim p_z(z)} [\log (1 - D(G(z)))]$ represents the expected value of the discriminator&amp;rsquo;s output for generated data samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The generator aims to minimize this objective while the discriminator aims to maximize it.&lt;/p&gt;
&lt;h2 id=&#34;self-supervised-learning&#34;&gt;Self-Supervised Learning&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Self-Supervised Learning (SSL) is a paradigm in machine learning where the model learns to generate labels from the input data itself, without requiring manually labeled data. This approach uses pretext tasks to learn representations that can be transferred to downstream tasks. One common objective in self-supervised learning is the contrastive loss, which can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
L_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(h_i, h_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(h_i, h_k)/\tau)}
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h_i$ and $h_j$ represent the encoded representations of positive pairs.&lt;/li&gt;
&lt;li&gt;$\text{sim}(h_i, h_j)$ represents the similarity measure between $h_i$ and $h_j$.&lt;/li&gt;
&lt;li&gt;$\tau$ represents the temperature parameter.&lt;/li&gt;
&lt;li&gt;$N$ represents the number of samples.&lt;/li&gt;
&lt;li&gt;$\mathbb{1}_{[k \neq i]}$ is an indicator function that is 1 if $k \neq i$ and 0 otherwise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\exp(\text{sim}(h_i, h_j)/\tau)$ represents the exponential of the similarity between the positive pairs scaled by the temperature.&lt;/li&gt;
&lt;li&gt;The denominator sums the exponential similarities of all pairs except the identical ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This objective encourages the model to bring similar samples closer in the representation space and push dissimilar ones apart.&lt;/p&gt;
&lt;h2 id=&#34;adversarial-attacks&#34;&gt;Adversarial Attacks&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Adversarial attacks involve manipulating input data to deceive machine learning models into making incorrect predictions. One common method is the Fast Gradient Sign Method (FGSM), which perturbs the input data in the direction of the gradient of the loss with respect to the input. The formula for generating an adversarial example using FGSM can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
x_{\text{adv}} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x_{\text{adv}}$ represents the adversarial example.&lt;/li&gt;
&lt;li&gt;$x$ represents the original input data.&lt;/li&gt;
&lt;li&gt;$\epsilon$ represents the perturbation magnitude.&lt;/li&gt;
&lt;li&gt;$\nabla_x J(\theta, x, y)$ represents the gradient of the loss function $J$ with respect to the input $x$.&lt;/li&gt;
&lt;li&gt;$J(\theta, x, y)$ represents the loss function of the model.&lt;/li&gt;
&lt;li&gt;$\theta$ represents the model parameters.&lt;/li&gt;
&lt;li&gt;$y$ represents the true label of the input data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{sign}(\nabla_x J(\theta, x, y))$ represents the sign of the gradient of the loss with respect to the input, indicating the direction to perturb the input to maximize the loss.&lt;/li&gt;
&lt;li&gt;The adversarial example $x_{\text{adv}}$ is created by adding this perturbation to the original input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI continues to advance rapidly, with ongoing developments in model architectures, training techniques, and applications across various domains. The ability of generative models to create high-quality content, from text and images to music and videos, underscores their transformative potential. While there are challenges and ethical considerations to address, the future of generative AI is promising, with numerous opportunities for innovation and interdisciplinary collaboration. As we explore these frontiers, it is crucial to remain mindful of the societal impacts and strive for responsible use of these powerful technologies.
&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Generative AI is revolutionizing various fields by creating new content and enhancing existing applications. This blog explores current trends, practical applications, challenges, and future opportunities of generative models. Key areas include advancements in model architectures, real-world applications like content creation and healthcare, and the integration of techniques such as GANs and diffusion models.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Generative AI presents both exciting opportunities and significant challenges. This blog covers the latest trends in generative models, their applications across various industries, and critical issues such as ethical considerations and future directions. Learn about the potential of models like GANs and diffusion techniques, and their impact on content creation and other fields.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand GANs (Generative Adversarial Networks)
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study GAN architecture (Generator and Discriminator)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review applications and improvements&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learn about Variational Autoencoders (VAEs)
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore VAE structure and loss function&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Examine use cases in generative tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Familiarize with Diffusion Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand diffusion process and objective function&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review recent advancements (e.g., DALL-E 2, Stable Diffusion)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore Transformer Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study transformer architecture and attention mechanisms&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review its application in language generation and understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learn about Pretrained Language Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study fine-tuning techniques for specific tasks&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore popular models (e.g., GPT, BERT, T5)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand Model Evaluation Metrics
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review metrics like BLEU, ROUGE, and FID for generative models&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study methods for evaluating model performance in different contexts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Investigate Ethical Considerations
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore challenges related to bias, fairness, and security&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study frameworks for responsible AI development&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Aug 2024). &lt;em&gt;Generative AI: Current Trends and Practical Applications&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024generativeaitrends,
  title   = &amp;quot;Generative AI: Current Trends and Practical Applications.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Aug&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Bender, E. M., Gebru, T., McMillan-Major, A., &amp;amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? &lt;em&gt;Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2102.02503&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2102.02503&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BROWN, T. B., MANE, D., LANGE, I., &amp;amp; et al. (2020). Language Models are Few-Shot Learners. &lt;em&gt;Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2005.14165&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CHESNEY, R., &amp;amp; CITRON, D. K. (2019). Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. &lt;em&gt;California Law Review&lt;/em&gt;, 107(6), 1753-1819. &lt;a href=&#34;https://doi.org/10.2139/ssrn.3213954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.2139/ssrn.3213954&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DENG, J., DONAHUE, J., &amp;amp; HAREL, M. (2009). ImageNet: A Large-Scale Hierarchical Image Database. &lt;em&gt;Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2009.5206848&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR.2009.5206848&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DOSOVITSKIY, A., BROSSARD, T., &amp;amp; SPRINGENBERG, J. (2017). Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks. &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 39(5), 939-949. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2016.2593826&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2016.2593826&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FRID-ADAR, M., ELIYAHU, S., &amp;amp; GOLDY, S. (2018). GAN-based Synthetic Medical Image Augmentation for Increased CNN Performance in Liver Lesion Classification. &lt;em&gt;IEEE Transactions on Medical Imaging&lt;/em&gt;, 37(6), 1334-1343. &lt;a href=&#34;https://doi.org/10.1109/TMI.2018.2813792&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TMI.2018.2813792&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;KINGMA, D. P., &amp;amp; WELLING, M. (2013). Auto-Encoding Variational Bayes. &lt;em&gt;Proceedings of the 2nd International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1312.6114&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RADFORD, A., WU, J., &amp;amp; AMODEI, D. (2021). Learning Transferable Visual Models From Natural Language Supervision. &lt;em&gt;Proceedings of the 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2103.00020&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RAMESH, A., MENG, C., &amp;amp; ZHANG, S. (2021). DALL·E: Creating Images from Text. &lt;em&gt;OpenAI&lt;/em&gt;. &lt;a href=&#34;https://openai.com/research/dall-e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://openai.com/research/dall-e&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;STRUBELL, E., GANASSI, M., &amp;amp; MCAFEE, P. (2019). Energy and Policy Considerations for Deep Learning in NLP. &lt;em&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1906.02243&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1906.02243&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WIESE, S., BOLAND, M., &amp;amp; TONG, A. (2019). A Survey on Machine Learning in Finance. &lt;em&gt;Proceedings of the 26th International Conference on Machine Learning (ICML 2019)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1910.02342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1910.02342&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VASWANI, A., SHAZEER, N., &amp;amp; PARMAR, N. (2017). Attention Is All You Need. &lt;em&gt;Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1706.03762&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Leveraging Large Language Models (LLMs) in Remote Sensing</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-07-29-leveraging-large-language-models-in-remote-sensing/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-07-29-leveraging-large-language-models-in-remote-sensing/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#understanding-large-language-models-llms&#34;&gt;Understanding Large Language Models (LLMs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#llms-in-lulc-classification&#34;&gt;LLMs in LULC Classification&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#methodology&#34;&gt;Methodology&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-lulc-classification-on-sentinel-2-imagery&#34;&gt;Case Study: LULC Classification on Sentinel-2 Imagery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Remote sensing has revolutionized the way we observe and understand the Earth’s surface. With the advent of satellites like Sentinel, Landsat-8, and THEOS, we have access to a plethora of high-resolution imagery that can be used for various applications, including Land Use/Land Cover (LULC) classification and image classification. However, analyzing and interpreting this vast amount of data is a complex task. Enter Large Language Models (LLMs), which have shown promise in various domains, including natural language processing, computer vision, and remote sensing.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In this blog, we will explore how LLMs can be applied to remote sensing, particularly in the domains of LULC and image classification. We will delve into the methodologies, algorithms, and techniques that can be utilized to harness the power of LLMs for these applications.
&lt;/p&gt;
&lt;h2 id=&#34;understanding-large-language-models-llms&#34;&gt;Understanding Large Language Models (LLMs)&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Large Language Models, such as GPT-4, are deep learning models that have been trained on vast amounts of text data. They are capable of understanding and generating human-like text, making them highly versatile for various applications. In the context of remote sensing, LLMs can be used to analyze and interpret imagery data, aiding in tasks like LULC classification and image classification.
&lt;/p&gt;
&lt;h3 id=&#34;llms-in-lulc-classification&#34;&gt;LLMs in LULC Classification&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
LULC classification involves categorizing different regions of an image into land use and land cover classes, such as forests, urban areas, water bodies, and agricultural land. Traditional methods for LULC classification include supervised and unsupervised learning techniques. LLMs, however, can enhance these methods by providing contextual understanding and improved feature extraction.
&lt;/p&gt;
&lt;h4 id=&#34;methodology&#34;&gt;Methodology&lt;/h4&gt;
&lt;p align=&#34;justify&#34;&gt;
The process of using LLMs for LULC classification can be summarized as follows:
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collect satellite imagery from sources like Sentinel, Landsat-8, and THEOS.&lt;/li&gt;
&lt;li&gt;Perform image correction and normalization to ensure consistency in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Extraction&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use convolutional neural networks (CNNs) to extract features from the satellite images.&lt;/li&gt;
&lt;li&gt;Integrate LLMs to enhance feature extraction by incorporating contextual information from related text data (e.g., environmental reports, land use documentation).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Training&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train a classification model using the extracted features. The model can be a hybrid of CNNs and LLMs, where the CNN handles the spatial features and the LLM provides contextual understanding.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification and Validation&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply the trained model to classify the satellite images into LULC categories.&lt;/li&gt;
&lt;li&gt;Validate the model using ground truth data and performance metrics like accuracy, precision, recall, and F1-score.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;case-study-lulc-classification-on-sentinel-2-imagery&#34;&gt;Case Study: LULC Classification on Sentinel-2 Imagery&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
To illustrate the application of LLMs in LULC classification, let’s consider a case study using Sentinel-2 imagery. Sentinel-2 provides high-resolution optical imagery, which is ideal for detailed LULC classification.
&lt;/p&gt;
&lt;h3 id=&#34;data-collection-and-preprocessing&#34;&gt;Data Collection and Preprocessing&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
We collected Sentinel-2 imagery for a region with diverse land cover types. The images were preprocessed to correct for atmospheric effects and normalize the reflectance values.
&lt;/p&gt;
&lt;h3 id=&#34;feature-extraction&#34;&gt;Feature Extraction&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
A pre-trained CNN, such as ResNet-50, was used to extract spatial features from the images. Simultaneously, a large corpus of environmental text data was fed into an LLM to extract contextual features.
&lt;/p&gt;
&lt;h3 id=&#34;model-training-and-classification&#34;&gt;Model Training and Classification&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The extracted features were combined and fed into a hybrid classification model. The model was trained using labeled ground truth data. The results showed a significant improvement in classification accuracy compared to traditional methods.
&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The integration of LLMs in remote sensing, particularly for LULC and image classification, holds immense potential. By combining the spatial feature extraction capabilities of CNNs with the contextual understanding of LLMs, we can achieve more accurate and meaningful classifications. As remote sensing technology continues to evolve, the role of advanced AI models like LLMs will become increasingly crucial in unlocking new insights from satellite imagery.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In this blog, we explored the methodologies and techniques for leveraging LLMs in remote sensing applications. The case study on Sentinel-2 imagery demonstrated the practical benefits of this approach. As we move forward, further research and development in this field will undoubtedly lead to more innovative and effective solutions for remote sensing challenges.
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</title>
      <link>https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/tgcKR97Ea8I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Semantic segmentation is vital in remote sensing, particularly for identifying and categorizing different land use and land cover types. In regions like Thailand, where agriculture is central to the economy, precise segmentation of satellite imagery can enhance our ability to track crop health, predict yields, and improve resource management. Our model, &lt;em&gt;MeViT&lt;/em&gt; (Medium-Resolution Vision Transformer), is specifically designed to classify agricultural crops like para rubber, corn, and pineapple across Thailand’s varied landscapes.&lt;/p&gt;
&lt;h2 id=&#34;background-on-vision-transformers&#34;&gt;Background on Vision Transformers&lt;/h2&gt;
&lt;p&gt;Unlike traditional convolutional neural networks (CNNs), which are excellent at capturing local spatial hierarchies, Vision Transformers excel at modeling long-range dependencies through self-attention mechanisms. This unique structure allows &lt;em&gt;MeViT&lt;/em&gt; to interpret both local and global features, enhancing its effectiveness in agricultural land segmentation tasks where accuracy and detail are paramount.&lt;/p&gt;
&lt;h2 id=&#34;mevit-architecture&#34;&gt;MeViT Architecture&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;MeViT&lt;/em&gt; leverages a multi-branch architecture tailored for medium-resolution images, balancing computational efficiency with high-quality feature extraction. This design approach enables the model to capture details across multiple spatial scales, which is crucial for segmenting complex land use patterns in agricultural imagery.&lt;/p&gt;
&lt;p&gt;In particular, the revised mixed-scale convolutional feedforward network (MixCFN) in &lt;em&gt;MeViT&lt;/em&gt; incorporates multiple depth-wise convolution paths, further refining feature extraction by allowing the model to focus on different spatial scales. This enhanced architecture achieves an efficient trade-off between model complexity and performance, making it well-suited for large-scale image analysis tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured_backup.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;experimental-results-and-evaluation&#34;&gt;Experimental Results and Evaluation&lt;/h2&gt;
&lt;p&gt;We extensively tested &lt;em&gt;MeViT&lt;/em&gt; on Thailand’s Landsat-8 dataset, focusing on para rubber, corn, and pineapple classifications. Compared to other models, including state-of-the-art architectures like HRViT and SegFormer, &lt;em&gt;MeViT&lt;/em&gt; demonstrated notable improvements in precision and segmentation accuracy, proving its efficacy in challenging, real-world datasets. This establishes &lt;em&gt;MeViT&lt;/em&gt; as a leading tool in medium-resolution satellite imagery analysis, surpassing previous Vision Transformer models and CNN-based methods in delivering high-quality semantic segmentation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;MeViT&lt;/em&gt; presents a significant advancement in Vision Transformer applications, setting a new standard for semantic segmentation in remote sensing. By combining multi-branch ViT architectures with optimized convolutional modules, &lt;em&gt;MeViT&lt;/em&gt; delivers efficient, accurate LULC classification on satellite imagery, supporting agricultural insights and sustainable resource management across Thailand. This work contributes to the broader field of environmental monitoring and opens up new possibilities for enhanced remote sensing techniques globally.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</title>
      <link>https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama</title>
      <link>https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks</title>
      <link>https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province</title>
      <link>https://kaopanboonyuen.github.io/publication/rainfall-prediction-a-machine-learning-approach/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/rainfall-prediction-a-machine-learning-approach/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus</title>
      <link>https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks</title>
      <link>https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</title>
      <link>https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</title>
      <link>https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution</title>
      <link>https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning</title>
      <link>https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transportation Mobility Factor Extraction Using Image Recognition Techniques</title>
      <link>https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</title>
      <link>https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices</title>
      <link>https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</title>
      <link>https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery</title>
      <link>https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Road map extraction from satellite imagery using connected component analysis and landscape metrics</title>
      <link>https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image Vectorization of Road Satellite Data Sets</title>
      <link>https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
