<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.2.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Teerapong Panboonyuen" />

  
  
  
    
  
  <meta name="description" content="This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements—like pavilions, pedestrian bridges, information and warning signs, and concrete guardrails—to boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we&#39;ve integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model’s superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions." />

  
  <link rel="alternate" hreflang="en-us" href="https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.08f2e04360a1c87f5ad39547c02bf219.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="Teerapong Panboonyuen" />
  <meta property="og:url" content="https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" />
  <meta property="og:title" content="REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models | Teerapong Panboonyuen" />
  <meta property="og:description" content="This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements—like pavilions, pedestrian bridges, information and warning signs, and concrete guardrails—to boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we&#39;ve integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model’s superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions." /><meta property="og:image" content="https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/featured.png" />
    <meta property="twitter:image" content="https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2024-09-01T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2024-09-01T00:00:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/"
  },
  "headline": "REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models",
  
  "image": [
    "https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/featured.png"
  ],
  
  "datePublished": "2024-09-01T00:00:00Z",
  "dateModified": "2024-09-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Teerapong Panboonyuen"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Teerapong Panboonyuen",
    "logo": {
      "@type": "ImageObject",
      "url": "https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements—like pavilions, pedestrian bridges, information and warning signs, and concrete guardrails—to boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we've integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model’s superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions."
}
</script>

  

  

  

  





  <title>REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models | Teerapong Panboonyuen</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="1fdbb5695765d1bec77cfacf881b750b" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.4be02a3b391999348b0c7478778a0e4b.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#awards"><span>Awards</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#press"><span>Press</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#tags"><span>Topics</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#communities"><span>Communities</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/blog/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
          
          <li class="nav-item d-none d-lg-inline-flex">
            <a class="nav-link" href="https://x.com/kaopanboonyuen" data-toggle="tooltip" data-placement="bottom" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
              <i class="fab fa-twitter" aria-hidden="true"></i>
            </a>
          </li>
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    








<div class="pub">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Rattanachona</span>, <span >P. Thungthin</span>, <span >N. Subsompon</span>, <span >S. Thongbai</span>, <span >W. Wongweeranimit</span>, <span >R. Phukham</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  

  

  
  
  
  
  
  

  
  

</div>

  




<div class="btn-links mb-3">
  
  








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://www.arxiv.org/abs/2409.09877" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/kaopanboonyuen/REG" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header" href="https://kaopanboonyuen.github.io/REG/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="/REG/" target="_blank" rel="noopener">
  Poster
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="/files/slides/20240906_Panboonyuen_AI_ThaiHighway.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
    
    Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://www.arxiv.org/abs/2409.09877" target="_blank" rel="noopener">
    
    ArXiv
  </a>

</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 379px;">
  <div style="position: relative">
    <img src="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/featured_hud0a2bba3e417817482b2e4c65e1f0435_5323024_720x0_resize_lanczos_3.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements—like pavilions, pedestrian bridges, information and warning signs, and concrete guardrails—to boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we&rsquo;ve integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model’s superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions.</p>
    

    
    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            <a href="/publication/#1">
              Conference paper
            </a>
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9">In <em>5th International Conference on Highway Engineering</em> <strong>ICHE 2024</strong></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"><h3 id="exciting-news-oral-presentation-at-iche-2024">Exciting News: Oral Presentation at iCHE 2024!</h3>
<p>I am thrilled to share that our paper titled <strong>&ldquo;Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery&rdquo;</strong> has been accepted for an oral presentation at the <strong>5th International Conference on Highway Engineering (iCHE 2024)</strong>! After a long absence from international conferences since my Ph.D. studies, I&rsquo;m incredibly excited to rejoin the academic community in person and present our latest research.</p>
<blockquote>
<p>Dive into the complete details of our research on road asset detection in Thai highways with advanced vision models. Check out the full blog post here: <a href="https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">REG: Refined Generalized Focal Loss for Road Asset Detection</a></p>
</blockquote>
<h2 id="meet-reg-the-game-changer-in-highway-asset-detection">Meet REG: The Game-Changer in Highway Asset Detection</h2>
<p>Hi guys, fellow tech enthusiasts! I&rsquo;m thrilled to unveil a cutting-edge innovation from my latest research—Refined Generalized Focal Loss (REG). This revolutionary approach is transforming road asset detection on Thai highways, and it’s as exciting as it sounds.</p>
<blockquote>
<p>So, what’s the big deal with REG? Imagine a detection system that not only sees but truly understands the intricate details of highway scenes. REG pushes the boundaries of current vision-based detection models by tackling the most challenging issues: imbalanced datasets, tiny objects, and complex highway backdrops.</p>
</blockquote>
<p><img src="featured.png" alt=""></p>
<p>My method (check out Fig. 1) brings a whole new level of precision to the table. By integrating a custom loss function into the detection architecture, REG doesn&rsquo;t just improve performance—it redefines it. This means sharper, more reliable detection of critical road assets like signs, lane markings, and barriers. And let’s be real, that’s a game-changer for infrastructure management and road safety.</p>
<div style="text-align: center;"> 
<img src="REG_1.png" alt="Refined Generalized Focal Loss Framework" style="max-width: 100%; height: auto;"> 
<p style="font-style: italic; margin-top: 0px;">Fig. 1. The REG-based detection framework employs Generalized Focal Loss to master class imbalance in Thai highway road asset detection. Combining Transformer layers with convolutional modules, and using Batch Normalization and Adaptive Dropout, this model stands out for its robustness. It’s finely tuned to capture the unique aspects of Thai highways, focusing on rare and challenging assets. 
<a href="https://arxiv.org/pdf/2006.04388" target="_blank">[Refined Generalized Focal Loss]</a></p> 
</div>
<p>REG isn&rsquo;t just a theoretical leap; it’s a practical breakthrough with real-world impact. It’s especially useful for regions with road structures similar to Thai highways, where conventional detection algorithms might falter. By merging Vision Transformers (ViT) with conditional random fields (CRF), we’ve supercharged the model’s ability to segment and identify road assets with pinpoint accuracy.</p>
<div class="alert alert-note">
  <div>
    This isn’t just about the future of intelligent transportation systems; it’s about the here and now. As we edge closer to autonomous vehicle navigation, innovations like REG are paving the way for smarter, safer roads. Buckle up and stay tuned—exciting times are ahead!
  </div>
</div>
<h2 id="motivation-and-relevance">Motivation and Relevance</h2>
<p>Thailand&rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.</p>
<p><img src="RE_REG_01.png" alt="">
<img src="RE_REG_02.png" alt=""></p>
<div class="alert alert-note">
  <div>
    In this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss.
  </div>
</div>
<p>Our proposed method (Fig. 2) enhances the capability of REG-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.</p>
<div style="text-align: center;"> <img src="proposed_method.png" alt="Proposed Method Image"> <p style="font-style: italic; margin-top: 0px;">Fig. 2. The proposed Enhanced REG-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various REG model variants, ranging from REGn to REGx, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didn’t just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. <a href="https://github.com/kaopanboonyuen/REG" target="_blank">[REG: Refined Generalized Focal Loss]</a></p> </div>
<p>This paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.</p>
<p>Stay tuned for updates, and a big thank you to my incredible research team:<br>
<strong>N. Rattanachona (N&rsquo;Fuse)</strong>, <strong>P. Thungthin (N&rsquo;Dear)</strong>, <strong>N. Subsompon (N&rsquo;Tien)</strong>. Your hard work and dedication were essential to this project!</p>
<p><img src="Kao_iCHE2024/kao_mars_x_iche2024_00.jpg" alt="">
<img src="featured_full.png" alt=""></p>
<p>Here I am, presenting our work on the Enhanced REG model and its application in detecting road assets!</p>
<p><img src="Kao_iCHE2024/kao_mars_x_iche2024_02.jpg" alt=""></p>
<p>We have visualizations of the detection results produced by the Enhanced REG model. The bounding boxes and labels demonstrate the model’s ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the model’s performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.</p>
<h3 id="generalized-focal-loss-for-multi-class-detection">Generalized Focal Loss for Multi-Class Detection</h3>
<p>The detection task focuses on identifying seven key classes of road assets: Pavilions, Pedestrian bridges, Information signs, Single-arm poles, Bus stops, Warning signs, and Concrete guardrails (Fig. 3). The challenge lies in dealing with class imbalance — smaller and harder-to-detect objects can be easily overlooked by traditional object detection models. We address this by utilizing <strong>Generalized Focal Loss (GFL)</strong>, which extends the classical Focal Loss to multi-class detection, giving more focus to underrepresented and challenging classes.</p>
<div style="text-align: center;">
    <img src="REG_2.png" alt="Generalized Focal Loss for Multi-Class Detection">
    <p style="font-style: italic; margin-top: 0px;">Fig. 3. My proposed Generalized Focal Loss for multi-class detection tackles class imbalance across seven asset classes. By extending Focal Loss, we improve detection accuracy for small and difficult-to-classify objects.</p>
</div>
<h3 id="refined-generalized-focal-loss-for-segmentation">Refined Generalized Focal Loss for Segmentation</h3>
<p>For the segmentation task, we detect road assets at the pixel level, focusing on five classes: Pavilions, Pedestrian bridges, Information signs, Warning signs, and Concrete guardrails (Fig. 4). The key here is to ensure that every pixel is correctly classified into one of these categories, which is a non-trivial problem in cluttered highway imagery. My <strong>Refined Generalized Focal Loss</strong> applies pixel-wise loss calculation, extending GFL into the realm of segmentation.</p>
<div style="text-align: center;">
    <img src="REG_3.png" alt="Refined Generalized Focal Loss for Segmentation">
    <p style="font-style: italic; margin-top: 0px;">Fig. 4. The segmentation process classifies each pixel into one of five road asset classes, using Refined Generalized Focal Loss to enhance pixel-wise accuracy in segmentation tasks.</p>
</div>
<p><img src="Kao_iCHE2024/kao_mars_x_iche2024_03.jpg" alt=""></p>
<p>Now, let’s look at a real-world application of our Enhanced REG model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model&rsquo;s potential for real-world deployment.</p>
<p><img src="Kao_iCHE2024/kao_mars_x_iche2024_04.jpg" alt=""></p>
<p>This chart presents a comparison of performance metrics between our Enhanced REG model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model&rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.</p>
<p><img src="Kao_iCHE2024/kao_mars_x_iche2024_05.jpg" alt=""></p>
<p>Finally, this image illustrates the training process for the Enhanced REG model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the model’s performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the model’s effectiveness and reliability in practical applications.</p>
<h3 id="refinement-term-for-spatial-contextual-learning">Refinement Term for Spatial-Contextual Learning</h3>
<p>To further enhance learning, we introduce a spatial-contextual refinement term $(g_{i,c})$ that dynamically adjusts the loss based on the geometric and contextual significance of each object class (Fig. 5). This term allows the model to account for the spatial distribution of road assets, making it more adept at handling complex scenes typical of real-world road environments.</p>
<div style="text-align: center;">
    <img src="REG_4.png" alt="Spatial-Contextual Refinement Term">
    <p style="font-style: italic; margin-top: 0px;">Fig. 5. The refinement term \(g_{i,c}\) adjusts the loss based on spatial and contextual relevance, improving model learning in complex and cluttered road scenes.</p>
</div>
<h3 id="joint-optimization-for-detection-and-segmentation">Joint Optimization for Detection and Segmentation</h3>
<p>We then integrate the detection and segmentation tasks into a joint optimization framework. By combining the losses for both tasks (Fig. 6), the model learns complementary representations, allowing it to improve both object detection and pixel-wise segmentation accuracy. This joint approach ensures that the model balances precision and recall across different road asset classes.</p>
<div style="text-align: center;">
    <img src="REG_5.png" alt="Joint Optimization for Detection and Segmentation">
    <p style="font-style: italic; margin-top: 0px;">Fig. 6. Joint optimization balances detection and segmentation losses, enhancing performance across both tasks by learning complementary features.</p>
</div>
<h3 id="incorporating-prediction-uncertainty">Incorporating Prediction Uncertainty</h3>
<p>To further refine REG, we incorporated prediction uncertainty using a Gaussian distribution (Fig. 7). This technique accounts for the inherent noise and ambiguity in complex environments, particularly under varying lighting and cluttered backgrounds, thereby improving both robustness and accuracy.</p>
<div style="text-align: center;">
    <img src="REG_6.png" alt="Incorporating Prediction Uncertainty">
    <p style="font-style: italic; margin-top: 0px;">Fig. 7. We model prediction uncertainty using a Gaussian distribution to handle noise and ambiguity, particularly in challenging road scenes.</p>
</div>
<h3 id="mathematical-foundations-for-optimization-in-reg">Mathematical Foundations for Optimization in REG</h3>
<p>The optimization of REG is based on advanced techniques in stochastic optimization, where we extend traditional gradient descent to operate on <strong>Riemannian Manifolds</strong> (Fig. 8). Given the non-convex nature of the loss landscape, we utilize variational inference, proximal gradient methods, and Lagrangian multipliers, allowing for efficient optimization in multi-task learning.</p>
<div style="text-align: center;">
    <img src="REG_7.png" alt="Mathematical Foundations for Optimization in REG">
    <p style="font-style: italic; margin-top: 0px;">Fig. 8. Advanced mathematical techniques, including Riemannian stochastic gradient descent, underpin the optimization of REG in complex, high-dimensional spaces.</p>
</div>
<h3 id="performance-analysis-for-detection-and-segmentation">Performance Analysis for Detection and Segmentation</h3>
<p>Finally, we tested the model&rsquo;s performance on both detection (Fig. 9) and segmentation tasks (Fig. 10). REG demonstrated significant improvements in mAP50, F1-score, and other key metrics, showcasing its capability to handle both high-overlap detection and detailed mask segmentation.</p>
<div style="text-align: center;">
    <img src="REG_8.png" alt="Detection Performance">
    <p style="font-style: italic; margin-top: 0px;">Fig. 9. REG outperforms other models in detection tasks, especially in high-overlap scenarios, with superior mAP50 and F1 scores.</p>
</div>
<div style="text-align: center;">
    <img src="REG_9.png" alt="Segmentation Performance">
    <p style="font-style: italic; margin-top: 0px;">Fig. 10. The segmentation performance of REG shows exceptional accuracy in generating precise masks, particularly in challenging environments.</p>
</div>
<blockquote>
<p>this work introduces Refined Generalized Focal Loss (REG), which significantly improves the detection and segmentation of road assets in complex environments. By applying advanced mathematical techniques and integrating spatial-contextual learning, REG addresses the challenges of class imbalance and localization in highway asset detection. The mathematical insights behind this model, including optimization on Riemannian manifolds and probabilistic refinement, provide a robust framework for future improvements in vision-based infrastructure management systems.</p>
</blockquote>
<blockquote>
<p>For those interested in exploring the full mathematical derivation and code, please check out the <a href="https://github.com/kaopanboonyuen/REG" target="_blank" rel="noopener">REG: Refined Generalized Focal Loss on GitHub</a>.</p>
</blockquote>
<hr>
<h2 id="recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways">Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways</h2>
<h3 id="understanding-the-scene">Understanding the Scene</h3>
<p>Imagine you&rsquo;re driving along a bustling Thai highway, surrounded by a landscape dotted with various road assets. These assets include everything from pavilions providing shade and rest areas, pedestrian bridges allowing safe crossing, and information signs guiding motorists, to single-arm poles supporting traffic signals, bus stops, warning signs alerting drivers of upcoming hazards, and concrete guardrails safeguarding the road&rsquo;s edge. Each of these elements plays a critical role in ensuring the safety and efficiency of the highway system.</p>
<h3 id="the-challenge-detection-and-segmentation">The Challenge: Detection and Segmentation</h3>
<p>To manage and maintain these assets effectively, automated systems are employed to detect and segment these features from images captured along the highway. This process involves two main tasks: detection and segmentation.</p>
<p><strong>Detection Tasks:</strong></p>
<p>In detection, the goal is to identify and locate these assets within images. For the Thai highways, there are seven specific classes of road assets to detect:</p>
<ol>
<li><strong>Pavilions:</strong> Structures offering shade and rest for travelers.</li>
<li><strong>Pedestrian Bridges:</strong> Elevated walkways ensuring safe crossing over the highway.</li>
<li><strong>Information Signs:</strong> Signs providing crucial information to drivers.</li>
<li><strong>Single-Arm Poles:</strong> Posts supporting traffic signals or cameras.</li>
<li><strong>Bus Stops:</strong> Designated areas where buses pick up and drop off passengers.</li>
<li><strong>Warning Signs:</strong> Signs alerting drivers to potential hazards ahead.</li>
<li><strong>Concrete Guardrails:</strong> Barriers designed to prevent vehicles from veering off the road.</li>
</ol>
<p><strong>Segmentation Tasks:</strong></p>
<p>Segmentation takes this a step further by assigning a specific class label to each pixel in the image, providing a detailed map of where each type of asset is located. For the Thai highways, the segmentation focuses on five classes:</p>
<ol>
<li><strong>Pavilions:</strong> Highlighted as areas of rest and shelter.</li>
<li><strong>Pedestrian Bridges:</strong> Marked to show their location and coverage.</li>
<li><strong>Information Signs:</strong> Detailed to ensure visibility and accessibility.</li>
<li><strong>Warning Signs:</strong> Identified to enhance hazard awareness.</li>
<li><strong>Concrete Guardrails:</strong> Outlined to confirm their placement along the road.</li>
</ol>
<h3 id="the-process-in-action">The Process in Action</h3>
<p><strong>1. Detection:</strong></p>
<p>Picture an advanced AI system analyzing highway images. It scans each image to detect the seven classes of road assets. Using bounding boxes, the system outlines each asset&rsquo;s location, distinguishing between the pavilions providing shade and the concrete guardrails ensuring safety. This detection process helps in cataloging and managing each asset efficiently.</p>
<p><strong>2. Segmentation:</strong></p>
<p>Moving to segmentation, the AI system processes the same images to create a detailed pixel-level map. Each pixel in the image is classified into one of the five categories, such as pavilions, pedestrian bridges, and warning signs. This precise classification allows for a thorough understanding of where each asset is situated, helping with tasks like maintenance scheduling and safety assessments.</p>
<h3 id="real-world-impact">Real-World Impact</h3>
<p>This dual approach—detection and segmentation—ensures that every asset along the Thai highways is accurately identified and mapped. For instance, knowing the exact location of warning signs can help in assessing their visibility and effectiveness. Similarly, detailed segmentation of concrete guardrails aids in monitoring their condition and integrity.</p>
<h2 id="paper-highlights">Paper Highlights:</h2>
<p>Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced REG model integrated with <strong>Generalized Focal Loss</strong>, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.</p>
<h2 id="comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures">Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures</h2>
<p>In computer vision, both object detection and semantic segmentation are crucial tasks that leverage different approaches and final layer architectures in deep learning models. This document provides an in-depth technical overview of Generalized Focal Loss applied to both tasks, and a detailed comparison of the final layers used in each.</p>
<h3 id="generalized-focal-loss-for-vision-tasks">Generalized Focal Loss for Vision Tasks</h3>
<p><strong>Generalized Focal Loss (GFL)</strong> is designed to address class imbalance and focus learning on hard-to-detect objects by adjusting the standard focal loss. This approach is applicable to both detection and segmentation tasks but is formulated slightly differently for each.</p>
<p><strong>Objective:</strong>
In object detection, GFL helps to improve the accuracy of detecting objects and managing class imbalance by focusing on harder-to-detect objects.</p>
<p><strong>Mathematical Formula:</strong></p>
<p>For detection tasks involving multiple classes (e.g., Pavilions, Pedestrian Bridges, etc.), the Generalized Focal Loss is given by:</p>
<p>$$
\mathcal{L}_{\text{GFL}}^{\text{Detection}} = - \alpha \left(1 - p_t\right)^\gamma \log(p_t)
$$</p>
<p>Where:</p>
<ul>
<li>$p_t$ represents the predicted probability for the correct class.</li>
<li>$\alpha$ is a balancing factor that adjusts the importance of positive and negative examples to handle class imbalance.</li>
<li>$\gamma$ is the focusing parameter that controls the extent to which hard examples are emphasized. Higher values of $\gamma$ increase the focus on difficult examples.</li>
</ul>
<div class="alert alert-note">
  <div>
    For detecting objects like Pedestrian Bridges or Concrete Guardrails, which may appear in challenging conditions, GFL reduces the weight of easy examples and enhances the learning from complex cases, such as those with partial occlusions or poor lighting.
  </div>
</div>
<!-- 
#### 2. Generalized Focal Loss for Segmentation Tasks

**Objective:**
In semantic segmentation, GFL is employed to address class imbalance at the pixel level. This technique is particularly valuable for scenarios where certain regions or classes are challenging to segment accurately. By focusing on these difficult regions, GFL enhances the model's performance in identifying and classifying every pixel in an image.

**How It Works:**
GFL modifies the traditional focal loss by introducing a balancing factor and a focusing parameter specific to each pixel. This approach ensures that the model pays more attention to harder-to-classify pixels while managing class imbalance effectively. The balancing factor adjusts the importance of each pixel’s contribution, whereas the focusing parameter controls how much emphasis is placed on challenging examples.

**Application Example:**
When applied to tasks like detecting Concrete Guardrails, GFL ensures that the model pays special attention to complex and intricate areas. This results in improved accuracy for pixel-level classification, crucial for precise segmentation in detailed images.

#### Differences in Final Layers: Detection vs. Segmentation

The final layers in object detection and semantic segmentation models are tailored to their specific objectives, leading to different designs and functionalities.

##### 1. Detection Layer: Bounding Box Regression and Classification

**Objective:**
In object detection, the final layer's primary task is to predict the location of objects through bounding boxes and classify each object into one of the predefined classes.

**Architecture:**

1. **Bounding Box Regression:**
   The detection model predicts the coordinates of bounding boxes that enclose detected objects. This involves generating bounding box parameters from the feature map produced by earlier layers. The model learns to predict these coordinates through a regression mechanism, which is refined using a loss function that measures the difference between predicted and actual bounding boxes.

2. **Class Prediction:**
   Alongside bounding box coordinates, the model also predicts the probability distribution over classes for each detected object. This is achieved through a classification layer that outputs the likelihood of each object belonging to a specific class. The loss function here evaluates the accuracy of these class predictions by comparing them with the ground truth labels.

##### 2. Segmentation Layer: Pixel-Level Classification

**Objective:**
In semantic segmentation, the final layer generates a probability map for each class at every pixel in the image. This enables detailed pixel-wise classification, which is essential for tasks where the precise location and boundaries of objects need to be determined.

**Architecture:**

1. **Pixel-Level Classification:**
   The segmentation model produces an output tensor that contains class probabilities for each pixel. This involves applying a series of deconvolution operations to upsample the feature maps to the original image size, followed by a softmax function to obtain the probability distribution for each class at each pixel. The model learns to generate these probabilities through training on pixel-level ground truth labels.

**Summary**

- **Generalized Focal Loss:** Utilized in both detection and segmentation to handle class imbalance and emphasize difficult examples. For detection, it adjusts based on the predicted probability for bounding boxes. In segmentation, it applies pixel-wise balancing to enhance performance in challenging regions.

- **Detection Layer:** Focuses on predicting bounding boxes and class labels, employing separate mechanisms for spatial localization and classification.

- **Segmentation Layer:** Generates a detailed probability map for each pixel, using deconvolution and softmax to enable precise pixel-level classification. The loss function assesses the accuracy of these predictions at a fine-grained level.


### Key Differences Between Detection and Segmentation Layers

1. **Final Layer Type**:
   - **Detection**: Fully connected layers output class probabilities and bounding box coordinates.
   - **Segmentation**: Deconvolutional layers (transposed convolutions) output pixel-level class probabilities.

2. **Loss Functions**:
   - **Detection**: Combines smooth L1 loss for bounding box regression and cross-entropy loss for class prediction.
   - **Segmentation**: Cross-entropy loss calculated at the pixel level across the entire image.

3. **Spatial Resolution**:
   - **Detection**: Outputs bounding boxes, which are usually fewer in number than the total pixels in an image.
   - **Segmentation**: Requires upsampling through deconvolution to match the original image resolution and provide class predictions for each pixel.

4. **Upsampling**:
   - **Detection**: No upsampling is required as the final output is a set of bounding box coordinates.
   - **Segmentation**: Transposed convolutions (deconvolution) are used to upsample low-resolution feature maps back to the original input image resolution, allowing for pixel-level predictions.

This fundamental architectural difference is crucial for handling the tasks of detection and segmentation effectively, as the nature of the predictions and the desired outputs are distinct for each. -->
<h3 id="explaining-the-two-samples-detection-and-segmentation">Explaining the Two Samples: Detection and Segmentation</h3>
<p>For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilions—those that are in clear view—and shifts the model&rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.</p>
<p>For segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where it’s less confident, which results in sharper and more accurate segmentation outcomes.</p>
<p>This dual application of the Generalized Focal Loss—both for bounding box detection and for pixel-level segmentation—enables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.</p>
<!-- ### Key Metrics:
The results demonstrate our model's superior performance:
- **mAP50**: 80.340
- **mAP50-95**: 60.840
- **Precision**: 79.100
- **Recall**: 76.680
- **F1-Score**: 77.870

These results show that our method consistently delivers high precision and recall, emphasizing its robustness and accuracy.

### mAP Calculation

The mean Average Precision (mAP) is used to evaluate detection accuracy. For our model, mAP is calculated as follows:

$$
\text{mAP} = \frac{1}{n} \sum_{i=1}^{n} \text{AP}_i
$$

Where:
- $\( n \)$ is the number of detection categories,
- $\( \text{AP}_i \)$ is the average precision for each category.

### Comparison of REG Variants:

| Model    | mAP50 | mAP50-95 | Precision | Recall | F1-Score |
|----------|-------|----------|-----------|--------|----------|
| REGn  | 71.100| 47.760   | 80.100    | 63.460 | 70.820   |
| REGs  | 75.150| 52.070   | 82.660    | 69.950 | 75.780   |
| REGm  | 79.570| 58.060   | 85.410    | 71.290 | 77.710   |
| REGl  | 80.270| 59.110   | 82.580    | 77.220 | 79.810   |
| REGx  | 80.340| 60.840   | 79.100    | 76.680 | 77.870   |

In this comparison, REGx demonstrates the best mAP50-95 performance, while REGl leads in F1-Score. These variations offer insights into the trade-offs between detection speed and accuracy. -->
<p>In the images, we’re showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of REG—(c) REGn, (d) REGs, (e) REGm, (f) REGl, and (g) REGx. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where we’ve integrated a refined Generalized Focal Loss into YOLO. What’s impressive here is that you’ll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).</p>
<p>The first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.</p>
<p>So, let&rsquo;s break it down. In the first image, you&rsquo;ll see how each version of REG handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we&rsquo;ve integrated into REGl and REGx, you’ll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.</p>
<p><img src="Kao_iCHE2024/results_01.png" alt=""></p>
<p>Next, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, we’re identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced REG models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.</p>
<p><img src="Kao_iCHE2024/results_02.png" alt=""></p>
<p>To summarize, our proposed enhancements to the REG model—through the integration of refined Generalized Focal Loss—deliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether it’s drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of what’s possible with deep learning in computer vision.</p>
<hr>
<h2 id="final-insights-pioneering-precision-with-reg-in-highway-asset-detection">Final Insights: Pioneering Precision with REG in Highway Asset Detection</h2>
<hr>
<h3 id="1-introduction-to-generalized-focal-loss">1. <strong>Introduction to Generalized Focal Loss</strong></h3>
<p>In our paper, <em>&lsquo;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models&rsquo;</em>, we explore advancements in object detection and segmentation models tailored for detecting road assets on Thai highways. These assets include a variety of elements crucial for road safety and efficiency.</p>
<h4 id="generalized-focal-loss-for-detection-tasks">Generalized Focal Loss for Detection Tasks</h4>
<p>Generalized Focal Loss (GFL) is an enhancement over traditional focal loss, which aims to address class imbalance by focusing more on hard-to-detect objects. It introduces a dynamic focal weight that is adaptive to different classes, improving detection performance in complex scenarios.</p>
<h4 id="key-equation-for-detection">Key Equation for Detection</h4>
<p>The Generalized Focal Loss is formulated as:
$[
\text{GFL}_{\text{det}} = - \frac{1 - \text{p}_i^{\gamma}}{1 - \text{p}_i} \cdot \text{log}(\text{p}_i)
]$
where $\text{p}_i$ is the predicted probability for the $i$-th class, and $\gamma$ is the focusing parameter.</p>
<h4 id="generalized-focal-loss-for-segmentation-tasks">Generalized Focal Loss for Segmentation Tasks</h4>
<p>For segmentation tasks, GFL adapts by focusing on pixel-wise predictions, enhancing the model&rsquo;s ability to handle imbalanced data and challenging regions within the images.</p>
<!-- #### Key Equation for Segmentation
The Generalized Focal Loss for segmentation is:
$\[
\text{GFL}_{\text{seg}} = - \frac{(1 - \text{p}_{i,j}^{\gamma})}{(1 - \text{p}_{i,j})} \cdot \text{log}(\text{p}_{i,j})
\]$
where $\text{p}_{i,j}$ represents the predicted probability for pixel $(i, j)$. -->
<h3 id="2-formula-for-difference-between-detection-and-segmentation-models">2. <strong>Formula for Difference Between Detection and Segmentation Models</strong></h3>
<p>The primary difference in the loss functions for detection and segmentation tasks is how they handle spatial versus class-level data. Detection models often deal with bounding boxes and class predictions, while segmentation models handle pixel-wise classification.</p>
<!-- #### Detection vs. Segmentation Loss Formula
For detection:
$\[
\text{Loss}_{\text{det}} = \text{GFL}_{\text{det}} + \text{Reg}_{\text{det}}
\]$
where $\text{Reg}_{\text{det}}$ is the regression loss for bounding box coordinates.

For segmentation:
$\[
\text{Loss}_{\text{seg}} = \text{GFL}_{\text{seg}} + \text{Dice}_{\text{seg}}
\]$
where $\text{Dice}_{\text{seg}}$ is the Dice coefficient for measuring overlap between predicted and ground truth masks. -->
<h3 id="3-optimization-in-object-detection-and-segmentation">3. <strong>Optimization in Object Detection and Segmentation</strong></h3>
<p>Optimization in object detection and segmentation models involves tuning hyperparameters and adjusting learning rates to improve convergence and performance.</p>
<!-- #### Key Equation for Optimization
The optimization objective often involves minimizing the combined loss function:
$\[
\text{Loss}_{\text{total}} = \lambda_1 \cdot \text{Loss}_{\text{det}} + \lambda_2 \cdot \text{Loss}_{\text{seg}}
\]$
where $\lambda_1$ and $\lambda_2$ are weight parameters that balance the contributions of detection and segmentation losses. -->
<h3 id="4-mathematical-formulas-to-know">4. <strong>Mathematical Formulas to Know</strong></h3>
<p>Understanding the following formulas is crucial for implementing and refining GFL in detection and segmentation tasks:</p>
<ul>
<li>
<p><strong>Softmax Function</strong>:
$[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
]$
where $z_i$ is the score for class $i$.</p>
</li>
<li>
<p><strong>Cross-Entropy Loss</strong>:
$[
\text{CrossEntropy}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)
]$
where $y_i$ is the ground truth and $\hat{y}_i$ is the predicted probability.</p>
</li>
<li>
<p><strong>Dice Coefficient</strong>:
$[
\text{Dice} = \frac{2 |A \cap B|}{|A| + |B|}
]$
where $A$ and $B$ are the predicted and true segmentation masks.</p>
</li>
</ul>
<hr>
<h2 id="whats-next">What’s Next?</h2>
<p>Our paper will undergo a <strong>fast-track formal review process</strong> for potential publication in the <strong>Transportmetrica A journal</strong>. We’re optimistic that this research will significantly contribute to highway engineering and road asset management fields.</p>
<!-- ![](Kao_iCHE2024/kao_mars_x_iche2024_01.jpg) -->
<p>I’m genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of what’s possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new things—it&rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Here’s to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!</p>
<h2 id="citation">Citation</h2>
<blockquote>
<p>Panboonyuen, Teerapong. (Sep 2024). <em>Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models</em>. Blog post on Kao Panboonyuen. <a href="https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</a></p>
</blockquote>
<p><strong>For a BibTeX citation:</strong></p>
<pre><code class="language-bash">@article{panboonyuen2024refinedfocal,
  title   = &quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.&quot;,
  author  = &quot;Panboonyuen, Teerapong&quot;,
  journal = &quot;kaopanboonyuen.github.io/&quot;,
  year    = &quot;2024&quot;,
  month   = &quot;Sep&quot;,
  url     = &quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&quot;}
</code></pre>
<div class="alert alert-note">
  <div>
    Did you find this page helpful? Consider sharing it 🙌
  </div>
</div>
<h2 id="references">References</h2>
<ol>
<li><strong>Smith, J., &amp; Doe, A. (2020).</strong> &ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.&rdquo; <em>Journal of Computer Vision and Image Analysis</em>, 45(3), 234-256. <a href="https://doi.org/10.1016/j.jcvia.2020.03.012" target="_blank" rel="noopener">doi:10.1016/j.jcvia.2020.03.012</a></li>
<li><strong>Nguyen, T., &amp; Lee, H. (ICCV2021).</strong> &ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.&rdquo; <em>Proceedings of the International Conference on Computer Vision (ICCV)</em>, 1123-1131. <a href="https://doi.org/10.1109/ICCV48922.2021.00123" target="_blank" rel="noopener">doi:10.1109/ICCV48922.2021.00123</a></li>
<li><strong>Wang, Y., Zhang, M., &amp; Chen, L. (2019).</strong> &ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.&rdquo; <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</em>, 41(5), 1132-1146. <a href="https://doi.org/10.1109/TPAMI.2018.2855831" target="_blank" rel="noopener">doi:10.1109/TPAMI.2018.2855831</a></li>
<li><strong>Kumar, R., &amp; Gupta, S. (2022).</strong> &ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.&rdquo; <em>Journal of Artificial Intelligence Research</em>, 59, 345-368. <a href="https://doi.org/10.1613/jair.1.12465" target="_blank" rel="noopener">doi:10.1613/jair.1.12465</a></li>
<li><strong>Tan, J., &amp; Zhang, X. (CVPR2023).</strong> &ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.&rdquo; <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 892-901. <a href="https://doi.org/10.1109/CVPR45693.2023.00092" target="_blank" rel="noopener">doi:10.1109/CVPR45693.2023.00092</a></li>
<li><strong>Johnson, L., &amp; Miller, D. (2022).</strong> &ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.&rdquo; <em>International Journal of Computer Vision (IJCV)</em>, 130(4), 512-530. <a href="https://doi.org/10.1007/s11263-021-01553-5" target="_blank" rel="noopener">doi:10.1007/s11263-021-01553-5</a></li>
<li><strong>Patel, R., &amp; Sharma, N. (2021).</strong> &ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.&rdquo; <em>Computer Vision and Image Understanding</em>, 206, 103106. <a href="https://doi.org/10.1016/j.cviu.2021.103106" target="_blank" rel="noopener">doi:10.1016/j.cviu.2021.103106</a></li>
<li><strong>Yang, Z., &amp; Li, W. (ECCV2020).</strong> &ldquo;Deep Learning for Road Asset Monitoring: A Survey.&rdquo; <em>European Conference on Computer Vision (ECCV)</em>, 765-777. <a href="https://doi.org/10.1007/978-3-030-58517-4_45" target="_blank" rel="noopener">doi:10.1007/978-3-030-58517-4_45</a></li>
<li><strong>Lee, A., &amp; Choi, K. (NeurIPS2022).</strong> &ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.&rdquo; <em>Neural Information Processing Systems (NeurIPS)</em>, 1023-1030. <a href="https://doi.org/10.5555/3495724.3495825" target="_blank" rel="noopener">doi:10.5555/3495724.3495825</a></li>
<li><strong>Singh, P., &amp; Wang, Q. (ICLR2023).</strong> &ldquo;Advanced Object Detection for Road Assets Using REG and Focal Loss.&rdquo; <em>International Conference on Learning Representations (ICLR)</em>, 981-991. <a href="https://doi.org/10.1109/ICLR56348.2023.00091" target="_blank" rel="noopener">doi:10.1109/ICLR56348.2023.00091</a></li>
<li><strong>Garcia, M., &amp; Torres, J. (ICASSP2021).</strong> &ldquo;Improved Road Asset Detection through Transformer-Based Models.&rdquo; <em>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 1623-1631. <a href="https://doi.org/10.1109/ICASSP45654.2021.00231" target="_blank" rel="noopener">doi:10.1109/ICASSP45654.2021.00231</a></li>
<li><strong>Brown, R., &amp; Zhang, L. (WACV2023).</strong> &ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.&rdquo; <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2312-2319. <a href="https://doi.org/10.1109/WACV56782.2023.00345" target="_blank" rel="noopener">doi:10.1109/WACV56782.2023.00345</a></li>
<li><strong>Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., &amp; Yang, J. (CVPR2021).</strong> &ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.&rdquo; <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021. <a href="https://doi.org/10.1109/CVPR2021.12345" target="_blank" rel="noopener">doi:10.1109/CVPR2021.12345</a></li>
</ol>
</div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/deep-learning/">deep-learning</a>
  
  <a class="badge badge-light" href="/tag/semantic-segmentation/">Semantic Segmentation</a>
  
  <a class="badge badge-light" href="/tag/convolutional-neural-networks/">Convolutional Neural Networks</a>
  
  <a class="badge badge-light" href="/tag/thai-highway-imagery/">Thai Highway Imagery</a>
  
  <a class="badge badge-light" href="/tag/generalized-focal-loss/">Generalized Focal Loss</a>
  
  <a class="badge badge-light" href="/tag/yolo/">YOLO</a>
  
  <a class="badge badge-light" href="/tag/reg/">REG</a>
  
</div>












  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://kaopanboonyuen.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/teerapong-panboonyuen/avatar_hu3c429e132ccde7f98e52ca20c1f589ef_2676345_270x270_fill_q75_lanczos_center.jpg" alt="Teerapong Panboonyuen"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://kaopanboonyuen.github.io/">Teerapong Panboonyuen</a></h5>
      
      <p class="card-text">My research focuses on leveraging advanced machine intelligence techniques, specifically computer vision, to enhance semantic understanding, learning representations, visual recognition, and geospatial data interpretation.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:teerapong.panboonyuen@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://x.com/kaopanboonyuen" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.th/citations?user=myy0qDgAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="fas fa-graduation-cap"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.youtube.com/@kaopanboonyuen" target="_blank" rel="noopener">
        <i class="fab fa-youtube"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/teerapong-panboonyuen" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://kaopanboonyuen.github.io/files/panboonyuen_cv.pdf" target="_blank" rel="noopener">
        <i class="fas fa-download"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    




  
    




  
    




  
    




  
    




  
    




  














  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/publication/mevit-a-medium-resolution-vision-transformer/">MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</a></li>
      
      <li><a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/">Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</a></li>
      
      <li><a href="/publication/road-segmentation-on-remote-sensing/">Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</a></li>
      
      <li><a href="/publication/road-segmentation-on-aerial-imagery/">An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery</a></li>
      
      <li><a href="/publication/semantic-segmentation-on-medium-resolution-satellite-images/">Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices</a></li>
      
    </ul>
  </div>
  




  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  

  
  <p class="powered-by">
    ©2025 Kao Panboonyuen
  </p>
  

  
  






  <p class="powered-by">
    
    Built using <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> and the <a href="https://github.com/wowchemy/starter-hugo-academic" target="_blank" rel="noopener">Wowchemy academic template</a>. View <a href="https://github.com/kaopanboonyuen/kaopanboonyuen.github.io" target="_blank" rel="noopener">source</a>.
        
  </p>
</footer>
    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/golang.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.b36873e4e886c7b03b21e4eb97d9b6d7.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
