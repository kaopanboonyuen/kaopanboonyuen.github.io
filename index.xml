<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Teerapong Panboonyuen</title>
    <link>https://kaopanboonyuen.github.io/</link>
      <atom:link href="https://kaopanboonyuen.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Teerapong Panboonyuen</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â©2026 Kao Panboonyuen</copyright><lastBuildDate>Thu, 01 Jan 2026 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png</url>
      <title>Teerapong Panboonyuen</title>
      <link>https://kaopanboonyuen.github.io/</link>
    </image>
    
    <item>
      <title>Lecture 01 â€” What Is a Multimodal LLM, Really?</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-01-what-is-multimodal-llm/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-01-what-is-multimodal-llm/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~3 hours (deep foundational lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-lecture-matters&#34;&gt;ğŸŒ Why This Lecture Matters&lt;/h2&gt;
&lt;p&gt;Before code.&lt;br&gt;
Before models.&lt;br&gt;
Before GPUs.&lt;/p&gt;
&lt;p&gt;We must answer &lt;strong&gt;one fundamental question&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What does it mean for a machine to understand the world through many senses?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Multimodal LLMs are not just:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bigger models&lt;/li&gt;
&lt;li&gt;more parameters&lt;/li&gt;
&lt;li&gt;more data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They represent a &lt;strong&gt;shift in how intelligence is built&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-big-picture&#34;&gt;ğŸ§  The Big Picture&lt;/h2&gt;
&lt;p&gt;Humans are multimodal by nature:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Human Sense&lt;/th&gt;
&lt;th&gt;AI Modality&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Images / Video&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hearing&lt;/td&gt;
&lt;td&gt;Audio&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Language&lt;/td&gt;
&lt;td&gt;Text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Memory&lt;/td&gt;
&lt;td&gt;Documents&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reasoning&lt;/td&gt;
&lt;td&gt;LLM&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A &lt;strong&gt;multimodal LLM&lt;/strong&gt; attempts to &lt;strong&gt;unify perception and reasoning&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-formal-definition-intuitive&#34;&gt;ğŸ§© Formal Definition (Intuitive)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;A &lt;strong&gt;Multimodal Large Language Model (MLLM)&lt;/strong&gt; is a system that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;processes &lt;strong&gt;multiple input modalities&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;aligns them into a &lt;strong&gt;shared representation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;performs &lt;strong&gt;reasoning primarily through language&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Key idea:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Language is the reasoning interface.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-modalities-vs-models-important-distinction&#34;&gt;ğŸ”¬ Modalities vs Models (Important Distinction)&lt;/h2&gt;
&lt;p&gt;âŒ Multimodal â‰  multiple models glued together&lt;br&gt;
âœ… Multimodal = &lt;strong&gt;coherent representation space&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Image â†’ Vision Encoder â”
Audio â†’ Audio Encoder  â”œâ”€&amp;gt; Shared Latent Space â†’ LLM â†’ Output
Text  â†’ Text Encoder   â”˜

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-llms-became-the-brain&#34;&gt;ğŸ§  Why LLMs Became the â€œBrainâ€&lt;/h2&gt;
&lt;p&gt;Historically:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vision models â†’ pattern recognition&lt;/li&gt;
&lt;li&gt;Audio models â†’ signal processing&lt;/li&gt;
&lt;li&gt;NLP models â†’ reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LLMs won because they:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;handle &lt;strong&gt;symbolic abstraction&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;perform &lt;strong&gt;long-chain reasoning&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;generalize across tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;LLMs are not just text models â€” they are reasoning engines.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-components-of-a-multimodal-llm&#34;&gt;ğŸ§© Core Components of a Multimodal LLM&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Purpose&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Modality Encoder&lt;/td&gt;
&lt;td&gt;Convert raw input â†’ embeddings&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Projection Layer&lt;/td&gt;
&lt;td&gt;Align modality to language space&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LLM Backbone&lt;/td&gt;
&lt;td&gt;Reasoning &amp;amp; generation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Output Head&lt;/td&gt;
&lt;td&gt;Decode answers&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-component-deep-dive&#34;&gt;ğŸ” Component Deep Dive&lt;/h2&gt;
&lt;h3 id=&#34;1-modality-encoders&#34;&gt;1ï¸âƒ£ Modality Encoders&lt;/h3&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image â†’ ViT, CNN&lt;/li&gt;
&lt;li&gt;Audio â†’ Whisper, Wav2Vec&lt;/li&gt;
&lt;li&gt;Video â†’ Frame encoder + temporal model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Role:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Convert &lt;strong&gt;raw signals&lt;/strong&gt; into &lt;strong&gt;semantic vectors&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-projection--alignment-layer-critical&#34;&gt;2ï¸âƒ£ Projection / Alignment Layer (CRITICAL)&lt;/h3&gt;
&lt;p&gt;This is the &lt;strong&gt;most underrated component&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Purpose:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;map non-text embeddings â†’ LLM token space&lt;/li&gt;
&lt;li&gt;enable cross-modal attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without good alignment:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The LLM sees noise, not meaning.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-llm-backbone&#34;&gt;3ï¸âƒ£ LLM Backbone&lt;/h3&gt;
&lt;p&gt;Usually:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decoder-only Transformer&lt;/li&gt;
&lt;li&gt;Pretrained on massive text corpora&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Why reuse?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;language encodes &lt;strong&gt;world knowledge&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;reasoning already learned&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mental-model-very-important&#34;&gt;ğŸ§  Mental Model (Very Important)&lt;/h2&gt;
&lt;p&gt;Think of a multimodal LLM as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Perception â†’ Translation â†’ Thought â†’ Expression&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;perception = encoders&lt;/li&gt;
&lt;li&gt;translation = projection&lt;/li&gt;
&lt;li&gt;thought = LLM&lt;/li&gt;
&lt;li&gt;expression = output&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-systems-conceptual&#34;&gt;ğŸ”„ Example Systems (Conceptual)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Input&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Image Captioning&lt;/td&gt;
&lt;td&gt;Image&lt;/td&gt;
&lt;td&gt;Text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VQA&lt;/td&gt;
&lt;td&gt;Image + Question&lt;/td&gt;
&lt;td&gt;Answer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ASR&lt;/td&gt;
&lt;td&gt;Audio&lt;/td&gt;
&lt;td&gt;Text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video QA&lt;/td&gt;
&lt;td&gt;Video + Text&lt;/td&gt;
&lt;td&gt;Text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Doc QA&lt;/td&gt;
&lt;td&gt;PDF&lt;/td&gt;
&lt;td&gt;Answer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-misconceptions&#34;&gt;âš ï¸ Common Misconceptions&lt;/h2&gt;
&lt;h3 id=&#34;-myth-1-bigger--better&#34;&gt;âŒ Myth 1: Bigger = Better&lt;/h3&gt;
&lt;p&gt;Truth:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Alignment quality &amp;gt; parameter count&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-myth-2-multimodal-means-end-to-end-training&#34;&gt;âŒ Myth 2: Multimodal means end-to-end training&lt;/h3&gt;
&lt;p&gt;Truth:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most systems are &lt;strong&gt;composed + aligned&lt;/strong&gt;, not trained from scratch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-myth-3-vision-models-can-reason&#34;&gt;âŒ Myth 3: Vision models can reason&lt;/h3&gt;
&lt;p&gt;Truth:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Reasoning happens in &lt;strong&gt;language space&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--conceptual&#34;&gt;ğŸ§ª Knowledge Check â€” Conceptual&lt;/h2&gt;
&lt;h3 id=&#34;q1-objective&#34;&gt;Q1 (Objective)&lt;/h3&gt;
&lt;p&gt;What is the primary role of an LLM in a multimodal system?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Reasoning and generation across aligned modalities.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2-true--false&#34;&gt;Q2 (True / False)&lt;/h3&gt;
&lt;p&gt;Multimodal LLMs require a separate reasoning engine for each modality.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mathematical-intuition-lightweight&#34;&gt;ğŸ§  Mathematical Intuition (Lightweight)&lt;/h2&gt;
&lt;p&gt;Each modality produces vectors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Image â†’ â„â¿
Audio â†’ â„áµ
Text  â†’ â„áµ

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Projection learns:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
â„â¿, â„áµ â†’ â„áµ

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the LLM can attend uniformly.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Alignment = learning a shared geometry of meaning&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--alignment&#34;&gt;ğŸ§ª Knowledge Check â€” Alignment&lt;/h2&gt;
&lt;h3 id=&#34;q3-mcq&#34;&gt;Q3 (MCQ)&lt;/h3&gt;
&lt;p&gt;What happens if embeddings are poorly aligned?&lt;/p&gt;
&lt;p&gt;A) Slower inference&lt;br&gt;
B) Higher memory usage&lt;br&gt;
C) Hallucinations&lt;br&gt;
D) Overfitting&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;C) Hallucinations&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-multimodal-llms-emerged-now&#34;&gt;ğŸ§  Why Multimodal LLMs Emerged &lt;em&gt;Now&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Three forces converged:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Foundation models&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cheap large-scale pretraining&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformers with attention&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Multimodality was &lt;em&gt;impossible&lt;/em&gt; without scalable language reasoning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-beginner--advanced-progression&#34;&gt;ğŸŒ± Beginner â†’ Advanced Progression&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Level&lt;/th&gt;
&lt;th&gt;Focus&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Beginner&lt;/td&gt;
&lt;td&gt;What is multimodal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Intermediate&lt;/td&gt;
&lt;td&gt;Architecture &amp;amp; alignment&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Advanced&lt;/td&gt;
&lt;td&gt;Training strategies, evaluation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Expert&lt;/td&gt;
&lt;td&gt;Agents, reasoning, ethics&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This course follows &lt;strong&gt;that arc intentionally&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--systems-thinking&#34;&gt;ğŸ§ª Knowledge Check â€” Systems Thinking&lt;/h2&gt;
&lt;h3 id=&#34;q4-objective&#34;&gt;Q4 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is language used as the shared interface instead of vision?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because language is symbolic, compositional, and supports reasoning.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-human-centered-perspective&#34;&gt;ğŸ§  Human-Centered Perspective&lt;/h2&gt;
&lt;p&gt;Humans:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;perceive multimodally&lt;/li&gt;
&lt;li&gt;reason symbolically&lt;/li&gt;
&lt;li&gt;communicate linguistically&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multimodal LLMs mirror this &lt;strong&gt;cognitive pipeline&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;But remember:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Understanding â‰  Consciousness&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-limitations-be-honest&#34;&gt;âš ï¸ Limitations (Be Honest)&lt;/h2&gt;
&lt;p&gt;Multimodal LLMs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hallucinate&lt;/li&gt;
&lt;li&gt;inherit bias&lt;/li&gt;
&lt;li&gt;lack grounding&lt;/li&gt;
&lt;li&gt;do not &lt;em&gt;understand&lt;/em&gt; like humans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Awareness is responsibility.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--ethics-awareness&#34;&gt;ğŸ§ª Knowledge Check â€” Ethics Awareness&lt;/h2&gt;
&lt;h3 id=&#34;q5-true--false&#34;&gt;Q5 (True / False)&lt;/h3&gt;
&lt;p&gt;Multimodal LLMs truly understand the world.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False â€” they model correlations, not lived experience.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaways&#34;&gt;âœ… Final Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multimodal LLMs unify perception + reasoning&lt;/li&gt;
&lt;li&gt;Language is the cognitive backbone&lt;/li&gt;
&lt;li&gt;Alignment is more important than scale&lt;/li&gt;
&lt;li&gt;Understanding systems &amp;gt; using tools&lt;/li&gt;
&lt;li&gt;Responsibility is part of intelligence&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection-very-important&#34;&gt;ğŸŒ Final Reflection (Very Important)&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If machines can see and hear, what remains uniquely human?&lt;/summary&gt;
  &lt;p&gt;Values, wisdom, empathy, responsibility.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>CS101-LC01 â€” What is LeetCode? (Why Everyone Uses It)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc01-intro/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc01-intro/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~30 minutes&lt;/p&gt;
&lt;h2 id=&#34;-why-this-chapter-matters&#34;&gt;ğŸ¯ Why This Chapter Matters&lt;/h2&gt;
&lt;p&gt;Before solving problems, you must understand &lt;strong&gt;why&lt;/strong&gt; you are solving them.&lt;/p&gt;
&lt;p&gt;LeetCode is not just a website.&lt;br&gt;
It is a &lt;strong&gt;thinking framework&lt;/strong&gt; used worldwide to evaluate engineers.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-leetcode&#34;&gt;ğŸ¤” What is LeetCode?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;LeetCode&lt;/strong&gt; is an online platform for practicing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Algorithms&lt;/li&gt;
&lt;li&gt;Data Structures&lt;/li&gt;
&lt;li&gt;Logical problem solving&lt;/li&gt;
&lt;li&gt;Interview-style coding questions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Problems range from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Beginner (arrays, loops)&lt;/li&gt;
&lt;li&gt;Intermediate (hash maps, trees)&lt;/li&gt;
&lt;li&gt;Advanced (DP, graphs, system thinking)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-who-uses-leetcode&#34;&gt;ğŸŒ Who Uses LeetCode?&lt;/h2&gt;
&lt;p&gt;LeetCode-style questions are used by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google&lt;/li&gt;
&lt;li&gt;Meta (Facebook)&lt;/li&gt;
&lt;li&gt;Amazon&lt;/li&gt;
&lt;li&gt;Microsoft&lt;/li&gt;
&lt;li&gt;Apple&lt;/li&gt;
&lt;li&gt;Netflix&lt;/li&gt;
&lt;li&gt;Startups &amp;amp; research labs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even if the interview is &lt;strong&gt;not on LeetCode&lt;/strong&gt;,&lt;br&gt;
the &lt;strong&gt;problem patterns are the same&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-leetcode-is-really-training&#34;&gt;ğŸ§  What LeetCode Is REALLY Training&lt;/h2&gt;
&lt;p&gt;LeetCode is NOT about memorizing code.&lt;/p&gt;
&lt;p&gt;It trains you to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Break big problems into smaller ones&lt;/li&gt;
&lt;li&gt;Recognize patterns&lt;/li&gt;
&lt;li&gt;Choose the right data structure&lt;/li&gt;
&lt;li&gt;Optimize time &amp;amp; memory&lt;/li&gt;
&lt;li&gt;Explain your reasoning clearly&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-interviewers-look-for&#34;&gt;ğŸ” What Interviewers Look For&lt;/h2&gt;
&lt;p&gt;Interviewers are evaluating &lt;strong&gt;how you think&lt;/strong&gt;, not just the final answer.&lt;/p&gt;
&lt;p&gt;They care about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clear understanding of the problem&lt;/li&gt;
&lt;li&gt;Logical step-by-step reasoning&lt;/li&gt;
&lt;li&gt;Clean, readable code&lt;/li&gt;
&lt;li&gt;Correct time complexity (Big-O)&lt;/li&gt;
&lt;li&gt;Ability to improve a slow solution&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interview-reality-check&#34;&gt;ğŸ§ª Interview Reality Check&lt;/h2&gt;
&lt;p&gt;You are usually expected to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Explain the problem in your own words&lt;/li&gt;
&lt;li&gt;Start with a simple (even slow) solution&lt;/li&gt;
&lt;li&gt;Analyze time complexity&lt;/li&gt;
&lt;li&gt;Improve it&lt;/li&gt;
&lt;li&gt;Communicate clearly&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Perfect code â‰  Perfect interview&lt;br&gt;
Clear thinking = Pass&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-beginner-misunderstanding&#34;&gt;ğŸ§  Common Beginner Misunderstanding&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Does LeetCode measure intelligence?&lt;/summary&gt;
  &lt;p&gt;âŒ No&lt;br&gt;
âœ… It measures &lt;strong&gt;problem-solving patterns&lt;/strong&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;Someone who practiced patterns will outperform&lt;br&gt;
someone who is â€œsmart but untrainedâ€.&lt;/p&gt;
&lt;p&gt;This is &lt;strong&gt;learnable&lt;/strong&gt;, not talent-based.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-examples-of-patterns-you-will-learn&#34;&gt;ğŸ§© Examples of Patterns You Will Learn&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pattern&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Two pointers&lt;/td&gt;
&lt;td&gt;Palindrome check&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hash map&lt;/td&gt;
&lt;td&gt;Two Sum&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sliding window&lt;/td&gt;
&lt;td&gt;Subarray problems&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix sum&lt;/td&gt;
&lt;td&gt;Range queries&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stack&lt;/td&gt;
&lt;td&gt;Valid parentheses&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BFS / DFS&lt;/td&gt;
&lt;td&gt;Tree &amp;amp; graph traversal&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Once you see the pattern,&lt;br&gt;
many problems feel &lt;strong&gt;similar&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-leetcode-is-used-in-exams&#34;&gt;ğŸ« Why LeetCode is Used in Exams&lt;/h2&gt;
&lt;p&gt;LeetCode-style questions test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logic&lt;/li&gt;
&lt;li&gt;Edge cases&lt;/li&gt;
&lt;li&gt;Complexity awareness&lt;/li&gt;
&lt;li&gt;Coding discipline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These skills apply to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Midterm / final exams&lt;/li&gt;
&lt;li&gt;Programming contests&lt;/li&gt;
&lt;li&gt;Research code&lt;/li&gt;
&lt;li&gt;Real-world software&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-how-to-use-leetcode-correctly&#34;&gt;ğŸ How to Use LeetCode Correctly&lt;/h2&gt;
&lt;p&gt;âŒ Wrong way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copy solution&lt;/li&gt;
&lt;li&gt;Memorize code&lt;/li&gt;
&lt;li&gt;Skip explanation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;âœ… Right way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Understand the idea&lt;/li&gt;
&lt;li&gt;Rewrite in your own style&lt;/li&gt;
&lt;li&gt;Analyze complexity&lt;/li&gt;
&lt;li&gt;Explain out loud&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you can &lt;strong&gt;teach it&lt;/strong&gt;, you truly understand it.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-goal-of-this-course&#34;&gt;ğŸ¯ Goal of This Course&lt;/h2&gt;
&lt;p&gt;By the end of this course, you should be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Look at a problem&lt;/li&gt;
&lt;li&gt;Identify its pattern&lt;/li&gt;
&lt;li&gt;Choose the right approach&lt;/li&gt;
&lt;li&gt;Write clean, efficient code&lt;/li&gt;
&lt;li&gt;Explain your solution confidently&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That is exactly what interviewers want.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-message&#34;&gt;ğŸš€ Final Message&lt;/h2&gt;
&lt;p&gt;LeetCode is not a test of intelligence.&lt;br&gt;
It is training for &lt;strong&gt;structured thinking&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you practice correctly,&lt;br&gt;
you will think like a software engineer.&lt;/p&gt;
&lt;p&gt;And that skill lasts a lifetime ğŸ’¡&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DE101-PD01 â€” Pandas &amp; the Data Engineering Mindset</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd01-intro/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd01-intro/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~45â€“60 minutes&lt;/p&gt;
&lt;h2 id=&#34;-why-this-chapter-matters&#34;&gt;ğŸ¯ Why This Chapter Matters&lt;/h2&gt;
&lt;p&gt;Before learning Pandas syntax, you must learn &lt;strong&gt;how engineers think about data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Pandas is not just:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A tool for Jupyter notebooks&lt;/li&gt;
&lt;li&gt;A library for homework&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pandas is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A bridge between raw data and real systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-pandas-actually-is&#34;&gt;ğŸ§  What Pandas Actually Is&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Pandas&lt;/strong&gt; is the core Python library for working with &lt;strong&gt;structured data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It sits between:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Raw files (CSV, JSON, Excel, Parquet)&lt;/li&gt;
&lt;li&gt;Databases (PostgreSQL, MySQL, BigQuery)&lt;/li&gt;
&lt;li&gt;Data warehouses &amp;amp; lakes&lt;/li&gt;
&lt;li&gt;Machine learning pipelines&lt;/li&gt;
&lt;li&gt;Analytics dashboards&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If data has rows and columns, Pandas is usually involved.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-data-engineering-pipeline-high-level&#34;&gt;ğŸ—ï¸ The Data Engineering Pipeline (High Level)&lt;/h2&gt;
&lt;p&gt;Every real data system follows this flow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ingest data (files, APIs, logs)&lt;/li&gt;
&lt;li&gt;Clean and validate&lt;/li&gt;
&lt;li&gt;Transform and enrich&lt;/li&gt;
&lt;li&gt;Aggregate and summarize&lt;/li&gt;
&lt;li&gt;Store or feed to ML models&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pandas lives mostly in &lt;strong&gt;steps 2â€“4&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pandas-is-not-just-for-notebooks&#34;&gt;ğŸ§  Pandas Is NOT Just for Notebooks&lt;/h2&gt;
&lt;p&gt;Beginner mistake:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œPandas is only for exploration.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Professional reality:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Production ETL jobs use Pandas&lt;/li&gt;
&lt;li&gt;Feature engineering uses Pandas&lt;/li&gt;
&lt;li&gt;Data validation uses Pandas&lt;/li&gt;
&lt;li&gt;Research pipelines start in Pandas&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-real-world-use-cases&#34;&gt;ğŸ§ª Example: Real-World Use Cases&lt;/h2&gt;
&lt;h3 id=&#34;google&#34;&gt;Google&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Experiment analysis&lt;/li&gt;
&lt;li&gt;Metric validation&lt;/li&gt;
&lt;li&gt;Feature debugging&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;meta&#34;&gt;Meta&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A/B testing&lt;/li&gt;
&lt;li&gt;User behavior analysis&lt;/li&gt;
&lt;li&gt;Dataset sanity checks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;openai&#34;&gt;OpenAI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset preprocessing&lt;/li&gt;
&lt;li&gt;Filtering &amp;amp; labeling&lt;/li&gt;
&lt;li&gt;Feature extraction&lt;/li&gt;
&lt;li&gt;Evaluation pipelines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All start with Pandas.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-good-pandas-code-has-these-properties&#34;&gt;ğŸ§  Good Pandas Code Has These Properties&lt;/h2&gt;
&lt;h3 id=&#34;1-readable&#34;&gt;1ï¸âƒ£ Readable&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[df[&amp;quot;country&amp;quot;] == &amp;quot;US&amp;quot;][&amp;quot;revenue&amp;quot;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anyone should understand it.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-testable&#34;&gt;2ï¸âƒ£ Testable&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;assert df[&amp;quot;age&amp;quot;].min() &amp;gt;= 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bad data is worse than no data.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-deterministic&#34;&gt;3ï¸âƒ£ Deterministic&lt;/h3&gt;
&lt;p&gt;Same input â†’ same output
No hidden randomness.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-scalable-within-reason&#34;&gt;4ï¸âƒ£ Scalable (Within Reason)&lt;/h3&gt;
&lt;p&gt;Good Pandas code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Works for 1k rows&lt;/li&gt;
&lt;li&gt;Still works for 10M rows&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pandas-vs-sql-vs-spark&#34;&gt;ğŸ§  Pandas vs SQL vs Spark&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tool&lt;/th&gt;
&lt;th&gt;Best For&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Pandas&lt;/td&gt;
&lt;td&gt;Prototyping, analysis, ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SQL&lt;/td&gt;
&lt;td&gt;Large-scale aggregation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;Distributed big data&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Engineers &lt;strong&gt;combine&lt;/strong&gt; tools, not worship one.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pandas-as-a-thinking-tool&#34;&gt;ğŸ§© Pandas as a Thinking Tool&lt;/h2&gt;
&lt;p&gt;Pandas teaches you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data modeling&lt;/li&gt;
&lt;li&gt;Schema awareness&lt;/li&gt;
&lt;li&gt;Edge cases&lt;/li&gt;
&lt;li&gt;Performance thinking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These skills transfer to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;Spark&lt;/li&gt;
&lt;li&gt;Flink&lt;/li&gt;
&lt;li&gt;DuckDB&lt;/li&gt;
&lt;li&gt;Polars&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-beginner-mistakes&#34;&gt;ğŸ§  Common Beginner Mistakes&lt;/h2&gt;
&lt;p&gt;âŒ Writing everything in one line
âŒ Ignoring dtypes
âŒ Silent NaNs
âŒ Copy-paste pipelines
âŒ No validation&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-minimal-example-end-to-end-thinking&#34;&gt;ğŸ§ª Minimal Example (End-to-End Thinking)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

df = pd.read_csv(&amp;quot;users.csv&amp;quot;)

df = (
    df
    .dropna(subset=[&amp;quot;age&amp;quot;])
    .assign(age=lambda x: x[&amp;quot;age&amp;quot;].astype(int))
)

assert df[&amp;quot;age&amp;quot;].min() &amp;gt;= 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is production thinking.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pandas--testing&#34;&gt;ğŸ§  Pandas + Testing&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def test_no_negative_age(df):
    assert (df[&amp;quot;age&amp;quot;] &amp;gt;= 0).all()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Engineers test data, not just code.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-when-pandas-is-enough&#34;&gt;ğŸ§  When Pandas Is Enough&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Is Pandas enough for big data?&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Pandas is perfect for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prototyping&lt;/li&gt;
&lt;li&gt;Medium-scale data (up to tens of millions of rows)&lt;/li&gt;
&lt;li&gt;ML feature engineering&lt;/li&gt;
&lt;li&gt;Research workflows&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It often pairs with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;Spark&lt;/li&gt;
&lt;li&gt;Arrow&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-when-pandas-is-not-enough&#34;&gt;ğŸš§ When Pandas Is NOT Enough&lt;/h2&gt;
&lt;p&gt;Signs you should scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Memory errors&lt;/li&gt;
&lt;li&gt;Very slow joins&lt;/li&gt;
&lt;li&gt;Daily batch jobs taking hours&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At that point:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep logic&lt;/li&gt;
&lt;li&gt;Change engine&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-engineers-rule-of-thumb&#34;&gt;ğŸ§  Engineerâ€™s Rule of Thumb&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Prototype in Pandas
Validate logic
Scale only when needed&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is how real teams work.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ Final Takeaway&lt;/h2&gt;
&lt;p&gt;Pandas is not a toy.
It is a &lt;strong&gt;thinking framework&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you master Pandas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You understand data&lt;/li&gt;
&lt;li&gt;You avoid silent bugs&lt;/li&gt;
&lt;li&gt;You build reliable systems&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Lecture 01 â€” What Is AI, Really?</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-01-intro/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-01-intro/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~2 hours (foundational lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-big-question&#34;&gt;ğŸ§  The Big Question&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What does it mean for a machine to be intelligent?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Before tools, before code, before models â€”&lt;br&gt;
we must understand &lt;strong&gt;what intelligence actually is&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Is intelligence:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;memorizing facts?&lt;/li&gt;
&lt;li&gt;following rules?&lt;/li&gt;
&lt;li&gt;learning from experience?&lt;/li&gt;
&lt;li&gt;reasoning?&lt;/li&gt;
&lt;li&gt;creating new ideas?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI forces us to rethink &lt;strong&gt;what it means to think&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-a-short-history-of-ai-very-important&#34;&gt;ğŸ§© A Short History of AI (Very Important)&lt;/h2&gt;
&lt;h3 id=&#34;1-rule-based-systems-old-ai&#34;&gt;1ï¸âƒ£ Rule-Based Systems (Old AI)&lt;/h3&gt;
&lt;p&gt;Early AI worked like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;IF email contains &amp;quot;free money&amp;quot;
THEN mark as spam
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ… Works for simple cases
âŒ Breaks when rules explode&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If intelligence = rules, then more rules = smarter machine?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;No.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-learning-based-systems-modern-ai&#34;&gt;2ï¸âƒ£ Learning-Based Systems (Modern AI)&lt;/h3&gt;
&lt;p&gt;Instead of rules, we show &lt;strong&gt;examples&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spam email âŒ&lt;/li&gt;
&lt;li&gt;normal email âœ…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The machine &lt;strong&gt;learns patterns&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This shift is called &lt;strong&gt;Machine Learning&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ai-for-kids-and-adults&#34;&gt;ğŸŒ± AI for Kids (and Adults)&lt;/h2&gt;
&lt;p&gt;Imagine teaching a robot:&lt;/p&gt;
&lt;p&gt;ğŸ¶ What is a dog?
ğŸ± What is a cat?&lt;/p&gt;
&lt;p&gt;Two ways:&lt;/p&gt;
&lt;h3 id=&#34;-rule-way&#34;&gt;âŒ Rule Way&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dogs have 4 legs&lt;/li&gt;
&lt;li&gt;Dogs bark&lt;/li&gt;
&lt;li&gt;Dogs have fur&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;toy dogs?&lt;/li&gt;
&lt;li&gt;robots?&lt;/li&gt;
&lt;li&gt;pictures?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rules break.&lt;/p&gt;
&lt;h3 id=&#34;-learning-way&#34;&gt;âœ… Learning Way&lt;/h3&gt;
&lt;p&gt;Show &lt;strong&gt;many examples&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The robot learns patterns.&lt;/p&gt;
&lt;p&gt;ğŸ‘‰ This is &lt;strong&gt;Machine Learning&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-machine-learning&#34;&gt;ğŸ§  What Is Machine Learning?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning = Learning patterns from data to make predictions or decisions&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Instead of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;writing rules&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;collect data&lt;/li&gt;
&lt;li&gt;define objectives&lt;/li&gt;
&lt;li&gt;let the model learn&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-ai-simple-definition&#34;&gt;ğŸ¤– What Is AI (Simple Definition)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Artificial Intelligence is a system that perceives, learns, reasons, and acts to achieve goals.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Key words:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Perceive&lt;/strong&gt; (see, hear, read)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt; (from data)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reason&lt;/strong&gt; (make decisions)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Act&lt;/strong&gt; (produce output)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-narrow-ai-vs-human-intelligence&#34;&gt;ğŸ§  Narrow AI vs Human Intelligence&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Human&lt;/th&gt;
&lt;th&gt;AI&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;General intelligence&lt;/td&gt;
&lt;td&gt;Narrow intelligence&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Learns few examples&lt;/td&gt;
&lt;td&gt;Needs lots of data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Understands meaning&lt;/td&gt;
&lt;td&gt;Learns patterns&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Has values&lt;/td&gt;
&lt;td&gt;Needs alignment&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;AI today is &lt;strong&gt;powerful but narrow&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-from-ml-to-deep-learning&#34;&gt;ğŸ§  From ML to Deep Learning&lt;/h2&gt;
&lt;h3 id=&#34;-machine-learning&#34;&gt;ğŸ”¹ Machine Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Uses features&lt;/li&gt;
&lt;li&gt;Human-designed representations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-deep-learning&#34;&gt;ğŸ”¹ Deep Learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Learns features automatically&lt;/li&gt;
&lt;li&gt;Uses neural networks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deep Learning made AI &lt;strong&gt;work at scale&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-a-neural-network&#34;&gt;ğŸ§  What Is a Neural Network?&lt;/h2&gt;
&lt;p&gt;Inspired by the brain (loosely).&lt;/p&gt;
&lt;p&gt;Basic idea:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Input â†’ Layers â†’ Output
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It learns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;weights&lt;/li&gt;
&lt;li&gt;connections&lt;/li&gt;
&lt;li&gt;representations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Neural networks power:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;vision&lt;/li&gt;
&lt;li&gt;speech&lt;/li&gt;
&lt;li&gt;language&lt;/li&gt;
&lt;li&gt;games&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-nlp&#34;&gt;ğŸ—£ï¸ What Is NLP?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;NLP = Natural Language Processing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Goal:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Teach machines to understand human language.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;translation&lt;/li&gt;
&lt;li&gt;chatbots&lt;/li&gt;
&lt;li&gt;summarization&lt;/li&gt;
&lt;li&gt;search&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-an-llm&#34;&gt;ğŸ“š What Is an LLM?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;LLM = Large Language Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;Gemini&lt;/li&gt;
&lt;li&gt;Claude&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LLMs are trained on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;books&lt;/li&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;li&gt;websites&lt;/li&gt;
&lt;li&gt;conversations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-idea&#34;&gt;Key idea:&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;LLMs predict the &lt;strong&gt;next word&lt;/strong&gt; â€” very well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Butâ€¦&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prediction â‰  understanding&lt;/li&gt;
&lt;li&gt;fluency â‰  truth&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-how-chatgpt-really-works-simple&#34;&gt;ğŸ’¬ How ChatGPT Really Works (Simple)&lt;/h2&gt;
&lt;p&gt;ChatGPT:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reads your input&lt;/li&gt;
&lt;li&gt;Converts words â†’ numbers&lt;/li&gt;
&lt;li&gt;Uses a &lt;strong&gt;Transformer&lt;/strong&gt; model&lt;/li&gt;
&lt;li&gt;Predicts the next token&lt;/li&gt;
&lt;li&gt;Repeats&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It feels intelligent because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;patterns of language encode reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But it does &lt;strong&gt;not think like humans&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-generative-ai&#34;&gt;ğŸ¨ What Is Generative AI?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Generative AI creates new content&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Text âœï¸&lt;/li&gt;
&lt;li&gt;Images ğŸ–¼ï¸&lt;/li&gt;
&lt;li&gt;Audio ğŸ§&lt;/li&gt;
&lt;li&gt;Video ğŸ¥&lt;/li&gt;
&lt;li&gt;Code ğŸ’»&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It does not copy.
It &lt;strong&gt;samples from learned distributions&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-diffusion-models-image-ai&#34;&gt;ğŸ–¼ï¸ Diffusion Models (Image AI)&lt;/h2&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stable Diffusion&lt;/li&gt;
&lt;li&gt;DALLÂ·E&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Idea:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add noise to images&lt;/li&gt;
&lt;li&gt;Learn how to remove noise&lt;/li&gt;
&lt;li&gt;Generate new images from noise&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Creation = reversing chaos.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-gan&#34;&gt;ğŸ­ What Is GAN?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GAN = Generative Adversarial Network&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Two models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generator ğŸ¨ (creates fake)&lt;/li&gt;
&lt;li&gt;Discriminator ğŸ•µï¸ (detects fake)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They compete.&lt;/p&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;very realistic images&lt;/li&gt;
&lt;li&gt;unstable training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GANs taught us:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Creativity can emerge from competition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-an-ai-agent&#34;&gt;ğŸ¤– What Is an AI Agent?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;An AI agent is a system that can observe, decide, and act autonomously.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Agent = LLM + tools + memory + goals&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AutoGPT&lt;/li&gt;
&lt;li&gt;AI assistants&lt;/li&gt;
&lt;li&gt;Game-playing bots&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-agentic-ai&#34;&gt;ğŸ§  What Is Agentic AI?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Agentic AI&lt;/strong&gt; means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;planning&lt;/li&gt;
&lt;li&gt;tool usage&lt;/li&gt;
&lt;li&gt;multi-step reasoning&lt;/li&gt;
&lt;li&gt;self-reflection&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not just answering â€”
but &lt;strong&gt;doing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This is the future direction of AI.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-intelligence--consciousness&#34;&gt;ğŸ§  Intelligence â‰  Consciousness&lt;/h2&gt;
&lt;p&gt;Important truth:&lt;/p&gt;
&lt;p&gt;AI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;âŒ has no emotions&lt;/li&gt;
&lt;li&gt;âŒ has no awareness&lt;/li&gt;
&lt;li&gt;âŒ has no intent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It simulates intelligence.&lt;/p&gt;
&lt;p&gt;Humans:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;have values&lt;/li&gt;
&lt;li&gt;have meaning&lt;/li&gt;
&lt;li&gt;have responsibility&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-ethics-matters&#34;&gt;âš ï¸ Why Ethics Matters&lt;/h2&gt;
&lt;p&gt;AI learns from us.&lt;/p&gt;
&lt;p&gt;Bias in data â†’ bias in AI
Power without wisdom â†’ danger&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Smart systems require wise humans.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaways&#34;&gt;âœ… Final Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;AI is &lt;strong&gt;not magic&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;AI is &lt;strong&gt;math + data + optimization&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Intelligence can be &lt;strong&gt;simulated&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Understanding matters more than tools&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;If AI becomes very intelligent, what should remain uniquely human?&lt;/summary&gt;
  &lt;p&gt;Values, ethics, responsibility, wisdom.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Lecture 02 â€” How to Think Like a Multimodal System Designer</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-02-thinking-like-multimodal-architect/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-02-thinking-like-multimodal-architect/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~3â€“4 hours (core system-design lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-lecture-matters&#34;&gt;ğŸŒ Why This Lecture Matters&lt;/h2&gt;
&lt;p&gt;Most people learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ“¦ APIs&lt;/li&gt;
&lt;li&gt;ğŸ§© libraries&lt;/li&gt;
&lt;li&gt;âš™ï¸ frameworks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Very few learn:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;How to design an intelligent system from first principles.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This lecture transforms you from:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;model user&lt;/em&gt; â†’ &lt;strong&gt;multimodal system architect&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-architects-mindset&#34;&gt;ğŸ§  The Architectâ€™s Mindset&lt;/h2&gt;
&lt;p&gt;A multimodal architect does &lt;strong&gt;not&lt;/strong&gt; ask first:&lt;/p&gt;
&lt;p&gt;âŒ â€œWhich model should I use?â€&lt;/p&gt;
&lt;p&gt;They ask:&lt;/p&gt;
&lt;p&gt;âœ… What problem am I solving?&lt;br&gt;
âœ… What information is available?&lt;br&gt;
âœ… What modality carries the signal?&lt;br&gt;
âœ… What errors are acceptable?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-first-principle-1-start-from-the-task-not-the-model&#34;&gt;ğŸ—ï¸ First Principle #1: Start From the Task, Not the Model&lt;/h2&gt;
&lt;p&gt;Every intelligent system begins with a &lt;strong&gt;task definition&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;ask-these-questions-always&#34;&gt;Ask These Questions (Always)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What is the &lt;strong&gt;input&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;What is the &lt;strong&gt;output&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;What transformation is required?&lt;/li&gt;
&lt;li&gt;What failure is unacceptable?&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt; Medical image diagnosis&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Aspect&lt;/th&gt;
&lt;th&gt;Decision&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Input&lt;/td&gt;
&lt;td&gt;Image + text report&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Output&lt;/td&gt;
&lt;td&gt;Text explanation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Risk&lt;/td&gt;
&lt;td&gt;False negative&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Requirement&lt;/td&gt;
&lt;td&gt;Human-in-the-loop&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--task-thinking&#34;&gt;ğŸ§ª Knowledge Check â€” Task Thinking&lt;/h2&gt;
&lt;h3 id=&#34;q1-objective&#34;&gt;Q1 (Objective)&lt;/h3&gt;
&lt;p&gt;Why should task definition come before model selection?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because architecture depends on constraints, risk, and signal â€” not tools.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-first-principle-2-modalities-are-information-channels&#34;&gt;ğŸ§  First Principle #2: Modalities Are Information Channels&lt;/h2&gt;
&lt;p&gt;Each modality has &lt;strong&gt;strengths and weaknesses&lt;/strong&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Strength&lt;/th&gt;
&lt;th&gt;Weakness&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Text&lt;/td&gt;
&lt;td&gt;Reasoning&lt;/td&gt;
&lt;td&gt;No raw perception&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image&lt;/td&gt;
&lt;td&gt;Spatial&lt;/td&gt;
&lt;td&gt;No abstraction&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Audio&lt;/td&gt;
&lt;td&gt;Emotion, tone&lt;/td&gt;
&lt;td&gt;Noise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video&lt;/td&gt;
&lt;td&gt;Time&lt;/td&gt;
&lt;td&gt;Cost &amp;amp; complexity&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Good design uses the minimum modality required.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-over-engineering-trap-very-common&#34;&gt;âŒ Over-Engineering Trap (Very Common)&lt;/h2&gt;
&lt;p&gt;Many beginners think:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œMore modalities = smarter AIâ€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reality:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;More modalities = more noise, cost, and failure modes&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--modalities&#34;&gt;ğŸ§ª Knowledge Check â€” Modalities&lt;/h2&gt;
&lt;h3 id=&#34;q2-mcq&#34;&gt;Q2 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which modality is the most expensive to annotate?&lt;/p&gt;
&lt;p&gt;A) Text&lt;br&gt;
B) Image&lt;br&gt;
C) Audio&lt;br&gt;
D) Video&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;D) Video&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-first-principle-3-separate-perception-from-reasoning&#34;&gt;ğŸ§  First Principle #3: Separate Perception from Reasoning&lt;/h2&gt;
&lt;p&gt;One of the &lt;strong&gt;most important design rules&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;correct-separation&#34;&gt;Correct Separation&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
Perception â†’ Representation â†’ Reasoning â†’ Action

&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Perception = encoders&lt;/li&gt;
&lt;li&gt;Reasoning = LLM&lt;/li&gt;
&lt;li&gt;Action = output or tool use&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-this-matters&#34;&gt;Why This Matters&lt;/h3&gt;
&lt;p&gt;If you mix everything:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;debugging becomes impossible&lt;/li&gt;
&lt;li&gt;errors propagate&lt;/li&gt;
&lt;li&gt;evaluation is unclear&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Clean boundaries create reliable systems.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--architecture&#34;&gt;ğŸ§ª Knowledge Check â€” Architecture&lt;/h2&gt;
&lt;h3 id=&#34;q3-true--false&#34;&gt;Q3 (True / False)&lt;/h3&gt;
&lt;p&gt;Vision models should handle logical reasoning.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-first-principle-4-alignment-is-the-bottleneck&#34;&gt;ğŸ§  First Principle #4: Alignment Is the Bottleneck&lt;/h2&gt;
&lt;p&gt;Alignment answers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;How does non-language data become â€œthinkableâ€?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Bad alignment â†’&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hallucinations&lt;/li&gt;
&lt;li&gt;irrelevant answers&lt;/li&gt;
&lt;li&gt;false confidence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Good alignment â†’&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reasoning&lt;/li&gt;
&lt;li&gt;grounding&lt;/li&gt;
&lt;li&gt;generalization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-alignment-design-choices&#34;&gt;ğŸ§© Alignment Design Choices&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;When to Use&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Linear projection&lt;/td&gt;
&lt;td&gt;Simple tasks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MLP&lt;/td&gt;
&lt;td&gt;Moderate complexity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cross-attention&lt;/td&gt;
&lt;td&gt;High alignment need&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Q-Former&lt;/td&gt;
&lt;td&gt;Vision-language fusion&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--alignment&#34;&gt;ğŸ§ª Knowledge Check â€” Alignment&lt;/h2&gt;
&lt;h3 id=&#34;q4-mcq&#34;&gt;Q4 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which component most directly affects hallucination?&lt;/p&gt;
&lt;p&gt;A) Tokenizer&lt;br&gt;
B) Alignment layer&lt;br&gt;
C) GPU size&lt;br&gt;
D) Dataset size&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;B) Alignment layer&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-first-principle-5-think-in-pipelines-not-models&#34;&gt;ğŸ§  First Principle #5: Think in Pipelines, Not Models&lt;/h2&gt;
&lt;p&gt;Architects think in &lt;strong&gt;pipelines&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;example-image-question-answering&#34;&gt;Example: Image Question Answering&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
Image â†’ Vision Encoder
Question â†’ Text Encoder
â†“
Alignment
â†“
LLM
â†“
Answer

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each box is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;replaceable&lt;/li&gt;
&lt;li&gt;testable&lt;/li&gt;
&lt;li&gt;improvable&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--pipeline-thinking&#34;&gt;ğŸ§ª Knowledge Check â€” Pipeline Thinking&lt;/h2&gt;
&lt;h3 id=&#34;q5-objective&#34;&gt;Q5 (Objective)&lt;/h3&gt;
&lt;p&gt;Why should components be modular?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;To allow debugging, replacement, and independent improvement.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-beginner--advanced-design-levels&#34;&gt;ğŸ§  Beginner â†’ Advanced Design Levels&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Level&lt;/th&gt;
&lt;th&gt;Focus&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Beginner&lt;/td&gt;
&lt;td&gt;Single modality&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Intermediate&lt;/td&gt;
&lt;td&gt;Multimodal fusion&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Advanced&lt;/td&gt;
&lt;td&gt;RAG + tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Expert&lt;/td&gt;
&lt;td&gt;Agents + feedback loops&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You &lt;strong&gt;do not&lt;/strong&gt; jump levels.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-case-study-1--image-captioning&#34;&gt;ğŸ§  Case Study 1 â€” Image Captioning&lt;/h2&gt;
&lt;h3 id=&#34;design-decision&#34;&gt;Design Decision&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Choice&lt;/th&gt;
&lt;th&gt;Reason&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Frozen vision encoder&lt;/td&gt;
&lt;td&gt;Stability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Small projection&lt;/td&gt;
&lt;td&gt;Efficiency&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pretrained LLM&lt;/td&gt;
&lt;td&gt;Reasoning&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;This works because &lt;strong&gt;task complexity is low&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-case-study-2--medical-vqa-high-risk&#34;&gt;ğŸ§  Case Study 2 â€” Medical VQA (High Risk)&lt;/h2&gt;
&lt;p&gt;Changes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HITL required&lt;/li&gt;
&lt;li&gt;Conservative decoding&lt;/li&gt;
&lt;li&gt;Explanation mandatory&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Risk changes architecture.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--risk-awareness&#34;&gt;ğŸ§ª Knowledge Check â€” Risk Awareness&lt;/h2&gt;
&lt;h3 id=&#34;q6-true--false&#34;&gt;Q6 (True / False)&lt;/h3&gt;
&lt;p&gt;The same architecture fits both chatbots and medical diagnosis.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-first-principle-6-evaluation-shapes-design&#34;&gt;ğŸ§  First Principle #6: Evaluation Shapes Design&lt;/h2&gt;
&lt;p&gt;If you canâ€™t measure it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;You canâ€™t trust it.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Evaluation informs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;architecture choice&lt;/li&gt;
&lt;li&gt;data needs&lt;/li&gt;
&lt;li&gt;alignment method&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(We go deep in Lecture 09.)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-thinking-beyond-accuracy&#34;&gt;ğŸ§  Thinking Beyond Accuracy&lt;/h2&gt;
&lt;p&gt;Architects evaluate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;robustness&lt;/li&gt;
&lt;li&gt;calibration&lt;/li&gt;
&lt;li&gt;failure modes&lt;/li&gt;
&lt;li&gt;ethical impact&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--evaluation-thinking&#34;&gt;ğŸ§ª Knowledge Check â€” Evaluation Thinking&lt;/h2&gt;
&lt;h3 id=&#34;q7-objective&#34;&gt;Q7 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is accuracy alone insufficient?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because it hides bias, uncertainty, and rare failures.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-architects-checklist-very-practical&#34;&gt;ğŸ§  Architectâ€™s Checklist (Very Practical)&lt;/h2&gt;
&lt;p&gt;Before coding, ask:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Is this modality necessary?&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Is language the reasoning layer?&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Is alignment explicit?&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Are risks identified?&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Can humans intervene?&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-human-centered-design-core-philosophy&#34;&gt;ğŸŒ± Human-Centered Design (Core Philosophy)&lt;/h2&gt;
&lt;p&gt;Multimodal AI should:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assist humans&lt;/li&gt;
&lt;li&gt;explain decisions&lt;/li&gt;
&lt;li&gt;accept correction&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Architects design for humility, not dominance.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-knowledge-check--reflection&#34;&gt;ğŸ§ª Final Knowledge Check â€” Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;What separates an AI architect from a model user?&lt;/summary&gt;
  &lt;p&gt;System-level thinking, responsibility, and first-principle design.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaways&#34;&gt;âœ… Final Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Architects start from tasks&lt;/li&gt;
&lt;li&gt;Modalities are information channels&lt;/li&gt;
&lt;li&gt;Alignment is critical&lt;/li&gt;
&lt;li&gt;Pipelines beat monoliths&lt;/li&gt;
&lt;li&gt;Ethics influence architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-9&#34;&gt;
  &lt;summary&gt;If AI systems fail, who is responsible?&lt;/summary&gt;
  &lt;p&gt;The humans who designed them.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>CS101-LC02 â€” Big-O Intuition</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc02-big-o/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc02-big-o/</guid>
      <description>&lt;h2 id=&#34;-why-big-o-matters&#34;&gt;ğŸ¯ Why Big-O Matters&lt;/h2&gt;
&lt;p&gt;Big-O is &lt;strong&gt;NOT&lt;/strong&gt; about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exact seconds&lt;/li&gt;
&lt;li&gt;CPU speed&lt;/li&gt;
&lt;li&gt;Programming language&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Big-O is about &lt;strong&gt;scaling&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â“ If input becomes 10Ã— bigger, how much slower will your program be?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thatâ€™s what interviewers, professors, and engineers care about.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-simple-definition&#34;&gt;ğŸ§  Simple Definition&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Big-O describes how an algorithmâ€™s runtime or memory grows as input size increases.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We focus on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Worst-case behavior&lt;/li&gt;
&lt;li&gt;Growth trend&lt;/li&gt;
&lt;li&gt;Ignoring constants&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-world-analogy&#34;&gt;ğŸ• Real-World Analogy&lt;/h2&gt;
&lt;p&gt;Imagine finding your name in a list of students.&lt;/p&gt;
&lt;h3 id=&#34;case-1-you-know-your-seat-number&#34;&gt;Case 1: You know your seat number&lt;/h3&gt;
&lt;p&gt;â†’ You check once&lt;br&gt;
âœ… Constant time â†’ &lt;strong&gt;O(1)&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;case-2-names-listed-randomly&#34;&gt;Case 2: Names listed randomly&lt;/h3&gt;
&lt;p&gt;â†’ You scan one by one&lt;br&gt;
ğŸ“ˆ Linear â†’ &lt;strong&gt;O(n)&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;case-3-names-sorted-alphabetically&#34;&gt;Case 3: Names sorted alphabetically&lt;/h3&gt;
&lt;p&gt;â†’ You keep halving the list&lt;br&gt;
âš¡ Fast â†’ &lt;strong&gt;O(log n)&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-big-o-classes-must-memorize&#34;&gt;ğŸ“Š Common Big-O Classes (Must Memorize)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Big-O&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Real Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;O(1)&lt;/td&gt;
&lt;td&gt;Constant&lt;/td&gt;
&lt;td&gt;Always same time&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O(log n)&lt;/td&gt;
&lt;td&gt;Logarithmic&lt;/td&gt;
&lt;td&gt;Divide &amp;amp; conquer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;td&gt;Linear&lt;/td&gt;
&lt;td&gt;Scan once&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O(n log n)&lt;/td&gt;
&lt;td&gt;Log-linear&lt;/td&gt;
&lt;td&gt;Efficient sorting&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O(nÂ²)&lt;/td&gt;
&lt;td&gt;Quadratic&lt;/td&gt;
&lt;td&gt;Nested loops&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;O(2â¿)&lt;/td&gt;
&lt;td&gt;Exponential&lt;/td&gt;
&lt;td&gt;Very slow&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-o1--constant-time&#34;&gt;âš™ï¸ O(1) â€” Constant Time&lt;/h2&gt;
&lt;p&gt;Time does &lt;strong&gt;not&lt;/strong&gt; depend on input size.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_first(arr):
    return arr[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No matter how big &lt;code&gt;arr&lt;/code&gt; is â†’ still 1 step.&lt;/p&gt;
&lt;p&gt;ğŸ“Œ Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Array index access&lt;/li&gt;
&lt;li&gt;Hash map lookup&lt;/li&gt;
&lt;li&gt;Stack push/pop&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-on--linear-time&#34;&gt;ğŸ“ˆ O(n) â€” Linear Time&lt;/h2&gt;
&lt;p&gt;Runtime grows &lt;strong&gt;directly&lt;/strong&gt; with input size.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def linear_search(arr, x):
    for v in arr:
        if v == x:
            return True
    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If input doubles â†’ time doubles.&lt;/p&gt;
&lt;p&gt;ğŸ“Œ Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Counting frequency&lt;/li&gt;
&lt;li&gt;Finding max/min&lt;/li&gt;
&lt;li&gt;Scanning data&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-olog-n--logarithmic-time&#34;&gt;âš¡ O(log n) â€” Logarithmic Time&lt;/h2&gt;
&lt;p&gt;Each step cuts the problem in half.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def binary_search(arr, x):
    l, r = 0, len(arr) - 1
    while l &amp;lt;= r:
        mid = (l + r) // 2
        if arr[mid] == x:
            return True
        elif arr[mid] &amp;lt; x:
            l = mid + 1
        else:
            r = mid - 1
    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even for &lt;strong&gt;1 million elements&lt;/strong&gt;, only ~20 steps!&lt;/p&gt;
&lt;p&gt;ğŸ“Œ Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Binary search&lt;/li&gt;
&lt;li&gt;Tree traversal (balanced)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-on-log-n--efficient-algorithms&#34;&gt;ğŸ§  O(n log n) â€” Efficient Algorithms&lt;/h2&gt;
&lt;p&gt;Often comes from:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Divide problem&lt;/li&gt;
&lt;li&gt;Solve subproblems&lt;/li&gt;
&lt;li&gt;Combine results&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sorted(nums)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python uses &lt;strong&gt;Timsort&lt;/strong&gt; â†’ &lt;code&gt;O(n log n)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;ğŸ“Œ Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Merge sort&lt;/li&gt;
&lt;li&gt;Quick sort (average case)&lt;/li&gt;
&lt;li&gt;Heap sort&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-on--quadratic-time&#34;&gt;ğŸš¨ O(nÂ²) â€” Quadratic Time&lt;/h2&gt;
&lt;p&gt;Usually caused by &lt;strong&gt;nested loops&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(n):
    for j in range(n):
        do_something()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;n = 10,000&lt;/code&gt;
â†’ &lt;strong&gt;100 million operations&lt;/strong&gt; ğŸ˜±&lt;/p&gt;
&lt;p&gt;ğŸ“Œ Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pair comparisons&lt;/li&gt;
&lt;li&gt;Brute-force two sum&lt;/li&gt;
&lt;li&gt;Duplicate detection (naive)&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;Nested loops usually mean?&lt;/summary&gt;
  &lt;p&gt;O(nÂ²)&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-on-is-dangerous&#34;&gt;ğŸ”¥ Why O(nÂ²) Is Dangerous&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;n&lt;/th&gt;
&lt;th&gt;Operations&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;10,000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1,000&lt;/td&gt;
&lt;td&gt;1,000,000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10,000&lt;/td&gt;
&lt;td&gt;100,000,000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is why LeetCode &lt;strong&gt;TLE (Time Limit Exceeded)&lt;/strong&gt; happens.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-exam-style-questions&#34;&gt;ğŸ§ª Exam-Style Questions&lt;/h2&gt;
&lt;h3 id=&#34;q1&#34;&gt;Q1&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for x in arr:
    print(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ… &lt;strong&gt;O(n)&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2&#34;&gt;Q2&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(n):
    for j in range(i):
        print(i, j)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ… &lt;strong&gt;O(nÂ²)&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3&#34;&gt;Q3&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 1
while i &amp;lt; n:
    i *= 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ… &lt;strong&gt;O(log n)&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-space-complexity-also-important&#34;&gt;ğŸ§  Space Complexity (Also Important)&lt;/h2&gt;
&lt;p&gt;Big-O also applies to &lt;strong&gt;memory usage&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seen = set()
for x in arr:
    seen.add(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Time: O(n)&lt;/li&gt;
&lt;li&gt;Space: O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interviewers love this question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œCan you trade space for time?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-world-engineering-insight&#34;&gt;ğŸ¯ Real-World Engineering Insight&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Situation&lt;/th&gt;
&lt;th&gt;Preferred&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Small data&lt;/td&gt;
&lt;td&gt;Simple code&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Large data&lt;/td&gt;
&lt;td&gt;Better Big-O&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Real-time system&lt;/td&gt;
&lt;td&gt;Predictable complexity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AI pipelines&lt;/td&gt;
&lt;td&gt;Vectorized / O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-golden-interview-rule&#34;&gt;ğŸ Golden Interview Rule&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;First make it work.
Then make it fast.
Then explain why.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you can &lt;strong&gt;explain Big-O clearly&lt;/strong&gt;, you already sound like a professional.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ‰ Final Takeaway&lt;/h2&gt;
&lt;p&gt;Big-O is not math.
Big-O is &lt;strong&gt;thinking about scale&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Once you see it everywhere:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LeetCode becomes easier&lt;/li&gt;
&lt;li&gt;Code becomes cleaner&lt;/li&gt;
&lt;li&gt;Interviews become calmer ğŸ˜„&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Youâ€™re officially past beginner level ğŸš€&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DE101-PD02 â€” Pandas Series, DataFrame, and Indexing (Real-World Mindset)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd02-core-structures/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd02-core-structures/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~45â€“60 minutes&lt;/p&gt;
&lt;h2 id=&#34;-why-pandas-matters-in-the-real-world&#34;&gt;ğŸ¯ Why Pandas Matters in the Real World&lt;/h2&gt;
&lt;p&gt;At companies like &lt;strong&gt;Google, Meta, OpenAI&lt;/strong&gt;, Pandas is often used for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data exploration (EDA)&lt;/li&gt;
&lt;li&gt;Debugging datasets before training models&lt;/li&gt;
&lt;li&gt;Analyzing experiments / A-B tests&lt;/li&gt;
&lt;li&gt;Cleaning logs and user behavior data&lt;/li&gt;
&lt;li&gt;Prototyping ideas before moving to Spark / BigQuery&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pandas is not about speed.&lt;br&gt;
It is about &lt;strong&gt;thinking clearly with data&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-pandas-data-structures&#34;&gt;ğŸ§± Core Pandas Data Structures&lt;/h2&gt;
&lt;p&gt;Pandas has &lt;strong&gt;two fundamental objects&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Series&lt;/strong&gt; â†’ 1D labeled array&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DataFrame&lt;/strong&gt; â†’ 2D table (rows + columns)&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pandas-series&#34;&gt;ğŸ“Œ Pandas Series&lt;/h2&gt;
&lt;p&gt;Think of a &lt;strong&gt;Series&lt;/strong&gt; as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A column in a table&lt;/li&gt;
&lt;li&gt;A vector with labels&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

goals = pd.Series([91, 85, 70], index=[&amp;quot;Messi&amp;quot;, &amp;quot;Ronaldo&amp;quot;, &amp;quot;Mbappe&amp;quot;])
print(goals)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Messi      91
Ronaldo   85
Mbappe    70
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;key-ideas&#34;&gt;Key Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Series = &lt;strong&gt;data + index&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Index gives meaning, not just position&lt;/li&gt;
&lt;li&gt;Used heavily in statistics and ML features&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pandas-dataframe&#34;&gt;ğŸ“Š Pandas DataFrame&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;DataFrame&lt;/strong&gt; is like an Excel table or SQL table.&lt;/p&gt;
&lt;p&gt;Letâ€™s simulate a &lt;strong&gt;FIFA World Cup dataset&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = {
    &amp;quot;player&amp;quot;: [&amp;quot;Messi&amp;quot;, &amp;quot;Mbappe&amp;quot;, &amp;quot;Neymar&amp;quot;, &amp;quot;Ronaldo&amp;quot;],
    &amp;quot;country&amp;quot;: [&amp;quot;Argentina&amp;quot;, &amp;quot;France&amp;quot;, &amp;quot;Brazil&amp;quot;, &amp;quot;Portugal&amp;quot;],
    &amp;quot;goals&amp;quot;: [7, 8, 6, 1],
    &amp;quot;age&amp;quot;: [35, 23, 30, 37]
}

df = pd.DataFrame(data)
print(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;output-1&#34;&gt;Output&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;     player     country  goals  age
0     Messi   Argentina      7   35
1    Mbappe      France      8   23
2    Neymar      Brazil      6   30
3   Ronaldo    Portugal      1   37
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-how-engineers-think-about-dataframes&#34;&gt;ğŸ§  How Engineers Think About DataFrames&lt;/h2&gt;
&lt;p&gt;At big tech companies, engineers think:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rows â†’ observations (players, users, events)&lt;/li&gt;
&lt;li&gt;Columns â†’ features (age, goals, clicks)&lt;/li&gt;
&lt;li&gt;Index â†’ identity or order&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bad indexing = confusing analysis.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-column-selection-most-common-operation&#34;&gt;ğŸ” Column Selection (Most Common Operation)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;player&amp;quot;]
df[&amp;quot;goals&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a &lt;strong&gt;Series&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-loc--label-based-indexing-human-thinking&#34;&gt;ğŸ“ &lt;code&gt;.loc&lt;/code&gt; â€” Label-Based Indexing (Human Thinking)&lt;/h2&gt;
&lt;p&gt;Use &lt;code&gt;.loc&lt;/code&gt; when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You care about &lt;strong&gt;meaning&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You think in terms of labels&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# All rows, specific columns
df.loc[:, [&amp;quot;player&amp;quot;, &amp;quot;goals&amp;quot;]]

# Filter rows by condition
df.loc[df[&amp;quot;country&amp;quot;] == &amp;quot;Argentina&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;example-find-messis-record&#34;&gt;Example: Find Messiâ€™s record&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[df[&amp;quot;player&amp;quot;] == &amp;quot;Messi&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-iloc--position-based-indexing-machine-thinking&#34;&gt;ğŸ“ &lt;code&gt;.iloc&lt;/code&gt; â€” Position-Based Indexing (Machine Thinking)&lt;/h2&gt;
&lt;p&gt;Use &lt;code&gt;.iloc&lt;/code&gt; when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You care about &lt;strong&gt;position&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You donâ€™t trust index labels&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# First row
df.iloc[0]

# First two rows, first two columns
df.iloc[0:2, 0:2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Think of &lt;code&gt;.iloc&lt;/code&gt; like Python slicing.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-loc-vs-iloc-interview-favorite&#34;&gt;âš ï¸ &lt;code&gt;.loc&lt;/code&gt; vs &lt;code&gt;.iloc&lt;/code&gt; (Interview Favorite)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Based on&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.loc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;labels&lt;/td&gt;
&lt;td&gt;df.loc[0] âŒ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.iloc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;positions&lt;/td&gt;
&lt;td&gt;df.iloc[0] âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Interviewers love asking this.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-boolean-filtering-sql-where-equivalent&#34;&gt;ğŸ” Boolean Filtering (SQL WHERE Equivalent)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Players older than 30
df[df[&amp;quot;age&amp;quot;] &amp;gt; 30]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Real-world usage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Filter active users&lt;/li&gt;
&lt;li&gt;Find failed experiments&lt;/li&gt;
&lt;li&gt;Analyze edge cases&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-query--clean--readable-filtering&#34;&gt;ğŸ§® &lt;code&gt;.query()&lt;/code&gt; â€” Clean &amp;amp; Readable Filtering&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.query(&amp;quot;goals &amp;gt;= 7 and age &amp;lt; 30&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why engineers love &lt;code&gt;.query()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Looks like SQL&lt;/li&gt;
&lt;li&gt;Cleaner than long boolean chains&lt;/li&gt;
&lt;li&gt;Easier to debug&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-combining-conditions&#34;&gt;ğŸ§© Combining Conditions&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[(df[&amp;quot;goals&amp;quot;] &amp;gt; 5) &amp;amp; (df[&amp;quot;country&amp;quot;] != &amp;quot;Brazil&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âš ï¸ Must use &lt;code&gt;&amp;amp;&lt;/code&gt; instead of &lt;code&gt;and&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-index-design-very-important&#34;&gt;ğŸ§  Index Design (Very Important)&lt;/h2&gt;
&lt;p&gt;You can set a meaningful index:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.set_index(&amp;quot;player&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[&amp;quot;Messi&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cleaner&lt;/li&gt;
&lt;li&gt;Faster lookup&lt;/li&gt;
&lt;li&gt;Less error-prone&lt;/li&gt;
&lt;/ul&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Why index design matters in production?&lt;/summary&gt;
  &lt;p&gt;Good index = clear logic + faster access + fewer bugs&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-world-use-case-meta--openai-style&#34;&gt;ğŸ§ª Real-World Use Case (Meta / OpenAI Style)&lt;/h2&gt;
&lt;p&gt;Imagine this DataFrame:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rows = model experiments&lt;/li&gt;
&lt;li&gt;Columns = loss, accuracy, timestamp&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Engineers use Pandas to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Filter failed runs&lt;/li&gt;
&lt;li&gt;Compare metrics&lt;/li&gt;
&lt;li&gt;Debug data leakage&lt;/li&gt;
&lt;li&gt;Sanity-check training data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before scaling â†’ Pandas first.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-pandas-patterns-you-must-know&#34;&gt;ğŸ“¦ Common Pandas Patterns You MUST Know&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
df.tail()
df.shape
df.columns
df.info()
df.describe()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are used &lt;strong&gt;daily&lt;/strong&gt; by professionals.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interview-insight&#34;&gt;ğŸ§  Interview Insight&lt;/h2&gt;
&lt;p&gt;Interviewers expect you to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Know &lt;code&gt;.loc&lt;/code&gt; vs &lt;code&gt;.iloc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Filter rows correctly&lt;/li&gt;
&lt;li&gt;Avoid chained indexing&lt;/li&gt;
&lt;li&gt;Write readable Pandas code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Clean Pandas code = clean thinking.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-summary&#34;&gt;ğŸ Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Series = labeled vector&lt;/li&gt;
&lt;li&gt;DataFrame = table&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.loc&lt;/code&gt; = label-based&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.iloc&lt;/code&gt; = position-based&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.query()&lt;/code&gt; = readable filtering&lt;/li&gt;
&lt;li&gt;Indexing = design decision&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Master these, and you already think like a data engineer.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-next-chapter&#34;&gt;ğŸš€ Next Chapter&lt;/h2&gt;
&lt;p&gt;ğŸ‘‰ &lt;strong&gt;DE101-PD03 â€” GroupBy, Aggregation, and Analytics Thinking&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is where Pandas becomes powerful.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lecture 02 â€” Python: The Language of AI</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-02-python/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-02-python/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~2â€“3 hours (core recap lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-python&#34;&gt;ğŸ Why Python?&lt;/h2&gt;
&lt;p&gt;Python is the &lt;strong&gt;language of thinking&lt;/strong&gt; for AI and algorithms.&lt;/p&gt;
&lt;p&gt;It is used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ¤– Artificial Intelligence&lt;/li&gt;
&lt;li&gt;ğŸ“Š Data Science&lt;/li&gt;
&lt;li&gt;ğŸ§  Research &amp;amp; prototyping&lt;/li&gt;
&lt;li&gt;ğŸ§© Coding interviews (LeetCode, Google, Meta)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If you know Python well, you can think clearly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mental-model-important&#34;&gt;ğŸ§  Mental Model (IMPORTANT)&lt;/h2&gt;
&lt;p&gt;Before syntax, remember this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Python executes line by line, top to bottom.&lt;/strong&gt;&lt;br&gt;
Variables point to objects.&lt;br&gt;
Everything is an object.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This mental model helps you avoid 80% of bugs.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-variables--data-types&#34;&gt;1ï¸âƒ£ Variables &amp;amp; Data Types&lt;/h2&gt;
&lt;h3 id=&#34;-variables&#34;&gt;ğŸ“¦ Variables&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = 10
name = &amp;quot;AI&amp;quot;
pi = 3.14
is_active = True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Python is &lt;strong&gt;dynamically typed&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = 10
x = &amp;quot;now a string&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-core-data-types&#34;&gt;ğŸ”¢ Core Data Types&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;int&lt;/td&gt;
&lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;float&lt;/td&gt;
&lt;td&gt;&lt;code&gt;3.14&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;str&lt;/td&gt;
&lt;td&gt;&lt;code&gt;&amp;quot;hello&amp;quot;&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;bool&lt;/td&gt;
&lt;td&gt;&lt;code&gt;True&lt;/code&gt;, &lt;code&gt;False&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;-quiz&#34;&gt;â“ Quiz&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Is Python statically typed?&lt;/summary&gt;
  &lt;p&gt;No. Python is dynamically typed.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-control-flow-if--else&#34;&gt;2ï¸âƒ£ Control Flow (if / else)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = 5

if x &amp;gt; 0:
    print(&amp;quot;Positive&amp;quot;)
elif x == 0:
    print(&amp;quot;Zero&amp;quot;)
else:
    print(&amp;quot;Negative&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-interview-tip&#34;&gt;ğŸ§  Interview Tip&lt;/h3&gt;
&lt;p&gt;Indentation &lt;strong&gt;is logic&lt;/strong&gt; in Python.&lt;/p&gt;
&lt;h3 id=&#34;-quiz-1&#34;&gt;â“ Quiz&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What happens if indentation is wrong?&lt;/summary&gt;
  &lt;p&gt;The program raises an IndentationError or behaves incorrectly.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-loops-for--while&#34;&gt;3ï¸âƒ£ Loops (for / while)&lt;/h2&gt;
&lt;h3 id=&#34;-for-loop-most-common&#34;&gt;ğŸ” for-loop (most common)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(5):
    print(i)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-while-loop&#34;&gt;ğŸ”„ while-loop&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;count = 0
while count &amp;lt; 3:
    print(count)
    count += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-interview-tip-1&#34;&gt;ğŸ§  Interview Tip&lt;/h3&gt;
&lt;p&gt;Use &lt;code&gt;for&lt;/code&gt; unless you &lt;em&gt;must&lt;/em&gt; use &lt;code&gt;while&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-quiz-2&#34;&gt;â“ Quiz&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;When is while-loop dangerous?&lt;/summary&gt;
  &lt;p&gt;When the stopping condition is incorrect â†’ infinite loop.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-lists-most-important-for-leetcode&#34;&gt;4ï¸âƒ£ Lists (Most Important for LeetCode)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nums = [1, 2, 3, 4]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;common-operations&#34;&gt;Common operations&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nums.append(5)
nums.pop()
nums[0]
nums[-1]
len(nums)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;looping-over-list&#34;&gt;Looping over list&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for n in nums:
    print(n)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;with-index&#34;&gt;With index&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, n in enumerate(nums):
    print(i, n)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-quiz-3&#34;&gt;â“ Quiz&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;What is the time complexity of list append?&lt;/summary&gt;
  &lt;p&gt;Amortized O(1)&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-strings-text--sequence&#34;&gt;5ï¸âƒ£ Strings (Text = Sequence)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = &amp;quot;machine&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Strings behave like lists:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s[0]        # &#39;m&#39;
s[-1]       # &#39;e&#39;
s[1:4]      # &#39;ach&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;useful-methods&#34;&gt;Useful methods&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s.lower()
s.upper()
s.split()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-quiz-4&#34;&gt;â“ Quiz&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Are strings mutable?&lt;/summary&gt;
  &lt;p&gt;No. Strings are immutable.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;6-dictionaries-keyvalue-thinking&#34;&gt;6ï¸âƒ£ Dictionaries (Keyâ€“Value Thinking)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;student = {
    &amp;quot;name&amp;quot;: &amp;quot;Kao&amp;quot;,
    &amp;quot;age&amp;quot;: 25,
    &amp;quot;major&amp;quot;: &amp;quot;AI&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;access&#34;&gt;Access&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;student[&amp;quot;name&amp;quot;]
student.get(&amp;quot;age&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;loop&#34;&gt;Loop&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for key, value in student.items():
    print(key, value)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-interview-tip-2&#34;&gt;ğŸ§  Interview Tip&lt;/h3&gt;
&lt;p&gt;Dictionaries give &lt;strong&gt;O(1)&lt;/strong&gt; average lookup.&lt;/p&gt;
&lt;h3 id=&#34;-quiz-5&#34;&gt;â“ Quiz&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Why are dictionaries powerful?&lt;/summary&gt;
  &lt;p&gt;Fast lookup using hashing.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;7-sets-uniqueness&#34;&gt;7ï¸âƒ£ Sets (Uniqueness)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nums = [1, 2, 2, 3]
unique = set(nums)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;use-cases&#34;&gt;Use cases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Remove duplicates&lt;/li&gt;
&lt;li&gt;Fast membership check&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if 3 in unique:
    print(&amp;quot;Found&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-quiz-6&#34;&gt;â“ Quiz&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;Does a set preserve order?&lt;/summary&gt;
  &lt;p&gt;No.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;8-functions-reusable-thinking&#34;&gt;8ï¸âƒ£ Functions (Reusable Thinking)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def add(a, b):
    return a + b
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;default-arguments&#34;&gt;Default arguments&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def greet(name=&amp;quot;AI&amp;quot;):
    print(&amp;quot;Hello&amp;quot;, name)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-quiz-7&#34;&gt;â“ Quiz&lt;/h3&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;Why use functions?&lt;/summary&gt;
  &lt;p&gt;Abstraction, reuse, clarity.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;9-common-built-ins-leetcode-gold&#34;&gt;9ï¸âƒ£ Common Built-ins (LeetCode Gold)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;len()
sum()
min()
max()
sorted()
range()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nums = [3, 1, 2]
sorted(nums)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-time-complexity-awareness-very-important&#34;&gt;ğŸ”Ÿ Time Complexity Awareness (VERY IMPORTANT)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Operation&lt;/th&gt;
&lt;th&gt;Typical Cost&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;List access&lt;/td&gt;
&lt;td&gt;O(1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dict lookup&lt;/td&gt;
&lt;td&gt;O(1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;List search&lt;/td&gt;
&lt;td&gt;O(n)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sorting&lt;/td&gt;
&lt;td&gt;O(n log n)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;Python coding interviews are &lt;strong&gt;algorithm interviews&lt;/strong&gt;, not syntax tests.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-for-ai-mindset&#34;&gt;ğŸ§  Python for AI Mindset&lt;/h2&gt;
&lt;p&gt;Python is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ§© How we express logic&lt;/li&gt;
&lt;li&gt;ğŸ§  How we describe algorithms&lt;/li&gt;
&lt;li&gt;ğŸ¤– How we talk to machines&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Clean Python = Clear Thinking&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>Lecture 03 â€” Training Paradigms: Pretraining, Fine-tuning, and Training from Scratch</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-03-training-paradigms/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-03-training-paradigms/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~3â€“4 hours (core learning lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-lecture-matters&#34;&gt;ğŸŒ Why This Lecture Matters&lt;/h2&gt;
&lt;p&gt;Every modern AI system answers &lt;strong&gt;one critical question&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;How was this intelligence created?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Understanding &lt;em&gt;training paradigms&lt;/em&gt; means understanding:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;capability&lt;/li&gt;
&lt;li&gt;limitation&lt;/li&gt;
&lt;li&gt;cost&lt;/li&gt;
&lt;li&gt;risk&lt;/li&gt;
&lt;li&gt;ethics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This lecture teaches you &lt;strong&gt;how intelligence is shaped&lt;/strong&gt;, not just deployed.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-three-ways-machines-learn&#34;&gt;ğŸ§  The Three Ways Machines Learn&lt;/h2&gt;
&lt;p&gt;All modern multimodal systems are trained using &lt;strong&gt;one (or more)&lt;/strong&gt; of these paradigms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ğŸ§± Training from Scratch&lt;/li&gt;
&lt;li&gt;ğŸ”§ Fine-tuning&lt;/li&gt;
&lt;li&gt;ğŸ§  Pretraining (Foundation Models)&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-big-picture-comparison&#34;&gt;ğŸ§© Big Picture Comparison&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Paradigm&lt;/th&gt;
&lt;th&gt;Data Size&lt;/th&gt;
&lt;th&gt;Cost&lt;/th&gt;
&lt;th&gt;Flexibility&lt;/th&gt;
&lt;th&gt;Risk&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scratch&lt;/td&gt;
&lt;td&gt;Massive&lt;/td&gt;
&lt;td&gt;ğŸ’°ğŸ’°ğŸ’°ğŸ’°&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Very High&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pretraining&lt;/td&gt;
&lt;td&gt;Huge&lt;/td&gt;
&lt;td&gt;ğŸ’°ğŸ’°ğŸ’°&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fine-tuning&lt;/td&gt;
&lt;td&gt;Smallâ€“Medium&lt;/td&gt;
&lt;td&gt;ğŸ’°&lt;/td&gt;
&lt;td&gt;Lowâ€“Medium&lt;/td&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Most real systems use pretrained + fine-tuned models.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-paradigm-1--training-from-scratch&#34;&gt;ğŸ§± Paradigm 1 â€” Training From Scratch&lt;/h2&gt;
&lt;h3 id=&#34;what-it-means&#34;&gt;What It Means&lt;/h3&gt;
&lt;p&gt;Training &lt;strong&gt;all weights from random initialization&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;No prior knowledge.
No shortcuts.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;when-it-makes-sense-rare&#34;&gt;When It Makes Sense (Rare)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;New modality (e.g., brain signals)&lt;/li&gt;
&lt;li&gt;Fundamental research&lt;/li&gt;
&lt;li&gt;Extreme domain shift&lt;/li&gt;
&lt;li&gt;National-scale infrastructure&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-its-dangerous&#34;&gt;Why Itâ€™s Dangerous&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Requires massive data&lt;/li&gt;
&lt;li&gt;Requires massive compute&lt;/li&gt;
&lt;li&gt;High chance of bias&lt;/li&gt;
&lt;li&gt;Easy to fail silently&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Training from scratch is not bravery â€” itâ€™s responsibility.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--scratch-training&#34;&gt;ğŸ§ª Knowledge Check â€” Scratch Training&lt;/h2&gt;
&lt;h3 id=&#34;q1-true--false&#34;&gt;Q1 (True / False)&lt;/h3&gt;
&lt;p&gt;Training from scratch is the best choice for most applications.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2-objective&#34;&gt;Q2 (Objective)&lt;/h3&gt;
&lt;p&gt;Name one valid reason to train from scratch.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;When no pretrained model exists for the modality or domain.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-paradigm-2--pretraining-foundation-models&#34;&gt;ğŸ§  Paradigm 2 â€” Pretraining (Foundation Models)&lt;/h2&gt;
&lt;h3 id=&#34;what-is-pretraining&#34;&gt;What Is Pretraining?&lt;/h3&gt;
&lt;p&gt;Learning &lt;strong&gt;general-purpose representations&lt;/strong&gt; from massive unlabeled or weakly labeled data.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT (text)&lt;/li&gt;
&lt;li&gt;CLIP (imageâ€“text)&lt;/li&gt;
&lt;li&gt;Whisper (audio)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-pretraining-works&#34;&gt;Why Pretraining Works&lt;/h3&gt;
&lt;p&gt;Because the world is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;repetitive&lt;/li&gt;
&lt;li&gt;structured&lt;/li&gt;
&lt;li&gt;statistically learnable&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Pretraining captures &lt;strong&gt;world regularities&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;multimodal-pretraining&#34;&gt;Multimodal Pretraining&lt;/h3&gt;
&lt;p&gt;Typical objective:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
(Image, Text) â†’ Predict missing modality

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates &lt;strong&gt;cross-modal alignment&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--pretraining&#34;&gt;ğŸ§ª Knowledge Check â€” Pretraining&lt;/h2&gt;
&lt;h3 id=&#34;q3-mcq&#34;&gt;Q3 (MCQ)&lt;/h3&gt;
&lt;p&gt;What is the main goal of pretraining?&lt;/p&gt;
&lt;p&gt;A) Task accuracy&lt;br&gt;
B) Memorization&lt;br&gt;
C) General representation learning&lt;br&gt;
D) Deployment speed&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;C) General representation learning&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-paradigm-3--fine-tuning&#34;&gt;ğŸ”§ Paradigm 3 â€” Fine-tuning&lt;/h2&gt;
&lt;h3 id=&#34;what-is-fine-tuning&#34;&gt;What Is Fine-tuning?&lt;/h3&gt;
&lt;p&gt;Adapting a pretrained model to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a specific task&lt;/li&gt;
&lt;li&gt;a specific domain&lt;/li&gt;
&lt;li&gt;a specific behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;types-of-fine-tuning&#34;&gt;Types of Fine-tuning&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Full fine-tuning&lt;/td&gt;
&lt;td&gt;Update all weights&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Partial&lt;/td&gt;
&lt;td&gt;Update some layers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PEFT&lt;/td&gt;
&lt;td&gt;LoRA, adapters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Instruction tuning&lt;/td&gt;
&lt;td&gt;Align behavior&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-fine-tuning-is-powerful&#34;&gt;Why Fine-tuning Is Powerful&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Low data requirement&lt;/li&gt;
&lt;li&gt;Low compute&lt;/li&gt;
&lt;li&gt;Fast iteration&lt;/li&gt;
&lt;li&gt;Safer behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Fine-tuning is &lt;strong&gt;how most intelligence is specialized&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--fine-tuning&#34;&gt;ğŸ§ª Knowledge Check â€” Fine-tuning&lt;/h2&gt;
&lt;h3 id=&#34;q4-true--false&#34;&gt;Q4 (True / False)&lt;/h3&gt;
&lt;p&gt;Fine-tuning always requires large datasets.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5-objective&#34;&gt;Q5 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is PEFT (e.g., LoRA) popular?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It reduces memory and compute while preserving performance.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pretraining-vs-fine-tuning-mental-model&#34;&gt;ğŸ§  Pretraining vs Fine-tuning (Mental Model)&lt;/h2&gt;
&lt;p&gt;Think of a human:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pretraining = education&lt;/li&gt;
&lt;li&gt;Fine-tuning = job training&lt;/li&gt;
&lt;li&gt;Scratch = growing up alone on an island&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-multimodal-specific-considerations&#34;&gt;ğŸ§  Multimodal-Specific Considerations&lt;/h2&gt;
&lt;h3 id=&#34;what-can-be-fine-tuned&#34;&gt;What Can Be Fine-tuned?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Encoders&lt;/li&gt;
&lt;li&gt;Projection layers&lt;/li&gt;
&lt;li&gt;LLM&lt;/li&gt;
&lt;li&gt;Output heads&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;common-strategy-best-practice&#34;&gt;Common Strategy (Best Practice)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Strategy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Encoder&lt;/td&gt;
&lt;td&gt;Freeze&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Projection&lt;/td&gt;
&lt;td&gt;Train&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LLM&lt;/td&gt;
&lt;td&gt;PEFT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Head&lt;/td&gt;
&lt;td&gt;Train&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Alignment layers are usually the sweet spot.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--multimodal-strategy&#34;&gt;ğŸ§ª Knowledge Check â€” Multimodal Strategy&lt;/h2&gt;
&lt;h3 id=&#34;q6-mcq&#34;&gt;Q6 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which component is most commonly fine-tuned first?&lt;/p&gt;
&lt;p&gt;A) Tokenizer&lt;br&gt;
B) Vision encoder&lt;br&gt;
C) Projection layer&lt;br&gt;
D) Dataset&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;C) Projection layer&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-overfitting--catastrophic-forgetting&#34;&gt;âš ï¸ Overfitting &amp;amp; Catastrophic Forgetting&lt;/h2&gt;
&lt;p&gt;Fine-tuning risks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;forgetting general knowledge&lt;/li&gt;
&lt;li&gt;over-specialization&lt;/li&gt;
&lt;li&gt;bias amplification&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mitigations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;low learning rate&lt;/li&gt;
&lt;li&gt;freezing layers&lt;/li&gt;
&lt;li&gt;mixed data&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-paradigm-comparison-by-task&#34;&gt;ğŸ§  Paradigm Comparison by Task&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Best Paradigm&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Image captioning&lt;/td&gt;
&lt;td&gt;Pretrained + light FT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Medical QA&lt;/td&gt;
&lt;td&gt;FT + HITL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;New sensor modality&lt;/td&gt;
&lt;td&gt;Scratch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Internal company data&lt;/td&gt;
&lt;td&gt;RAG or FT&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--decision-making&#34;&gt;ğŸ§ª Knowledge Check â€” Decision Making&lt;/h2&gt;
&lt;h3 id=&#34;q7-objective&#34;&gt;Q7 (Objective)&lt;/h3&gt;
&lt;p&gt;Why might RAG be preferred over fine-tuning?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;When knowledge changes frequently or data cannot be embedded in weights.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-training-is-not-just-optimization&#34;&gt;ğŸ§  Training Is Not Just Optimization&lt;/h2&gt;
&lt;p&gt;Training choices encode:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;values&lt;/li&gt;
&lt;li&gt;assumptions&lt;/li&gt;
&lt;li&gt;power&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Who chooses the data chooses the behavior.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ethical-considerations&#34;&gt;ğŸŒ± Ethical Considerations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pretraining data bias&lt;/li&gt;
&lt;li&gt;Fine-tuning reinforcement of norms&lt;/li&gt;
&lt;li&gt;Scratch training without safeguards&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ethics begins &lt;strong&gt;before training starts&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--ethics&#34;&gt;ğŸ§ª Knowledge Check â€” Ethics&lt;/h2&gt;
&lt;h3 id=&#34;q8-true--false&#34;&gt;Q8 (True / False)&lt;/h3&gt;
&lt;p&gt;Bias can only be fixed during deployment.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False â€” bias enters during data and training.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-architects-decision-tree-practical&#34;&gt;ğŸ§  Architectâ€™s Decision Tree (Practical)&lt;/h2&gt;
&lt;p&gt;Ask:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is there a pretrained model?&lt;/li&gt;
&lt;li&gt;Is the domain stable?&lt;/li&gt;
&lt;li&gt;Is data private?&lt;/li&gt;
&lt;li&gt;Is behavior critical?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then choose:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RAG&lt;/li&gt;
&lt;li&gt;Fine-tuning&lt;/li&gt;
&lt;li&gt;Scratch (rare)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaways&#34;&gt;âœ… Final Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Training defines intelligence&lt;/li&gt;
&lt;li&gt;Pretraining builds foundations&lt;/li&gt;
&lt;li&gt;Fine-tuning specializes behavior&lt;/li&gt;
&lt;li&gt;Scratch training is exceptional&lt;/li&gt;
&lt;li&gt;Ethics is embedded in data&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-9&#34;&gt;
  &lt;summary&gt;If a model learns from biased data, who is accountable?&lt;/summary&gt;
  &lt;p&gt;The humans who selected, curated, and approved the data.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>CS101-LC03 â€” Python for Interviews (From Zero to Thinking Like an Engineer)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc03-python-recap/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc03-python-recap/</guid>
      <description>&lt;h2 id=&#34;-why-python-matters-in-interviews-real-world-edition&#34;&gt;ğŸ¯ Why Python Matters in Interviews (Real World Edition)&lt;/h2&gt;
&lt;p&gt;Python is not tested because it is easy.&lt;/p&gt;
&lt;p&gt;Python is tested because:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;It exposes how you think.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Interviewers look for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can you break a problem into steps?&lt;/li&gt;
&lt;li&gt;Do you choose the right container?&lt;/li&gt;
&lt;li&gt;Can you read your own code after 6 months?&lt;/li&gt;
&lt;li&gt;Do you understand cost (time &amp;amp; memory)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python is a &lt;strong&gt;mirror of your thinking quality&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-how-strong-python-engineers-think&#34;&gt;ğŸ§  How Strong Python Engineers Think&lt;/h2&gt;
&lt;p&gt;Before writing code, they ask:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is the &lt;strong&gt;input shape&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;What must I &lt;strong&gt;track&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;What repeats? (loops)&lt;/li&gt;
&lt;li&gt;What must be &lt;strong&gt;fast lookup&lt;/strong&gt;? (set / dict)&lt;/li&gt;
&lt;li&gt;What logic can become a function?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This chapter builds that instinct.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-absolute-basics-interview-warm-up&#34;&gt;1. Absolute Basics (Interview Warm-Up)&lt;/h2&gt;
&lt;p&gt;These problems look &lt;em&gt;deceptively easy&lt;/em&gt;, yet &lt;strong&gt;many candidates fail them&lt;/strong&gt; â€” not because they canâ€™t code, but because they &lt;strong&gt;donâ€™t think flexibly&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ’¡ The goal is &lt;strong&gt;not&lt;/strong&gt; to memorize one solution,&lt;br&gt;
but to understand &lt;strong&gt;why many solutions are possible&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;11-problem--count-prime-numbers&#34;&gt;1.1 Problem â€” Count Prime Numbers&lt;/h2&gt;
&lt;h3 id=&#34;task&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;Given an integer &lt;code&gt;n&lt;/code&gt;, &lt;strong&gt;count how many prime numbers are strictly less than &lt;code&gt;n&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A prime number is a number:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;greater than 1&lt;/li&gt;
&lt;li&gt;divisible &lt;strong&gt;only&lt;/strong&gt; by 1 and itself&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;12-baseline-reference-solution-most-common&#34;&gt;1.2 Baseline Reference Solution (Most Common)&lt;/h2&gt;
&lt;p&gt;This is the solution &lt;strong&gt;everyone knows&lt;/strong&gt; â€” and thatâ€™s exactly why itâ€™s &lt;em&gt;not enough&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def count_primes(n):
    def is_prime(x):
        if x &amp;lt; 2:
            return False
        for i in range(2, int(x**0.5) + 1):
            if x % i == 0:
                return False
        return True

    count = 0
    for i in range(n):
        if is_prime(i):
            count += 1
    return count
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ… Correct
âŒ Predictable
âŒ Shows &lt;strong&gt;only one way of thinking&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;13-how-strong-candidates-think-about-is_primex&#34;&gt;1.3 How Strong Candidates &lt;em&gt;Think&lt;/em&gt; About &lt;code&gt;is_prime(x)&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;A strong engineer does &lt;strong&gt;not&lt;/strong&gt; ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œHow do I write &lt;code&gt;is_prime&lt;/code&gt;?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;They ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;â€œWhat conditions must be violated for a number to be prime?â€&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This mindset unlocks &lt;strong&gt;multiple valid implementations&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;131-idea-1--divisor-search-for-loop&#34;&gt;1.3.1 Idea 1 â€” Divisor Search (For Loop)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Thinking:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œIf a number has &lt;em&gt;any&lt;/em&gt; divisor other than 1 and itself, itâ€™s not prime.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Stop early when a divisor is found&lt;/li&gt;
&lt;li&gt;No need to check beyond âˆšx&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This leads to the classic &lt;code&gt;for&lt;/code&gt;-loop solution.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;132-idea-2--same-logic-different-control-flow-while-loop&#34;&gt;1.3.2 Idea 2 â€” Same Logic, Different Control Flow (&lt;code&gt;while&lt;/code&gt; loop)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Thinking:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThe logic doesnâ€™t depend on &lt;code&gt;for&lt;/code&gt; â€” it depends on &lt;em&gt;iteration&lt;/em&gt;.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def is_prime(x):
    if x &amp;lt; 2:
        return False
    i = 2
    while i * i &amp;lt;= x:
        if x % i == 0:
            return False
        i += 1
    return True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ¨ Same math
âœ¨ Same correctness
âœ¨ &lt;strong&gt;Different reasoning style&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This shows &lt;strong&gt;control-flow mastery&lt;/strong&gt;, not memorization.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;133-idea-3--mathematical-elimination&#34;&gt;1.3.3 Idea 3 â€” Mathematical Elimination&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Thinking:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œMost numbers are not prime. Can I eliminate many cases early?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def is_prime(x):
    if x &amp;lt; 2:
        return False
    if x == 2:
        return True
    if x % 2 == 0:
        return False

    i = 3
    while i * i &amp;lt;= x:
        if x % i == 0:
            return False
        i += 2
    return True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ¯ Insight:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Skip even numbers entirely&lt;/li&gt;
&lt;li&gt;Half the work, same correctness&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This demonstrates &lt;strong&gt;performance awareness&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;134-idea-4--conceptual-shift-precomputation&#34;&gt;1.3.4 Idea 4 â€” Conceptual Shift (Precomputation)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Thinking:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhy test each number independently?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Instead:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Precompute primes once&lt;/li&gt;
&lt;li&gt;Reuse results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This leads naturally to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sieve of Eratosthenes&lt;/li&gt;
&lt;li&gt;Boolean arrays&lt;/li&gt;
&lt;li&gt;Timeâ€“space tradeoffs&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸš€ This is how &lt;strong&gt;simple problems evolve into algorithms&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;14-what-interviewers-actually-evaluate&#34;&gt;1.4 What Interviewers &lt;em&gt;Actually&lt;/em&gt; Evaluate&lt;/h2&gt;
&lt;p&gt;They are &lt;strong&gt;not&lt;/strong&gt; checking if you remember &lt;code&gt;sqrt(x)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;They look for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;âœ” Logical decomposition&lt;/li&gt;
&lt;li&gt;âœ” Early termination&lt;/li&gt;
&lt;li&gt;âœ” Control-flow flexibility&lt;/li&gt;
&lt;li&gt;âœ” Mathematical intuition&lt;/li&gt;
&lt;li&gt;âœ” Ability to explain &lt;em&gt;why&lt;/em&gt; a solution works&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;15-teaching-message&#34;&gt;1.5 Teaching Message&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;If you can write the same logic in multiple ways,
you truly understand it.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Anyone can copy:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(2, int(x**0.5) + 1):
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Only strong thinkers can say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThatâ€™s just &lt;em&gt;one&lt;/em&gt; way to express the idea.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-interview-follow-up--get-all-prime-numbers&#34;&gt;2. Interview Follow-Up â€” Get &lt;em&gt;All&lt;/em&gt; Prime Numbers&lt;/h2&gt;
&lt;p&gt;After you solve &lt;strong&gt;counting primes&lt;/strong&gt;, a strong interviewer often asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â“ &lt;em&gt;â€œOkay. How would you return &lt;strong&gt;all prime numbers&lt;/strong&gt;?â€&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is where &lt;strong&gt;thinking depth&lt;/strong&gt; matters.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;21-first-question-to-ask-yourself-very-important&#34;&gt;2.1 First Question to Ask Yourself (Very Important)&lt;/h2&gt;
&lt;p&gt;A weak candidate immediately writes code.&lt;/p&gt;
&lt;p&gt;A strong candidate pauses and asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ§  &lt;strong&gt;â€œUp to what range?â€&lt;/strong&gt;&lt;br&gt;
ğŸ§  &lt;strong&gt;â€œOnce or many queries?â€&lt;/strong&gt;&lt;br&gt;
ğŸ§  &lt;strong&gt;â€œDo I care about speed or memory?â€&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These questions decide the solution.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;22-method-1--brute-force-trial-division&#34;&gt;2.2 Method 1 â€” Brute Force (Trial Division)&lt;/h2&gt;
&lt;h3 id=&#34;idea&#34;&gt;Idea&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œA number is prime if it survives divisor testing.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is the &lt;strong&gt;most direct&lt;/strong&gt; extension of &lt;code&gt;is_prime(x)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def all_primes_bruteforce(n):
    primes = []

    for x in range(2, n):
        is_prime = True
        for i in range(2, int(x**0.5) + 1):
            if x % i == 0:
                is_prime = False
                break
        if is_prime:
            primes.append(x)

    return primes
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;why-this-still-matters&#34;&gt;Why This Still Matters&lt;/h3&gt;
&lt;p&gt;âœ” Easy to explain
âœ” Easy to verify
âœ” Good for &lt;strong&gt;small &lt;code&gt;n&lt;/code&gt;&lt;/strong&gt;
âŒ Repeats work many times&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Interview signal: &lt;em&gt;â€œI understand the baseline before optimizing.â€&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;23-method-2--control-flow-mastery-while-based-thinking&#34;&gt;2.3 Method 2 â€” Control-Flow Mastery (While-Based Thinking)&lt;/h2&gt;
&lt;h3 id=&#34;idea-1&#34;&gt;Idea&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThe algorithm doesnâ€™t depend on &lt;code&gt;for&lt;/code&gt; â€” only on logic.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def all_primes_while(n):
    primes = []

    x = 2
    while x &amp;lt; n:
        i = 2
        prime = True
        while i * i &amp;lt;= x:
            if x % i == 0:
                prime = False
                break
            i += 1
        if prime:
            primes.append(x)
        x += 1

    return primes
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;why-interviewers-like-this&#34;&gt;Why Interviewers Like This&lt;/h3&gt;
&lt;p&gt;âœ” Shows &lt;strong&gt;loop control confidence&lt;/strong&gt;
âœ” Demonstrates flexibility
âœ” Not locked to syntax patterns&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Same math, different brain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;24-method-3--mathematical-elimination-smarter-trial-division&#34;&gt;2.4 Method 3 â€” Mathematical Elimination (Smarter Trial Division)&lt;/h2&gt;
&lt;h3 id=&#34;idea-2&#34;&gt;Idea&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œMost numbers are obviously not prime.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Key observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 is the only even prime&lt;/li&gt;
&lt;li&gt;Skip all even numbers&lt;/li&gt;
&lt;li&gt;Check only odd divisors&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def all_primes_optimized(n):
    if n &amp;lt;= 2:
        return []

    primes = [2]

    for x in range(3, n, 2):
        prime = True
        for i in range(3, int(x**0.5) + 1, 2):
            if x % i == 0:
                prime = False
                break
        if prime:
            primes.append(x)

    return primes
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;interview-gold-signal&#34;&gt;Interview Gold Signal&lt;/h3&gt;
&lt;p&gt;âœ” Cuts work ~50%
âœ” Shows &lt;strong&gt;mathematical reasoning&lt;/strong&gt;
âœ” Same correctness, better performance&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;25-method-4--conceptual-leap-sieve-of-eratosthenes&#34;&gt;2.5 Method 4 â€” Conceptual Leap (Sieve of Eratosthenes)&lt;/h2&gt;
&lt;h3 id=&#34;idea-shift-this-is-the-key-moment&#34;&gt;Idea Shift (This Is the Key Moment)&lt;/h3&gt;
&lt;p&gt;Instead of asking:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œIs this number prime?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;â€œWhich numbers are definitely NOT prime?â€&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This flips the problem.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sieve-algorithm&#34;&gt;Sieve Algorithm&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def all_primes_sieve(n):
    if n &amp;lt;= 2:
        return []

    is_prime = [True] * n
    is_prime[0] = is_prime[1] = False

    p = 2
    while p * p &amp;lt; n:
        if is_prime[p]:
            for multiple in range(p * p, n, p):
                is_prime[multiple] = False
        p += 1

    return [i for i in range(n) if is_prime[i]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;why-this-impresses-interviewers&#34;&gt;Why This Impresses Interviewers&lt;/h3&gt;
&lt;p&gt;âœ” Classic algorithm knowledge
âœ” Time complexity: &lt;strong&gt;O(n log log n)&lt;/strong&gt;
âœ” No repeated primality checks
âœ” Shows &lt;strong&gt;algorithmic maturity&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This is where warm-up ends and &lt;em&gt;real CS begins&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;26-method-5--incremental-prime-building-reuse-knowledge&#34;&gt;2.6 Method 5 â€” Incremental Prime Building (Reuse Knowledge)&lt;/h2&gt;
&lt;h3 id=&#34;idea-3&#34;&gt;Idea&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œTo test a number, I only need &lt;strong&gt;previous primes&lt;/strong&gt;.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def all_primes_incremental(n):
    primes = []

    for x in range(2, n):
        prime = True
        for p in primes:
            if p * p &amp;gt; x:
                break
            if x % p == 0:
                prime = False
                break
        if prime:
            primes.append(x)

    return primes
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;why-this-is-subtle-and-powerful&#34;&gt;Why This Is Subtle and Powerful&lt;/h3&gt;
&lt;p&gt;âœ” Avoids redundant checks
âœ” Natural optimization
âœ” Bridges brute force â†’ sieve thinking&lt;/p&gt;
&lt;p&gt;Interviewers often smile at this one ğŸ˜„&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;27-how-to-explain-this-like-a-pro-talking-points&#34;&gt;2.7 How to Explain This Like a Pro (Talking Points)&lt;/h2&gt;
&lt;p&gt;When asked &lt;strong&gt;â€œHow do you get all primes?â€&lt;/strong&gt;, say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThere are multiple valid strategies, depending on constraints.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then briefly summarize:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Key Idea&lt;/th&gt;
&lt;th&gt;Best When&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Brute force&lt;/td&gt;
&lt;td&gt;Test each number&lt;/td&gt;
&lt;td&gt;Small &lt;code&gt;n&lt;/code&gt;, clarity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Optimized trial&lt;/td&gt;
&lt;td&gt;Skip evens&lt;/td&gt;
&lt;td&gt;Medium &lt;code&gt;n&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Incremental&lt;/td&gt;
&lt;td&gt;Reuse primes&lt;/td&gt;
&lt;td&gt;Streaming primes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sieve&lt;/td&gt;
&lt;td&gt;Mark composites&lt;/td&gt;
&lt;td&gt;Large &lt;code&gt;n&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;28-teaching-message-very-strong&#34;&gt;2.8 Teaching Message (Very Strong)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The problem is not about primes.&lt;/strong&gt;
It is about &lt;strong&gt;how you think, structure, and optimize&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If a student can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write it in &lt;strong&gt;multiple ways&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Explain &lt;strong&gt;why each works&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Choose based on &lt;strong&gt;constraints&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸ‘‰ They are interview-ready.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-containers--indexing--the-hidden-power-behind-easy-problems&#34;&gt;3. Containers &amp;amp; Indexing â€” The Hidden Power Behind Easy Problems&lt;/h2&gt;
&lt;p&gt;Many interview problems are labeled &lt;em&gt;easy&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;They are not testing difficulty.&lt;br&gt;
They are testing &lt;strong&gt;how well you control containers and indexing&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;31-containers-you-must-master-not-memorize&#34;&gt;3.1 Containers You Must Master (Not Memorize)&lt;/h2&gt;
&lt;p&gt;Before writing any algorithm, ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ§  &lt;em&gt;â€œWhat container represents my idea most clearly?â€&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Container&lt;/th&gt;
&lt;th&gt;When to Use&lt;/th&gt;
&lt;th&gt;Interview Signal&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;list&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Ordered, indexed data&lt;/td&gt;
&lt;td&gt;Iteration, slicing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;set&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Uniqueness, fast lookup&lt;/td&gt;
&lt;td&gt;Optimization thinking&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Mapping, frequency&lt;/td&gt;
&lt;td&gt;Real-world modeling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;tuple&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fixed structure&lt;/td&gt;
&lt;td&gt;Safety &amp;amp; intent&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Most LeetCode problems are &lt;strong&gt;container problems in disguise&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;32-indexing--where-real-control-begins&#34;&gt;3.2 Indexing â€” Where Real Control Begins&lt;/h2&gt;
&lt;h3 id=&#34;basic-indexing&#34;&gt;Basic Indexing&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;arr = [10, 20, 30, 40, 50]

arr[0]    # 10
arr[1]    # 20
arr[-1]   # 50
arr[-2]   # 40
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Thinking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Positive index â†’ from the front&lt;/li&gt;
&lt;li&gt;Negative index â†’ from the back&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Interview hint: Negative indexing often removes edge-case code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;33-slicing-syntax-start--stop--step&#34;&gt;3.3 Slicing Syntax (Start : Stop : Step)&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;non-negotiable knowledge&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;arr[start : stop : step]
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;start&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Where to begin (inclusive)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;stop&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Where to end (exclusive)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;step&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;How to move&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;331-basic-slicing-warm-up&#34;&gt;3.3.1 Basic Slicing (Warm-Up)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;arr = [0, 1, 2, 3, 4, 5, 6]

arr[1:5]     # [1, 2, 3, 4]
arr[:4]      # [0, 1, 2, 3]
arr[3:]      # [3, 4, 5, 6]
arr[:]       # full copy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Thinking:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œPython slicing avoids off-by-one bugs.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;34-step--where-you-impress-people&#34;&gt;3.4 Step â€” Where You Impress People&lt;/h2&gt;
&lt;h3 id=&#34;skipping-elements&#34;&gt;Skipping Elements&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;arr[::2]     # every 2nd element
arr[1::2]    # odd indices
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimizing prime checks&lt;/li&gt;
&lt;li&gt;Sampling data&lt;/li&gt;
&lt;li&gt;Window-based problems&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;reverse-without-a-loop--interview-favorite&#34;&gt;Reverse Without a Loop (ğŸ”¥ Interview Favorite)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;arr[::-1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No temp variables
No loop
No bugs&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Many candidates forget this. Interviewers do not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;35-advanced-slicing-patterns-real-leetcode-use&#34;&gt;3.5 Advanced Slicing Patterns (Real LeetCode Use)&lt;/h2&gt;
&lt;h3 id=&#34;process-only-odd-numbers&#34;&gt;Process Only Odd Numbers&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nums = list(range(20))
odds = nums[1::2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prime optimization&lt;/li&gt;
&lt;li&gt;Parity problems&lt;/li&gt;
&lt;li&gt;Bit manipulation prep&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sliding-window-intuition-without-writing-it-yet&#34;&gt;Sliding Window Intuition (Without Writing It Yet)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;window_size = 3
for i in range(len(arr) - window_size + 1):
    window = arr[i : i + window_size]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  This idea powers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Max subarray&lt;/li&gt;
&lt;li&gt;Moving average&lt;/li&gt;
&lt;li&gt;Time-series features&lt;/li&gt;
&lt;li&gt;NLP n-grams&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;36-rangestart-stop-step--the-loop-twin-of-slicing&#34;&gt;3.6 &lt;code&gt;range(start, stop, step)&lt;/code&gt; â€” The Loop Twin of Slicing&lt;/h2&gt;
&lt;p&gt;Many people misuse &lt;code&gt;range&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Strong candidates &lt;strong&gt;design it&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;range(start, stop, step)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;examples-that-matter&#34;&gt;Examples That Matter&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;range(5)           # 0,1,2,3,4
range(2, 10)       # 2..9
range(1, 10, 2)    # odd numbers
range(10, 0, -1)   # reverse loop
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;prime-optimization-example-tie-back&#34;&gt;Prime Optimization Example (Tie Back)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(3, int(x**0.5) + 1, 2):
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Insight:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start at 3&lt;/li&gt;
&lt;li&gt;Stop at âˆšx&lt;/li&gt;
&lt;li&gt;Step by 2 (skip evens)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is &lt;strong&gt;not syntax&lt;/strong&gt; â€” it is &lt;strong&gt;thinking encoded into iteration&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;37-containers--indexing--real-world-thinking&#34;&gt;3.7 Containers + Indexing = Real-World Thinking&lt;/h2&gt;
&lt;h3 id=&#34;example-marking-non-primes-sieve-recall&#34;&gt;Example: Marking Non-Primes (Sieve Recall)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;is_prime = [True] * n
is_prime[0] = is_prime[1] = False
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Container choice: list&lt;/li&gt;
&lt;li&gt;Index = number itself&lt;/li&gt;
&lt;li&gt;Value = property of that number&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This is how abstract math becomes code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;38-interviewer-mental-checklist-they-wont-tell-you&#34;&gt;3.8 Interviewer Mental Checklist (They Wonâ€™t Tell You)&lt;/h2&gt;
&lt;p&gt;They silently ask:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does this candidate know slicing?&lt;/li&gt;
&lt;li&gt;Do they avoid unnecessary loops?&lt;/li&gt;
&lt;li&gt;Can they control iteration boundaries?&lt;/li&gt;
&lt;li&gt;Can they reverse, skip, and window cleanly?&lt;/li&gt;
&lt;li&gt;Do they understand start/stop/step?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If yes â†’ &lt;strong&gt;strong hire signal&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;39-teaching-message-very-strong&#34;&gt;3.9 Teaching Message (Very Strong)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Containers are not data.&lt;/strong&gt;
&lt;strong&gt;They are thinking tools.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you master:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Indexing&lt;/li&gt;
&lt;li&gt;Slicing&lt;/li&gt;
&lt;li&gt;Step control&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your code is shorter&lt;/li&gt;
&lt;li&gt;Your logic is clearer&lt;/li&gt;
&lt;li&gt;Your interviews go smoother&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;310-final-impression-line-use-this-verbally&#34;&gt;3.10 Final Impression Line (Use This Verbally)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œMost problems are solved not by clever algorithms,
but by precise control of containers and iteration.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ğŸ”¥ This framing &lt;strong&gt;always&lt;/strong&gt; impresses.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-applied-problem--lionel-messi-at-fc-barcelona-&#34;&gt;4. Applied Problem â€” Lionel Messi at FC Barcelona ğŸ&lt;/h2&gt;
&lt;p&gt;Abstract concepts only stick when people can &lt;strong&gt;see them in real life&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So letâ€™s apply containers and indexing to a story &lt;strong&gt;everyone knows&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;41-problem-story-real-world-framing&#34;&gt;4.1 Problem Story (Real-World Framing)&lt;/h2&gt;
&lt;p&gt;Lionel Messi played for &lt;strong&gt;FC Barcelona&lt;/strong&gt; from &lt;strong&gt;2004 to 2021&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Each season, we record how many goals he scored in all competitions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messi_goals = [
    1,   # 2004
    6,   # 2005
    17,  # 2006
    16,  # 2007
    38,  # 2008
    47,  # 2009
    53,  # 2010
    73,  # 2011 (peak)
    60,  # 2012
    41,  # 2013
    58,  # 2014
    41,  # 2015
    54,  # 2016
    45,  # 2017
    51,  # 2018
    36,  # 2019
    31   # 2020
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Container choice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;list&lt;/code&gt; â†’ ordered by time&lt;/li&gt;
&lt;li&gt;index â†’ season offset&lt;/li&gt;
&lt;li&gt;value â†’ performance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is already &lt;strong&gt;data modeling&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;42-problem-1--first-last-and-peak-seasons&#34;&gt;4.2 Problem 1 â€” First, Last, and Peak Seasons&lt;/h2&gt;
&lt;h3 id=&#34;task-1&#34;&gt;Task&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;First season goals&lt;/li&gt;
&lt;li&gt;Last season goals&lt;/li&gt;
&lt;li&gt;Peak scoring season&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution-indexing&#34;&gt;Solution (Indexing)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;first_season = messi_goals[0]
last_season = messi_goals[-1]
peak_goals = max(messi_goals)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Thinking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Index &lt;code&gt;0&lt;/code&gt; â†’ beginning of career&lt;/li&gt;
&lt;li&gt;Index &lt;code&gt;-1&lt;/code&gt; â†’ end of career&lt;/li&gt;
&lt;li&gt;Built-in functions reduce logic noise&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;43-problem-2--prime-messi-era-slicing&#34;&gt;4.3 Problem 2 â€” Prime Messi Era (Slicing)&lt;/h2&gt;
&lt;h3 id=&#34;story&#34;&gt;Story&lt;/h3&gt;
&lt;p&gt;Fans often say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œMessiâ€™s &lt;strong&gt;prime years&lt;/strong&gt; were from 2009 to 2012.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;task-2&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;Extract only those seasons.&lt;/p&gt;
&lt;h3 id=&#34;thinking-first&#34;&gt;Thinking First&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data starts at 2004&lt;/li&gt;
&lt;li&gt;2009 is index &lt;code&gt;5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;2012 is index &lt;code&gt;8&lt;/code&gt; (exclusive â†’ stop at 9)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prime_years = messi_goals[5:9]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[47, 53, 73, 60]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  This is &lt;strong&gt;timeline slicing&lt;/strong&gt; â€” used everywhere in real systems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;44-problem-3--every-other-season-step-control&#34;&gt;4.4 Problem 3 â€” Every Other Season (Step Control)&lt;/h2&gt;
&lt;h3 id=&#34;story-1&#34;&gt;Story&lt;/h3&gt;
&lt;p&gt;Analysts want to study &lt;strong&gt;long-term consistency&lt;/strong&gt;, skipping every other season.&lt;/p&gt;
&lt;h3 id=&#34;task-3&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;Select goals from alternating seasons.&lt;/p&gt;
&lt;h3 id=&#34;solution-1&#34;&gt;Solution&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alternate_seasons = messi_goals[::2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Meaning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start at beginning&lt;/li&gt;
&lt;li&gt;Go to end&lt;/li&gt;
&lt;li&gt;Step = 2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This pattern appears in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling&lt;/li&gt;
&lt;li&gt;Signal processing&lt;/li&gt;
&lt;li&gt;Optimization loops&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;45-problem-4--late-career-decline-analysis-negative-indexing&#34;&gt;4.5 Problem 4 â€” Late-Career Decline Analysis (Negative Indexing)&lt;/h2&gt;
&lt;h3 id=&#34;story-2&#34;&gt;Story&lt;/h3&gt;
&lt;p&gt;Sports analysts often study &lt;strong&gt;late-career performance&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;task-4&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;Extract the &lt;strong&gt;last 5 seasons&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;solution-2&#34;&gt;Solution&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;late_career = messi_goals[-5:]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Why this is powerful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No need to know list length&lt;/li&gt;
&lt;li&gt;Works even if data grows&lt;/li&gt;
&lt;li&gt;Avoids edge-case bugs&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;46-problem-5--reverse-career-timeline-interview-favorite&#34;&gt;4.6 Problem 5 â€” Reverse Career Timeline (Interview Favorite)&lt;/h2&gt;
&lt;h3 id=&#34;story-3&#34;&gt;Story&lt;/h3&gt;
&lt;p&gt;A visualization tool wants data &lt;strong&gt;from last season to first&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;task-5&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;Reverse the list.&lt;/p&gt;
&lt;h3 id=&#34;solution-3&#34;&gt;Solution&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;reverse_timeline = messi_goals[::-1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ”¥ No loop
ğŸ”¥ No temp variable
ğŸ”¥ Clean and expressive&lt;/p&gt;
&lt;p&gt;Interviewers &lt;em&gt;love&lt;/em&gt; this.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;47-problem-6--sliding-window-form-analysis&#34;&gt;4.7 Problem 6 â€” Sliding Window (Form Analysis)&lt;/h2&gt;
&lt;h3 id=&#34;story-4&#34;&gt;Story&lt;/h3&gt;
&lt;p&gt;Coaches analyze &lt;strong&gt;3-season form windows&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;task-6&#34;&gt;Task&lt;/h3&gt;
&lt;p&gt;Compute 3-season goal windows.&lt;/p&gt;
&lt;h3 id=&#34;solution-4&#34;&gt;Solution&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;window_size = 3
windows = []

for i in range(len(messi_goals) - window_size + 1):
    window = messi_goals[i : i + window_size]
    windows.append(window)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  This exact pattern appears in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stock prices&lt;/li&gt;
&lt;li&gt;Time-series forecasting&lt;/li&gt;
&lt;li&gt;NLP n-grams&lt;/li&gt;
&lt;li&gt;Sports analytics&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;48-problem-7--labeling-seasons-with-a-dictionary&#34;&gt;4.8 Problem 7 â€” Labeling Seasons with a Dictionary&lt;/h2&gt;
&lt;h3 id=&#34;story-5&#34;&gt;Story&lt;/h3&gt;
&lt;p&gt;We want to map &lt;strong&gt;season â†’ goals&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;solution-container-upgrade&#34;&gt;Solution (Container Upgrade)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;years = list(range(2004, 2004 + len(messi_goals)))
messi_by_year = dict(zip(years, messi_goals))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can ask:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messi_by_year[2011]   # 73
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  This is &lt;strong&gt;real-world data modeling&lt;/strong&gt;, not toy code.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;49-interviewer-level-insight-say-this-out-loud&#34;&gt;4.9 Interviewer-Level Insight (Say This Out Loud)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œI model time-series data using ordered containers,
then use indexing and slicing to express questions clearly.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This sentence alone changes how interviewers see you.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;410-teaching-message-strong--memorable&#34;&gt;4.10 Teaching Message (Strong &amp;amp; Memorable)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Containers tell the story.&lt;/strong&gt;
&lt;strong&gt;Indexing lets you ask questions.&lt;/strong&gt;
&lt;strong&gt;Slicing answers them cleanly.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If students understand this section, they are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LeetCode-ready&lt;/li&gt;
&lt;li&gt;Interview-ready&lt;/li&gt;
&lt;li&gt;Real-world-ready&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-olympic-puzzle--the-messi-challenge-advanced-edition-&#34;&gt;5. Olympic Puzzle â€” The Messi Challenge (Advanced Edition) ğŸ…âš½&lt;/h2&gt;
&lt;p&gt;This puzzle looks fun.&lt;/p&gt;
&lt;p&gt;In reality, it tests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;loop control&lt;/li&gt;
&lt;li&gt;state tracking&lt;/li&gt;
&lt;li&gt;early exits&lt;/li&gt;
&lt;li&gt;skipping logic&lt;/li&gt;
&lt;li&gt;recursion thinking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Exactly what &lt;strong&gt;top interviews&lt;/strong&gt; look for.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;51-the-dataset-given&#34;&gt;5.1 The Dataset (Given)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messi_goals = [
    1, 6, 17, 16, 38, 47, 53, 73, 60,
    41, 58, 41, 54, 45, 51, 36, 31
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each element is goals per season, in order.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;52-olympic-rules-strict&#34;&gt;5.2 Olympic Rules (Strict)&lt;/h2&gt;
&lt;p&gt;You &lt;strong&gt;must use&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;âœ… &lt;code&gt;for&lt;/code&gt; loop
âœ… &lt;code&gt;while&lt;/code&gt; loop
âœ… &lt;code&gt;break&lt;/code&gt;
âœ… &lt;code&gt;continue&lt;/code&gt;
âœ… indexing or slicing
âœ… at least one container
â­ recursion = &lt;strong&gt;bonus gold medal&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;âŒ No &lt;code&gt;max()&lt;/code&gt;
âŒ No sorting
âŒ No libraries&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;53-the-challenge-&#34;&gt;5.3 The Challenge ğŸ§©&lt;/h2&gt;
&lt;h3 id=&#34;main-task&#34;&gt;Main Task&lt;/h3&gt;
&lt;p&gt;Find the &lt;strong&gt;longest consecutive streak&lt;/strong&gt; where:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Messi scored &lt;strong&gt;more than 40 goals&lt;/strong&gt;
in &lt;strong&gt;back-to-back seasons&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then output:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Length of the streak&lt;/li&gt;
&lt;li&gt;Goals in the streak&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;54-think-like-an-engineer-say-this-first&#34;&gt;5.4 Think Like an Engineer (Say This First)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThis is sequential data, so I will scan it linearly
and track state transitions.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That sentence alone is interview gold.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;55-core-scan--while-loop--continue&#34;&gt;5.5 Core Scan â€” &lt;code&gt;while&lt;/code&gt; Loop + &lt;code&gt;continue&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 0
current = []
longest = []

while i &amp;lt; len(messi_goals):

    # Skip low-scoring seasons early
    if messi_goals[i] &amp;lt;= 40:
        current = []
        i += 1
        continue   # â¬… skip rest of loop immediately

    current.append(messi_goals[i])

    if len(current) &amp;gt; len(longest):
        longest = current.copy()

    i += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;why-this-is-powerful&#34;&gt;Why This Is Powerful&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;continue&lt;/code&gt; avoids deep nesting&lt;/li&gt;
&lt;li&gt;logic is &lt;strong&gt;flat and readable&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;state resets cleanly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interviewers &lt;em&gt;love&lt;/em&gt; this pattern.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;56-early-exit-logic--break&#34;&gt;5.6 Early Exit Logic â€” &lt;code&gt;break&lt;/code&gt;&lt;/h2&gt;
&lt;h3 id=&#34;scenario&#34;&gt;Scenario&lt;/h3&gt;
&lt;p&gt;What if the interviewer adds:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œStop once you find a streak of &lt;strong&gt;5 seasons&lt;/strong&gt;.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now &lt;code&gt;break&lt;/code&gt; becomes necessary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if len(longest) == 5:
    break   # â¬… mission complete, exit loop early
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Insight:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;break&lt;/code&gt; expresses &lt;strong&gt;confidence&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You stop because you &lt;em&gt;know&lt;/em&gt; the goal is reached&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;57-verifying-the-result&#34;&gt;5.7 Verifying the Result&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Longest streak length:&amp;quot;, len(longest))
print(&amp;quot;Goals:&amp;quot;, longest)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Expected:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Longest streak length: 5
Goals: [47, 53, 73, 60, 41]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;58-using-a-for-loop--index-recovery&#34;&gt;5.8 Using a &lt;code&gt;for&lt;/code&gt; Loop â€” Index Recovery&lt;/h2&gt;
&lt;p&gt;Now locate &lt;strong&gt;where&lt;/strong&gt; the streak starts.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;start_index = -1

for i in range(len(messi_goals)):
    if messi_goals[i:i+len(longest)] == longest:
        start_index = i
        break   # â¬… first match is enough
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;constructs-used-here&#34;&gt;Constructs Used Here&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;for&lt;/code&gt; loop&lt;/li&gt;
&lt;li&gt;slicing&lt;/li&gt;
&lt;li&gt;comparison&lt;/li&gt;
&lt;li&gt;&lt;code&gt;break&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;59-bonus---recursive-thinking-very-impressive&#34;&gt;5.9 BONUS ğŸ¥‡ â€” Recursive Thinking (Very Impressive)&lt;/h2&gt;
&lt;h3 id=&#34;idea-4&#34;&gt;Idea&lt;/h3&gt;
&lt;p&gt;Instead of scanning manually, ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhat is the longest valid streak starting at index &lt;code&gt;i&lt;/code&gt;?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That question is &lt;strong&gt;recursive by nature&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;recursive-function&#34;&gt;Recursive Function&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def streak_from(i):
    if i &amp;gt;= len(messi_goals):
        return []

    if messi_goals[i] &amp;lt;= 40:
        return []

    return [messi_goals[i]] + streak_from(i + 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;using-recursion-to-find-the-best-streak&#34;&gt;Using Recursion to Find the Best Streak&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;best = []

for i in range(len(messi_goals)):
    current = streak_from(i)
    if len(current) &amp;gt; len(best):
        best = current
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Why This Is Gold&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clear base case&lt;/li&gt;
&lt;li&gt;Clear recursive step&lt;/li&gt;
&lt;li&gt;Matches human reasoning&lt;/li&gt;
&lt;li&gt;Shows algorithmic maturity&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;510-interviewer-level-explanation-say-this&#34;&gt;5.10 Interviewer-Level Explanation (Say This)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œI used iteration for efficiency
and recursion to express the problem naturally.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That sentence &lt;strong&gt;changes your level instantly&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;511-why-this-puzzle-is-olympic-level-&#34;&gt;5.11 Why This Puzzle Is Olympic-Level ğŸ†&lt;/h2&gt;
&lt;p&gt;It combines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;container modeling&lt;/li&gt;
&lt;li&gt;indexing &amp;amp; slicing&lt;/li&gt;
&lt;li&gt;&lt;code&gt;for&lt;/code&gt; vs &lt;code&gt;while&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;break&lt;/code&gt; &amp;amp; &lt;code&gt;continue&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;recursion&lt;/li&gt;
&lt;li&gt;edge-case awareness&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All in &lt;strong&gt;one coherent story&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;512-teaching-message-very-strong&#34;&gt;5.12 Teaching Message (Very Strong)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Loops control time.&lt;/strong&gt;
&lt;strong&gt;Containers control space.&lt;/strong&gt;
&lt;strong&gt;Recursion controls structure.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If someone understands all three â€”
they understand programming.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;6-final-boss-puzzle--messi-legendary-season-run--complete-edition&#34;&gt;6. FINAL BOSS PUZZLE â€” Messi Legendary Season Run ğŸğŸ† (Complete Edition)&lt;/h2&gt;
&lt;p&gt;This is the &lt;strong&gt;ultimate recap problem&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It tests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;logic composition&lt;/li&gt;
&lt;li&gt;condition design&lt;/li&gt;
&lt;li&gt;container modeling&lt;/li&gt;
&lt;li&gt;loop mastery&lt;/li&gt;
&lt;li&gt;real-world reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If someone solves this &lt;strong&gt;cleanly and explains it&lt;/strong&gt; â€”&lt;br&gt;
they are &lt;strong&gt;ready for any interview&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;61-the-dataset-multi-dimensional-reality&#34;&gt;6.1 The Dataset (Multi-Dimensional Reality)&lt;/h2&gt;
&lt;p&gt;Each season now has &lt;strong&gt;four attributes&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seasons = [
    {&amp;quot;goals&amp;quot;: 1,  &amp;quot;assists&amp;quot;: 0,  &amp;quot;yellow&amp;quot;: 0, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 6,  &amp;quot;assists&amp;quot;: 3,  &amp;quot;yellow&amp;quot;: 1, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 17, &amp;quot;assists&amp;quot;: 7,  &amp;quot;yellow&amp;quot;: 2, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 16, &amp;quot;assists&amp;quot;: 11, &amp;quot;yellow&amp;quot;: 1, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 38, &amp;quot;assists&amp;quot;: 17, &amp;quot;yellow&amp;quot;: 3, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 47, &amp;quot;assists&amp;quot;: 18, &amp;quot;yellow&amp;quot;: 2, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 53, &amp;quot;assists&amp;quot;: 24, &amp;quot;yellow&amp;quot;: 1, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 73, &amp;quot;assists&amp;quot;: 30, &amp;quot;yellow&amp;quot;: 2, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 60, &amp;quot;assists&amp;quot;: 16, &amp;quot;yellow&amp;quot;: 3, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 41, &amp;quot;assists&amp;quot;: 14, &amp;quot;yellow&amp;quot;: 4, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 58, &amp;quot;assists&amp;quot;: 27, &amp;quot;yellow&amp;quot;: 2, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 41, &amp;quot;assists&amp;quot;: 19, &amp;quot;yellow&amp;quot;: 3, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 54, &amp;quot;assists&amp;quot;: 16, &amp;quot;yellow&amp;quot;: 2, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 45, &amp;quot;assists&amp;quot;: 18, &amp;quot;yellow&amp;quot;: 3, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 51, &amp;quot;assists&amp;quot;: 19, &amp;quot;yellow&amp;quot;: 1, &amp;quot;red&amp;quot;: 0},
    {&amp;quot;goals&amp;quot;: 36, &amp;quot;assists&amp;quot;: 14, &amp;quot;yellow&amp;quot;: 4, &amp;quot;red&amp;quot;: 1},
    {&amp;quot;goals&amp;quot;: 31, &amp;quot;assists&amp;quot;: 9,  &amp;quot;yellow&amp;quot;: 2, &amp;quot;red&amp;quot;: 0}
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  This is &lt;strong&gt;real data modeling&lt;/strong&gt; â€” not toy arrays.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;62-the-legendary-run-definition-hard-logic&#34;&gt;6.2 The Legendary Run Definition (Hard Logic)&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;Legendary Run&lt;/strong&gt; is a &lt;strong&gt;consecutive streak&lt;/strong&gt; of seasons where:&lt;/p&gt;
&lt;h3 id=&#34;performance-conditions&#34;&gt;Performance Conditions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;goals &lt;strong&gt;&amp;gt; 40&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;assists &lt;strong&gt;â‰¥ 15&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;discipline-conditions&#34;&gt;Discipline Conditions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;red cards == &lt;strong&gt;0&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;yellow cards &lt;strong&gt;â‰¤ 3&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;aggregate-constraints&#34;&gt;Aggregate Constraints&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;streak length &lt;strong&gt;â‰¥ 3&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;average goals &lt;strong&gt;&amp;gt; 50 OR&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;(average assists &lt;strong&gt;&amp;gt; 20 AND max goals â‰¥ 70&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes â€” this is &lt;strong&gt;intentional complexity&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;63-output-requirements&#34;&gt;6.3 Output Requirements&lt;/h2&gt;
&lt;p&gt;Return:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Legendary streak (list of seasons)&lt;/li&gt;
&lt;li&gt;Start index&lt;/li&gt;
&lt;li&gt;End index&lt;/li&gt;
&lt;li&gt;Average goals&lt;/li&gt;
&lt;li&gt;Average assists&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;64-olympic-rules-no-shortcuts&#34;&gt;6.4 Olympic Rules (No Shortcuts)&lt;/h2&gt;
&lt;p&gt;You MUST use:&lt;/p&gt;
&lt;p&gt;âœ… &lt;code&gt;while&lt;/code&gt; loop
âœ… &lt;code&gt;for&lt;/code&gt; loop
âœ… &lt;code&gt;break&lt;/code&gt;
âœ… &lt;code&gt;continue&lt;/code&gt;
âœ… indexing
âœ… slicing
âœ… &lt;code&gt;list&lt;/code&gt; and &lt;code&gt;dict&lt;/code&gt;
â­ recursion (required)&lt;/p&gt;
&lt;p&gt;âŒ No &lt;code&gt;sum()&lt;/code&gt;
âŒ No &lt;code&gt;max()&lt;/code&gt;
âŒ No sorting
âŒ No libraries&lt;/p&gt;
&lt;p&gt;There is &lt;strong&gt;only one correct result&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;65-recursive-helpers-no-built-ins-allowed&#34;&gt;6.5 Recursive Helpers (No Built-ins Allowed)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def recursive_sum(arr, key):
    if not arr:
        return 0
    return arr[0][key] + recursive_sum(arr[1:], key)

def recursive_max(arr, key):
    if len(arr) == 1:
        return arr[0][key]
    rest_max = recursive_max(arr[1:], key)
    return arr[0][key] if arr[0][key] &amp;gt; rest_max else rest_max
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  This proves &lt;strong&gt;algorithmic understanding&lt;/strong&gt;, not memorization.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;66-core-scan--nested-while--continue&#34;&gt;6.6 Core Scan â€” Nested &lt;code&gt;while&lt;/code&gt; + &lt;code&gt;continue&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 0
best = {
    &amp;quot;streak&amp;quot;: [],
    &amp;quot;start&amp;quot;: -1,
    &amp;quot;end&amp;quot;: -1,
    &amp;quot;avg_goals&amp;quot;: 0,
    &amp;quot;avg_assists&amp;quot;: 0
}

while i &amp;lt; len(seasons):

    s = seasons[i]

    # Early rejection (discipline + performance)
    if not (
        s[&amp;quot;goals&amp;quot;] &amp;gt; 40 and
        s[&amp;quot;assists&amp;quot;] &amp;gt;= 15 and
        s[&amp;quot;red&amp;quot;] == 0 and
        s[&amp;quot;yellow&amp;quot;] &amp;lt;= 3
    ):
        i += 1
        continue

    j = i
    current = []
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;67-build-the-consecutive-run&#34;&gt;6.7 Build the Consecutive Run&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    while j &amp;lt; len(seasons):
        s = seasons[j]

        if (
            s[&amp;quot;goals&amp;quot;] &amp;gt; 40 and
            s[&amp;quot;assists&amp;quot;] &amp;gt;= 15 and
            s[&amp;quot;red&amp;quot;] == 0 and
            s[&amp;quot;yellow&amp;quot;] &amp;lt;= 3
        ):
            current.append(s)
            j += 1
        else:
            break
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Uses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;indexing&lt;/li&gt;
&lt;li&gt;&lt;code&gt;while&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;compound conditions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;break&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;68-validate-the-run-complex-logic-zone&#34;&gt;6.8 Validate the Run (Complex Logic Zone)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    if len(current) &amp;gt;= 3:
        avg_goals = recursive_sum(current, &amp;quot;goals&amp;quot;) / len(current)
        avg_assists = recursive_sum(current, &amp;quot;assists&amp;quot;) / len(current)
        max_goals = recursive_max(current, &amp;quot;goals&amp;quot;)

        if (
            avg_goals &amp;gt; 50 or
            (avg_assists &amp;gt; 20 and max_goals &amp;gt;= 70)
        ):
            if avg_goals &amp;gt; best[&amp;quot;avg_goals&amp;quot;]:
                best[&amp;quot;streak&amp;quot;] = current
                best[&amp;quot;start&amp;quot;] = i
                best[&amp;quot;end&amp;quot;] = j - 1
                best[&amp;quot;avg_goals&amp;quot;] = avg_goals
                best[&amp;quot;avg_assists&amp;quot;] = avg_assists
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ”¥ This block alone tests &lt;strong&gt;real logical maturity&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;69-smart-exit--break-with-confidence&#34;&gt;6.9 Smart Exit â€” &lt;code&gt;break&lt;/code&gt; with Confidence&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    if len(current) &amp;gt;= 5:
        break

    i = j
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why this matters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shows optimization instinct&lt;/li&gt;
&lt;li&gt;Avoids brute force thinking&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;610-final-output&#34;&gt;6.10 Final Output&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Legendary Run:&amp;quot;, best[&amp;quot;streak&amp;quot;])
print(&amp;quot;Start Index:&amp;quot;, best[&amp;quot;start&amp;quot;])
print(&amp;quot;End Index:&amp;quot;, best[&amp;quot;end&amp;quot;])
print(&amp;quot;Avg Goals:&amp;quot;, best[&amp;quot;avg_goals&amp;quot;])
print(&amp;quot;Avg Assists:&amp;quot;, best[&amp;quot;avg_assists&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;expected-result&#34;&gt;Expected Result&lt;/h3&gt;
&lt;p&gt;The legendary run corresponds to Messiâ€™s &lt;strong&gt;2009â€“2013 peak&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;611-where-everything-appears-final-recap&#34;&gt;6.11 Where EVERYTHING Appears (Final Recap)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Concept&lt;/th&gt;
&lt;th&gt;Used&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;list / dict&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;indexing&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;slicing logic&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;for loop&lt;/td&gt;
&lt;td&gt;validation &amp;amp; explanation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;while loop&lt;/td&gt;
&lt;td&gt;core scan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;break&lt;/td&gt;
&lt;td&gt;boundary control&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;continue&lt;/td&gt;
&lt;td&gt;early rejection&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AND / OR logic&lt;/td&gt;
&lt;td&gt;heavy&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;recursion&lt;/td&gt;
&lt;td&gt;aggregation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;state tracking&lt;/td&gt;
&lt;td&gt;best run&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is &lt;strong&gt;complete mastery&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;612-ultimate-teaching-message-mic-drop&#34;&gt;6.12 Ultimate Teaching Message (Mic Drop)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Easy problems test syntax.&lt;/strong&gt;
&lt;strong&gt;Hard problems test logic.&lt;/strong&gt;
&lt;strong&gt;Great problems test judgment.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If someone solves this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They can reason&lt;/li&gt;
&lt;li&gt;They can model&lt;/li&gt;
&lt;li&gt;They can explain&lt;/li&gt;
&lt;li&gt;They can build real systems&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;613-final-line-&#34;&gt;6.13 Final Line ğŸâš½ğŸ”¥&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œMessi wasnâ€™t great because he scored goals.
He was great because he satisfied &lt;em&gt;every condition&lt;/em&gt;
under pressure â€” every season.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;7-final-ultimate-cheat-sheet--messi-edition-&#34;&gt;7. Final Ultimate Cheat Sheet â€” Messi Edition ğŸâš½&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;A Complete Recap of Python Thinking&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This section is a &lt;strong&gt;one-page mental model&lt;/strong&gt; of Python fundamentals.&lt;/p&gt;
&lt;p&gt;If you understand &lt;strong&gt;everything here&lt;/strong&gt;,&lt;br&gt;
you understand &lt;strong&gt;how to think in Python&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;71-data-modeling-everything-starts-here&#34;&gt;7.1 Data Modeling (Everything Starts Here)&lt;/h2&gt;
&lt;p&gt;Messiâ€™s Barcelona career modeled as structured data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;season = {
    &amp;quot;goals&amp;quot;: 73,
    &amp;quot;assists&amp;quot;: 30,
    &amp;quot;yellow&amp;quot;: 2,
    &amp;quot;red&amp;quot;: 0
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Key idea:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dict&lt;/code&gt; models &lt;strong&gt;real-world entities&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;keys = attributes&lt;/li&gt;
&lt;li&gt;values = facts&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;72-logic-conditions--and--or--not-core-thinking&#34;&gt;7.2 Logic Conditions â€” AND / OR / NOT (Core Thinking)&lt;/h2&gt;
&lt;h3 id=&#34;example-legendary-season-rule&#34;&gt;Example: Legendary Season Rule&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;is_legendary = (
    season[&amp;quot;goals&amp;quot;] &amp;gt; 40 and
    season[&amp;quot;assists&amp;quot;] &amp;gt;= 15 and
    season[&amp;quot;red&amp;quot;] == 0 and
    season[&amp;quot;yellow&amp;quot;] &amp;lt;= 3
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;mixed-logic-interview-favorite&#34;&gt;Mixed Logic (Interview Favorite)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;is_iconic = (
    season[&amp;quot;goals&amp;quot;] &amp;gt; 50 or
    (season[&amp;quot;assists&amp;quot;] &amp;gt; 20 and season[&amp;quot;goals&amp;quot;] &amp;gt;= 70)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Rule of thumb:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;and&lt;/code&gt; â†’ &lt;strong&gt;all must pass&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;or&lt;/code&gt; â†’ &lt;strong&gt;at least one passes&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;parentheses = &lt;strong&gt;thinking clarity&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;73-while-loop--when-you-dont-know-the-end&#34;&gt;7.3 &lt;code&gt;while&lt;/code&gt; Loop â€” When You Donâ€™t Know the End&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 0
while i &amp;lt; len(seasons):
    if seasons[i][&amp;quot;red&amp;quot;] &amp;gt; 0:
        i += 1
        continue
    i += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Use &lt;code&gt;while&lt;/code&gt; when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;scanning&lt;/li&gt;
&lt;li&gt;searching&lt;/li&gt;
&lt;li&gt;state-based logic&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;74-continue--skip-noise-early&#34;&gt;7.4 &lt;code&gt;continue&lt;/code&gt; â€” Skip Noise Early&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if season[&amp;quot;goals&amp;quot;] &amp;lt;= 40:
    continue
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ” avoids deep nesting
âœ” keeps logic flat
âœ” very interview-friendly&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;75-break--stop-with-confidence&#34;&gt;7.5 &lt;code&gt;break&lt;/code&gt; â€” Stop With Confidence&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if len(streak) &amp;gt;= 5:
    break
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  &lt;code&gt;break&lt;/code&gt; means:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œI know Iâ€™m done.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Strong signal of control.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;76-for-loop--when-order-is-known&#34;&gt;7.6 &lt;code&gt;for&lt;/code&gt; Loop â€” When Order Is Known&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for season in seasons:
    print(season[&amp;quot;goals&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with index:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(len(seasons)):
    print(i, seasons[i][&amp;quot;goals&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Use &lt;code&gt;for&lt;/code&gt; when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;iteration count is clear&lt;/li&gt;
&lt;li&gt;data is stable&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;77-indexing--slicing--timeline-control&#34;&gt;7.7 Indexing &amp;amp; Slicing â€” Timeline Control&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seasons[0]      # first season
seasons[-1]     # last season
seasons[5:9]    # Messi prime
seasons[::-1]   # reverse career
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Slicing = &lt;strong&gt;asking questions about time&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;78-functions-def--name-your-thinking&#34;&gt;7.8 Functions (&lt;code&gt;def&lt;/code&gt;) â€” Name Your Thinking&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def is_legendary(season):
    return (
        season[&amp;quot;goals&amp;quot;] &amp;gt; 40 and
        season[&amp;quot;assists&amp;quot;] &amp;gt;= 15 and
        season[&amp;quot;red&amp;quot;] == 0
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compress logic&lt;/li&gt;
&lt;li&gt;improve readability&lt;/li&gt;
&lt;li&gt;reduce bugs&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;79-decorators--behavior-on-top-of-behavior-advanced-basic&#34;&gt;7.9 Decorators â€” Behavior on Top of Behavior (Advanced Basic)&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;You want to &lt;strong&gt;add behavior&lt;/strong&gt; without changing logic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_check(func):
    def wrapper(season):
        result = func(season)
        print(&amp;quot;Checked season â†’&amp;quot;, result)
        return result
    return wrapper
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;usage&#34;&gt;Usage&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@log_check
def is_goat_season(season):
    return season[&amp;quot;goals&amp;quot;] &amp;gt;= 70
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Decorators =
&lt;strong&gt;logic + policy separation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Very impressive when explained simply.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;710-classes--modeling-the-real-world&#34;&gt;7.10 Classes â€” Modeling the Real World&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MessiSeason:
    def __init__(self, goals, assists, yellow, red):
        self.goals = goals
        self.assists = assists
        self.yellow = yellow
        self.red = red

    def is_clean(self):
        return self.red == 0 and self.yellow &amp;lt;= 3

    def is_legendary(self):
        return self.goals &amp;gt; 40 and self.assists &amp;gt;= 15 and self.is_clean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Usage:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;season_2011 = MessiSeason(73, 30, 2, 0)
season_2011.is_legendary()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ§  Classes =
&lt;strong&gt;data + behavior together&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is real software design.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;711-how-everything-fits-together-mental-map&#34;&gt;7.11 How Everything Fits Together (Mental Map)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Concept&lt;/th&gt;
&lt;th&gt;Purpose&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;dict / class&lt;/td&gt;
&lt;td&gt;model reality&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;if / and / or&lt;/td&gt;
&lt;td&gt;express rules&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;while&lt;/td&gt;
&lt;td&gt;scan &amp;amp; search&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;continue&lt;/td&gt;
&lt;td&gt;skip noise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;break&lt;/td&gt;
&lt;td&gt;stop early&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;for&lt;/td&gt;
&lt;td&gt;controlled iteration&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;slicing&lt;/td&gt;
&lt;td&gt;timeline logic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;def&lt;/td&gt;
&lt;td&gt;name thinking&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;decorator&lt;/td&gt;
&lt;td&gt;extend behavior&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;class&lt;/td&gt;
&lt;td&gt;design systems&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This is the &lt;strong&gt;full stack of thinking&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;712-final-teaching-message-ultimate&#34;&gt;7.12 Final Teaching Message (Ultimate)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Syntax is vocabulary.&lt;/strong&gt;
&lt;strong&gt;Logic is grammar.&lt;/strong&gt;
&lt;strong&gt;Structure is storytelling.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Messi didnâ€™t become the GOAT by knowing rules â€”
he mastered &lt;strong&gt;when and how&lt;/strong&gt; to apply them.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;713-final-line-&#34;&gt;7.13 Final Line ğŸğŸ”¥&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œIf you can model Messiâ€™s career in code,
you can model almost anything in the real world.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-problem-2--frequency-counter-real-life&#34;&gt;ğŸ”¢ Problem 2 â€” Frequency Counter (Real Life)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Count how many times each number appears.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def frequency(arr):
    freq = {}
    for x in arr:
        freq[x] = freq.get(x, 0) + 1
    return freq
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used everywhere:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logs&lt;/li&gt;
&lt;li&gt;NLP tokens&lt;/li&gt;
&lt;li&gt;Event analytics&lt;/li&gt;
&lt;li&gt;ML preprocessing&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-iteration-patterns-fundamental-thinking&#34;&gt;ğŸ” Iteration Patterns (Fundamental Thinking)&lt;/h2&gt;
&lt;h3 id=&#34;looping-over-values&#34;&gt;Looping Over Values&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for x in arr:
    print(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You donâ€™t care about position&lt;/li&gt;
&lt;li&gt;Logic depends only on value&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;looping-with-index-important&#34;&gt;Looping With Index (IMPORTANT)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i, x in enumerate(arr):
    print(i, x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Position matters&lt;/li&gt;
&lt;li&gt;Sliding window&lt;/li&gt;
&lt;li&gt;Adjacent comparison&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;looping-with-range-indexing-power&#34;&gt;Looping With Range (Indexing Power)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(0, len(arr), 2):
    print(arr[i])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how &lt;strong&gt;real indexing problems are solved&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-containers-the-core-of-python-power&#34;&gt;ğŸ§º Containers (The Core of Python Power)&lt;/h2&gt;
&lt;h3 id=&#34;list--ordered-mutable&#34;&gt;List â€” Ordered, Mutable&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nums = [1, 2, 3]
nums.append(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Order matters&lt;/li&gt;
&lt;li&gt;Sequential access&lt;/li&gt;
&lt;li&gt;Sliding window&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;tuple--fixed-safe&#34;&gt;Tuple â€” Fixed, Safe&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;point = (3, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Coordinates&lt;/li&gt;
&lt;li&gt;Keys in dict&lt;/li&gt;
&lt;li&gt;Immutable records&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;set--fast-membership&#34;&gt;Set â€” Fast Membership&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seen = set()
if x in seen:
    print(&amp;quot;Duplicate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ Average lookup: &lt;strong&gt;O(1)&lt;/strong&gt;
Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deduplication&lt;/li&gt;
&lt;li&gt;Cycle detection&lt;/li&gt;
&lt;li&gt;Graphs&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;dictionary--the-interview-weapon&#34;&gt;Dictionary â€” The Interview Weapon&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;freq = {}
for x in arr:
    freq[x] = freq.get(x, 0) + 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Counting&lt;/li&gt;
&lt;li&gt;Mapping&lt;/li&gt;
&lt;li&gt;Caching&lt;/li&gt;
&lt;li&gt;DP states&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If you master dictionaries, &lt;strong&gt;half of LeetCode disappears&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-searching-patterns&#34;&gt;ğŸ” Searching Patterns&lt;/h2&gt;
&lt;h3 id=&#34;linear-search-baseline&#34;&gt;Linear Search (Baseline)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def linear_search(arr, target):
    for x in arr:
        if x == target:
            return True
    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ O(n)&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;binary-search-mindset-shift&#34;&gt;Binary Search (Mindset Shift)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def binary_search(arr, target):
    l, r = 0, len(arr) - 1

    while l &amp;lt;= r:
        mid = (l + r) // 2
        if arr[mid] == target:
            return True
        elif arr[mid] &amp;lt; target:
            l = mid + 1
        else:
            r = mid - 1

    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ O(log n)&lt;/p&gt;
&lt;p&gt;Interviewers care more about:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Why binary search works&lt;/strong&gt;, not syntax.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vector-thinking-ai--data-foundation&#34;&gt;ğŸ§® Vector Thinking (AI &amp;amp; Data Foundation)&lt;/h2&gt;
&lt;h3 id=&#34;vector-addition&#34;&gt;Vector Addition&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def add_vectors(a, b):
    return [a[i] + b[i] for i in range(len(a))]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;dot-product&#34;&gt;Dot Product&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def dot(a, b):
    return sum(a[i] * b[i] for i in range(len(a)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neural networks&lt;/li&gt;
&lt;li&gt;Similarity search&lt;/li&gt;
&lt;li&gt;Attention mechanisms&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-matrix-thinking-2d-indexing&#34;&gt;ğŸ§± Matrix Thinking (2D Indexing)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;matrix = [
    [1, 2, 3],
    [4, 5, 6]
]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;traversal&#34;&gt;Traversal&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(len(matrix)):
    for j in range(len(matrix[0])):
        print(matrix[i][j])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Index control = &lt;strong&gt;engineering maturity&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;matrix-multiplication-classic-filter&#34;&gt;Matrix Multiplication (Classic Filter)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def matmul(A, B):
    result = [[0]*len(B[0]) for _ in range(len(A))]

    for i in range(len(A)):
        for j in range(len(B[0])):
            for k in range(len(B)):
                result[i][j] += A[i][k] * B[k][j]

    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ O(nÂ³)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-functions--thinking-units&#34;&gt;ğŸ§  Functions = Thinking Units&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def square(x: int) -&amp;gt; int:
    return x * x
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do one thing&lt;/li&gt;
&lt;li&gt;Have clear inputs/outputs&lt;/li&gt;
&lt;li&gt;Are testable&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-classes-engineer-level&#34;&gt;ğŸ§± Classes (Engineer Level)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Vector:
    def __init__(self, values):
        self.values = values

    def dot(self, other):
        return sum(a*b for a, b in zip(self.values, other.values))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why classes matter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Modeling real systems&lt;/li&gt;
&lt;li&gt;Clean APIs&lt;/li&gt;
&lt;li&gt;Interview design questions&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-inheritance-abstraction-skill&#34;&gt;ğŸ§¬ Inheritance (Abstraction Skill)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Matrix(Vector):
    def shape(self):
        return (len(self.values), len(self.values[0]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frameworks&lt;/li&gt;
&lt;li&gt;ML pipelines&lt;/li&gt;
&lt;li&gt;Systems design&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pythonic-patterns-interviewers-love&#34;&gt;ğŸ§  Pythonic Patterns Interviewers Love&lt;/h2&gt;
&lt;h3 id=&#34;list-comprehension&#34;&gt;List Comprehension&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[x*x for x in range(10) if x % 2 == 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;dictionary-comprehension&#34;&gt;Dictionary Comprehension&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{x: arr.count(x) for x in set(arr)}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;any--all&#34;&gt;any / all&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;any(x &amp;lt; 0 for x in arr)
all(x &amp;gt; 0 for x in arr)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-interview-mistakes&#34;&gt;ğŸš¨ Common Interview Mistakes&lt;/h2&gt;
&lt;p&gt;âŒ Writing code before thinking
âŒ Ignoring complexity
âŒ Using wrong container
âŒ Overengineering simple logic&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ Final Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Python interviews are not about Python.&lt;/p&gt;
&lt;p&gt;They are about &lt;strong&gt;structured thinking&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Count primes&lt;/li&gt;
&lt;li&gt;Track frequencies&lt;/li&gt;
&lt;li&gt;Control indices&lt;/li&gt;
&lt;li&gt;Design functions&lt;/li&gt;
&lt;li&gt;Choose containers wisely&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You are &lt;strong&gt;ready for real interviews&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>DE101-PD03 â€” Data Cleaning &amp; Transformation (Production Mindset)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd03-data-cleaning/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd03-data-cleaning/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~60 minutes&lt;/p&gt;
&lt;h2 id=&#34;-why-data-cleaning-is-the-real-job&#34;&gt;ğŸ¯ Why Data Cleaning Is the Real Job&lt;/h2&gt;
&lt;p&gt;In the real world:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;70â€“80% of data work = cleaning &amp;amp; transformation&lt;/li&gt;
&lt;li&gt;Models fail more often due to &lt;strong&gt;bad data&lt;/strong&gt;, not bad algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At companies like &lt;strong&gt;Google, Meta, OpenAI&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;If your data is wrong, your model is wrong.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-data-cleaning&#34;&gt;ğŸ§  What Is Data Cleaning?&lt;/h2&gt;
&lt;p&gt;Data cleaning means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Making data &lt;strong&gt;usable&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Making assumptions &lt;strong&gt;explicit&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Removing ambiguity&lt;/li&gt;
&lt;li&gt;Preventing silent bugs&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-real-world-data-problems&#34;&gt;ğŸ§¹ Common Real-World Data Problems&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Problem&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Missing values&lt;/td&gt;
&lt;td&gt;NaN age, null clicks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wrong data type&lt;/td&gt;
&lt;td&gt;&amp;ldquo;30&amp;rdquo; instead of 30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Inconsistent labels&lt;/td&gt;
&lt;td&gt;&amp;ldquo;USA&amp;rdquo;, &amp;ldquo;U.S.&amp;rdquo;, &amp;ldquo;United States&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Duplicate rows&lt;/td&gt;
&lt;td&gt;Same user logged twice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Outliers&lt;/td&gt;
&lt;td&gt;Age = 999&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-dataset-user-analytics&#34;&gt;ğŸ“Š Example Dataset (User Analytics)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np

data = {
    &amp;quot;user_id&amp;quot;: [1, 2, 3, 3],
    &amp;quot;age&amp;quot;: [25, None, &amp;quot;30&amp;quot;, 30],
    &amp;quot;country&amp;quot;: [&amp;quot;US&amp;quot;, &amp;quot;USA&amp;quot;, &amp;quot;us&amp;quot;, &amp;quot;USA&amp;quot;],
    &amp;quot;clicks&amp;quot;: [10, np.nan, 5, 5]
}

df = pd.DataFrame(data)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-1-inspect-before-cleaning-critical&#34;&gt;ğŸ” Step 1: Inspect Before Cleaning (Critical)&lt;/h2&gt;
&lt;p&gt;Never clean blindly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info()
df.describe(include=&amp;quot;all&amp;quot;)
df.isna().sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This answers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which columns are broken?&lt;/li&gt;
&lt;li&gt;How many missing values?&lt;/li&gt;
&lt;li&gt;Are types correct?&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-removing-missing-data-dropna&#34;&gt;ğŸ—‘ï¸ Removing Missing Data (&lt;code&gt;dropna&lt;/code&gt;)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.dropna()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âš ï¸ Dangerous in production:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You may delete important users&lt;/li&gt;
&lt;li&gt;Always know &lt;strong&gt;why&lt;/strong&gt; you drop&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Better:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.dropna(subset=[&amp;quot;age&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-filling-missing-data-fillna&#34;&gt;ğŸ©¹ Filling Missing Data (&lt;code&gt;fillna&lt;/code&gt;)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;clicks&amp;quot;] = df[&amp;quot;clicks&amp;quot;].fillna(0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Common strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mean / median (numeric)&lt;/li&gt;
&lt;li&gt;Mode (categorical)&lt;/li&gt;
&lt;li&gt;Zero (counts)&lt;/li&gt;
&lt;li&gt;Forward fill (time series)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-fixing-data-types-astype&#34;&gt;ğŸ”¢ Fixing Data Types (&lt;code&gt;astype&lt;/code&gt;)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;age&amp;quot;] = df[&amp;quot;age&amp;quot;].astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This often fails â†’ must clean first:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;age&amp;quot;] = pd.to_numeric(df[&amp;quot;age&amp;quot;], errors=&amp;quot;coerce&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Engineers always expect type issues.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-renaming-for-clarity&#34;&gt;ğŸ·ï¸ Renaming for Clarity&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.rename(columns={&amp;quot;clicks&amp;quot;: &amp;quot;num_clicks&amp;quot;})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why this matters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Self-documenting code&lt;/li&gt;
&lt;li&gt;Fewer misunderstandings&lt;/li&gt;
&lt;li&gt;Cleaner downstream pipelines&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-deduplication-very-common&#34;&gt;ğŸ§  Deduplication (Very Common)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.drop_duplicates(subset=[&amp;quot;user_id&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User logs&lt;/li&gt;
&lt;li&gt;Event streams&lt;/li&gt;
&lt;li&gt;Experiment tracking&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-normalizing-categorical-data&#34;&gt;ğŸŒ Normalizing Categorical Data&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;country&amp;quot;] = (
    df[&amp;quot;country&amp;quot;]
    .str.lower()
    .replace({&amp;quot;us&amp;quot;: &amp;quot;usa&amp;quot;})
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Small inconsistency â†’ huge analytics bug.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-feature-transformation&#34;&gt;ğŸ”„ Feature Transformation&lt;/h2&gt;
&lt;p&gt;Create new features from old ones:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;is_active&amp;quot;] = df[&amp;quot;num_clicks&amp;quot;] &amp;gt; 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is &lt;strong&gt;feature engineering&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-outlier-handling&#34;&gt;ğŸ§ª Outlier Handling&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df[df[&amp;quot;age&amp;quot;] &amp;lt; 100]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Outliers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Break averages&lt;/li&gt;
&lt;li&gt;Break models&lt;/li&gt;
&lt;li&gt;Must be justified, not guessed&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-assign--functional-style&#34;&gt;ğŸ§© &lt;code&gt;.assign()&lt;/code&gt; â€” Functional Style&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.assign(
    age_next_year=lambda x: x[&amp;quot;age&amp;quot;] + 1,
    clicks_per_age=lambda x: x[&amp;quot;num_clicks&amp;quot;] / x[&amp;quot;age&amp;quot;]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No mutation. Clear logic.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-method-chaining-professional-style&#34;&gt;ğŸ”— Method Chaining (Professional Style)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = (
    df
    .drop_duplicates(&amp;quot;user_id&amp;quot;)
    .assign(
        age=lambda x: pd.to_numeric(x[&amp;quot;age&amp;quot;], errors=&amp;quot;coerce&amp;quot;),
        country=lambda x: x[&amp;quot;country&amp;quot;].str.lower()
    )
    .dropna(subset=[&amp;quot;age&amp;quot;])
    .query(&amp;quot;age &amp;lt; 100&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how &lt;strong&gt;production Pandas code looks&lt;/strong&gt;.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Why method chaining is preferred in real projects?&lt;/summary&gt;
  &lt;p&gt;Readable, debuggable, testable, reproducible&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-pipeline-thinking-interview-gold&#34;&gt;ğŸ§  Pipeline Thinking (Interview Gold)&lt;/h2&gt;
&lt;p&gt;Engineers think in pipelines:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Load data&lt;/li&gt;
&lt;li&gt;Inspect&lt;/li&gt;
&lt;li&gt;Clean&lt;/li&gt;
&lt;li&gt;Transform&lt;/li&gt;
&lt;li&gt;Validate&lt;/li&gt;
&lt;li&gt;Export&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each step should be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explicit&lt;/li&gt;
&lt;li&gt;Reproducible&lt;/li&gt;
&lt;li&gt;Testable&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-validation-checks-often-missed&#34;&gt;ğŸ§ª Validation Checks (Often Missed)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;assert df[&amp;quot;age&amp;quot;].min() &amp;gt; 0
assert df[&amp;quot;user_id&amp;quot;].is_unique
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These prevent silent failures.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-world-case-meta--openai&#34;&gt;ğŸ¢ Real-World Case (Meta / OpenAI)&lt;/h2&gt;
&lt;p&gt;Before training models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pandas pipeline cleans data&lt;/li&gt;
&lt;li&gt;Validates assumptions&lt;/li&gt;
&lt;li&gt;Logs anomalies&lt;/li&gt;
&lt;li&gt;Produces clean tables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Only then â†’ Spark / TensorFlow / PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-beginner-mistakes&#34;&gt;âš ï¸ Common Beginner Mistakes&lt;/h2&gt;
&lt;p&gt;âŒ Overusing &lt;code&gt;dropna()&lt;/code&gt;
âŒ Ignoring data types
âŒ Modifying DataFrame in-place blindly
âŒ No validation checks&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-summary&#34;&gt;ğŸ Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cleaning is the real job&lt;/li&gt;
&lt;li&gt;Inspect before modifying&lt;/li&gt;
&lt;li&gt;Be explicit, not clever&lt;/li&gt;
&lt;li&gt;Method chaining = professional style&lt;/li&gt;
&lt;li&gt;Pipelines prevent bugs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Good data â†’ good decisions â†’ good models.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-next-chapter&#34;&gt;ğŸš€ Next Chapter&lt;/h2&gt;
&lt;p&gt;ğŸ‘‰ &lt;strong&gt;DE101-PD04 â€” GroupBy, Aggregation, and Business Analytics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is where insights are born.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lecture 03 â€” Math Behind Intelligence</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-03-math/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-03-math/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~2â€“3 hours (core AI math intuition)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-math-matters-truth-first&#34;&gt;ğŸ“ Why Math Matters (Truth First)&lt;/h2&gt;
&lt;p&gt;Letâ€™s be honest.&lt;/p&gt;
&lt;p&gt;Most people think:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œAI = magic + code + GPUâ€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reality:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI = math making good guesses under uncertainty&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;AI does NOT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;know the truth&lt;/li&gt;
&lt;li&gt;understand like humans&lt;/li&gt;
&lt;li&gt;think with meaning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI &lt;strong&gt;estimates probabilities&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-one-sentence-that-explains-all-ai&#34;&gt;ğŸ§  One Sentence That Explains All AI&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI is a machine that guessesâ€¦ and learns how to guess better.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Math is how we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;define â€œbetterâ€&lt;/li&gt;
&lt;li&gt;measure mistakes&lt;/li&gt;
&lt;li&gt;improve guesses&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-idea-uncertainty-is-everywhere&#34;&gt;ğŸ¯ Core Idea: Uncertainty Is Everywhere&lt;/h2&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is this email spam? â“&lt;/li&gt;
&lt;li&gt;Is this image a cat or dog? â“&lt;/li&gt;
&lt;li&gt;What word comes next? â“&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI never knows 100%.&lt;/p&gt;
&lt;p&gt;So it asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œHow confident am I?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That confidence is &lt;strong&gt;probability&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-probability-human-version&#34;&gt;ğŸ“Š Probability (Human Version)&lt;/h2&gt;
&lt;p&gt;Imagine this situation:&lt;/p&gt;
&lt;p&gt;You hear &lt;strong&gt;meowing&lt;/strong&gt; behind a door ğŸ±&lt;br&gt;
Whatâ€™s behind the door?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cat? ğŸ±&lt;/li&gt;
&lt;li&gt;Dog? ğŸ¶&lt;/li&gt;
&lt;li&gt;Robot cat? ğŸ¤–ğŸ±&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You update your belief based on evidence.&lt;/p&gt;
&lt;p&gt;That is probability.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-bayes-rule-the-brain-of-ai&#34;&gt;ğŸ§® Bayesâ€™ Rule (The Brain of AI)&lt;/h2&gt;
&lt;h3 id=&#34;-formula-dont-panic&#34;&gt;ğŸ§  Formula (donâ€™t panic)&lt;/h3&gt;
&lt;p&gt;$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$&lt;/p&gt;
&lt;h3 id=&#34;-human-translation&#34;&gt;ğŸ§’ Human Translation&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œHow likely A is, &lt;strong&gt;after&lt;/strong&gt; seeing evidence B.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-funny-example-is-your-friend-late&#34;&gt;ğŸ˜„ Funny Example: Is Your Friend Late?&lt;/h2&gt;
&lt;p&gt;Let:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt; = â€œFriend is lazyâ€&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;B&lt;/strong&gt; = â€œFriend is lateâ€&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We want:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Is your friend lazy &lt;strong&gt;given&lt;/strong&gt; that they are late?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;step-by-step&#34;&gt;Step-by-step&lt;/h3&gt;
&lt;p&gt;Assume:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(friend is lazy) = 0.3&lt;/li&gt;
&lt;li&gt;P(friend is late | lazy) = 0.8&lt;/li&gt;
&lt;li&gt;P(friend is late) = 0.5&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now calculate:&lt;/p&gt;
&lt;p&gt;$$
P(lazy|late) = \frac{0.8 \times 0.3}{0.5} = 0.48
$$&lt;/p&gt;
&lt;p&gt;ğŸ‘‰ You are now &lt;strong&gt;48% suspicious&lt;/strong&gt; ğŸ˜„&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-bayes-matters-in-ai&#34;&gt;ğŸ¤– Why Bayes Matters in AI&lt;/h2&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spam detection&lt;/li&gt;
&lt;li&gt;medical diagnosis&lt;/li&gt;
&lt;li&gt;robotics&lt;/li&gt;
&lt;li&gt;decision-making&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI is constantly updating beliefs.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-loss-function--how-ai-feels-pain&#34;&gt;ğŸ“‰ Loss Function â€” How AI Feels Pain&lt;/h2&gt;
&lt;p&gt;AI learns by &lt;strong&gt;making mistakes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Loss function answers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œHow bad was this mistake?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-guessing-a-number&#34;&gt;ğŸ¯ Example: Guessing a Number&lt;/h2&gt;
&lt;p&gt;True value = 10&lt;br&gt;
AI predicts = 7&lt;/p&gt;
&lt;p&gt;Error = 3&lt;/p&gt;
&lt;p&gt;But &lt;strong&gt;how painful is that?&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mean-squared-error-mse&#34;&gt;ğŸ”¹ Mean Squared Error (MSE)&lt;/h2&gt;
&lt;p&gt;Formula:
$$
MSE = \frac{1}{n} \sum (y - \hat{y})^2
$$&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
True = 10
Predicted = 7
Error = 3
Squared = 9

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why square?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;no negative errors&lt;/li&gt;
&lt;li&gt;punish big mistakes more&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-cross-entropy-very-important&#34;&gt;ğŸ¤¯ Cross-Entropy (VERY IMPORTANT)&lt;/h2&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;classification&lt;/li&gt;
&lt;li&gt;language models&lt;/li&gt;
&lt;li&gt;ChatGPT&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;simple-idea&#34;&gt;Simple idea:&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œHow surprised is the model?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-coin-example-easy&#34;&gt;ğŸ² Coin Example (Easy)&lt;/h2&gt;
&lt;p&gt;True answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heads = 1&lt;/li&gt;
&lt;li&gt;Tails = 0&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model predicts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Heads = 0.9&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cross-entropy loss:
$$
L = -\log(0.9) \approx 0.105
$$&lt;/p&gt;
&lt;p&gt;Good prediction â†’ small loss ğŸ˜„&lt;br&gt;
Bad prediction â†’ huge loss ğŸ˜±&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-chatgpt-uses-cross-entropy&#34;&gt;ğŸ’¬ Why ChatGPT Uses Cross-Entropy&lt;/h2&gt;
&lt;p&gt;ChatGPT predicts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhat is the next word?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If itâ€™s confident and correct â†’ low loss&lt;br&gt;
Confident but wrong â†’ BIG loss&lt;/p&gt;
&lt;p&gt;Thatâ€™s how it learns language.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-neural-network--function-machine&#34;&gt;ğŸ§  Neural Network = Function Machine&lt;/h2&gt;
&lt;p&gt;A neural network is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;A very flexible math function&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;
Input â†’ Weighted sum â†’ Activation â†’ Output

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-feedforward-forward-pass&#34;&gt;ğŸ” Feedforward (Forward Pass)&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;thinking&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Input data&lt;/li&gt;
&lt;li&gt;Multiply by weights&lt;/li&gt;
&lt;li&gt;Apply activation&lt;/li&gt;
&lt;li&gt;Output prediction&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;No learning yet.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-backpropagation-learning-moment&#34;&gt;ğŸ˜– Backpropagation (Learning Moment)&lt;/h2&gt;
&lt;p&gt;Backpropagation answers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhich weights caused the mistake?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute loss&lt;/li&gt;
&lt;li&gt;Send error backward&lt;/li&gt;
&lt;li&gt;Update weights slightly&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thatâ€™s learning.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-gradient-descent-walking-down-a-hill&#34;&gt;ğŸ“‰ Gradient Descent (Walking Down a Hill)&lt;/h2&gt;
&lt;p&gt;Imagine youâ€™re blindfolded ğŸ§‘â€ğŸ¦¯ on a hill.&lt;/p&gt;
&lt;p&gt;You:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;feel the slope&lt;/li&gt;
&lt;li&gt;step downhill&lt;/li&gt;
&lt;li&gt;repeat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eventually â†’ bottom.&lt;/p&gt;
&lt;p&gt;Thatâ€™s gradient descent.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tiny-numeric-example&#34;&gt;ğŸ§® Tiny Numeric Example&lt;/h2&gt;
&lt;p&gt;Loss = (w âˆ’ 5)Â²&lt;/p&gt;
&lt;p&gt;Start:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;w = 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gradient:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;derivative = 2(w âˆ’ 5) = -8&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Update:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
w = w - learning_rate * gradient
w = 1 - 0.1 * (-8)
w = 1.8

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Closer to 5 ğŸ‘&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-math-is-not-scary&#34;&gt;ğŸ§  Why Math Is Not Scary&lt;/h2&gt;
&lt;p&gt;Math in AI is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;measuring belief&lt;/li&gt;
&lt;li&gt;measuring mistakes&lt;/li&gt;
&lt;li&gt;adjusting guesses&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not magic.
Not monsters.
Just logic.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-how-all-math-connects&#34;&gt;ğŸ¤– How All Math Connects&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Concept&lt;/th&gt;
&lt;th&gt;Purpose&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Probability&lt;/td&gt;
&lt;td&gt;Belief&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bayes&lt;/td&gt;
&lt;td&gt;Update belief&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Loss&lt;/td&gt;
&lt;td&gt;Measure mistake&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cross-entropy&lt;/td&gt;
&lt;td&gt;Surprise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient&lt;/td&gt;
&lt;td&gt;Direction to improve&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Backprop&lt;/td&gt;
&lt;td&gt;Credit assignment&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-big-insight&#34;&gt;ğŸŒ Final Big Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI does not understand truth.&lt;br&gt;
It minimizes loss.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Understanding this makes you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a better engineer&lt;/li&gt;
&lt;li&gt;a better researcher&lt;/li&gt;
&lt;li&gt;a wiser human&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Why do humans reason probabilistically instead of perfectly?&lt;/summary&gt;
  &lt;p&gt;Because the world is uncertain â€” and intelligence means adapting.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Lecture 04 â€” Audio â†” Text â†” Reasoning: Teaching Machines to Listen</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-04-audio-text/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-04-audio-text/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~4â€“5 hours (theory + code)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-audiotext-matters&#34;&gt;ğŸŒ Why Audioâ€“Text Matters&lt;/h2&gt;
&lt;p&gt;Sound is the &lt;strong&gt;first interface of intelligence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Before writing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;humans listened&lt;/li&gt;
&lt;li&gt;learned tone&lt;/li&gt;
&lt;li&gt;sensed emotion&lt;/li&gt;
&lt;li&gt;reacted in real time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Audioâ€“Text systems allow machines to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;transcribe speech&lt;/li&gt;
&lt;li&gt;understand intent&lt;/li&gt;
&lt;li&gt;reason about meaning&lt;/li&gt;
&lt;li&gt;respond naturally&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;If vision gives machines eyes, audio gives them ears.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-an-audiotext-multimodal-system&#34;&gt;ğŸ§  What Is an Audioâ€“Text Multimodal System?&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;Audioâ€“Text-to-Text&lt;/strong&gt; system takes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ§ audio input (speech, sound)&lt;/li&gt;
&lt;li&gt;ğŸ§  converts it to representations&lt;/li&gt;
&lt;li&gt;âœï¸ produces text output&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Speech-to-text (ASR)&lt;/li&gt;
&lt;li&gt;Audio question answering&lt;/li&gt;
&lt;li&gt;Voice assistants&lt;/li&gt;
&lt;li&gt;Meeting summarization&lt;/li&gt;
&lt;li&gt;Emotion-aware chatbots&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-high-level-architecture&#34;&gt;ğŸ§© High-Level Architecture&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;
Raw Audio
â†“
Audio Encoder (Whisper / Wav2Vec)
â†“
Projection / Alignment
â†“
LLM (Reasoning)
â†“
Text Output

&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Audio models perceive. LLMs reason.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-understanding-audio-first-principles&#34;&gt;ğŸ§ Understanding Audio (First Principles)&lt;/h2&gt;
&lt;p&gt;Audio is a &lt;strong&gt;time-domain signal&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Amplitude vs Time

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Key challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;noise&lt;/li&gt;
&lt;li&gt;accents&lt;/li&gt;
&lt;li&gt;speed variation&lt;/li&gt;
&lt;li&gt;emotion&lt;/li&gt;
&lt;li&gt;silence&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-audio-is-harder-than-text&#34;&gt;ğŸ§  Why Audio Is Harder Than Text&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Aspect&lt;/th&gt;
&lt;th&gt;Audio&lt;/th&gt;
&lt;th&gt;Text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Noise&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ambiguity&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Temporal&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Emotion&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Rare&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Audio carries meaning beyond words.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--fundamentals&#34;&gt;ğŸ§ª Knowledge Check â€” Fundamentals&lt;/h2&gt;
&lt;h3 id=&#34;q1-objective&#34;&gt;Q1 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is audio considered a high-entropy modality?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because it varies continuously in time, tone, speed, and noise.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-audio-tasks&#34;&gt;ğŸ§  Core Audio Tasks&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ASR&lt;/td&gt;
&lt;td&gt;Speech â†’ Text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Audio QA&lt;/td&gt;
&lt;td&gt;Audio + Question â†’ Answer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Emotion Recognition&lt;/td&gt;
&lt;td&gt;Audio â†’ Emotion&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Speaker ID&lt;/td&gt;
&lt;td&gt;Audio â†’ Identity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reasoning&lt;/td&gt;
&lt;td&gt;Audio â†’ Meaning&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This lecture focuses on &lt;strong&gt;ASR + reasoning&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-audio-encoders-the-ears&#34;&gt;ğŸ§ Audio Encoders (The Ears)&lt;/h2&gt;
&lt;p&gt;Popular models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Whisper&lt;/strong&gt; (OpenAI)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wav2Vec 2.0&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;HuBERT&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They convert waveform â†’ embeddings.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-encoder-output&#34;&gt;ğŸ§  Encoder Output&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;
Audio waveform â†’ Frame-level embeddings â†’ Sequence

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pooled&lt;/li&gt;
&lt;li&gt;projected&lt;/li&gt;
&lt;li&gt;aligned to language space&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--encoders&#34;&gt;ğŸ§ª Knowledge Check â€” Encoders&lt;/h2&gt;
&lt;h3 id=&#34;q2-mcq&#34;&gt;Q2 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which model is widely used for multilingual ASR?&lt;/p&gt;
&lt;p&gt;A) ResNet&lt;br&gt;
B) Whisper&lt;br&gt;
C) CLIP&lt;br&gt;
D) BERT&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;B) Whisper&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-lab-1--speech-to-text-with-whisper&#34;&gt;ğŸ Python Lab 1 â€” Speech-to-Text with Whisper&lt;/h2&gt;
&lt;h3 id=&#34;-install-dependencies&#34;&gt;ğŸ“¦ Install Dependencies&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers accelerate torch torchaudio
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-load-model&#34;&gt;ğŸ§  Load Model&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import WhisperProcessor, WhisperForConditionalGeneration
import torchaudio

model_name = &amp;quot;openai/whisper-small&amp;quot;

processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-load-audio&#34;&gt;ğŸ§ Load Audio&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;waveform, sample_rate = torchaudio.load(&amp;quot;sample.wav&amp;quot;)

inputs = processor(
    waveform.squeeze(),
    sampling_rate=sample_rate,
    return_tensors=&amp;quot;pt&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-generate-text&#34;&gt;âœï¸ Generate Text&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with torch.no_grad():
    predicted_ids = model.generate(inputs[&amp;quot;input_features&amp;quot;])

text = processor.batch_decode(
    predicted_ids,
    skip_special_tokens=True
)

print(text)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ‰ You just built a speech-to-text system.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--code-understanding&#34;&gt;ğŸ§ª Knowledge Check â€” Code Understanding&lt;/h2&gt;
&lt;h3 id=&#34;q3-objective&#34;&gt;Q3 (Objective)&lt;/h3&gt;
&lt;p&gt;What is the role of the processor in Whisper?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It converts raw audio into model-ready features and decodes outputs.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-adding-reasoning-with-an-llm&#34;&gt;ğŸ§  Adding Reasoning with an LLM&lt;/h2&gt;
&lt;p&gt;ASR alone is &lt;strong&gt;not intelligence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We add an LLM to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;summarize&lt;/li&gt;
&lt;li&gt;answer questions&lt;/li&gt;
&lt;li&gt;extract intent&lt;/li&gt;
&lt;li&gt;reason&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-full-audiotext-reasoning-pipeline&#34;&gt;ğŸ— Full Audioâ€“Text Reasoning Pipeline&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Audio â†’ ASR â†’ Transcript â†’ Prompt â†’ LLM â†’ Answer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This separation is &lt;strong&gt;architecturally clean&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-lab-2--audio-qa-with-llm&#34;&gt;ğŸ Python Lab 2 â€” Audio QA with LLM&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM

llm_name = &amp;quot;meta-llama/Llama-3-8B-Instruct&amp;quot;

tokenizer = AutoTokenizer.from_pretrained(llm_name)
llm = AutoModelForCausalLM.from_pretrained(llm_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-prompting-with-transcript&#34;&gt;ğŸ§  Prompting with Transcript&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;transcript = text[0]

prompt = f&amp;quot;&amp;quot;&amp;quot;
You are an assistant.
Audio transcript:
{transcript}

Question:
What is the main topic of this audio?
&amp;quot;&amp;quot;&amp;quot;

inputs = tokenizer(prompt, return_tensors=&amp;quot;pt&amp;quot;)

outputs = llm.generate(**inputs, max_new_tokens=100)

answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(answer)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--system-design&#34;&gt;ğŸ§ª Knowledge Check â€” System Design&lt;/h2&gt;
&lt;h3 id=&#34;q4-true--false&#34;&gt;Q4 (True / False)&lt;/h3&gt;
&lt;p&gt;ASR models alone can reason about audio content.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-audiotext-alignment-strategies&#34;&gt;ğŸ§  Audioâ€“Text Alignment Strategies&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Strategy&lt;/th&gt;
&lt;th&gt;Use Case&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Transcript-only&lt;/td&gt;
&lt;td&gt;Simple QA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Timestamp alignment&lt;/td&gt;
&lt;td&gt;Video/audio sync&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Embedding fusion&lt;/td&gt;
&lt;td&gt;Emotion + content&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Multitask training&lt;/td&gt;
&lt;td&gt;Robust systems&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-failure-modes&#34;&gt;âš ï¸ Common Failure Modes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Background noise&lt;/li&gt;
&lt;li&gt;Code-switching&lt;/li&gt;
&lt;li&gt;Accents&lt;/li&gt;
&lt;li&gt;Domain-specific terms&lt;/li&gt;
&lt;li&gt;Emotional nuance loss&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Architects design for failure, not perfection.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--failures&#34;&gt;ğŸ§ª Knowledge Check â€” Failures&lt;/h2&gt;
&lt;h3 id=&#34;q5-mcq&#34;&gt;Q5 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which issue is hardest for ASR?&lt;/p&gt;
&lt;p&gt;A) Silence
B) Clear speech
C) Accents
D) Typed text&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;C) Accents&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-evaluation-metrics&#34;&gt;ğŸ§  Evaluation Metrics&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;WER&lt;/td&gt;
&lt;td&gt;Word Error Rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CER&lt;/td&gt;
&lt;td&gt;Character Error Rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QA Accuracy&lt;/td&gt;
&lt;td&gt;Reasoning quality&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Latency&lt;/td&gt;
&lt;td&gt;Real-time ability&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--evaluation&#34;&gt;ğŸ§ª Knowledge Check â€” Evaluation&lt;/h2&gt;
&lt;h3 id=&#34;q6-objective&#34;&gt;Q6 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is low WER not sufficient for good QA?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because reasoning depends on semantic correctness, not just word accuracy.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ethics--audio-ai&#34;&gt;ğŸŒ± Ethics &amp;amp; Audio AI&lt;/h2&gt;
&lt;p&gt;Risks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;surveillance&lt;/li&gt;
&lt;li&gt;consent violation&lt;/li&gt;
&lt;li&gt;accent discrimination&lt;/li&gt;
&lt;li&gt;voice cloning&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Listening without permission is not intelligence.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--ethics&#34;&gt;ğŸ§ª Knowledge Check â€” Ethics&lt;/h2&gt;
&lt;h3 id=&#34;q7-true--false&#34;&gt;Q7 (True / False)&lt;/h3&gt;
&lt;p&gt;Audio AI systems should always inform users they are listening.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;True.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-human-in-the-loop-hitl&#34;&gt;ğŸ§  Human-in-the-Loop (HITL)&lt;/h2&gt;
&lt;p&gt;Best practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;humans review transcripts&lt;/li&gt;
&lt;li&gt;corrections feed back&lt;/li&gt;
&lt;li&gt;confidence thresholds trigger review&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaways&#34;&gt;âœ… Final Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Audio is rich but noisy&lt;/li&gt;
&lt;li&gt;Encoders perceive, LLMs reason&lt;/li&gt;
&lt;li&gt;Separation of ASR and reasoning is powerful&lt;/li&gt;
&lt;li&gt;Python pipelines are modular&lt;/li&gt;
&lt;li&gt;Ethics matter deeply for audio&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;If a machine understands your voice, what responsibility does it have?&lt;/summary&gt;
  &lt;p&gt;To respect consent, privacy, and human dignity.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>DE101-PD04 â€” GroupBy, Aggregation &amp; Joins (Analytics &amp; Interview Mastery)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd04-groupby-joins/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd04-groupby-joins/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~75â€“90 minutes&lt;/p&gt;
&lt;h2 id=&#34;-why-this-chapter-is-critical&#34;&gt;ğŸ¯ Why This Chapter Is Critical&lt;/h2&gt;
&lt;p&gt;If Pandas were a weapon:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cleaning = sharpening&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GroupBy &amp;amp; Join = actual power&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most real-world questions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â€œWhich group performs best?â€&lt;/li&gt;
&lt;li&gt;â€œHow do metrics change by segment?â€&lt;/li&gt;
&lt;li&gt;â€œHow do we combine datasets correctly?â€&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-groupby-mental-model-most-important-idea&#34;&gt;ğŸ§  GroupBy Mental Model (Most Important Idea)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GroupBy = Split â†’ Apply â†’ Combine&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Split data into groups&lt;/li&gt;
&lt;li&gt;Apply aggregation or transformation&lt;/li&gt;
&lt;li&gt;Combine results into a new table&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This mindset works in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;Spark&lt;/li&gt;
&lt;li&gt;BigQuery&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-dataset-e-commerce-analytics&#34;&gt;ğŸ“Š Example Dataset (E-Commerce Analytics)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

data = {
    &amp;quot;user_id&amp;quot;: [1, 2, 1, 3, 2, 3, 1],
    &amp;quot;country&amp;quot;: [&amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;FR&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;FR&amp;quot;, &amp;quot;US&amp;quot;],
    &amp;quot;revenue&amp;quot;: [100, 200, 150, 80, 120, 60, 90],
    &amp;quot;device&amp;quot;: [&amp;quot;mobile&amp;quot;, &amp;quot;desktop&amp;quot;, &amp;quot;mobile&amp;quot;, &amp;quot;mobile&amp;quot;, &amp;quot;mobile&amp;quot;, &amp;quot;desktop&amp;quot;, &amp;quot;desktop&amp;quot;]
}

df = pd.DataFrame(data)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-basic-groupby&#34;&gt;ğŸ“¦ Basic GroupBy&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;country&amp;quot;)[&amp;quot;revenue&amp;quot;].sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;country
FR    140
US    660
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This answers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œHow much revenue comes from each country?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-multiple-aggregations&#34;&gt;ğŸ”¢ Multiple Aggregations&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;country&amp;quot;)[&amp;quot;revenue&amp;quot;].agg([&amp;quot;sum&amp;quot;, &amp;quot;mean&amp;quot;, &amp;quot;count&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used daily in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;KPI dashboards&lt;/li&gt;
&lt;li&gt;Business reports&lt;/li&gt;
&lt;li&gt;Model evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-groupby-multiple-columns&#34;&gt;ğŸ§  GroupBy Multiple Columns&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby([&amp;quot;country&amp;quot;, &amp;quot;device&amp;quot;])[&amp;quot;revenue&amp;quot;].sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you are thinking like an analyst.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-resetting-index-very-common&#34;&gt;ğŸ§® Resetting Index (Very Common)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;country&amp;quot;, as_index=False)[&amp;quot;revenue&amp;quot;].sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;country&amp;quot;)[&amp;quot;revenue&amp;quot;].sum().reset_index()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This makes output usable for joins.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-transform--keep-original-shape&#34;&gt;ğŸ§© &lt;code&gt;.transform()&lt;/code&gt; â€” Keep Original Shape&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;country_avg_revenue&amp;quot;] = (
    df.groupby(&amp;quot;country&amp;quot;)[&amp;quot;revenue&amp;quot;].transform(&amp;quot;mean&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You need group stats per row&lt;/li&gt;
&lt;li&gt;Feature engineering for ML&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-apply--flexible-but-dangerous&#34;&gt;ğŸ” &lt;code&gt;.apply()&lt;/code&gt; â€” Flexible but Dangerous&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;country&amp;quot;).apply(lambda x: x[&amp;quot;revenue&amp;quot;].max() - x[&amp;quot;revenue&amp;quot;].min())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âš ï¸ Powerful but slower.
Use only when aggregation cannot express logic.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sql-vs-pandas-groupby&#34;&gt;ğŸ§  SQL vs Pandas GroupBy&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SQL&lt;/th&gt;
&lt;th&gt;Pandas&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GROUP BY&lt;/td&gt;
&lt;td&gt;groupby()&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SUM&lt;/td&gt;
&lt;td&gt;sum()&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AVG&lt;/td&gt;
&lt;td&gt;mean()&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;COUNT&lt;/td&gt;
&lt;td&gt;count()&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;SQL vs Pandas GroupBy?&lt;/summary&gt;
  &lt;p&gt;Conceptually identical â€” only syntax differs&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-joins-matter&#34;&gt;ğŸ”— Why Joins Matter&lt;/h2&gt;
&lt;p&gt;Real-world data is &lt;strong&gt;never in one table&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You often have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;users table&lt;/li&gt;
&lt;li&gt;orders table&lt;/li&gt;
&lt;li&gt;events table&lt;/li&gt;
&lt;li&gt;experiments table&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Joins connect the story.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-tables&#34;&gt;ğŸ“Š Example Tables&lt;/h2&gt;
&lt;h3 id=&#34;users&#34;&gt;Users&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;users = pd.DataFrame({
    &amp;quot;user_id&amp;quot;: [1, 2, 3],
    &amp;quot;country&amp;quot;: [&amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;FR&amp;quot;]
})
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;orders&#34;&gt;Orders&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;orders = pd.DataFrame({
    &amp;quot;order_id&amp;quot;: [101, 102, 103, 104],
    &amp;quot;user_id&amp;quot;: [1, 2, 1, 4],
    &amp;quot;amount&amp;quot;: [250, 300, 150, 500]
})
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-inner-join&#34;&gt;ğŸ”— Inner Join&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.merge(users, orders, on=&amp;quot;user_id&amp;quot;, how=&amp;quot;inner&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keeps only matching rows.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-left-join-most-common&#34;&gt;ğŸ”— Left Join (Most Common)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.merge(users, orders, on=&amp;quot;user_id&amp;quot;, how=&amp;quot;left&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Users are primary&lt;/li&gt;
&lt;li&gt;Orders may be missing&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-right-join&#34;&gt;ğŸ”— Right Join&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.merge(users, orders, on=&amp;quot;user_id&amp;quot;, how=&amp;quot;right&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Less common but useful for audits.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-outer-join&#34;&gt;ğŸ”— Outer Join&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.merge(users, orders, on=&amp;quot;user_id&amp;quot;, how=&amp;quot;outer&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data reconciliation&lt;/li&gt;
&lt;li&gt;Finding missing relationships&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-join-on-different-column-names&#34;&gt;ğŸ§  Join on Different Column Names&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.merge(
    users,
    orders,
    left_on=&amp;quot;user_id&amp;quot;,
    right_on=&amp;quot;user_id&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In real data, column names rarely match.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-join-explosion-critical-warning&#34;&gt;âš ï¸ Join Explosion (Critical Warning)&lt;/h2&gt;
&lt;p&gt;Many-to-many joins can &lt;strong&gt;multiply rows&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;users.merge(orders, on=&amp;quot;user_id&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Always ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhat is the cardinality?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-validation-after-join&#34;&gt;ğŸ§ª Validation After Join&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;assert merged[&amp;quot;user_id&amp;quot;].notna().all()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Production engineers always validate joins.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-world-case-google--meta--openai&#34;&gt;ğŸ§  Real-World Case (Google / Meta / OpenAI)&lt;/h2&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Join user metadata&lt;/li&gt;
&lt;li&gt;Join experiment assignment&lt;/li&gt;
&lt;li&gt;Aggregate metrics per variant&lt;/li&gt;
&lt;li&gt;Decide product direction&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bad join â†’ wrong decision â†’ $$$ lost&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interview-patterns&#34;&gt;ğŸ§  Interview Patterns&lt;/h2&gt;
&lt;p&gt;Interviewers test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GroupBy logic&lt;/li&gt;
&lt;li&gt;Correct join type&lt;/li&gt;
&lt;li&gt;Index reset&lt;/li&gt;
&lt;li&gt;Avoiding row explosion&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-summary&#34;&gt;ğŸ Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GroupBy = Split â†’ Apply â†’ Combine&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.agg()&lt;/code&gt; for metrics&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.transform()&lt;/code&gt; for ML features&lt;/li&gt;
&lt;li&gt;Joins connect reality&lt;/li&gt;
&lt;li&gt;Validate after join&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you master this chapter,
you can survive &lt;strong&gt;most analytics interviews&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-next-chapter&#34;&gt;ğŸš€ Next Chapter&lt;/h2&gt;
&lt;p&gt;ğŸ‘‰ &lt;strong&gt;DE101-PD05 â€” Time Series &amp;amp; Window Operations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is where senior-level thinking begins.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-01 â€” Sorting an Array</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-01-sorting/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-01-sorting/</guid>
      <description>&lt;h2 id=&#34;-problem-statement&#34;&gt;ğŸ§© Problem Statement&lt;/h2&gt;
&lt;p&gt;Given an integer array &lt;code&gt;nums&lt;/code&gt;, return the array sorted in &lt;strong&gt;ascending order&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This problem looks simple, but interviewers often use it to evaluate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your understanding of &lt;strong&gt;algorithmic trade-offs&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Your knowledge of &lt;strong&gt;time vs space complexity&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Whether you know &lt;strong&gt;when NOT to use built-in functions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-input---output&#34;&gt;ğŸ“¥ Sample Input / ğŸ“¤ Output&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Input&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[3, 1, 2]&lt;/td&gt;
&lt;td&gt;[1, 2, 3]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[5, 4, 3, 2, 1]&lt;/td&gt;
&lt;td&gt;[1, 2, 3, 4, 5]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[1]&lt;/td&gt;
&lt;td&gt;[1]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[2, 2, 1]&lt;/td&gt;
&lt;td&gt;[1, 2, 2]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight-read-this-first&#34;&gt;ğŸ§  Interviewer Insight (Read This First)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â— &lt;strong&gt;Important:&lt;/strong&gt;&lt;br&gt;
In real interviews, the question is &lt;strong&gt;not&lt;/strong&gt; â€œCan you sort an array?â€&lt;br&gt;
It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which sorting algorithm do you choose?&lt;/li&gt;
&lt;li&gt;Why is it optimal &lt;strong&gt;for this scenario&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;What are the time &amp;amp; space trade-offs?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Below we show &lt;strong&gt;all major sorting algorithms&lt;/strong&gt; with &lt;strong&gt;clear intuition&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-1--python-built-in-sort-recommended-in-practice&#34;&gt;âœ… Approach 1 â€” Python Built-in Sort (Recommended in Practice)&lt;/h2&gt;
&lt;h3 id=&#34;-idea&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Python uses &lt;strong&gt;Timsort&lt;/strong&gt;, a hybrid of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Merge Sort&lt;/li&gt;
&lt;li&gt;Insertion Sort&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stable&lt;/li&gt;
&lt;li&gt;Extremely fast in real-world data&lt;/li&gt;
&lt;li&gt;Optimized for partially sorted arrays&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-code&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sort_array(nums):
    # Python&#39;s built-in sort uses Timsort
    return sorted(nums)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n log n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ &lt;strong&gt;Use this in interviews unless explicitly forbidden.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-2--bubble-sort-educational-only&#34;&gt;ğŸ§© Approach 2 â€” Bubble Sort (Educational Only)&lt;/h2&gt;
&lt;h3 id=&#34;-idea-1&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Repeatedly swap adjacent elements if they are in the wrong order.&lt;/p&gt;
&lt;p&gt;Think of large numbers &lt;strong&gt;â€œbubbling upâ€&lt;/strong&gt; to the end.&lt;/p&gt;
&lt;h3 id=&#34;-code-1&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def bubble_sort(nums):
    n = len(nums)
    for i in range(n):
        for j in range(0, n - i - 1):
            if nums[j] &amp;gt; nums[j + 1]:
                nums[j], nums[j + 1] = nums[j + 1], nums[j]
    return nums
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-1&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(nÂ²)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸ”´ Rarely used in practice, but good for teaching basics.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-3--selection-sort&#34;&gt;ğŸ§© Approach 3 â€” Selection Sort&lt;/h2&gt;
&lt;h3 id=&#34;-idea-2&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Repeatedly find the &lt;strong&gt;minimum element&lt;/strong&gt; and move it to the front.&lt;/p&gt;
&lt;h3 id=&#34;-code-2&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def selection_sort(nums):
    n = len(nums)
    for i in range(n):
        min_idx = i
        for j in range(i + 1, n):
            if nums[j] &amp;lt; nums[min_idx]:
                min_idx = j
        nums[i], nums[min_idx] = nums[min_idx], nums[i]
    return nums
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-2&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(nÂ²)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¡ Simple but inefficient for large data.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-4--insertion-sort&#34;&gt;ğŸ§© Approach 4 â€” Insertion Sort&lt;/h2&gt;
&lt;h3 id=&#34;-idea-3&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Build the sorted array &lt;strong&gt;one element at a time&lt;/strong&gt;, similar to sorting playing cards.&lt;/p&gt;
&lt;p&gt;Very efficient for &lt;strong&gt;nearly sorted arrays&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-code-3&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def insertion_sort(nums):
    for i in range(1, len(nums)):
        key = nums[i]
        j = i - 1
        while j &amp;gt;= 0 and nums[j] &amp;gt; key:
            nums[j + 1] = nums[j]
            j -= 1
        nums[j + 1] = key
    return nums
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-3&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(nÂ²) worst case, &lt;strong&gt;O(n)&lt;/strong&gt; best case&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Excellent when data is almost sorted.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-5--merge-sort-divide--conquer&#34;&gt;ğŸ§© Approach 5 â€” Merge Sort (Divide &amp;amp; Conquer)&lt;/h2&gt;
&lt;h3 id=&#34;-idea-4&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Divide the array into halves&lt;/li&gt;
&lt;li&gt;Sort each half&lt;/li&gt;
&lt;li&gt;Merge them back together&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-code-4&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def merge_sort(nums):
    if len(nums) &amp;lt;= 1:
        return nums

    mid = len(nums) // 2
    left = merge_sort(nums[:mid])
    right = merge_sort(nums[mid:])

    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0

    while i &amp;lt; len(left) and j &amp;lt; len(right):
        if left[i] &amp;lt; right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1

    result.extend(left[i:])
    result.extend(right[j:])
    return result
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-4&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n log n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Very popular in interviews.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-6--quick-sort-average-fastest&#34;&gt;ğŸ§© Approach 6 â€” Quick Sort (Average Fastest)&lt;/h2&gt;
&lt;h3 id=&#34;-idea-5&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pick a pivot&lt;/li&gt;
&lt;li&gt;Partition elements into smaller &amp;amp; larger&lt;/li&gt;
&lt;li&gt;Recursively sort&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-code-5&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def quick_sort(nums):
    if len(nums) &amp;lt;= 1:
        return nums

    pivot = nums[len(nums) // 2]
    left = [x for x in nums if x &amp;lt; pivot]
    middle = [x for x in nums if x == pivot]
    right = [x for x in nums if x &amp;gt; pivot]

    return quick_sort(left) + middle + quick_sort(right)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-5&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n log n) average, O(nÂ²) worst&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Very fast in practice, but discuss worst case.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-interview-advice&#34;&gt;ğŸ¯ Final Interview Advice&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Situation&lt;/th&gt;
&lt;th&gt;Best Choice&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Real-world code&lt;/td&gt;
&lt;td&gt;&lt;code&gt;sorted()&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Teaching / basics&lt;/td&gt;
&lt;td&gt;Bubble / Selection&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Nearly sorted data&lt;/td&gt;
&lt;td&gt;Insertion Sort&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Guaranteed performance&lt;/td&gt;
&lt;td&gt;Merge Sort&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Average fastest&lt;/td&gt;
&lt;td&gt;Quick Sort&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ’¬ &lt;strong&gt;Interview Tip:&lt;/strong&gt;
Always say &lt;em&gt;why&lt;/em&gt; you choose an algorithm, not just &lt;em&gt;how&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway&#34;&gt;ğŸ§  Takeaway&lt;/h2&gt;
&lt;p&gt;Sorting is not about memorizing code.
Itâ€™s about &lt;strong&gt;choosing the right tool for the problem&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This single question can separate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Junior developers&lt;/li&gt;
&lt;li&gt;From &lt;strong&gt;strong algorithmic thinkers&lt;/strong&gt; ğŸš€&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Lecture 04 â€” Data: The Fuel of AI</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-04-data/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-04-data/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~1.5â€“2 hours (core understanding lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-data-simple-but-deep&#34;&gt;ğŸ“¦ What Is Data? (Simple but Deep)&lt;/h2&gt;
&lt;p&gt;Data is &lt;strong&gt;recorded experience&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Everything AI knows comes from data.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Images ğŸ–¼ï¸ â†’ what the world looks like&lt;/li&gt;
&lt;li&gt;Text ğŸ“š â†’ how humans think &amp;amp; speak&lt;/li&gt;
&lt;li&gt;Audio ğŸ§ â†’ how humans sound&lt;/li&gt;
&lt;li&gt;Numbers ğŸ”¢ â†’ measurements, behavior, signals&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;No data = no learning&lt;br&gt;
Better data = better intelligence&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-a-powerful-truth&#34;&gt;ğŸ§  A Powerful Truth&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI does not see reality.&lt;br&gt;
AI sees data.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This difference explains almost every AI failure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-types-of-data-with-intuition&#34;&gt;ğŸ§© Types of Data (With Intuition)&lt;/h2&gt;
&lt;h3 id=&#34;-image-data&#34;&gt;ğŸ–¼ï¸ Image Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Photos&lt;/li&gt;
&lt;li&gt;Medical scans&lt;/li&gt;
&lt;li&gt;Satellite images&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI learns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;shapes&lt;/li&gt;
&lt;li&gt;textures&lt;/li&gt;
&lt;li&gt;patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But &lt;strong&gt;not meaning&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-text-data&#34;&gt;ğŸ“š Text Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Books&lt;/li&gt;
&lt;li&gt;Articles&lt;/li&gt;
&lt;li&gt;Code&lt;/li&gt;
&lt;li&gt;Conversations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI learns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grammar&lt;/li&gt;
&lt;li&gt;logic patterns&lt;/li&gt;
&lt;li&gt;style&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not truth.
Not intention.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-audio-data&#34;&gt;ğŸ§ Audio Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Speech&lt;/li&gt;
&lt;li&gt;Music&lt;/li&gt;
&lt;li&gt;Noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI learns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;frequency patterns&lt;/li&gt;
&lt;li&gt;pronunciation&lt;/li&gt;
&lt;li&gt;rhythm&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-tabular--numeric-data&#34;&gt;ğŸ”¢ Tabular / Numeric Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Excel sheets&lt;/li&gt;
&lt;li&gt;Databases&lt;/li&gt;
&lt;li&gt;Logs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;finance&lt;/li&gt;
&lt;li&gt;healthcare&lt;/li&gt;
&lt;li&gt;business&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-video-data&#34;&gt;ğŸ¥ Video Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Images over time&lt;/li&gt;
&lt;li&gt;Motion + sound&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hardest data type.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-multimodal-data-modern-ai&#34;&gt;ğŸ”€ Multimodal Data (Modern AI)&lt;/h3&gt;
&lt;p&gt;Text + Image + Audio together.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-4V&lt;/li&gt;
&lt;li&gt;Gemini&lt;/li&gt;
&lt;li&gt;CLIP&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Multimodal AI sees the world more like humans â€” but still imperfectly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-data-vs-information-vs-knowledge&#34;&gt;ğŸ§  Data vs Information vs Knowledge&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Concept&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Data&lt;/td&gt;
&lt;td&gt;Raw facts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Information&lt;/td&gt;
&lt;td&gt;Structured data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Knowledge&lt;/td&gt;
&lt;td&gt;Patterns learned&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Wisdom&lt;/td&gt;
&lt;td&gt;Human judgment&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;AI stops at &lt;strong&gt;patterns&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Humans add meaning.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-funny-example-teaching-a-kid-vs-ai&#34;&gt;ğŸ˜„ Funny Example: Teaching a Kid vs AI&lt;/h2&gt;
&lt;p&gt;You show a kid:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 picture of a dog ğŸ¶&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The kid understands.&lt;/p&gt;
&lt;p&gt;You show an AI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 picture of a dog&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The AI learns nothing.&lt;/p&gt;
&lt;p&gt;AI needs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;thousands&lt;/li&gt;
&lt;li&gt;millions&lt;/li&gt;
&lt;li&gt;billions of examples&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Why?
Because AI does not &lt;strong&gt;understand context&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-garbage-in--garbage-out-gigo&#34;&gt;âš ï¸ Garbage In = Garbage Out (GIGO)&lt;/h2&gt;
&lt;p&gt;Classic rule:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Bad data â†’ bad AI&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Biased hiring data â†’ biased hiring AI&lt;/li&gt;
&lt;li&gt;Incomplete medical data â†’ dangerous predictions&lt;/li&gt;
&lt;li&gt;Noisy labels â†’ confused models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Models donâ€™t fix data problems.
They &lt;strong&gt;amplify them&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-bias-clear-definition&#34;&gt;ğŸ§  What Is Bias? (Clear Definition)&lt;/h2&gt;
&lt;p&gt;Bias means:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data does not represent reality fairly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Historical inequality&lt;/li&gt;
&lt;li&gt;Sampling errors&lt;/li&gt;
&lt;li&gt;Human prejudice&lt;/li&gt;
&lt;li&gt;Missing groups&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI learns &lt;strong&gt;our past&lt;/strong&gt;, not our ideals.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-world-example-easy-to-feel&#34;&gt;ğŸ˜¬ Real-World Example (Easy to Feel)&lt;/h2&gt;
&lt;p&gt;If all training images of â€œCEOâ€ are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mostly men&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI learns:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;CEO = man&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Not because AI is evil â€”&lt;br&gt;
but because &lt;strong&gt;data told it so&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ethics-for-kids-simple-question&#34;&gt;ğŸ§’ Ethics for Kids (Simple Question)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Would you trust a robot that learned everything from &lt;strong&gt;one person&lt;/strong&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course not.&lt;/p&gt;
&lt;p&gt;Diversity of data = fairness.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ethics-for-adults-serious-truth&#34;&gt;âš–ï¸ Ethics for Adults (Serious Truth)&lt;/h2&gt;
&lt;p&gt;Data decisions are &lt;strong&gt;power decisions&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who is included?&lt;/li&gt;
&lt;li&gt;Who is missing?&lt;/li&gt;
&lt;li&gt;Who defines labels?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These choices shape society.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-data-is-not-neutral&#34;&gt;ğŸ§  Data Is Not Neutral&lt;/h2&gt;
&lt;p&gt;Important myth:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œData is objective.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reality:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data is collected by humans&lt;/li&gt;
&lt;li&gt;Labeled by humans&lt;/li&gt;
&lt;li&gt;Filtered by humans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Data contains values.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-data-lifecycle-how-ai-is-really-built&#34;&gt;ğŸ§ª Data Lifecycle (How AI Is Really Built)&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Collect data&lt;/li&gt;
&lt;li&gt;Clean data&lt;/li&gt;
&lt;li&gt;Label data&lt;/li&gt;
&lt;li&gt;Split data (train / val / test)&lt;/li&gt;
&lt;li&gt;Train model&lt;/li&gt;
&lt;li&gt;Evaluate&lt;/li&gt;
&lt;li&gt;Deploy&lt;/li&gt;
&lt;li&gt;Monitor drift&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Most failures happen at steps &lt;strong&gt;1â€“4&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-big-models-need-big-data&#34;&gt;ğŸ¤– Why Big Models Need Big Data&lt;/h2&gt;
&lt;p&gt;ChatGPT was trained on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;massive text corpora&lt;/li&gt;
&lt;li&gt;diverse sources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not because itâ€™s smart â€”&lt;br&gt;
but because &lt;strong&gt;scale matters&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;More data â†’ better generalization (to a point).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-small-data-still-matters&#34;&gt;ğŸ§  Small Data Still Matters&lt;/h2&gt;
&lt;p&gt;In:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;medicine&lt;/li&gt;
&lt;li&gt;law&lt;/li&gt;
&lt;li&gt;robotics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We must:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;be careful&lt;/li&gt;
&lt;li&gt;use domain knowledge&lt;/li&gt;
&lt;li&gt;avoid blind automation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-data-leakage-silent-killer&#34;&gt;ğŸ” Data Leakage (Silent Killer)&lt;/h2&gt;
&lt;p&gt;Data leakage = test data sneaks into training.&lt;/p&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fake high accuracy&lt;/li&gt;
&lt;li&gt;real-world failure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Always separate data properly.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-data-drift-why-models-fail-over-time&#34;&gt;ğŸŒ Data Drift (Why Models Fail Over Time)&lt;/h2&gt;
&lt;p&gt;World changes.
Data changes.
Models decay.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;language evolves&lt;/li&gt;
&lt;li&gt;behavior changes&lt;/li&gt;
&lt;li&gt;environments shift&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI must be &lt;strong&gt;maintained&lt;/strong&gt;, not worshiped.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-data-is-the-memory-of-ai&#34;&gt;ğŸ¤– Data Is the Memory of AI&lt;/h2&gt;
&lt;p&gt;AI has no childhood.
No intuition.
No experience.&lt;/p&gt;
&lt;p&gt;Data is its &lt;strong&gt;memory of the world&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Choose memory wisely.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-big-takeaways&#34;&gt;ğŸ§  Big Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Data shapes intelligence&lt;/li&gt;
&lt;li&gt;Bias is unavoidable, but manageable&lt;/li&gt;
&lt;li&gt;Ethics starts at data collection&lt;/li&gt;
&lt;li&gt;Models reflect us&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;If AI reflects society, what responsibility do we have as data creators?&lt;/summary&gt;
  &lt;p&gt;To be careful, inclusive, and wise.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Lecture 05 â€” Image â†” Text: Teaching Machines to See and Reason</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-05-image-text/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-05-image-text/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~5 hours (core multimodal lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-imagetext-is-so-important&#34;&gt;ğŸŒ Why Imageâ€“Text Is So Important&lt;/h2&gt;
&lt;p&gt;Vision is the &lt;strong&gt;dominant human sense&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Images contain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;spatial structure&lt;/li&gt;
&lt;li&gt;objects&lt;/li&gt;
&lt;li&gt;relationships&lt;/li&gt;
&lt;li&gt;context&lt;/li&gt;
&lt;li&gt;ambiguity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Teaching machines to &lt;strong&gt;see and explain&lt;/strong&gt; is one of the hardest problems in AI.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Seeing is easy. Understanding is hard.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-an-imagetext-multimodal-system&#34;&gt;ğŸ§  What Is an Imageâ€“Text Multimodal System?&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;Imageâ€“Text-to-Text&lt;/strong&gt; system takes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ–¼ an image&lt;/li&gt;
&lt;li&gt;âœï¸ optional text (question, instruction)&lt;/li&gt;
&lt;li&gt;ğŸ§  performs reasoning&lt;/li&gt;
&lt;li&gt;ğŸ—£ produces text&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image captioning&lt;/li&gt;
&lt;li&gt;Visual Question Answering (VQA)&lt;/li&gt;
&lt;li&gt;Image-based reasoning&lt;/li&gt;
&lt;li&gt;Medical imaging reports&lt;/li&gt;
&lt;li&gt;Autonomous perception explanations&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-high-level-architecture&#34;&gt;ğŸ§© High-Level Architecture&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;
Image
â†“
Vision Encoder (ViT / CNN)
â†“
Projection / Alignment
â†“
LLM (Reasoning)
â†“
Text Output

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Key idea:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Vision models perceive. LLMs reason.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-vision-alone-is-not-enough&#34;&gt;ğŸ§  Why Vision Alone Is Not Enough&lt;/h2&gt;
&lt;p&gt;Vision models are great at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;detecting patterns&lt;/li&gt;
&lt;li&gt;recognizing objects&lt;/li&gt;
&lt;li&gt;learning spatial features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But they struggle with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;logic&lt;/li&gt;
&lt;li&gt;explanation&lt;/li&gt;
&lt;li&gt;abstraction&lt;/li&gt;
&lt;li&gt;causality&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Images do not reason. Language does.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--fundamentals&#34;&gt;ğŸ§ª Knowledge Check â€” Fundamentals&lt;/h2&gt;
&lt;h3 id=&#34;q1-true--false&#34;&gt;Q1 (True / False)&lt;/h3&gt;
&lt;p&gt;A vision model alone can perform logical reasoning.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-imagetext-tasks&#34;&gt;ğŸ§  Core Imageâ€“Text Tasks&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Input&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Image Captioning&lt;/td&gt;
&lt;td&gt;Image&lt;/td&gt;
&lt;td&gt;Text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VQA&lt;/td&gt;
&lt;td&gt;Image + Question&lt;/td&gt;
&lt;td&gt;Answer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Grounding&lt;/td&gt;
&lt;td&gt;Image + Text&lt;/td&gt;
&lt;td&gt;Region&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image QA&lt;/td&gt;
&lt;td&gt;Image&lt;/td&gt;
&lt;td&gt;Explanation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OCR + Reasoning&lt;/td&gt;
&lt;td&gt;Image&lt;/td&gt;
&lt;td&gt;Text + Answer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This lecture focuses on &lt;strong&gt;captioning + VQA&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vision-encoders-the-eyes&#34;&gt;ğŸ‘ Vision Encoders (The Eyes)&lt;/h2&gt;
&lt;p&gt;Popular encoders:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ResNet (CNN-based)&lt;/li&gt;
&lt;li&gt;Vision Transformer (ViT)&lt;/li&gt;
&lt;li&gt;Swin Transformer&lt;/li&gt;
&lt;li&gt;ConvNeXt&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They convert pixels â†’ embeddings.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vision-encoder-output&#34;&gt;ğŸ§  Vision Encoder Output&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;
Image â†’ Patch embeddings â†’ Sequence of vectors

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each patch represents &lt;strong&gt;local visual semantics&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--vision&#34;&gt;ğŸ§ª Knowledge Check â€” Vision&lt;/h2&gt;
&lt;h3 id=&#34;q2-mcq&#34;&gt;Q2 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which model introduced patch-based vision processing?&lt;/p&gt;
&lt;p&gt;A) ResNet&lt;br&gt;
B) YOLO&lt;br&gt;
C) ViT&lt;br&gt;
D) AlexNet&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;C) ViT&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-alignment-problem-critical&#34;&gt;ğŸ§  The Alignment Problem (CRITICAL)&lt;/h2&gt;
&lt;p&gt;Vision embeddings â‰  language embeddings.&lt;/p&gt;
&lt;p&gt;Alignment answers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;How does a pixel become a word?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-alignment-strategies&#34;&gt;ğŸ§© Alignment Strategies&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Strategy&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Linear projection&lt;/td&gt;
&lt;td&gt;Simple, efficient&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MLP&lt;/td&gt;
&lt;td&gt;Nonlinear mapping&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cross-attention&lt;/td&gt;
&lt;td&gt;Deep fusion&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Q-Former&lt;/td&gt;
&lt;td&gt;Learned query alignment&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Most failures come from poor alignment.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--alignment&#34;&gt;ğŸ§ª Knowledge Check â€” Alignment&lt;/h2&gt;
&lt;h3 id=&#34;q3-objective&#34;&gt;Q3 (Objective)&lt;/h3&gt;
&lt;p&gt;What is the purpose of the projection layer?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;To map vision embeddings into the language embedding space.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-clip--a-foundational-breakthrough&#34;&gt;ğŸ§  CLIP â€” A Foundational Breakthrough&lt;/h2&gt;
&lt;p&gt;CLIP learned:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Image â†” Text similarity

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By contrastive learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;match image with correct caption&lt;/li&gt;
&lt;li&gt;separate mismatched pairs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Shared semantic space&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--clip&#34;&gt;ğŸ§ª Knowledge Check â€” CLIP&lt;/h2&gt;
&lt;h3 id=&#34;q4-mcq&#34;&gt;Q4 (MCQ)&lt;/h3&gt;
&lt;p&gt;What learning paradigm does CLIP use?&lt;/p&gt;
&lt;p&gt;A) Supervised classification&lt;br&gt;
B) Reinforcement learning&lt;br&gt;
C) Contrastive learning&lt;br&gt;
D) Autoencoding&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;C) Contrastive learning&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-lab-1--image-captioning-with-blip&#34;&gt;ğŸ Python Lab 1 â€” Image Captioning with BLIP&lt;/h2&gt;
&lt;h3 id=&#34;-install-dependencies&#34;&gt;ğŸ“¦ Install Dependencies&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install transformers pillow torch torchvision
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-load-model&#34;&gt;ğŸ§  Load Model&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

model_name = &amp;quot;Salesforce/blip-image-captioning-base&amp;quot;

processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-load-image&#34;&gt;ğŸ–¼ Load Image&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image = Image.open(&amp;quot;example.jpg&amp;quot;).convert(&amp;quot;RGB&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-generate-caption&#34;&gt;âœï¸ Generate Caption&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;inputs = processor(image, return_tensors=&amp;quot;pt&amp;quot;)

out = model.generate(**inputs, max_new_tokens=50)

caption = processor.decode(out[0], skip_special_tokens=True)
print(caption)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ‰ You just taught a machine to describe what it sees.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--code&#34;&gt;ğŸ§ª Knowledge Check â€” Code&lt;/h2&gt;
&lt;h3 id=&#34;q5-objective&#34;&gt;Q5 (Objective)&lt;/h3&gt;
&lt;p&gt;What role does the processor play in BLIP?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It preprocesses images and decodes generated tokens.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-adding-reasoning-with-an-llm&#34;&gt;ğŸ§  Adding Reasoning with an LLM&lt;/h2&gt;
&lt;p&gt;Captioning â‰  understanding.&lt;/p&gt;
&lt;p&gt;We add an LLM to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;answer questions&lt;/li&gt;
&lt;li&gt;infer relationships&lt;/li&gt;
&lt;li&gt;explain scenes&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-full-imagetext-reasoning-pipeline&#34;&gt;ğŸ— Full Imageâ€“Text Reasoning Pipeline&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Image â†’ Vision Encoder â†’ Alignment â†’ LLM â†’ Answer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Optional:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+ Question
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-lab-2--visual-question-answering&#34;&gt;ğŸ Python Lab 2 â€” Visual Question Answering&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;question = &amp;quot;What is the person doing in the image?&amp;quot;

prompt = f&amp;quot;&amp;quot;&amp;quot;
You are a vision-language assistant.
Image description:
{caption}

Question:
{question}
&amp;quot;&amp;quot;&amp;quot;

from transformers import AutoTokenizer, AutoModelForCausalLM

llm_name = &amp;quot;meta-llama/Llama-3-8B-Instruct&amp;quot;

tokenizer = AutoTokenizer.from_pretrained(llm_name)
llm = AutoModelForCausalLM.from_pretrained(llm_name)

inputs = tokenizer(prompt, return_tensors=&amp;quot;pt&amp;quot;)
outputs = llm.generate(**inputs, max_new_tokens=100)

answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(answer)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--reasoning&#34;&gt;ğŸ§ª Knowledge Check â€” Reasoning&lt;/h2&gt;
&lt;h3 id=&#34;q6-true--false&#34;&gt;Q6 (True / False)&lt;/h3&gt;
&lt;p&gt;Image captioning alone is sufficient for VQA.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-failure-modes&#34;&gt;ğŸ§  Common Failure Modes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hallucinated objects&lt;/li&gt;
&lt;li&gt;Missed small details&lt;/li&gt;
&lt;li&gt;Incorrect spatial relations&lt;/li&gt;
&lt;li&gt;Cultural bias&lt;/li&gt;
&lt;li&gt;Overconfidence&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Seeing â‰  understanding â‰  truth&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--failures&#34;&gt;ğŸ§ª Knowledge Check â€” Failures&lt;/h2&gt;
&lt;h3 id=&#34;q7-mcq&#34;&gt;Q7 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which failure is most dangerous?&lt;/p&gt;
&lt;p&gt;A) Slow inference
B) Hallucinated objects
C) Low resolution
D) Long captions&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;Correct Answer&lt;/summary&gt;
  &lt;p&gt;B) Hallucinated objects&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-evaluation-metrics&#34;&gt;ğŸ§  Evaluation Metrics&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Use&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BLEU / CIDEr&lt;/td&gt;
&lt;td&gt;Caption quality&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VQA Accuracy&lt;/td&gt;
&lt;td&gt;QA correctness&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Human Eval&lt;/td&gt;
&lt;td&gt;Trust &amp;amp; clarity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Calibration&lt;/td&gt;
&lt;td&gt;Confidence reliability&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--evaluation&#34;&gt;ğŸ§ª Knowledge Check â€” Evaluation&lt;/h2&gt;
&lt;h3 id=&#34;q8-objective&#34;&gt;Q8 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is human evaluation important for vision-language tasks?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because correctness depends on semantics and human perception.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ethics-in-visionlanguage-ai&#34;&gt;ğŸŒ± Ethics in Visionâ€“Language AI&lt;/h2&gt;
&lt;p&gt;Risks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;surveillance&lt;/li&gt;
&lt;li&gt;facial recognition abuse&lt;/li&gt;
&lt;li&gt;bias in datasets&lt;/li&gt;
&lt;li&gt;privacy violation&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;If a machine sees people, it must respect humanity.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-knowledge-check--ethics&#34;&gt;ğŸ§ª Knowledge Check â€” Ethics&lt;/h2&gt;
&lt;h3 id=&#34;q9-true--false&#34;&gt;Q9 (True / False)&lt;/h3&gt;
&lt;p&gt;Imageâ€“text systems can be ethically neutral.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-9&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;False.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-human-in-the-loop-best-practice&#34;&gt;ğŸ§  Human-in-the-Loop (Best Practice)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Human review for sensitive outputs&lt;/li&gt;
&lt;li&gt;Confidence thresholds&lt;/li&gt;
&lt;li&gt;Explainable responses&lt;/li&gt;
&lt;li&gt;Feedback-based refinement&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaways&#34;&gt;âœ… Final Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Vision provides perception&lt;/li&gt;
&lt;li&gt;Language provides reasoning&lt;/li&gt;
&lt;li&gt;Alignment is the hardest part&lt;/li&gt;
&lt;li&gt;Python pipelines are modular&lt;/li&gt;
&lt;li&gt;Ethics is not optional&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-10&#34;&gt;
  &lt;summary&gt;If a machine describes a human incorrectly, who is harmed?&lt;/summary&gt;
  &lt;p&gt;The human â€” therefore responsibility lies with designers.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>DE101-PD05 â€” Performance &amp; Production Patterns (From Pandas to Scale)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd05-performance/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd05-performance/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~90â€“120 minutes&lt;/p&gt;
&lt;h2 id=&#34;-why-this-chapter-matters&#34;&gt;ğŸ¯ Why This Chapter Matters&lt;/h2&gt;
&lt;p&gt;Most Pandas tutorials stop at:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œHere is how it worksâ€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Real engineers must ask:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWill this survive production?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Performance mistakes silently:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Waste memory&lt;/li&gt;
&lt;li&gt;Slow pipelines&lt;/li&gt;
&lt;li&gt;Break models&lt;/li&gt;
&lt;li&gt;Crash jobs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This chapter teaches you how professionals think.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mental-model-pandas-is-vectorized-python&#34;&gt;ğŸ§  Mental Model: Pandas Is Vectorized Python&lt;/h2&gt;
&lt;p&gt;Pandas is fast &lt;strong&gt;only when&lt;/strong&gt; you let it operate on whole columns.&lt;/p&gt;
&lt;p&gt;âŒ Bad:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(len(df)):
    df.loc[i, &amp;quot;x&amp;quot;] += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ… Good:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;x&amp;quot;] += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Vectorization = speed + clarity.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-avoid-python-loops-unless-necessary&#34;&gt;ğŸš« Avoid Python Loops (Unless Necessary)&lt;/h2&gt;
&lt;p&gt;Loops move work from C â†’ Python.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Approach&lt;/th&gt;
&lt;th&gt;Speed&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Python loop&lt;/td&gt;
&lt;td&gt;âŒ slow&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;.apply()&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;âš ï¸ medium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vectorized ops&lt;/td&gt;
&lt;td&gt;âœ… fastest&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-conditional-logic&#34;&gt;ğŸ§ª Example: Conditional Logic&lt;/h2&gt;
&lt;p&gt;âŒ Slow:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;flag&amp;quot;] = df[&amp;quot;value&amp;quot;].apply(lambda x: 1 if x &amp;gt; 0 else 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âœ… Fast:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;flag&amp;quot;] = (df[&amp;quot;value&amp;quot;] &amp;gt; 0).astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-use-the-right-data-types&#34;&gt;ğŸ§  Use the Right Data Types&lt;/h2&gt;
&lt;p&gt;Wrong dtypes waste memory.&lt;/p&gt;
&lt;h3 id=&#34;before&#34;&gt;Before&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.info()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;convert-strings-to-categories&#34;&gt;Convert strings to categories&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;country&amp;quot;] = df[&amp;quot;country&amp;quot;].astype(&amp;quot;category&amp;quot;)
df[&amp;quot;device&amp;quot;] = df[&amp;quot;device&amp;quot;].astype(&amp;quot;category&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can reduce memory &lt;strong&gt;10Ã—â€“100Ã—&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-numeric-downcasting&#34;&gt;ğŸ§  Numeric Downcasting&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;age&amp;quot;] = pd.to_numeric(df[&amp;quot;age&amp;quot;], downcast=&amp;quot;integer&amp;quot;)
df[&amp;quot;revenue&amp;quot;] = pd.to_numeric(df[&amp;quot;revenue&amp;quot;], downcast=&amp;quot;float&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in large ETL pipelines.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-memory-profiling&#34;&gt;ğŸ“¦ Memory Profiling&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.memory_usage(deep=True).sum() / 1024**2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Always measure before optimizing.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-copy-vs-view-danger-zone&#34;&gt;ğŸ§  Copy vs View (Danger Zone)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df2 = df[df[&amp;quot;age&amp;quot;] &amp;gt; 18]
df2[&amp;quot;flag&amp;quot;] = 1  # âš ï¸ SettingWithCopyWarning
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Safer:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df2 = df.loc[df[&amp;quot;age&amp;quot;] &amp;gt; 18].copy()
df2[&amp;quot;flag&amp;quot;] = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Silent bugs happen here.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-chunk-processing-large-files&#34;&gt;âš™ï¸ Chunk Processing (Large Files)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chunks = pd.read_csv(&amp;quot;large.csv&amp;quot;, chunksize=100_000)

for chunk in chunks:
    process(chunk)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used when data barely fits in memory.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-efficient-joins&#34;&gt;ğŸ§  Efficient Joins&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Join on indexed columns&lt;/li&gt;
&lt;li&gt;Avoid object dtype keys&lt;/li&gt;
&lt;li&gt;Filter before join&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;users = users.set_index(&amp;quot;user_id&amp;quot;)
orders = orders.set_index(&amp;quot;user_id&amp;quot;)

users.join(orders, how=&amp;quot;left&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sorting-is-expensive&#34;&gt;ğŸ§  Sorting Is Expensive&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.sort_values(&amp;quot;timestamp&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sorting is &lt;strong&gt;O(n log n)&lt;/strong&gt;
Avoid repeated sorts.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-use-query-for-readability&#34;&gt;ğŸ§  Use &lt;code&gt;query()&lt;/code&gt; for Readability&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.query(&amp;quot;country == &#39;US&#39; and revenue &amp;gt; 100&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Often faster and clearer than chained indexing.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-profiling-runtime&#34;&gt;ğŸ§ª Profiling Runtime&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%timeit df[&amp;quot;revenue&amp;quot;].sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In production:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Measure&lt;/li&gt;
&lt;li&gt;Compare&lt;/li&gt;
&lt;li&gt;Decide&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-parallelism-pandas-is-mostly-single-threaded&#34;&gt;ğŸ§  Parallelism: Pandas Is Mostly Single-Threaded&lt;/h2&gt;
&lt;p&gt;Workarounds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Split data&lt;/li&gt;
&lt;li&gt;Use multiprocessing&lt;/li&gt;
&lt;li&gt;Or switch tools&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-when-pandas-is-not-enough&#34;&gt;ğŸš€ When Pandas Is Not Enough&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;When does Pandas become slow?&lt;/summary&gt;
  &lt;p&gt;When data no longer fits in memory.&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;At this point, professionals move to:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tool&lt;/th&gt;
&lt;th&gt;When&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Dask&lt;/td&gt;
&lt;td&gt;Larger-than-memory Pandas&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;Distributed clusters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DuckDB&lt;/td&gt;
&lt;td&gt;Fast SQL analytics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Polars&lt;/td&gt;
&lt;td&gt;Modern fast DataFrame&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-google--meta--openai-pattern&#34;&gt;ğŸ§  Google / Meta / OpenAI Pattern&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Prototype in Pandas&lt;/li&gt;
&lt;li&gt;Validate logic&lt;/li&gt;
&lt;li&gt;Optimize&lt;/li&gt;
&lt;li&gt;Scale to Spark / SQL&lt;/li&gt;
&lt;li&gt;Monitor in production&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Same logic, different engine.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-interview-question&#34;&gt;ğŸ§ª Real Interview Question&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œYour Pandas pipeline is slow. What do you do?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Expected answer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Measure memory&lt;/li&gt;
&lt;li&gt;Check dtypes&lt;/li&gt;
&lt;li&gt;Remove loops&lt;/li&gt;
&lt;li&gt;Vectorize&lt;/li&gt;
&lt;li&gt;Reduce joins&lt;/li&gt;
&lt;li&gt;Scale tool if needed&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-golden-rules&#34;&gt;ğŸ Golden Rules&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Measure before optimizing&lt;/li&gt;
&lt;li&gt;Avoid Python loops&lt;/li&gt;
&lt;li&gt;Control memory&lt;/li&gt;
&lt;li&gt;Validate joins&lt;/li&gt;
&lt;li&gt;Think about scale early&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-thought&#34;&gt;ğŸš€ Final Thought&lt;/h2&gt;
&lt;p&gt;Pandas is not slow.
&lt;strong&gt;Bad usage is slow.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Engineers who understand performance
write code that survives real systems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-next-chapter&#34;&gt;ğŸ”œ Next Chapter&lt;/h2&gt;
&lt;p&gt;ğŸ‘‰ &lt;strong&gt;DE101-PD06 â€” Time Series, Rolling Windows &amp;amp; Feature Engineering&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is where ML &amp;amp; analytics meet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-02 â€” Prime Number Check</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-02-prime-number/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-02-prime-number/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;Given an integer &lt;code&gt;n&lt;/code&gt;, determine whether it is a prime number.&lt;/p&gt;
&lt;h2 id=&#34;sample-input--output&#34;&gt;Sample Input / Output&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Input&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;python-solution&#34;&gt;Python Solution&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def is_prime(n):
    &amp;quot;&amp;quot;&amp;quot;
    Check whether n is a prime number.
    &amp;quot;&amp;quot;&amp;quot;
    if n &amp;lt;= 1:
        return False

    for i in range(2, int(n ** 0.5) + 1):
        if n % i == 0:
            return False
    return True
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;complexity-analysis&#34;&gt;Complexity Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Time Complexity: &lt;strong&gt;O(âˆšn)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Space Complexity: &lt;strong&gt;O(1)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Lecture 05 â€” Types of Machine Learning</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-05-ml-types/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-05-ml-types/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~3 hours (core AI taxonomy lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-big-picture-read-this-first&#34;&gt;ğŸ§  Big Picture (Read This First)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning = systems that improve performance by learning from experience.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But here is the secret:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Not all experience teaches the same way.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Different problems â†’ different learning paradigms.&lt;/p&gt;
&lt;p&gt;If you choose the &lt;em&gt;wrong type&lt;/em&gt;, even the best model will fail.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-a-simple-mental-map&#34;&gt;ğŸ§­ A Simple Mental Map&lt;/h2&gt;
&lt;p&gt;Think of learning like school:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Learning Type&lt;/th&gt;
&lt;th&gt;Analogy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Supervised&lt;/td&gt;
&lt;td&gt;Teacher gives answers&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Unsupervised&lt;/td&gt;
&lt;td&gt;Student explores alone&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Semi-supervised&lt;/td&gt;
&lt;td&gt;Few answers, many questions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Self-supervised&lt;/td&gt;
&lt;td&gt;Student creates own homework&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reinforcement&lt;/td&gt;
&lt;td&gt;Learning by reward &amp;amp; punishment&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h1 id=&#34;1-supervised-learning&#34;&gt;1ï¸âƒ£ Supervised Learning&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;â€œLearning with a teacherâ€&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-definition&#34;&gt;ğŸ“˜ Definition&lt;/h2&gt;
&lt;p&gt;Learning from &lt;strong&gt;labeled data&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
(input â†’ correct output)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;image â†’ label (cat / dog)&lt;/li&gt;
&lt;li&gt;email â†’ spam / not spam&lt;/li&gt;
&lt;li&gt;house features â†’ price&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-intuition-human-version&#34;&gt;ğŸ§  Intuition (Human Version)&lt;/h2&gt;
&lt;p&gt;A teacher says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThis is a cat.â€&lt;br&gt;
â€œThis is NOT a cat.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The student adjusts until mistakes are small.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-classic-problems&#34;&gt;ğŸ§ª Classic Problems&lt;/h2&gt;
&lt;h3 id=&#34;-classification&#34;&gt;ğŸ”¹ Classification&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;spam detection&lt;/li&gt;
&lt;li&gt;medical diagnosis&lt;/li&gt;
&lt;li&gt;fraud detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-regression&#34;&gt;ğŸ”¹ Regression&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;house price prediction&lt;/li&gt;
&lt;li&gt;temperature forecasting&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mini-project-beginner-friendly&#34;&gt;ğŸ’» Mini Project (Beginner-Friendly)&lt;/h2&gt;
&lt;h3 id=&#34;-project-email-spam-classifier&#34;&gt;ğŸ¯ Project: Email Spam Classifier&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Steps:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Collect labeled emails&lt;/li&gt;
&lt;li&gt;Convert text â†’ numbers&lt;/li&gt;
&lt;li&gt;Train a classifier&lt;/li&gt;
&lt;li&gt;Evaluate accuracy&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer

texts = [&amp;quot;free money&amp;quot;, &amp;quot;meeting tomorrow&amp;quot;]
labels = [1, 0]  # 1=spam, 0=not spam

vec = CountVectorizer()
X = vec.fit_transform(texts)

model = LogisticRegression()
model.fit(X, labels)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ‘‰ This is &lt;strong&gt;real AI&lt;/strong&gt;, not magic.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-quiz&#34;&gt;â“ Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Why does supervised learning need labels?&lt;/summary&gt;
  &lt;p&gt;Because the loss function compares predictions with ground truth.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h1 id=&#34;2-unsupervised-learning&#34;&gt;2ï¸âƒ£ Unsupervised Learning&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;â€œLearning without answersâ€&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-definition-1&#34;&gt;ğŸ“˜ Definition&lt;/h2&gt;
&lt;p&gt;Learning &lt;strong&gt;patterns without labels&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The system asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhat structure exists here?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-funny-example&#34;&gt;ğŸ˜„ Funny Example&lt;/h2&gt;
&lt;p&gt;You walk into a party with strangers.&lt;/p&gt;
&lt;p&gt;No name tags.&lt;/p&gt;
&lt;p&gt;Your brain automatically groups people:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;similar clothes&lt;/li&gt;
&lt;li&gt;similar behavior&lt;/li&gt;
&lt;li&gt;similar interests&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thatâ€™s &lt;strong&gt;clustering&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-techniques&#34;&gt;ğŸ§ª Common Techniques&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;K-means clustering&lt;/li&gt;
&lt;li&gt;PCA (dimensionality reduction)&lt;/li&gt;
&lt;li&gt;Topic modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mini-project&#34;&gt;ğŸ’» Mini Project&lt;/h2&gt;
&lt;h3 id=&#34;-project-customer-segmentation&#34;&gt;ğŸ¯ Project: Customer Segmentation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.cluster import KMeans

X = [[20, 500], [25, 700], [45, 2000]]  # age, spending
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)

print(kmeans.labels_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;marketing&lt;/li&gt;
&lt;li&gt;recommendation systems&lt;/li&gt;
&lt;li&gt;anomaly detection&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-quiz-1&#34;&gt;â“ Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Is unsupervised learning useful without labels?&lt;/summary&gt;
  &lt;p&gt;Yes â€” it discovers structure and representations.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h1 id=&#34;3-semi-supervised-learning&#34;&gt;3ï¸âƒ£ Semi-Supervised Learning&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;â€œA few answers, many questionsâ€&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-definition-2&#34;&gt;ğŸ“˜ Definition&lt;/h2&gt;
&lt;p&gt;Small labeled dataset
+
Large unlabeled dataset&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-exists&#34;&gt;ğŸ§  Why This Exists&lt;/h2&gt;
&lt;p&gt;Labeling costs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;money ğŸ’°&lt;/li&gt;
&lt;li&gt;time â³&lt;/li&gt;
&lt;li&gt;experts ğŸ§ &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But raw data is everywhere.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-world-uses&#34;&gt;ğŸ§ª Real-World Uses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;medical images&lt;/li&gt;
&lt;li&gt;legal documents&lt;/li&gt;
&lt;li&gt;speech recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mini-project-idea&#34;&gt;ğŸ’» Mini Project Idea&lt;/h2&gt;
&lt;p&gt;Label:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100 images by hand&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let the model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;infer the rest&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is how &lt;strong&gt;real-world AI systems survive&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-quiz-2&#34;&gt;â“ Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Why is semi-supervised learning practical?&lt;/summary&gt;
  &lt;p&gt;Labels are expensive; data is cheap.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h1 id=&#34;4-self-supervised-learning&#34;&gt;4ï¸âƒ£ Self-Supervised Learning&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;â€œThe AI creates its own teacherâ€&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-definition-3&#34;&gt;ğŸ“˜ Definition&lt;/h2&gt;
&lt;p&gt;Labels are &lt;strong&gt;automatically generated from data itself&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-changed-ai-forever&#34;&gt;ğŸ¤¯ Why This Changed AI Forever&lt;/h2&gt;
&lt;p&gt;Instead of humans saying:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThis is the answerâ€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The system asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhat part of the data is missing?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-famous-examples&#34;&gt;ğŸ§ª Famous Examples&lt;/h2&gt;
&lt;h3 id=&#34;-nlp&#34;&gt;ğŸ”¹ NLP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Masked word prediction (BERT)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-vision&#34;&gt;ğŸ”¹ Vision&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Predict image rotation&lt;/li&gt;
&lt;li&gt;Contrastive learning (CLIP)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mini-project-conceptual&#34;&gt;ğŸ’» Mini Project (Conceptual)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Input: &amp;quot;AI is ____&amp;quot;
Target: &amp;quot;powerful&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simple idea trained &lt;strong&gt;LLMs&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-quiz-3&#34;&gt;â“ Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Why is self-supervised learning powerful?&lt;/summary&gt;
  &lt;p&gt;It scales with data, not human labeling.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h1 id=&#34;5-reinforcement-learning-rl&#34;&gt;5ï¸âƒ£ Reinforcement Learning (RL)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;â€œLearning by doingâ€&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-definition-4&#34;&gt;ğŸ“˜ Definition&lt;/h2&gt;
&lt;p&gt;Learning from &lt;strong&gt;reward signals&lt;/strong&gt;, not correct answers.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-intuition&#34;&gt;ğŸ•¹ï¸ Intuition&lt;/h2&gt;
&lt;p&gt;You train a dog ğŸ•:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Good behavior â†’ treat&lt;/li&gt;
&lt;li&gt;Bad behavior â†’ no treat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;No explicit instructions.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-key-components&#34;&gt;ğŸ§ª Key Components&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Term&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Agent&lt;/td&gt;
&lt;td&gt;Learner&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Environment&lt;/td&gt;
&lt;td&gt;World&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Action&lt;/td&gt;
&lt;td&gt;Choice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reward&lt;/td&gt;
&lt;td&gt;Feedback&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tiny-rl-example-concept&#34;&gt;ğŸ’» Tiny RL Example (Concept)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if reward &amp;gt; 0:
    increase_probability(action)
else:
    decrease_probability(action)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AlphaGo&lt;/li&gt;
&lt;li&gt;robotics&lt;/li&gt;
&lt;li&gt;recommendation systems&lt;/li&gt;
&lt;li&gt;game AI&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-quiz-4&#34;&gt;â“ Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;What is the explorationâ€“exploitation tradeoff?&lt;/summary&gt;
  &lt;p&gt;Balancing trying new actions vs using known good ones.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-how-these-power-modern-ai&#34;&gt;ğŸ§  How These Power Modern AI&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Learning Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ChatGPT&lt;/td&gt;
&lt;td&gt;Self-supervised + RLHF&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image AI&lt;/td&gt;
&lt;td&gt;Self-supervised + supervised&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Robots&lt;/td&gt;
&lt;td&gt;Reinforcement learning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recommenders&lt;/td&gt;
&lt;td&gt;Supervised + RL&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-big-insight&#34;&gt;ğŸ“ Final Big Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;There is no â€œbestâ€ learning type.
There is only &lt;strong&gt;the right tool for the problem&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Great AI engineers choose wisely.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If you had unlimited data but no labels, which learning paradigm would you choose?&lt;/summary&gt;
  &lt;p&gt;Self-supervised learning.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Lecture 06 â€” Videoâ€“Text Multimodal Intelligence</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-06-video-text/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-06-video-text/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~4â€“5 hours (advanced + practical lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-video-is-the-hardest-modality&#34;&gt;ğŸ¥ Why Video Is the Hardest Modality&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Video = Images + Time + Motion + Audio + Semantics&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Compared to text or images, video introduces:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â± Temporal dependency&lt;/li&gt;
&lt;li&gt;ğŸ Motion understanding&lt;/li&gt;
&lt;li&gt;ğŸ”Š Optional audio synchronization&lt;/li&gt;
&lt;li&gt;ğŸ§  Long-range reasoning&lt;/li&gt;
&lt;li&gt;ğŸ’¾ Massive compute &amp;amp; memory cost&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;If you master &lt;strong&gt;Videoâ€“Text&lt;/strong&gt;, you understand &lt;em&gt;true multimodal intelligence&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-videotext-totext&#34;&gt;ğŸ§  What Is Videoâ€“Text-toâ€“Text?&lt;/h2&gt;
&lt;p&gt;Videoâ€“Text models map:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ“¹ Video â†’ ğŸ“ Text&lt;/li&gt;
&lt;li&gt;ğŸ“¹ + â“ Question â†’ âœï¸ Answer&lt;/li&gt;
&lt;li&gt;ğŸ“¹ + ğŸ—£ Prompt â†’ ğŸ“– Explanation / Summary / Reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;common-tasks&#34;&gt;Common Tasks&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Video Captioning&lt;/td&gt;
&lt;td&gt;â€œA man is cooking pasta in a kitchen.â€&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video QA&lt;/td&gt;
&lt;td&gt;â€œWhat did the dog do after jumping?â€&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video Summarization&lt;/td&gt;
&lt;td&gt;â€œThis video shows a traffic accidentâ€¦â€&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Temporal Reasoning&lt;/td&gt;
&lt;td&gt;â€œWhat happened before the explosion?â€&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Instruction Following&lt;/td&gt;
&lt;td&gt;â€œExplain this experiment step-by-step.â€&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-video-is-fundamentally-different&#34;&gt;ğŸ§© Why Video Is Fundamentally Different&lt;/h2&gt;
&lt;h3 id=&#34;image-vs-video&#34;&gt;Image vs Video&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Image&lt;/th&gt;
&lt;th&gt;Video&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Static&lt;/td&gt;
&lt;td&gt;Temporal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Single embedding&lt;/td&gt;
&lt;td&gt;Sequence of embeddings&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Local reasoning&lt;/td&gt;
&lt;td&gt;Long-range reasoning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Easy batching&lt;/td&gt;
&lt;td&gt;Memory explosion&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Key idea:&lt;/strong&gt;&lt;br&gt;
Video understanding = &lt;strong&gt;sequence modeling + vision + alignment&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-thinking-like-a-videotext-architect&#34;&gt;ğŸ§  Thinking Like a Videoâ€“Text Architect&lt;/h2&gt;
&lt;h3 id=&#34;the-core-questions&#34;&gt;The Core Questions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What is the &lt;strong&gt;unit of time&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;How many frames matter?&lt;/li&gt;
&lt;li&gt;Do we need &lt;strong&gt;motion&lt;/strong&gt;, or just &lt;strong&gt;key frames&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Can we compress time?&lt;/li&gt;
&lt;li&gt;Where does language interact?&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-canonical-videotext-architecture&#34;&gt;ğŸ§± Canonical Videoâ€“Text Architecture&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;
Video Frames â†’ Vision Encoder â†’ Temporal Encoder â†’ LLM â†’ Text Output

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;components&#34;&gt;Components&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Examples&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Vision Encoder&lt;/td&gt;
&lt;td&gt;ViT, ConvNet, Swin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Temporal Encoder&lt;/td&gt;
&lt;td&gt;Transformer, LSTM, Mamba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fusion&lt;/td&gt;
&lt;td&gt;Cross-attention&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Language Model&lt;/td&gt;
&lt;td&gt;LLaMA, GPT, T5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-temporal-modeling-strategies&#34;&gt;â± Temporal Modeling Strategies&lt;/h2&gt;
&lt;h3 id=&#34;1-uniform-sampling&#34;&gt;1ï¸âƒ£ Uniform Sampling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sample every &lt;em&gt;n&lt;/em&gt; frames&lt;/li&gt;
&lt;li&gt;Cheap but may miss key events&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-keyframe-extraction&#34;&gt;2ï¸âƒ£ Keyframe Extraction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shot detection&lt;/li&gt;
&lt;li&gt;Scene change detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-learned-temporal-attention&#34;&gt;3ï¸âƒ£ Learned Temporal Attention&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Model decides which frames matter&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Rule:&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Not all frames are equally intelligent.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-temporal-reasoning--frame-understanding&#34;&gt;ğŸ§  Temporal Reasoning â‰  Frame Understanding&lt;/h2&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â€œWhat happened &lt;strong&gt;before&lt;/strong&gt; the fall?â€&lt;/li&gt;
&lt;li&gt;â€œWhy did the crowd start running?â€&lt;/li&gt;
&lt;li&gt;â€œWhat caused the explosion?â€&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Event ordering&lt;/li&gt;
&lt;li&gt;Causal reasoning&lt;/li&gt;
&lt;li&gt;Memory across time&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-videotext-alignment&#34;&gt;ğŸ”— Videoâ€“Text Alignment&lt;/h2&gt;
&lt;h3 id=&#34;alignment-objectives&#34;&gt;Alignment Objectives&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Frame â†” Word&lt;/li&gt;
&lt;li&gt;Segment â†” Sentence&lt;/li&gt;
&lt;li&gt;Event â†” Explanation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Losses commonly used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Contrastive loss (CLIP-style)&lt;/li&gt;
&lt;li&gt;Cross-entropy on generated text&lt;/li&gt;
&lt;li&gt;Temporal grounding loss&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-popular-videotext-models&#34;&gt;ğŸ§ª Popular Videoâ€“Text Models&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Key Idea&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;VideoBERT&lt;/td&gt;
&lt;td&gt;Treat frames as tokens&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Flamingo&lt;/td&gt;
&lt;td&gt;Perceiver-style attention&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;InternVideo&lt;/td&gt;
&lt;td&gt;Unified video representation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video-LLaMA&lt;/td&gt;
&lt;td&gt;Video + LLM alignment&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT-4V&lt;/td&gt;
&lt;td&gt;Proprietary multimodal reasoning&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-videotext-pipeline-conceptual&#34;&gt;ğŸ Python: Videoâ€“Text Pipeline (Conceptual)&lt;/h2&gt;
&lt;h3 id=&#34;step-1-load-video-frames&#34;&gt;Step 1: Load Video Frames&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2

def load_frames(video_path, max_frames=32):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while len(frames) &amp;lt; max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
    cap.release()
    return frames
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;step-2-encode-frames&#34;&gt;Step 2: Encode Frames&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch

with torch.no_grad():
    frame_embeddings = vision_encoder(frames)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;step-3-temporal-encoding&#34;&gt;Step 3: Temporal Encoding&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;video_embedding = temporal_transformer(frame_embeddings)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;step-4-language-generation&#34;&gt;Step 4: Language Generation&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;output = llm.generate(
    video_embedding=video_embedding,
    prompt=&amp;quot;Describe what is happening in this video&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-memory--efficiency-tricks-very-important&#34;&gt;ğŸ§  Memory &amp;amp; Efficiency Tricks (VERY IMPORTANT)&lt;/h2&gt;
&lt;h3 id=&#34;problem&#34;&gt;Problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Video = huge memory cost&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solutions&#34;&gt;Solutions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Frame pooling&lt;/li&gt;
&lt;li&gt;Token pruning&lt;/li&gt;
&lt;li&gt;Sliding windows&lt;/li&gt;
&lt;li&gt;Temporal compression&lt;/li&gt;
&lt;li&gt;Hierarchical attention&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Engineering intelligence matters as much as model size&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-evaluation-of-videotext-models&#34;&gt;ğŸ§ª Evaluation of Videoâ€“Text Models&lt;/h2&gt;
&lt;h3 id=&#34;automatic-metrics&#34;&gt;Automatic Metrics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BLEU&lt;/li&gt;
&lt;li&gt;METEOR&lt;/li&gt;
&lt;li&gt;CIDEr&lt;/li&gt;
&lt;li&gt;ROUGE&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;human-evaluation-best&#34;&gt;Human Evaluation (BEST)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Temporal correctness&lt;/li&gt;
&lt;li&gt;Causal reasoning&lt;/li&gt;
&lt;li&gt;Hallucination rate&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-failure-modes&#34;&gt;âš ï¸ Failure Modes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;âŒ Hallucinating events&lt;/li&gt;
&lt;li&gt;âŒ Mixing timelines&lt;/li&gt;
&lt;li&gt;âŒ Ignoring small but critical actions&lt;/li&gt;
&lt;li&gt;âŒ Overconfidence&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Video hallucination is &lt;strong&gt;more dangerous&lt;/strong&gt; than image hallucination.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-research-insight-important&#34;&gt;ğŸ§  Research Insight (Important)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Most â€œvideo understandingâ€ models are actually &lt;strong&gt;image models with memory&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;True video intelligence requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Event abstraction&lt;/li&gt;
&lt;li&gt;Temporal causality&lt;/li&gt;
&lt;li&gt;Long-horizon planning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-student-self-test-hidden-answers&#34;&gt;ğŸ§ª Student Self-Test (Hidden Answers)&lt;/h2&gt;
&lt;h3 id=&#34;q1--objective&#34;&gt;Q1 â€” Objective&lt;/h3&gt;
&lt;p&gt;What makes video harder than images?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Temporal dependency, motion, memory, and causal reasoning.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2--mcq&#34;&gt;Q2 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which component models time?&lt;/p&gt;
&lt;p&gt;A. Vision Encoder
B. Tokenizer
C. Temporal Encoder
D. Loss Function&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Temporal Encoder&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3--mcq&#34;&gt;Q3 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which is NOT a videoâ€“text task?&lt;/p&gt;
&lt;p&gt;A. Video captioning
B. Video QA
C. Video super-resolution
D. Video summarization&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Video super-resolution&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4--objective&#34;&gt;Q4 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is uniform frame sampling risky?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It may miss critical events or actions.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5--objective&#34;&gt;Q5 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is temporal hallucination?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Inventing events that never occurred in the video timeline.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If AI understands video perfectly, what responsibility do humans still hold?&lt;/summary&gt;
  &lt;p&gt;Interpretation, ethics, judgment, and accountability.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-key-takeaways&#34;&gt;âœ… Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Video is &lt;strong&gt;the hardest modality&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Time is the real challenge&lt;/li&gt;
&lt;li&gt;Compression = intelligence&lt;/li&gt;
&lt;li&gt;Reasoning &amp;gt; perception&lt;/li&gt;
&lt;li&gt;Human evaluation is critical&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>DE101-PD06 â€” Finding Insights with Pandas (From Data to Decisions)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd06-finging-insights/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-pandas-data-engineering-foundations/de101-pd06-finging-insights/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~120 minutes&lt;/p&gt;
&lt;h2 id=&#34;-goal-of-this-chapter&#34;&gt;ğŸ¯ Goal of This Chapter&lt;/h2&gt;
&lt;p&gt;This chapter teaches &lt;strong&gt;how to think&lt;/strong&gt;, not just how to code.&lt;/p&gt;
&lt;p&gt;You will learn:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to ask good questions&lt;/li&gt;
&lt;li&gt;How to extract insights from data&lt;/li&gt;
&lt;li&gt;How real companies analyze performance&lt;/li&gt;
&lt;li&gt;How to communicate findings clearly&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mock-dataset--fifa-world-cup-player-stats&#34;&gt;âš½ Mock Dataset â€” FIFA World Cup Player Stats&lt;/h2&gt;
&lt;p&gt;We will use a simplified dataset inspired by &lt;strong&gt;FIFA World Cup matches&lt;/strong&gt;,&lt;br&gt;
with focus on &lt;strong&gt;Lionel Messi&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

data = {
    &amp;quot;player&amp;quot;: [&amp;quot;Messi&amp;quot;, &amp;quot;Messi&amp;quot;, &amp;quot;Messi&amp;quot;, &amp;quot;Mbappe&amp;quot;, &amp;quot;Mbappe&amp;quot;, &amp;quot;Neymar&amp;quot;, &amp;quot;Modric&amp;quot;, &amp;quot;Messi&amp;quot;],
    &amp;quot;team&amp;quot;: [&amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;France&amp;quot;, &amp;quot;France&amp;quot;, &amp;quot;Brazil&amp;quot;, &amp;quot;Croatia&amp;quot;, &amp;quot;Argentina&amp;quot;],
    &amp;quot;match&amp;quot;: [1, 2, 3, 1, 2, 1, 1, 4],
    &amp;quot;goals&amp;quot;: [1, 2, 1, 2, 1, 1, 0, 1],
    &amp;quot;assists&amp;quot;: [0, 1, 0, 0, 1, 0, 1, 0],
    &amp;quot;shots&amp;quot;: [4, 6, 5, 7, 5, 4, 2, 3],
    &amp;quot;minutes&amp;quot;: [90, 90, 90, 90, 90, 90, 90, 90],
    &amp;quot;position&amp;quot;: [&amp;quot;FW&amp;quot;, &amp;quot;FW&amp;quot;, &amp;quot;FW&amp;quot;, &amp;quot;FW&amp;quot;, &amp;quot;FW&amp;quot;, &amp;quot;FW&amp;quot;, &amp;quot;MF&amp;quot;, &amp;quot;FW&amp;quot;]
}

df = pd.DataFrame(data)
df
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-20-insight-questions-with-answers&#34;&gt;ğŸ§  20 Insight Questions (with Answers)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q1-how-many-matches-did-each-player-play&#34;&gt;Q1ï¸âƒ£ How many matches did each player play?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;player&amp;quot;)[&amp;quot;match&amp;quot;].count()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Player usage &amp;amp; reliability.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q2-total-goals-per-player&#34;&gt;Q2ï¸âƒ£ Total goals per player?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;player&amp;quot;)[&amp;quot;goals&amp;quot;].sum().sort_values(ascending=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Who is the main scorer?&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q3-average-goals-per-match&#34;&gt;Q3ï¸âƒ£ Average goals per match?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;player&amp;quot;)[&amp;quot;goals&amp;quot;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Consistency vs volume.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q4-who-contributed-most-goals--assists&#34;&gt;Q4ï¸âƒ£ Who contributed most (goals + assists)?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.assign(contribution=df[&amp;quot;goals&amp;quot;] + df[&amp;quot;assists&amp;quot;]) \
  .groupby(&amp;quot;player&amp;quot;)[&amp;quot;contribution&amp;quot;].sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: True attacking impact.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q5-messis-total-world-cup-impact&#34;&gt;Q5ï¸âƒ£ Messiâ€™s total World Cup impact?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[df[&amp;quot;player&amp;quot;] == &amp;quot;Messi&amp;quot;][[&amp;quot;goals&amp;quot;, &amp;quot;assists&amp;quot;]].sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Player-specific deep dive.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q6-shots-to-goals-efficiency&#34;&gt;Q6ï¸âƒ£ Shots-to-goals efficiency?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.assign(efficiency=df[&amp;quot;goals&amp;quot;] / df[&amp;quot;shots&amp;quot;]) \
  .groupby(&amp;quot;player&amp;quot;)[&amp;quot;efficiency&amp;quot;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Finishing quality.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q7-which-team-scored-the-most-goals&#34;&gt;Q7ï¸âƒ£ Which team scored the most goals?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;team&amp;quot;)[&amp;quot;goals&amp;quot;].sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Team dominance.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q8-messis-share-of-argentina-goals&#34;&gt;Q8ï¸âƒ£ Messiâ€™s share of Argentina goals?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;total_arg_goals = df[df[&amp;quot;team&amp;quot;] == &amp;quot;Argentina&amp;quot;][&amp;quot;goals&amp;quot;].sum()
messi_goals = df[df[&amp;quot;player&amp;quot;] == &amp;quot;Messi&amp;quot;][&amp;quot;goals&amp;quot;].sum()

messi_goals / total_arg_goals
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Team dependency on star player.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q9-goals-by-position&#34;&gt;Q9ï¸âƒ£ Goals by position?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;position&amp;quot;)[&amp;quot;goals&amp;quot;].sum()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Tactical contribution.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q10-average-shots-per-match-per-player&#34;&gt;Q1ï¸âƒ£0ï¸âƒ£ Average shots per match per player?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;player&amp;quot;)[&amp;quot;shots&amp;quot;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Offensive involvement.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q11-which-match-had-the-most-goals&#34;&gt;Q1ï¸âƒ£1ï¸âƒ£ Which match had the most goals?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;match&amp;quot;)[&amp;quot;goals&amp;quot;].sum().sort_values(ascending=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: High-impact games.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q12-messi-trend-over-matches&#34;&gt;Q1ï¸âƒ£2ï¸âƒ£ Messi trend over matches?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[df[&amp;quot;player&amp;quot;] == &amp;quot;Messi&amp;quot;].sort_values(&amp;quot;match&amp;quot;)[[&amp;quot;match&amp;quot;, &amp;quot;goals&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Performance trajectory.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q13-player-with-best-goals-per-90-minutes&#34;&gt;Q1ï¸âƒ£3ï¸âƒ£ Player with best goals per 90 minutes?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.assign(goals_per_90=df[&amp;quot;goals&amp;quot;] / df[&amp;quot;minutes&amp;quot;] * 90) \
  .groupby(&amp;quot;player&amp;quot;)[&amp;quot;goals_per_90&amp;quot;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Fair comparison.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q14-assist-leaders&#34;&gt;Q1ï¸âƒ£4ï¸âƒ£ Assist leaders?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;player&amp;quot;)[&amp;quot;assists&amp;quot;].sum().sort_values(ascending=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Playmaking ability.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q15-messi-vs-mbappe-comparison&#34;&gt;Q1ï¸âƒ£5ï¸âƒ£ Messi vs Mbappe comparison?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[df[&amp;quot;player&amp;quot;].isin([&amp;quot;Messi&amp;quot;, &amp;quot;Mbappe&amp;quot;])] \
  .groupby(&amp;quot;player&amp;quot;)[[&amp;quot;goals&amp;quot;, &amp;quot;assists&amp;quot;, &amp;quot;shots&amp;quot;]].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Superstar comparison.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q16-which-team-relies-most-on-one-player&#34;&gt;Q1ï¸âƒ£6ï¸âƒ£ Which team relies most on one player?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;team_totals = df.groupby(&amp;quot;team&amp;quot;)[&amp;quot;goals&amp;quot;].sum()
player_totals = df.groupby([&amp;quot;team&amp;quot;, &amp;quot;player&amp;quot;])[&amp;quot;goals&amp;quot;].sum()

(player_totals / team_totals).sort_values(ascending=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Risk concentration.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q17-match-by-match-contribution-table&#34;&gt;Q1ï¸âƒ£7ï¸âƒ£ Match-by-match contribution table?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.assign(contribution=df[&amp;quot;goals&amp;quot;] + df[&amp;quot;assists&amp;quot;]) \
  .pivot_table(
      index=&amp;quot;match&amp;quot;,
      columns=&amp;quot;player&amp;quot;,
      values=&amp;quot;contribution&amp;quot;,
      aggfunc=&amp;quot;sum&amp;quot;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Game-level impact.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q18-who-is-most-consistent-scorer&#34;&gt;Q1ï¸âƒ£8ï¸âƒ£ Who is most consistent scorer?&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;player&amp;quot;)[&amp;quot;goals&amp;quot;].std()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Low variance = consistency.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q19-identify-underperformers-high-shots-low-goals&#34;&gt;Q1ï¸âƒ£9ï¸âƒ£ Identify underperformers (high shots, low goals)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.assign(conversion=df[&amp;quot;goals&amp;quot;] / df[&amp;quot;shots&amp;quot;]) \
  .sort_values(&amp;quot;conversion&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: Optimization opportunity.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;q20-executive-summary-table&#34;&gt;Q2ï¸âƒ£0ï¸âƒ£ Executive Summary Table&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.groupby(&amp;quot;player&amp;quot;).agg(
    matches=(&amp;quot;match&amp;quot;, &amp;quot;count&amp;quot;),
    goals=(&amp;quot;goals&amp;quot;, &amp;quot;sum&amp;quot;),
    assists=(&amp;quot;assists&amp;quot;, &amp;quot;sum&amp;quot;),
    shots=(&amp;quot;shots&amp;quot;, &amp;quot;sum&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸ“Œ Insight: One-table decision view.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-how-companies-use-this&#34;&gt;ğŸ§  How Companies Use This&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt; â†’ Experiment impact&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Meta&lt;/strong&gt; â†’ Creator performance&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; â†’ Model evaluation metrics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sports Analytics&lt;/strong&gt; â†’ Strategy decisions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Same logic, different domain.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-lesson&#34;&gt;ğŸ Final Lesson&lt;/h2&gt;
&lt;p&gt;Insight is not about charts.
Insight is about &lt;strong&gt;asking the right questions&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Pandas is your microscope.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-next-steps&#34;&gt;ğŸš€ Next Steps&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Add visualization&lt;/li&gt;
&lt;li&gt;Turn insights into dashboards&lt;/li&gt;
&lt;li&gt;Feed features into ML models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You now think like a &lt;strong&gt;Data Engineer + Analyst + AI Researcher&lt;/strong&gt; ğŸ§ ğŸ”¥&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-03 â€” Linear Search</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-03-linear-search/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-03-linear-search/</guid>
      <description>&lt;h2 id=&#34;-problem-statement&#34;&gt;ğŸ§© Problem Statement&lt;/h2&gt;
&lt;p&gt;Given an array &lt;code&gt;nums&lt;/code&gt; and a target value &lt;code&gt;x&lt;/code&gt;, return &lt;code&gt;True&lt;/code&gt; if &lt;code&gt;x&lt;/code&gt; exists in the array, otherwise return &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This problem evaluates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your understanding of &lt;strong&gt;search strategies&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Whether you know &lt;strong&gt;when sorted data matters&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Your ability to trade &lt;strong&gt;speed vs constraints&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-input---output&#34;&gt;ğŸ“¥ Sample Input / ğŸ“¤ Output&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nums&lt;/th&gt;
&lt;th&gt;x&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[1, 2, 3]&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[1, 2, 3]&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[5]&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[2, 4, 6]&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight&#34;&gt;ğŸ§  Interviewer Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â— Interviewers are not checking if you can scan an array.&lt;br&gt;
They want to know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is the array &lt;strong&gt;sorted&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Is random access allowed?&lt;/li&gt;
&lt;li&gt;How large is the data?&lt;/li&gt;
&lt;li&gt;Can preprocessing help?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Below are &lt;strong&gt;all major searching techniques&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-1--linear-search-baseline&#34;&gt;âœ… Approach 1 â€” Linear Search (Baseline)&lt;/h2&gt;
&lt;h3 id=&#34;-idea&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Check every element &lt;strong&gt;one by one&lt;/strong&gt; until the target is found.&lt;/p&gt;
&lt;p&gt;Works on &lt;strong&gt;any array&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-code&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def linear_search(nums, x):
    for v in nums:
        if v == x:
            return True
    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Use when the array is &lt;strong&gt;unsorted or very small&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-2--binary-search-sorted-array-required&#34;&gt;ğŸ§© Approach 2 â€” Binary Search (Sorted Array Required)&lt;/h2&gt;
&lt;h3 id=&#34;-idea-1&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Repeatedly divide the search space in half.&lt;/p&gt;
&lt;p&gt;Requires &lt;strong&gt;sorted input&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-code-1&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def binary_search(nums, x):
    left, right = 0, len(nums) - 1

    while left &amp;lt;= right:
        mid = (left + right) // 2

        if nums[mid] == x:
            return True
        elif nums[mid] &amp;lt; x:
            left = mid + 1
        else:
            right = mid - 1

    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-1&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(log n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ One of the &lt;strong&gt;most important interview algorithms&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-3--binary-search-recursive&#34;&gt;ğŸ§© Approach 3 â€” Binary Search (Recursive)&lt;/h2&gt;
&lt;h3 id=&#34;-idea-2&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Same logic as iterative binary search, implemented recursively.&lt;/p&gt;
&lt;h3 id=&#34;-code-2&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def binary_search_recursive(nums, x, left, right):
    if left &amp;gt; right:
        return False

    mid = (left + right) // 2

    if nums[mid] == x:
        return True
    elif nums[mid] &amp;lt; x:
        return binary_search_recursive(nums, x, mid + 1, right)
    else:
        return binary_search_recursive(nums, x, left, mid - 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-2&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(log n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(log n) (recursion stack)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¡ Use only if recursion is preferred.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-4--jump-search-sorted-array&#34;&gt;ğŸ§© Approach 4 â€” Jump Search (Sorted Array)&lt;/h2&gt;
&lt;h3 id=&#34;-idea-3&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Jump ahead by fixed steps (âˆšn) instead of checking every element.&lt;/p&gt;
&lt;p&gt;Useful when &lt;strong&gt;random access is costly&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-code-3&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math

def jump_search(nums, x):
    n = len(nums)
    step = int(math.sqrt(n))
    prev = 0

    while prev &amp;lt; n and nums[min(step, n) - 1] &amp;lt; x:
        prev = step
        step += int(math.sqrt(n))
        if prev &amp;gt;= n:
            return False

    for i in range(prev, min(step, n)):
        if nums[i] == x:
            return True

    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-3&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(âˆšn)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¡ Rare, but good for algorithm discussions.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-5--exponential-search&#34;&gt;ğŸ§© Approach 5 â€” Exponential Search&lt;/h2&gt;
&lt;h3 id=&#34;-idea-4&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Find range where target may exist&lt;/li&gt;
&lt;li&gt;Apply binary search inside that range&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Efficient for &lt;strong&gt;unbounded or infinite arrays&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-code-4&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def exponential_search(nums, x):
    if not nums:
        return False

    if nums[0] == x:
        return True

    i = 1
    while i &amp;lt; len(nums) and nums[i] &amp;lt;= x:
        i *= 2

    return binary_search(nums[:i], x)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-4&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(log n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Common in system-level interviews.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-6--hash-set-search-fast-lookup&#34;&gt;ğŸ§© Approach 6 â€” Hash Set Search (Fast Lookup)&lt;/h2&gt;
&lt;h3 id=&#34;-idea-5&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Convert the array into a set for &lt;strong&gt;O(1) average lookup&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-code-5&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def hash_search(nums, x):
    lookup = set(nums)
    return x in lookup
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-5&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n) preprocessing, O(1) lookup&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Excellent when &lt;strong&gt;many searches&lt;/strong&gt; are needed.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-choosing-the-right-search-algorithm&#34;&gt;ğŸ¯ Choosing the Right Search Algorithm&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scenario&lt;/th&gt;
&lt;th&gt;Best Choice&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Unsorted array&lt;/td&gt;
&lt;td&gt;Linear Search&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sorted array&lt;/td&gt;
&lt;td&gt;Binary Search&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Many queries&lt;/td&gt;
&lt;td&gt;Hash Set&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Very large data&lt;/td&gt;
&lt;td&gt;Binary / Exponential&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Teaching fundamentals&lt;/td&gt;
&lt;td&gt;Linear / Binary&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-interview-tip&#34;&gt;ğŸ§  Final Interview Tip&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Always ask:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is the data sorted?&lt;/li&gt;
&lt;li&gt;How many queries?&lt;/li&gt;
&lt;li&gt;Is extra memory allowed?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Answering these &lt;strong&gt;before coding&lt;/strong&gt; is what impresses interviewers.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;p&gt;Searching is not about speed alone.
Itâ€™s about &lt;strong&gt;choosing the right strategy under constraints&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you master these, &lt;strong&gt;binary search variations will feel easy&lt;/strong&gt; ğŸš€&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lecture 06 â€” Data Types &amp; Modalities</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-06-data-modalities/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-06-data-modalities/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~2â€“2.5 hours (core data understanding lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-big-idea-read-this-first&#34;&gt;ğŸŒ Big Idea (Read This First)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI does not see the world like humans.&lt;br&gt;
It sees data representations.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Understanding data modalities is understanding:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;what AI can learn&lt;/li&gt;
&lt;li&gt;what AI cannot learn&lt;/li&gt;
&lt;li&gt;why some problems are harder than others&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-data-is-everything-but-not-equal&#34;&gt;ğŸ“¦ Data Is Everything (But Not Equal)&lt;/h2&gt;
&lt;p&gt;AI performance is often limited by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data quality&lt;/li&gt;
&lt;li&gt;data quantity&lt;/li&gt;
&lt;li&gt;data structure&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Better model + bad data = bad AI&lt;br&gt;
Simple model + good data = strong AI&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-a-modality&#34;&gt;ğŸ§  What Is a Modality?&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;modality&lt;/strong&gt; is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A way information is represented.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Humans:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;see ğŸ‘€&lt;/li&gt;
&lt;li&gt;hear ğŸ‘‚&lt;/li&gt;
&lt;li&gt;read ğŸ“–&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;processes numbers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All modalities become &lt;strong&gt;numbers&lt;/strong&gt; eventually.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-1-tabular-data-the-quiet-workhorse&#34;&gt;ğŸ”¢ 1ï¸âƒ£ Tabular Data (The Quiet Workhorse)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;p&gt;Rows &amp;amp; columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Excel&lt;/li&gt;
&lt;li&gt;CSV&lt;/li&gt;
&lt;li&gt;databases&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-why-it-matters&#34;&gt;ğŸ§  Why it matters&lt;/h3&gt;
&lt;p&gt;Most real-world AI is still tabular.&lt;/p&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;finance (credit scoring)&lt;/li&gt;
&lt;li&gt;healthcare (risk prediction)&lt;/li&gt;
&lt;li&gt;business (forecasting)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-project&#34;&gt;ğŸ’» Mini Project&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

df = pd.DataFrame({
    &amp;quot;age&amp;quot;: [25, 40, 60],
    &amp;quot;income&amp;quot;: [30000, 70000, 120000]
})

df[&amp;quot;income&amp;quot;].mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Tabular data loves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;statistics&lt;/li&gt;
&lt;li&gt;classical ML&lt;/li&gt;
&lt;li&gt;interpretability&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-2-text-data-language-as-data&#34;&gt;ğŸ“š 2ï¸âƒ£ Text Data (Language as Data)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-1&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;sentences&lt;/li&gt;
&lt;li&gt;documents&lt;/li&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;li&gt;chat logs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-challenge&#34;&gt;ğŸ§  Challenge&lt;/h3&gt;
&lt;p&gt;Text has:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;order&lt;/li&gt;
&lt;li&gt;meaning&lt;/li&gt;
&lt;li&gt;ambiguity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Machines see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tokens&lt;/li&gt;
&lt;li&gt;embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-example&#34;&gt;ğŸ’» Mini Example&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;&amp;quot;AI is powerful&amp;quot;
â†’ [&amp;quot;AI&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;powerful&amp;quot;]
â†’ vectors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLP&lt;/li&gt;
&lt;li&gt;LLMs&lt;/li&gt;
&lt;li&gt;search engines&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-3-image-data-vision&#34;&gt;ğŸ–¼ï¸ 3ï¸âƒ£ Image Data (Vision)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-2&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;pixels&lt;/li&gt;
&lt;li&gt;grids of numbers&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-ai-does-not-see-objects&#34;&gt;ğŸ§  AI does NOT see objects.&lt;/h3&gt;
&lt;p&gt;It sees:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;edges&lt;/li&gt;
&lt;li&gt;textures&lt;/li&gt;
&lt;li&gt;patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-example-1&#34;&gt;ğŸ’» Mini Example&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

image = np.zeros((224, 224, 3))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;medical imaging&lt;/li&gt;
&lt;li&gt;self-driving&lt;/li&gt;
&lt;li&gt;facial recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-4-audio-data-sound-as-waves&#34;&gt;ğŸ§ 4ï¸âƒ£ Audio Data (Sound as Waves)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-3&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;waveforms&lt;/li&gt;
&lt;li&gt;frequencies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI learns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pitch&lt;/li&gt;
&lt;li&gt;rhythm&lt;/li&gt;
&lt;li&gt;patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-example-2&#34;&gt;ğŸ’» Mini Example&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Audio â†’ waveform â†’ spectrogram â†’ model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;speech recognition&lt;/li&gt;
&lt;li&gt;music generation&lt;/li&gt;
&lt;li&gt;voice assistants&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-5-video-data-the-hardest-modality&#34;&gt;ğŸ¥ 5ï¸âƒ£ Video Data (The Hardest Modality)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-4&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;p&gt;Images + time + audio&lt;/p&gt;
&lt;p&gt;Challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;massive data size&lt;/li&gt;
&lt;li&gt;temporal reasoning&lt;/li&gt;
&lt;li&gt;motion understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;action recognition&lt;/li&gt;
&lt;li&gt;surveillance&lt;/li&gt;
&lt;li&gt;robotics&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-6-multimodal-data-modern-ai&#34;&gt;ğŸ”€ 6ï¸âƒ£ Multimodal Data (Modern AI)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-5&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;p&gt;Multiple modalities together:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;text + image&lt;/li&gt;
&lt;li&gt;image + audio&lt;/li&gt;
&lt;li&gt;text + video&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-why-multimodal-ai-matters&#34;&gt;ğŸ¤– Why Multimodal AI Matters&lt;/h3&gt;
&lt;p&gt;Humans understand the world multimodally.&lt;/p&gt;
&lt;p&gt;Modern AI tries to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;align modalities&lt;/li&gt;
&lt;li&gt;share representations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CLIP&lt;/li&gt;
&lt;li&gt;GPT-4V&lt;/li&gt;
&lt;li&gt;Gemini&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-concept-example&#34;&gt;ğŸ’» Mini Concept Example&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Image of cat + &amp;quot;A cat sitting on a sofa&amp;quot;
â†’ same embedding space
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-multimodal-ai-is-hard&#34;&gt;ğŸ¤¯ Why Multimodal AI Is Hard&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Problem&lt;/th&gt;
&lt;th&gt;Reason&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Alignment&lt;/td&gt;
&lt;td&gt;Different data structures&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale&lt;/td&gt;
&lt;td&gt;Massive datasets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Noise&lt;/td&gt;
&lt;td&gt;Cross-modal ambiguity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Bias&lt;/td&gt;
&lt;td&gt;Uneven modality quality&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-data-complexity-vs-model-complexity&#34;&gt;ğŸ§  Data Complexity vs Model Complexity&lt;/h2&gt;
&lt;p&gt;Rule of thumb:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple data â†’ simple models&lt;/li&gt;
&lt;li&gt;Complex data â†’ deep models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wrong pairing = failure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-choosing-the-right-modality&#34;&gt;ğŸ§­ Choosing the Right Modality&lt;/h2&gt;
&lt;p&gt;Ask:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What signal matters most?&lt;/li&gt;
&lt;li&gt;What data is available?&lt;/li&gt;
&lt;li&gt;What errors are acceptable?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Good AI starts with &lt;strong&gt;data choice&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-real-world-mapping&#34;&gt;ğŸŒ Real-World Mapping&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Modalities&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ChatGPT&lt;/td&gt;
&lt;td&gt;Text&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPT-4V&lt;/td&gt;
&lt;td&gt;Text + Image&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Self-driving car&lt;/td&gt;
&lt;td&gt;Image + Video + Sensor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Voice assistant&lt;/td&gt;
&lt;td&gt;Audio + Text&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-big-insight&#34;&gt;ğŸ§  Final Big Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The world is rich.
Data is a simplified shadow of it.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Great AI engineers understand the gap.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;If you had unlimited compute but poor data, would AI succeed?&lt;/summary&gt;
  &lt;p&gt;No â€” data quality dominates.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Lecture 07 â€” Visual Question Answering (VQA) &amp; Document Question Answering (DocQA)</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-07-vqa-docqa/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-07-vqa-docqa/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~4â€“5 hours (applied multimodal reasoning)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-vqa--docqa-matter-to-humanity&#34;&gt;ğŸ‘ï¸ğŸ“„ Why VQA &amp;amp; DocQA Matter to Humanity&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;VQA &amp;amp; DocQA turn perception into understanding.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;They allow machines to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ‘ï¸ See&lt;/li&gt;
&lt;li&gt;ğŸ“„ Read&lt;/li&gt;
&lt;li&gt;â“ Understand questions&lt;/li&gt;
&lt;li&gt;ğŸ§  Reason&lt;/li&gt;
&lt;li&gt;âœï¸ Answer &lt;em&gt;grounded in evidence&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the foundation of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assistive AI&lt;/li&gt;
&lt;li&gt;Education AI&lt;/li&gt;
&lt;li&gt;Legal &amp;amp; medical AI&lt;/li&gt;
&lt;li&gt;Scientific discovery&lt;/li&gt;
&lt;li&gt;Accessibility technology&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-visual-question-answering-vqa&#34;&gt;ğŸ§  What Is Visual Question Answering (VQA)?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ–¼ Image&lt;/li&gt;
&lt;li&gt;â“ Natural language question&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;âœï¸ Text answer grounded in the image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; â€œHow many people are wearing helmets?â€&lt;br&gt;
&lt;strong&gt;A:&lt;/strong&gt; â€œTwo.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-document-question-answering-docqa&#34;&gt;ğŸ§  What Is Document Question Answering (DocQA)?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ“„ Document image / PDF&lt;/li&gt;
&lt;li&gt;â“ Question&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;âœï¸ Answer extracted or reasoned from the document&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; â€œWhat is the invoice total?â€&lt;br&gt;
&lt;strong&gt;A:&lt;/strong&gt; â€œ$1,245.50â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vqa-vs-docqa-key-differences&#34;&gt;ğŸ” VQA vs DocQA (Key Differences)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Aspect&lt;/th&gt;
&lt;th&gt;VQA&lt;/th&gt;
&lt;th&gt;DocQA&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Main Challenge&lt;/td&gt;
&lt;td&gt;Visual reasoning&lt;/td&gt;
&lt;td&gt;Text + layout understanding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Precision&lt;/td&gt;
&lt;td&gt;Often coarse&lt;/td&gt;
&lt;td&gt;Extremely precise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OCR Required&lt;/td&gt;
&lt;td&gt;Optional&lt;/td&gt;
&lt;td&gt;Mandatory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hallucination Risk&lt;/td&gt;
&lt;td&gt;Medium&lt;/td&gt;
&lt;td&gt;Very high&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Evaluation&lt;/td&gt;
&lt;td&gt;Semantic&lt;/td&gt;
&lt;td&gt;Exact match critical&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;DocQA punishes mistakes much harder than VQA.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-cognitive-skills-required&#34;&gt;ğŸ§  Cognitive Skills Required&lt;/h2&gt;
&lt;p&gt;Both tasks require:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visual grounding&lt;/li&gt;
&lt;li&gt;Cross-modal alignment&lt;/li&gt;
&lt;li&gt;Reasoning&lt;/li&gt;
&lt;li&gt;Attention control&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But DocQA adds:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Layout reasoning&lt;/li&gt;
&lt;li&gt;Reading order&lt;/li&gt;
&lt;li&gt;Table understanding&lt;/li&gt;
&lt;li&gt;Key-value extraction&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-canonical-architecture-unified-view&#34;&gt;ğŸ§± Canonical Architecture (Unified View)&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;
Image / Document
â†“
Vision Encoder (ViT / CNN)
â†“
OCR (DocQA only)
â†“
Layout / Spatial Encoder
â†“
Cross-Attention Fusion
â†“
LLM Reasoning
â†“
Text Answer

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vision-encoding&#34;&gt;ğŸ‘ï¸ Vision Encoding&lt;/h2&gt;
&lt;p&gt;Common backbones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ViT&lt;/li&gt;
&lt;li&gt;Swin Transformer&lt;/li&gt;
&lt;li&gt;CNN + FPN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Key requirement:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Preserve &lt;strong&gt;spatial information&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ocr-is-not-optional-for-docqa&#34;&gt;ğŸ”¤ OCR Is NOT Optional for DocQA&lt;/h2&gt;
&lt;p&gt;OCR extracts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Text&lt;/li&gt;
&lt;li&gt;Bounding boxes&lt;/li&gt;
&lt;li&gt;Confidence scores&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Popular OCR tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tesseract&lt;/li&gt;
&lt;li&gt;PaddleOCR&lt;/li&gt;
&lt;li&gt;EasyOCR&lt;/li&gt;
&lt;li&gt;TrOCR (Transformer-based)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Bad OCR = Impossible DocQA&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-layout-understanding-critical&#34;&gt;ğŸ§­ Layout Understanding (CRITICAL)&lt;/h2&gt;
&lt;p&gt;Documents are not sentences â€” they are &lt;strong&gt;spatial graphs&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;layout-features&#34;&gt;Layout Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;X, Y coordinates&lt;/li&gt;
&lt;li&gt;Width / height&lt;/li&gt;
&lt;li&gt;Reading order&lt;/li&gt;
&lt;li&gt;Font size / style&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LayoutLM&lt;/li&gt;
&lt;li&gt;LayoutLMv3&lt;/li&gt;
&lt;li&gt;DocFormer&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-reasoning-types-in-vqa--docqa&#34;&gt;ğŸ§  Reasoning Types in VQA &amp;amp; DocQA&lt;/h2&gt;
&lt;h3 id=&#34;vqa-reasoning&#34;&gt;VQA Reasoning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Counting&lt;/li&gt;
&lt;li&gt;Spatial (â€œleft ofâ€, â€œbehindâ€)&lt;/li&gt;
&lt;li&gt;Attribute recognition&lt;/li&gt;
&lt;li&gt;Commonsense&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;docqa-reasoning&#34;&gt;DocQA Reasoning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Lookup&lt;/li&gt;
&lt;li&gt;Aggregation&lt;/li&gt;
&lt;li&gt;Comparison&lt;/li&gt;
&lt;li&gt;Multi-hop reasoning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-simple-vqa-pipeline&#34;&gt;ğŸ Python: Simple VQA Pipeline&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image = load_image(&amp;quot;scene.jpg&amp;quot;)
question = &amp;quot;How many cars are parked?&amp;quot;

vision_features = vision_encoder(image)
answer = llm.generate(
    visual_features=vision_features,
    prompt=question
)

print(answer)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-docqa-pipeline-conceptual&#34;&gt;ğŸ Python: DocQA Pipeline (Conceptual)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;doc_image = load_image(&amp;quot;invoice.png&amp;quot;)
ocr_tokens, boxes = ocr_engine(doc_image)

doc_embedding = layout_encoder(ocr_tokens, boxes)

answer = llm.generate(
    document_embedding=doc_embedding,
    prompt=&amp;quot;What is the total amount?&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-prompt-engineering-for-vqa--docqa&#34;&gt;ğŸ§  Prompt Engineering for VQA &amp;amp; DocQA&lt;/h2&gt;
&lt;p&gt;Bad prompt:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œAnswer the question.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Good prompt:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œAnswer using only visible evidence. If uncertain, say â€˜Not foundâ€™.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Prompt discipline reduces hallucination.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-datasets-must-know&#34;&gt;ğŸ§ª Datasets (Must-Know)&lt;/h2&gt;
&lt;h3 id=&#34;vqa&#34;&gt;VQA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;VQA v2&lt;/li&gt;
&lt;li&gt;GQA&lt;/li&gt;
&lt;li&gt;CLEVR (synthetic reasoning)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;docqa&#34;&gt;DocQA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DocVQA&lt;/li&gt;
&lt;li&gt;FUNSD&lt;/li&gt;
&lt;li&gt;RVL-CDIP&lt;/li&gt;
&lt;li&gt;CORD&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-evaluation-metrics&#34;&gt;ğŸ“ Evaluation Metrics&lt;/h2&gt;
&lt;h3 id=&#34;vqa-1&#34;&gt;VQA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Accuracy&lt;/li&gt;
&lt;li&gt;Consensus-based scoring&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;docqa-1&#34;&gt;DocQA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exact Match (EM)&lt;/li&gt;
&lt;li&gt;F1 score&lt;/li&gt;
&lt;li&gt;String normalization critical&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;One wrong digit = wrong answer&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-hallucination-the-silent-killer&#34;&gt;âš ï¸ Hallucination: The Silent Killer&lt;/h2&gt;
&lt;p&gt;Common failure cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Answering from prior knowledge&lt;/li&gt;
&lt;li&gt;Guessing missing fields&lt;/li&gt;
&lt;li&gt;Confusing similar layouts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mitigation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evidence-aware prompting&lt;/li&gt;
&lt;li&gt;Answer verification&lt;/li&gt;
&lt;li&gt;Human-in-the-loop (later lecture)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-research-insight&#34;&gt;ğŸ§  Research Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;DocQA is closer to &lt;strong&gt;symbolic reasoning&lt;/strong&gt; than vision.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Strong DocQA models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Behave more like databases&lt;/li&gt;
&lt;li&gt;Require constraint enforcement&lt;/li&gt;
&lt;li&gt;Prefer abstention over guessing&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-student-knowledge-check-hidden-answers&#34;&gt;ğŸ§ª Student Knowledge Check (Hidden Answers)&lt;/h2&gt;
&lt;h3 id=&#34;q1--objective&#34;&gt;Q1 â€” Objective&lt;/h3&gt;
&lt;p&gt;What extra component does DocQA require compared to VQA?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;OCR and layout understanding.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2--mcq&#34;&gt;Q2 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which model explicitly encodes layout?&lt;/p&gt;
&lt;p&gt;A. CLIP
B. ViT
C. LayoutLM
D. ResNet&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. LayoutLM&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3--mcq&#34;&gt;Q3 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which metric is most important for DocQA?&lt;/p&gt;
&lt;p&gt;A. BLEU
B. ROUGE
C. Exact Match
D. Perplexity&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Exact Match&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4--objective&#34;&gt;Q4 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is hallucination more dangerous in DocQA?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because answers must be exact and legally or financially correct.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5--objective&#34;&gt;Q5 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is layout reasoning?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Understanding text based on spatial structure, not just content.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If AI can read documents perfectly, what must humans still verify?&lt;/summary&gt;
  &lt;p&gt;Truth, intent, context, and ethical consequences.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-key-takeaways&#34;&gt;âœ… Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;VQA = visual reasoning&lt;/li&gt;
&lt;li&gt;DocQA = precision reasoning&lt;/li&gt;
&lt;li&gt;OCR quality defines upper bound&lt;/li&gt;
&lt;li&gt;Layout is intelligence&lt;/li&gt;
&lt;li&gt;Abstaining is better than hallucinating&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-04 â€” Nested Loops (Thinking Beyond O(nÂ²))</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-04-nested-loop/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-04-nested-loop/</guid>
      <description>&lt;h2 id=&#34;-problem-statement&#34;&gt;ğŸ§© Problem Statement&lt;/h2&gt;
&lt;p&gt;Given an integer array &lt;code&gt;nums&lt;/code&gt;, determine whether the array contains &lt;strong&gt;any duplicate elements&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Return &lt;code&gt;True&lt;/code&gt; if &lt;strong&gt;any value appears at least twice&lt;/strong&gt;, otherwise return &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This problem intentionally encourages a &lt;strong&gt;nested-loop solution first&lt;/strong&gt;, and then tests whether you understand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How nested loops scale&lt;/li&gt;
&lt;li&gt;When they become inefficient&lt;/li&gt;
&lt;li&gt;How to optimize them&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-input---output&#34;&gt;ğŸ“¥ Sample Input / ğŸ“¤ Output&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nums&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[1, 2, 3, 1]&lt;/td&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[1, 2, 3, 4]&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[5]&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[2, 2, 2]&lt;/td&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight&#34;&gt;ğŸ§  Interviewer Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â— Interviewers &lt;strong&gt;expect&lt;/strong&gt; you to start with a nested loop.&lt;br&gt;
What they really care about is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can you analyze the complexity?&lt;/li&gt;
&lt;li&gt;Do you recognize inefficiency?&lt;/li&gt;
&lt;li&gt;Can you improve it?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Letâ€™s start from the &lt;strong&gt;baseline&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-1--brute-force-nested-loop&#34;&gt;ğŸ§© Approach 1 â€” Brute Force Nested Loop&lt;/h2&gt;
&lt;h3 id=&#34;-idea&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Compare &lt;strong&gt;every pair of elements&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If any two elements are equal â†’ duplicate found.&lt;/p&gt;
&lt;h3 id=&#34;-code&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def contains_duplicate(nums):
    n = len(nums)

    # Compare every pair (i, j)
    for i in range(n):
        for j in range(i + 1, n):
            if nums[i] == nums[j]:
                return True

    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(nÂ²)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¡ Correct but &lt;strong&gt;does not scale&lt;/strong&gt; for large input.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-nested-loops-are-expensive&#34;&gt;ğŸ” Why Nested Loops Are Expensive&lt;/h2&gt;
&lt;p&gt;If:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;n = 1,000 â†’ 1,000,000 comparisons&lt;/li&gt;
&lt;li&gt;n = 10,000 â†’ 100,000,000 comparisons&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸš¨ This is why interviewers worry about nested loops.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-2--early-exit-optimization&#34;&gt;ğŸ§© Approach 2 â€” Early Exit Optimization&lt;/h2&gt;
&lt;h3 id=&#34;-idea-1&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Still use nested loops, but &lt;strong&gt;short-circuit early&lt;/strong&gt; when a duplicate is found.&lt;/p&gt;
&lt;h3 id=&#34;-code-1&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def contains_duplicate_early_exit(nums):
    n = len(nums)

    for i in range(n):
        for j in range(i + 1, n):
            if nums[i] == nums[j]:
                # Exit immediately once duplicate is found
                return True

    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-1&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(nÂ²) worst case&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¡ Slight improvement, but still quadratic.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-3--nested-loop-with-sorting&#34;&gt;ğŸ§© Approach 3 â€” Nested Loop with Sorting&lt;/h2&gt;
&lt;h3 id=&#34;-idea-2&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;If we &lt;strong&gt;sort first&lt;/strong&gt;, duplicates become adjacent.&lt;/p&gt;
&lt;p&gt;Then we only need &lt;strong&gt;one loop&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-code-2&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def contains_duplicate_sort(nums):
    nums.sort()

    for i in range(1, len(nums)):
        if nums[i] == nums[i - 1]:
            return True

    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-2&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n log n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1) (ignoring sort internals)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Great improvement â€” shows algorithmic thinking.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-4--hash-set-optimal&#34;&gt;ğŸ§© Approach 4 â€” Hash Set (Optimal)&lt;/h2&gt;
&lt;h3 id=&#34;-idea-3&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Use a set to &lt;strong&gt;remember what we&amp;rsquo;ve seen&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Avoid nested loops entirely.&lt;/p&gt;
&lt;h3 id=&#34;-code-3&#34;&gt;ğŸ§‘â€ğŸ’» Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def contains_duplicate_hash(nums):
    seen = set()

    for v in nums:
        if v in seen:
            return True
        seen.add(v)

    return False
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-3&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ This is the &lt;strong&gt;best practical solution&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nested-loop-pattern-general-form&#34;&gt;ğŸ§  Nested Loop Pattern (General Form)&lt;/h2&gt;
&lt;p&gt;Most nested-loop problems follow this shape:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(n):
    for j in range(i + 1, n):
        # Compare or combine nums[i] and nums[j]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Typical problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pair comparison&lt;/li&gt;
&lt;li&gt;Duplicate detection&lt;/li&gt;
&lt;li&gt;Brute-force two-sum&lt;/li&gt;
&lt;li&gt;Matrix traversal&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-when-nested-loops-are-acceptable&#34;&gt;ğŸ¯ When Nested Loops Are ACCEPTABLE&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Scenario&lt;/th&gt;
&lt;th&gt;Acceptable?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;n â‰¤ 1,000&lt;/td&gt;
&lt;td&gt;âœ… Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;One-time computation&lt;/td&gt;
&lt;td&gt;âœ… Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Teaching logic&lt;/td&gt;
&lt;td&gt;âœ… Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Large datasets&lt;/td&gt;
&lt;td&gt;âŒ No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Performance-critical&lt;/td&gt;
&lt;td&gt;âŒ No&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interview-level-takeaway&#34;&gt;ğŸ§  Interview-Level Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â— Nested loops are not â€œbadâ€.
They are &lt;strong&gt;a starting point&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What impresses interviewers is when you say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThis works, but itâ€™s O(nÂ²). We can do better.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-advice&#34;&gt;ğŸ Final Advice&lt;/h2&gt;
&lt;p&gt;If you master nested loops:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You understand &lt;strong&gt;brute force&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You understand &lt;strong&gt;optimization paths&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You understand &lt;strong&gt;why better algorithms exist&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thatâ€™s the mindset of a &lt;strong&gt;strong programmer&lt;/strong&gt;, not just a coder ğŸš€&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lecture 07 â€” AI Fields &amp; Specializations</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-07-ai-fields/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-07-ai-fields/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~2â€“2.5 hours (AI landscape lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-big-truth&#34;&gt;ğŸŒ Big Truth&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI is not one field.&lt;br&gt;
AI is an ecosystem of specialized intelligences.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Understanding AI means understanding:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;what problem you are solving&lt;/li&gt;
&lt;li&gt;which field fits that problem&lt;/li&gt;
&lt;li&gt;how fields connect&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-ai-landscape-mental-map&#34;&gt;ğŸ§­ The AI Landscape (Mental Map)&lt;/h2&gt;
&lt;p&gt;Think of AI like a city ğŸ™ï¸:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NLP = language district&lt;/li&gt;
&lt;li&gt;Vision = visual district&lt;/li&gt;
&lt;li&gt;RL = decision-making district&lt;/li&gt;
&lt;li&gt;Generative AI = creative district&lt;/li&gt;
&lt;li&gt;LLMs = central brain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fields overlap and collaborate.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-1-natural-language-processing-nlp&#34;&gt;ğŸ—£ï¸ 1ï¸âƒ£ Natural Language Processing (NLP)&lt;/h1&gt;
&lt;h3 id=&#34;-what-it-is&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;p&gt;Teaching machines to understand and generate human language.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-core-problems&#34;&gt;ğŸ§  Core problems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;tokenization&lt;/li&gt;
&lt;li&gt;syntax&lt;/li&gt;
&lt;li&gt;semantics&lt;/li&gt;
&lt;li&gt;context&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-applications&#34;&gt;ğŸ§ª Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;translation&lt;/li&gt;
&lt;li&gt;chatbots&lt;/li&gt;
&lt;li&gt;search engines&lt;/li&gt;
&lt;li&gt;summarization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-example&#34;&gt;ğŸ’» Mini Example&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;AI changes the world&amp;quot;
tokens = text.split()
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-project-idea&#34;&gt;ğŸ¯ Mini Project Idea&lt;/h3&gt;
&lt;p&gt;Build a:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sentiment analyzer&lt;/li&gt;
&lt;li&gt;document summarizer&lt;/li&gt;
&lt;li&gt;chatbot&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-2-computer-vision-cv&#34;&gt;ğŸ‘ï¸ 2ï¸âƒ£ Computer Vision (CV)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-1&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;p&gt;Teaching machines to interpret images and videos.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-core-problems-1&#34;&gt;ğŸ§  Core problems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;object detection&lt;/li&gt;
&lt;li&gt;segmentation&lt;/li&gt;
&lt;li&gt;tracking&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-applications-1&#34;&gt;ğŸ§ª Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;self-driving cars&lt;/li&gt;
&lt;li&gt;medical imaging&lt;/li&gt;
&lt;li&gt;facial recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-example-1&#34;&gt;ğŸ’» Mini Example&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;image.shape  # height x width x channels
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-project-idea-1&#34;&gt;ğŸ¯ Mini Project Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;face detector&lt;/li&gt;
&lt;li&gt;image classifier&lt;/li&gt;
&lt;li&gt;medical image analyzer&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-3-large-language-models-llms&#34;&gt;ğŸ§  3ï¸âƒ£ Large Language Models (LLMs)&lt;/h2&gt;
&lt;h3 id=&#34;-what-they-are&#34;&gt;ğŸ“˜ What they are&lt;/h3&gt;
&lt;p&gt;Very large neural networks trained on massive text data.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-what-makes-them-special&#34;&gt;ğŸ§  What makes them special&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;emergent reasoning&lt;/li&gt;
&lt;li&gt;instruction following&lt;/li&gt;
&lt;li&gt;tool usage&lt;/li&gt;
&lt;li&gt;multimodal ability&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-examples&#34;&gt;ğŸ§ª Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ChatGPT&lt;/li&gt;
&lt;li&gt;Gemini&lt;/li&gt;
&lt;li&gt;Claude&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-project-idea-2&#34;&gt;ğŸ¯ Mini Project Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Q&amp;amp;A bot&lt;/li&gt;
&lt;li&gt;code assistant&lt;/li&gt;
&lt;li&gt;research summarizer&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-4-generative-ai&#34;&gt;ğŸ¨ 4ï¸âƒ£ Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-2&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;p&gt;AI that &lt;strong&gt;creates&lt;/strong&gt; new content.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-models&#34;&gt;ğŸ§  Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Diffusion models&lt;/li&gt;
&lt;li&gt;GANs&lt;/li&gt;
&lt;li&gt;Autoregressive models&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-applications-2&#34;&gt;ğŸ§ª Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;image generation&lt;/li&gt;
&lt;li&gt;music composition&lt;/li&gt;
&lt;li&gt;video synthesis&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-concept&#34;&gt;ğŸ’» Mini Concept&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Noise â†’ Model â†’ Image
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-project-idea-3&#34;&gt;ğŸ¯ Mini Project Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;image generator&lt;/li&gt;
&lt;li&gt;style transfer&lt;/li&gt;
&lt;li&gt;text-to-image demo&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-5-reinforcement-learning-rl&#34;&gt;ğŸ•¹ï¸ 5ï¸âƒ£ Reinforcement Learning (RL)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-3&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;p&gt;Learning through interaction and reward.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-core-idea&#34;&gt;ğŸ§  Core idea&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;No labels. Only feedback.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-applications-3&#34;&gt;ğŸ§ª Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;game AI (AlphaGo)&lt;/li&gt;
&lt;li&gt;robotics&lt;/li&gt;
&lt;li&gt;recommendation systems&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-concept-1&#34;&gt;ğŸ’» Mini Concept&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;State â†’ Action â†’ Reward â†’ Update
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-mini-project-idea-4&#34;&gt;ğŸ¯ Mini Project Idea&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;game-playing agent&lt;/li&gt;
&lt;li&gt;robot controller&lt;/li&gt;
&lt;li&gt;decision optimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-6-multimodal-ai-where-everything-meets&#34;&gt;ğŸ”€ 6ï¸âƒ£ Multimodal AI (Where Everything Meets)&lt;/h2&gt;
&lt;h3 id=&#34;-what-it-is-4&#34;&gt;ğŸ“˜ What it is&lt;/h3&gt;
&lt;p&gt;AI that understands multiple data types at once.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-examples-1&#34;&gt;ğŸ§ª Examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT-4V&lt;/li&gt;
&lt;li&gt;Gemini&lt;/li&gt;
&lt;li&gt;CLIP&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-why-it-matters&#34;&gt;ğŸ§  Why it matters&lt;/h3&gt;
&lt;p&gt;The real world is multimodal.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-fields-are-not-isolated&#34;&gt;ğŸ§  Fields Are Not Isolated&lt;/h2&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Self-driving car = CV + RL + sensors&lt;/li&gt;
&lt;li&gt;ChatGPT = NLP + LLM + RLHF&lt;/li&gt;
&lt;li&gt;Medical AI = Vision + NLP + statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-career-path-mapping-very-useful&#34;&gt;ğŸ“ Career Path Mapping (Very Useful)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Interest&lt;/th&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Language &amp;amp; logic&lt;/td&gt;
&lt;td&gt;NLP / LLM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Images &amp;amp; perception&lt;/td&gt;
&lt;td&gt;CV&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Decision-making&lt;/td&gt;
&lt;td&gt;RL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Creativity&lt;/td&gt;
&lt;td&gt;Generative AI&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Systems thinking&lt;/td&gt;
&lt;td&gt;Multimodal AI&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-big-insight&#34;&gt;ğŸŒ± Final Big Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Great AI systems are built by combining fields â€” not mastering only one.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Understanding the map makes you powerful.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-quiz&#34;&gt;â“ Final Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Can one model belong to multiple fields?&lt;/summary&gt;
  &lt;p&gt;Yes â€” modern AI systems are deeply interdisciplinary.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Lecture 08 â€” RAG, AI Agents &amp; Agentic Multimodal Systems</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-08-rag-agents/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-08-rag-agents/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~4â€“6 hours (modern AI systems design)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-lecture-changes-everything&#34;&gt;ğŸ§  Why This Lecture Changes Everything&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Models are not intelligent alone.&lt;br&gt;
Systems are.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;RAG and AI Agents represent a shift:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;âŒ From static models&lt;/li&gt;
&lt;li&gt;âœ… To &lt;em&gt;interactive, grounded, tool-using intelligence&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This lecture connects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLMs&lt;/li&gt;
&lt;li&gt;Multimodality&lt;/li&gt;
&lt;li&gt;Memory&lt;/li&gt;
&lt;li&gt;Tools&lt;/li&gt;
&lt;li&gt;Reasoning&lt;/li&gt;
&lt;li&gt;Real-world deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-retrieval-augmented-generation-rag&#34;&gt;ğŸ§© What Is Retrieval-Augmented Generation (RAG)?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;RAG = Knowledge + Reasoning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Instead of forcing the model to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;memorize everything&lt;/li&gt;
&lt;li&gt;hallucinate confidently&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We let it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Retrieve relevant information&lt;/li&gt;
&lt;li&gt;Reason over it&lt;/li&gt;
&lt;li&gt;Generate grounded answers&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-classical-llm-vs-rag&#34;&gt;ğŸ” Classical LLM vs RAG&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Aspect&lt;/th&gt;
&lt;th&gt;Classical LLM&lt;/th&gt;
&lt;th&gt;RAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Knowledge&lt;/td&gt;
&lt;td&gt;Frozen&lt;/td&gt;
&lt;td&gt;Dynamic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hallucination&lt;/td&gt;
&lt;td&gt;High&lt;/td&gt;
&lt;td&gt;Lower&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Updates&lt;/td&gt;
&lt;td&gt;Retrain&lt;/td&gt;
&lt;td&gt;Re-index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Traceability&lt;/td&gt;
&lt;td&gt;Poor&lt;/td&gt;
&lt;td&gt;Strong&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Enterprise-ready&lt;/td&gt;
&lt;td&gt;âŒ&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;RAG turns LLMs into â€œopen-book thinkers.â€&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-rag-pipeline&#34;&gt;ğŸ§  Core RAG Pipeline&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;
User Query
â†“
Embedding
â†“
Retriever (Vector DB)
â†“
Relevant Context
â†“
LLM Reasoning
â†“
Answer + Citations

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-can-be-retrieved&#34;&gt;ğŸ“¦ What Can Be Retrieved?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ“„ Documents&lt;/li&gt;
&lt;li&gt;ğŸ–¼ Images&lt;/li&gt;
&lt;li&gt;ğŸ¥ Videos&lt;/li&gt;
&lt;li&gt;ğŸ§¾ Tables&lt;/li&gt;
&lt;li&gt;ğŸ“Š Logs&lt;/li&gt;
&lt;li&gt;ğŸ§  Memories (Agent state)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Multimodal RAG = &lt;strong&gt;cross-modal retrieval + reasoning&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-embeddings-the-heart-of-rag&#34;&gt;ğŸ§  Embeddings: The Heart of RAG&lt;/h2&gt;
&lt;p&gt;Embedding models map meaning â†’ vectors.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Text: sentence-transformers&lt;/li&gt;
&lt;li&gt;Image: CLIP&lt;/li&gt;
&lt;li&gt;Video: InternVideo&lt;/li&gt;
&lt;li&gt;Document: Layout-aware embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Good retrieval beats bigger models.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-minimal-rag-example&#34;&gt;ğŸ Python: Minimal RAG Example&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;query = &amp;quot;What is transformer attention?&amp;quot;

q_emb = embedder.encode(query)
docs = vector_db.search(q_emb, top_k=5)

context = &amp;quot;\n&amp;quot;.join(docs)

answer = llm.generate(
    prompt=f&amp;quot;Answer using the context below:\n{context}\n\nQuestion:{query}&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-common-rag-failure-modes&#34;&gt;âš ï¸ Common RAG Failure Modes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Retrieving irrelevant chunks&lt;/li&gt;
&lt;li&gt;Context too long&lt;/li&gt;
&lt;li&gt;Context ignored&lt;/li&gt;
&lt;li&gt;Conflicting documents&lt;/li&gt;
&lt;li&gt;Over-trusting retrieved text&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mitigation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chunking strategy&lt;/li&gt;
&lt;li&gt;Reranking&lt;/li&gt;
&lt;li&gt;Instruction tuning&lt;/li&gt;
&lt;li&gt;Answer verification&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-an-ai-agent&#34;&gt;ğŸ¤– What Is an AI Agent?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;An agent is an LLM that can act.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Agent abilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decide next action&lt;/li&gt;
&lt;li&gt;Use tools&lt;/li&gt;
&lt;li&gt;Store memory&lt;/li&gt;
&lt;li&gt;Observe outcomes&lt;/li&gt;
&lt;li&gt;Iterate&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-agent-loop-canonical&#34;&gt;ğŸ§  Agent Loop (Canonical)&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Observe â†’ Think â†’ Act â†’ Reflect â†’ Repeat
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is &lt;strong&gt;not prompting&lt;/strong&gt; â€” it is &lt;strong&gt;control flow&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-agent-components&#34;&gt;ğŸ§© Agent Components&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Role&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LLM&lt;/td&gt;
&lt;td&gt;Reasoning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Memory&lt;/td&gt;
&lt;td&gt;State&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tools&lt;/td&gt;
&lt;td&gt;Actions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Planner&lt;/td&gt;
&lt;td&gt;Decomposition&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Executor&lt;/td&gt;
&lt;td&gt;Tool calling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Critic&lt;/td&gt;
&lt;td&gt;Self-evaluation&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tools-an-agent-can-use&#34;&gt;ğŸ›  Tools an Agent Can Use&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Search engines&lt;/li&gt;
&lt;li&gt;Databases&lt;/li&gt;
&lt;li&gt;Code execution&lt;/li&gt;
&lt;li&gt;APIs&lt;/li&gt;
&lt;li&gt;OCR&lt;/li&gt;
&lt;li&gt;Vision models&lt;/li&gt;
&lt;li&gt;File systems&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tools extend intelligence beyond tokens.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-simple-agent-skeleton&#34;&gt;ğŸ Python: Simple Agent Skeleton&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;while not task_done:
    thought = llm.think(state)
    action = planner.select(thought)
    result = tools.run(action)
    state.update(result)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-agentic-ai&#34;&gt;ğŸ§  What Is Agentic AI?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Agentic AI&lt;/strong&gt; means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Long-horizon goals&lt;/li&gt;
&lt;li&gt;Autonomous planning&lt;/li&gt;
&lt;li&gt;Self-correction&lt;/li&gt;
&lt;li&gt;Tool orchestration&lt;/li&gt;
&lt;li&gt;Memory persistence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Research agents&lt;/li&gt;
&lt;li&gt;Coding agents&lt;/li&gt;
&lt;li&gt;Multimodal assistants&lt;/li&gt;
&lt;li&gt;Auto-analysts&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-rag--agents--power&#34;&gt;ğŸ”— RAG + Agents = Power&lt;/h2&gt;
&lt;p&gt;RAG answers questions.
Agents &lt;strong&gt;decide what to retrieve and why&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Agent
  â”œâ”€â”€ Query RAG
  â”œâ”€â”€ Verify answer
  â”œâ”€â”€ Ask follow-up
  â”œâ”€â”€ Use tools
  â””â”€â”€ Deliver result
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;This is how &lt;strong&gt;real AI systems are built today&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-multimodal-agent-example&#34;&gt;ğŸ§  Multimodal Agent Example&lt;/h2&gt;
&lt;p&gt;Task:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œAnalyze this traffic video and explain why the accident occurred.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Agent flow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Extract video frames&lt;/li&gt;
&lt;li&gt;Retrieve traffic rules (RAG)&lt;/li&gt;
&lt;li&gt;Detect events&lt;/li&gt;
&lt;li&gt;Reason causality&lt;/li&gt;
&lt;li&gt;Generate explanation&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-risks-of-agentic-systems&#34;&gt;âš ï¸ Risks of Agentic Systems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tool misuse&lt;/li&gt;
&lt;li&gt;Infinite loops&lt;/li&gt;
&lt;li&gt;Overconfidence&lt;/li&gt;
&lt;li&gt;Hidden failures&lt;/li&gt;
&lt;li&gt;Alignment drift&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mitigation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guardrails&lt;/li&gt;
&lt;li&gt;Cost limits&lt;/li&gt;
&lt;li&gt;Human approval&lt;/li&gt;
&lt;li&gt;Logging&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-evaluating-rag--agents&#34;&gt;ğŸ“ Evaluating RAG &amp;amp; Agents&lt;/h2&gt;
&lt;h3 id=&#34;rag-evaluation&#34;&gt;RAG Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Retrieval recall&lt;/li&gt;
&lt;li&gt;Faithfulness&lt;/li&gt;
&lt;li&gt;Answer correctness&lt;/li&gt;
&lt;li&gt;Citation accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agent-evaluation&#34;&gt;Agent Evaluation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Task success rate&lt;/li&gt;
&lt;li&gt;Steps efficiency&lt;/li&gt;
&lt;li&gt;Error recovery&lt;/li&gt;
&lt;li&gt;Human satisfaction&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-research-insight&#34;&gt;ğŸ§  Research Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Intelligence is no longer &lt;strong&gt;inside the model&lt;/strong&gt;
It is &lt;strong&gt;distributed across systems&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The future:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Smaller models&lt;/li&gt;
&lt;li&gt;Better retrieval&lt;/li&gt;
&lt;li&gt;Smarter agents&lt;/li&gt;
&lt;li&gt;Human oversight&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-student-knowledge-check-hidden&#34;&gt;ğŸ§ª Student Knowledge Check (Hidden)&lt;/h2&gt;
&lt;h3 id=&#34;q1--objective&#34;&gt;Q1 â€” Objective&lt;/h3&gt;
&lt;p&gt;What problem does RAG primarily solve?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Hallucination and static knowledge.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2--mcq&#34;&gt;Q2 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which is NOT a core agent component?&lt;/p&gt;
&lt;p&gt;A. Memory
B. Planner
C. Tool interface
D. Dataset labeler&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;D. Dataset labeler&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3--mcq&#34;&gt;Q3 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Why combine RAG with agents?&lt;/p&gt;
&lt;p&gt;A. Reduce cost
B. Improve UI
C. Enable decision-driven retrieval
D. Increase model size&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Enable decision-driven retrieval&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4--objective&#34;&gt;Q4 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is agentic AI?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;AI systems that plan, act, use tools, and self-correct toward goals.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5--objective&#34;&gt;Q5 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is human oversight important for agents?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;To prevent unsafe, incorrect, or misaligned actions.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If AI agents can act autonomously, what must humans always control?&lt;/summary&gt;
  &lt;p&gt;Goals, values, boundaries, and accountability.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-key-takeaways&#34;&gt;âœ… Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RAG grounds intelligence&lt;/li&gt;
&lt;li&gt;Agents enable action&lt;/li&gt;
&lt;li&gt;Agentic AI is system-level intelligence&lt;/li&gt;
&lt;li&gt;Multimodal agents are the future&lt;/li&gt;
&lt;li&gt;Humans must remain in the loop&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-05 â€” Two Sum (Brute Force â†’ Hash Map)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-05-two-sum/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-05-two-sum/</guid>
      <description>&lt;h2 id=&#34;-problem-statement&#34;&gt;ğŸ§© Problem Statement&lt;/h2&gt;
&lt;p&gt;Given an integer array &lt;code&gt;nums&lt;/code&gt; and an integer &lt;code&gt;target&lt;/code&gt;, return the &lt;strong&gt;indices of two numbers&lt;/strong&gt; such that they add up to &lt;code&gt;target&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You may assume that &lt;strong&gt;exactly one solution exists&lt;/strong&gt;, and you may not use the same element twice.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-input---output&#34;&gt;ğŸ“¥ Sample Input / ğŸ“¤ Output&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;nums&lt;/th&gt;
&lt;th&gt;target&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[2,7,11,15]&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;[0,1]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[3,2,4]&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;[1,2]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[3,3]&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;[0,1]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[1,5,1,5]&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;[1,3]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;[-1,-2,-3,-4]&lt;/td&gt;
&lt;td&gt;-6&lt;/td&gt;
&lt;td&gt;[1,3]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-expectation&#34;&gt;ğŸ§  Interviewer Expectation&lt;/h2&gt;
&lt;p&gt;This problem tests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nested loop thinking&lt;/li&gt;
&lt;li&gt;Optimization awareness&lt;/li&gt;
&lt;li&gt;Hash map mastery&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-1--brute-force-nested-loop&#34;&gt;ğŸ§© Approach 1 â€” Brute Force (Nested Loop)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def two_sum_bruteforce(nums, target):
    n = len(nums)

    for i in range(n):
        for j in range(i + 1, n):
            if nums[i] + nums[j] == target:
                return [i, j]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time: &lt;strong&gt;O(nÂ²)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Space: &lt;strong&gt;O(1)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¡ Correct but inefficient.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-approach-2--hash-map-optimal&#34;&gt;ğŸ§© Approach 2 â€” Hash Map (Optimal)&lt;/h2&gt;
&lt;h3 id=&#34;-idea&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Store numbers we have seen and check if &lt;code&gt;target - current&lt;/code&gt; exists.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def two_sum(nums, target):
    seen = {}

    for i, v in enumerate(nums):
        complement = target - v

        if complement in seen:
            return [seen[complement], i]

        seen[v] = i
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;-complexity-1&#34;&gt;â±ï¸ Complexity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time: &lt;strong&gt;O(n)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Space: &lt;strong&gt;O(n)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Interview-ready solution.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œUse brute force to understand the problem.
Use hash maps to solve it like a professional.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-matrix-traversal-patterns&#34;&gt;ğŸ“˜ Matrix Traversal Patterns&lt;/h1&gt;
&lt;h2 id=&#34;-problem-statement-1&#34;&gt;ğŸ§© Problem Statement&lt;/h2&gt;
&lt;p&gt;Given a 2D matrix, traverse it in different patterns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Row-wise&lt;/li&gt;
&lt;li&gt;Column-wise&lt;/li&gt;
&lt;li&gt;Diagonal&lt;/li&gt;
&lt;li&gt;Spiral&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-matrix&#34;&gt;ğŸ“¥ Sample Matrix&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;1  2  3
4  5  6
7  8  9
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-row-wise-traversal&#34;&gt;ğŸ§© Row-wise Traversal&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for row in matrix:
    for value in row:
        print(value)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-column-wise-traversal&#34;&gt;ğŸ§© Column-wise Traversal&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rows, cols = len(matrix), len(matrix[0])

for c in range(cols):
    for r in range(rows):
        print(matrix[r][c])
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-diagonal-traversal&#34;&gt;ğŸ§© Diagonal Traversal&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;n = len(matrix)

for i in range(n):
    print(matrix[i][i])
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-spiral-traversal-interview-favorite&#34;&gt;ğŸ§© Spiral Traversal (Interview Favorite)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def spiral(matrix):
    res = []
    top, bottom = 0, len(matrix)-1
    left, right = 0, len(matrix[0])-1

    while top &amp;lt;= bottom and left &amp;lt;= right:
        for c in range(left, right+1):
            res.append(matrix[top][c])
        top += 1

        for r in range(top, bottom+1):
            res.append(matrix[r][right])
        right -= 1

        if top &amp;lt;= bottom:
            for c in range(right, left-1, -1):
                res.append(matrix[bottom][c])
            bottom -= 1

        if left &amp;lt;= right:
            for r in range(bottom, top-1, -1):
                res.append(matrix[r][left])
            left += 1

    return res
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity-2&#34;&gt;â±ï¸ Complexity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Time: &lt;strong&gt;O(m Ã— n)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Space: &lt;strong&gt;O(1)&lt;/strong&gt; (excluding output)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway-1&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;p&gt;Matrix traversal shows &lt;strong&gt;index control mastery&lt;/strong&gt;, critical for vision and AI tasks.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-prefix-sum-killing-nested-loops&#34;&gt;ğŸ“˜ Prefix Sum (Killing Nested Loops)&lt;/h1&gt;
&lt;h2 id=&#34;-problem-statement-2&#34;&gt;ğŸ§© Problem Statement&lt;/h2&gt;
&lt;p&gt;Given an array &lt;code&gt;nums&lt;/code&gt;, answer multiple range sum queries efficiently.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example&#34;&gt;ğŸ“¥ Example&lt;/h2&gt;
&lt;p&gt;nums = &lt;code&gt;[1, 2, 3, 4, 5]&lt;/code&gt;&lt;br&gt;
Query sum of range &lt;code&gt;[1,3]&lt;/code&gt; â†’ &lt;code&gt;2 + 3 + 4 = 9&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-brute-force&#34;&gt;ğŸ§© Brute Force&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def range_sum(nums, l, r):
    total = 0
    for i in range(l, r+1):
        total += nums[i]
    return total
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ &lt;strong&gt;O(n)&lt;/strong&gt; per query âŒ&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-prefix-sum-optimization&#34;&gt;ğŸ§© Prefix Sum Optimization&lt;/h2&gt;
&lt;h3 id=&#34;-idea-1&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Precompute cumulative sums once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_prefix(nums):
    prefix = [0]
    for v in nums:
        prefix.append(prefix[-1] + v)
    return prefix

def range_sum(prefix, l, r):
    return prefix[r+1] - prefix[l]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity-3&#34;&gt;â±ï¸ Complexity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Preprocessing: &lt;strong&gt;O(n)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Query: &lt;strong&gt;O(1)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸŸ¢ Essential for AI data pipelines.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway-2&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;p&gt;Prefix sums turn &lt;strong&gt;nested loops into math&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-sliding-window&#34;&gt;ğŸ“˜ Sliding Window&lt;/h1&gt;
&lt;h2 id=&#34;-problem-statement-3&#34;&gt;ğŸ§© Problem Statement&lt;/h2&gt;
&lt;p&gt;Given an array &lt;code&gt;nums&lt;/code&gt; and integer &lt;code&gt;k&lt;/code&gt;, find the &lt;strong&gt;maximum sum of any subarray of size k&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-1&#34;&gt;ğŸ“¥ Example&lt;/h2&gt;
&lt;p&gt;nums = &lt;code&gt;[2,1,5,1,3,2]&lt;/code&gt;, k = 3&lt;br&gt;
Output â†’ &lt;code&gt;9&lt;/code&gt; (&lt;code&gt;[5,1,3]&lt;/code&gt;)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-brute-force-1&#34;&gt;ğŸ§© Brute Force&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def max_sum(nums, k):
    max_sum = 0
    for i in range(len(nums) - k + 1):
        max_sum = max(max_sum, sum(nums[i:i+k]))
    return max_sum
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ &lt;strong&gt;O(nk)&lt;/strong&gt; âŒ&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sliding-window-optimization&#34;&gt;ğŸ§© Sliding Window Optimization&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def max_sum(nums, k):
    window_sum = sum(nums[:k])
    max_sum = window_sum

    for i in range(k, len(nums)):
        window_sum += nums[i]
        window_sum -= nums[i-k]
        max_sum = max(max_sum, window_sum)

    return max_sum
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity-4&#34;&gt;â±ï¸ Complexity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Time: &lt;strong&gt;O(n)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Space: &lt;strong&gt;O(1)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway-3&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;p&gt;Sliding window is &lt;strong&gt;mandatory knowledge&lt;/strong&gt; for interviews.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-linear-algebra-for-ai-vectors--matrices&#34;&gt;ğŸ“˜ Linear Algebra for AI (Vectors &amp;amp; Matrices)&lt;/h1&gt;
&lt;h2 id=&#34;-why-linear-algebra-matters&#34;&gt;ğŸ¯ Why Linear Algebra Matters&lt;/h2&gt;
&lt;p&gt;Linear algebra is the &lt;strong&gt;language of AI&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neural networks&lt;/li&gt;
&lt;li&gt;Embeddings&lt;/li&gt;
&lt;li&gt;Transformers&lt;/li&gt;
&lt;li&gt;Computer vision&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vectors&#34;&gt;ğŸ§® Vectors&lt;/h2&gt;
&lt;p&gt;A vector represents &lt;strong&gt;features&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = [1, 2, 3]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vector-addition&#34;&gt;â• Vector Addition&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def add(a, b):
    return [a[i] + b[i] for i in range(len(a))]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ O(n)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-dot-product-core-of-neural-networks&#34;&gt;âœ–ï¸ Dot Product (Core of Neural Networks)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def dot(a, b):
    return sum(a[i] * b[i] for i in range(len(a)))
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-matrices&#34;&gt;ğŸ§± Matrices&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A = [
    [1, 2],
    [3, 4]
]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-matrix-multiplication&#34;&gt;âœ–ï¸ Matrix Multiplication&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def matmul(A, B):
    res = [[0]*len(B[0]) for _ in range(len(A))]

    for i in range(len(A)):
        for j in range(len(B[0])):
            for k in range(len(B)):
                res[i][j] += A[i][k] * B[k][j]

    return res
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ &lt;strong&gt;O(nÂ³)&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ai-interpretation&#34;&gt;ğŸ§  AI Interpretation&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Math&lt;/th&gt;
&lt;th&gt;AI Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Vector&lt;/td&gt;
&lt;td&gt;Feature embedding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dot product&lt;/td&gt;
&lt;td&gt;Similarity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Matrix&lt;/td&gt;
&lt;td&gt;Layer weights&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MatMul&lt;/td&gt;
&lt;td&gt;Forward pass&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-thought&#34;&gt;ğŸ Final Thought&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;If algorithms are &lt;strong&gt;logic&lt;/strong&gt;,
linear algebra is &lt;strong&gt;meaning&lt;/strong&gt; in AI.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-06 â€” Probability for Machine Learning</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-06-probability-ai/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-06-probability-ai/</guid>
      <description>&lt;h2 id=&#34;-why-probability-matters&#34;&gt;ğŸ¯ Why Probability Matters&lt;/h2&gt;
&lt;p&gt;Probability is everywhere in AI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model uncertainty&lt;/li&gt;
&lt;li&gt;Classification confidence&lt;/li&gt;
&lt;li&gt;Loss functions&lt;/li&gt;
&lt;li&gt;Bayesian thinking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interviewers want &lt;strong&gt;intuition&lt;/strong&gt;, not formulas only.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-basic-definitions&#34;&gt;ğŸ§® Basic Definitions&lt;/h2&gt;
&lt;h3 id=&#34;probability&#34;&gt;Probability&lt;/h3&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;P(head) = 1 / 2
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-conditional-probability&#34;&gt;ğŸ² Conditional Probability&lt;/h2&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given it rains, whatâ€™s the chance traffic is heavy?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-independence&#34;&gt;ğŸ” Independence&lt;/h2&gt;
&lt;p&gt;Two events are independent if:
[
P(A \cap B) = P(A)P(B)
]&lt;/p&gt;
&lt;p&gt;Interview trick question âš ï¸
ğŸ‘‰ Independence â‰  no correlation always&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-bayes-theorem-very-important&#34;&gt;ğŸ§  Bayesâ€™ Theorem (VERY IMPORTANT)&lt;/h2&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spam detection&lt;/li&gt;
&lt;li&gt;Medical diagnosis&lt;/li&gt;
&lt;li&gt;Bayesian ML&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interview-question&#34;&gt;ğŸ§ª Interview Question&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Why Naive Bayes works despite false independence assumptions?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;âœ… Because it reduces variance and scales well.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Probability gives AI &lt;strong&gt;confidence&lt;/strong&gt;, not just predictions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-gradient-descent-interview-intuition&#34;&gt;ğŸ“˜ Gradient Descent (Interview Intuition)&lt;/h1&gt;
&lt;h2 id=&#34;-problem&#34;&gt;ğŸ¯ Problem&lt;/h2&gt;
&lt;p&gt;How do machines &lt;strong&gt;learn&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;By minimizing a loss function.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-loss-function&#34;&gt;ğŸ“‰ Loss Function&lt;/h2&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-gradient-descent-idea&#34;&gt;ğŸ§  Gradient Descent Idea&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Walk downhill in the direction of &lt;strong&gt;steepest descent&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-example&#34;&gt;ğŸ§‘â€ğŸ’» Python Example&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def gradient_descent(w, x, y, lr=0.01):
    grad = -2 * x * (y - w * x)
    return w - lr * grad
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-variants-interviewers-love&#34;&gt;â±ï¸ Variants Interviewers Love&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Idea&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Batch GD&lt;/td&gt;
&lt;td&gt;Use all data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SGD&lt;/td&gt;
&lt;td&gt;One sample&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mini-batch&lt;/td&gt;
&lt;td&gt;Best of both&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interview-trap&#34;&gt;ğŸš¨ Interview Trap&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Why not use a large learning rate?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;âŒ Overshooting
âŒ Divergence&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway-1&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Optimization is the &lt;strong&gt;engine&lt;/strong&gt; of AI.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-numpy-for-coding-interviews&#34;&gt;ğŸ“˜ NumPy for Coding Interviews&lt;/h1&gt;
&lt;h2 id=&#34;-why-numpy&#34;&gt;ğŸ¯ Why NumPy?&lt;/h2&gt;
&lt;p&gt;Interviewers expect:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vectorized thinking&lt;/li&gt;
&lt;li&gt;Clean math code&lt;/li&gt;
&lt;li&gt;Speed awareness&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-arrays-vs-lists&#34;&gt;ğŸ§® Arrays vs Lists&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

a = np.array([1, 2, 3])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why better?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Faster&lt;/li&gt;
&lt;li&gt;Cleaner&lt;/li&gt;
&lt;li&gt;Mathematical&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vector-operations&#34;&gt;â• Vector Operations&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a + b        # element-wise
a * b
np.dot(a,b) # dot product
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-matrix-multiplication&#34;&gt;ğŸ§± Matrix Multiplication&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A @ B
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Equivalent to:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.matmul(A, B)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-broadcasting-interview-favorite&#34;&gt;ğŸ“ Broadcasting (Interview Favorite)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;A + 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adds 1 to every element.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway-2&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;NumPy replaces &lt;strong&gt;loops with math&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;
---

# Time vs Space Trade-offs

## ğŸ¯ Interview Core Concept

You can optimize:
- Time
- Space

Rarely both.

---

## ğŸ§© Example: Duplicate Detection

### Less Space, More Time

```python
for i in range(n):
    for j in range(i+1, n):
        if nums[i] == nums[j]:
            return True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ O(nÂ²), ğŸ’¾ O(1)&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;more-space-less-time&#34;&gt;More Space, Less Time&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seen = set()
for v in nums:
    if v in seen:
        return True
    seen.add(v)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â±ï¸ O(n), ğŸ’¾ O(n)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interview-question-1&#34;&gt;ğŸ§  Interview Question&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Which would you choose?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Correct answer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Depends on constraints.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-takeaway-3&#34;&gt;ğŸ Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Strong engineers trade &lt;strong&gt;resources intentionally&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-07 â€” Indexing &amp; Containers Mastery</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-07-indexing-and-containers/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-07-indexing-and-containers/</guid>
      <description>&lt;h2 id=&#34;-why-this-lesson-exists&#34;&gt;ğŸ¯ Why This Lesson Exists&lt;/h2&gt;
&lt;p&gt;When you watch strong developers solve LeetCode or interview problems, youâ€™ll notice something:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;They donâ€™t write long code.&lt;br&gt;
They donâ€™t explain much.&lt;br&gt;
But their solution is &lt;strong&gt;short, fast, and confident&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The secret is &lt;strong&gt;indexing&lt;/strong&gt; and &lt;strong&gt;containers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;They know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When to use &lt;strong&gt;arrays vs lists&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;How to move pointers instead of copying data&lt;/li&gt;
&lt;li&gt;How to let &lt;strong&gt;data structures do the hard work&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This lesson is about &lt;strong&gt;thinking like them&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-you-will-train-here&#34;&gt;ğŸ§  What You Will Train Here&lt;/h2&gt;
&lt;p&gt;In this session, you will repeatedly practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Index-based traversal (two pointers, ranges, windows)&lt;/li&gt;
&lt;li&gt;Containers: &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;dict&lt;/code&gt;, &lt;code&gt;set&lt;/code&gt;, &lt;code&gt;deque&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Avoiding unnecessary loops&lt;/li&gt;
&lt;li&gt;Writing &lt;strong&gt;contest-level, interview-ready code&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All problems here are &lt;strong&gt;real LeetCode-style battlefield problems&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-example--text-justification&#34;&gt;ğŸ§© Core Example â€” Text Justification&lt;/h2&gt;
&lt;p&gt;This problem looks long.
Many beginners panic.&lt;/p&gt;
&lt;p&gt;Experienced devs see:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThis is just indexing + container control.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-statement--text-justification&#34;&gt;ğŸ“˜ Problem Statement â€” Text Justification&lt;/h2&gt;
&lt;p&gt;Given an array of strings &lt;code&gt;words&lt;/code&gt; and an integer &lt;code&gt;maxWidth&lt;/code&gt;, format the text so that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each line has &lt;strong&gt;exactly &lt;code&gt;maxWidth&lt;/code&gt; characters&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Text is &lt;strong&gt;fully justified&lt;/strong&gt; (left and right)&lt;/li&gt;
&lt;li&gt;Extra spaces are distributed &lt;strong&gt;as evenly as possible&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;If spaces do not divide evenly, &lt;strong&gt;left gaps get more&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;last line&lt;/strong&gt; is left-justified&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-input---output&#34;&gt;ğŸ“¥ Sample Input / ğŸ“¤ Output&lt;/h2&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;words = [&amp;quot;This&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;an&amp;quot;, &amp;quot;example&amp;quot;, &amp;quot;of&amp;quot;, &amp;quot;text&amp;quot;, &amp;quot;justification.&amp;quot;]
maxWidth = 16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[
  &amp;quot;This    is    an&amp;quot;,
  &amp;quot;example  of text&amp;quot;,
  &amp;quot;justification.  &amp;quot;
]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight&#34;&gt;ğŸ§  Interviewer Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â— Interviewers are NOT testing formatting.&lt;/p&gt;
&lt;p&gt;They are testing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can you &lt;strong&gt;group data using indices&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Can you manage &lt;strong&gt;ranges without slicing copies&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Do you understand &lt;strong&gt;greedy packing&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Can you control spaces mathematically?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you pass this problem cleanly,
they already trust your &lt;strong&gt;data-structure instincts&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-1--greedy-line-packing-indexing&#34;&gt;ğŸ§© Step 1 â€” Greedy Line Packing (Indexing)&lt;/h2&gt;
&lt;h3 id=&#34;-idea&#34;&gt;ğŸ’¡ Idea&lt;/h3&gt;
&lt;p&gt;Use indexing to pack as many words as possible into one line.&lt;/p&gt;
&lt;p&gt;We move a pointer &lt;code&gt;i&lt;/code&gt; forward.
No recursion.
No backtracking.
No copying.&lt;/p&gt;
&lt;p&gt;This is &lt;strong&gt;pure index control&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-packing-logic&#34;&gt;ğŸ§‘â€ğŸ’» Core Packing Logic&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = 0
n = len(words)

while i &amp;lt; n:
    line_len = len(words[i])
    j = i + 1

    while j &amp;lt; n and line_len + 1 + len(words[j]) &amp;lt;= maxWidth:
        line_len += 1 + len(words[j])
        j += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸŸ¢ Notice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;i&lt;/code&gt; and &lt;code&gt;j&lt;/code&gt; define a &lt;strong&gt;range&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;No new arrays are created&lt;/li&gt;
&lt;li&gt;This is how contest code stays fast&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-2--space-distribution-using-containers&#34;&gt;ğŸ§© Step 2 â€” Space Distribution Using Containers&lt;/h2&gt;
&lt;p&gt;Once words &lt;code&gt;[i : j]&lt;/code&gt; are fixed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Count total spaces needed&lt;/li&gt;
&lt;li&gt;Count gaps between words&lt;/li&gt;
&lt;li&gt;Distribute spaces using arithmetic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This avoids messy string operations.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-space-math-the-key-insight&#34;&gt;ğŸ§‘â€ğŸ’» Space Math (The Key Insight)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_words = j - i
total_spaces = maxWidth - sum(len(w) for w in words[i:j])
gaps = num_words - 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we branch:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Last line OR single word&lt;/strong&gt; â†’ left-justify&lt;/li&gt;
&lt;li&gt;Otherwise â†’ fully justify&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-full-justification-logic&#34;&gt;ğŸ§‘â€ğŸ’» Full Justification Logic&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if j == n or gaps == 0:
    line = &amp;quot; &amp;quot;.join(words[i:j])
    line += &amp;quot; &amp;quot; * (maxWidth - len(line))
else:
    space, extra = divmod(total_spaces, gaps)
    line = &amp;quot;&amp;quot;

    for k in range(i, j - 1):
        line += words[k]
        line += &amp;quot; &amp;quot; * (space + (1 if extra &amp;gt; 0 else 0))
        extra -= 1

    line += words[j - 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ğŸŸ¢ This is where &lt;strong&gt;containers + indexing shine&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity-analysis&#34;&gt;â±ï¸ Complexity Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1) extra (output excluded)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is &lt;strong&gt;optimal&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-problem-is-gold-for-interviews&#34;&gt;ğŸ§  Why This Problem Is Gold for Interviews&lt;/h2&gt;
&lt;p&gt;This single problem tests:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Skill&lt;/th&gt;
&lt;th&gt;Tested&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Index traversal&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Greedy strategy&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Container usage&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Edge cases&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Clean code&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If you solve this smoothly,
interviewers assume you can handle:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Two pointers&lt;/li&gt;
&lt;li&gt;Sliding windows&lt;/li&gt;
&lt;li&gt;String manipulation&lt;/li&gt;
&lt;li&gt;Array partitioning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-more-problems-you-should-practice-in-this-style&#34;&gt;ğŸ” More Problems You Should Practice in This Style&lt;/h2&gt;
&lt;p&gt;To fully absorb indexing + containers, repeat this mindset on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trapping Rain Water&lt;/li&gt;
&lt;li&gt;Container With Most Water&lt;/li&gt;
&lt;li&gt;Group Anagrams&lt;/li&gt;
&lt;li&gt;Minimum Window Substring&lt;/li&gt;
&lt;li&gt;Merge Intervals&lt;/li&gt;
&lt;li&gt;Rotate Image&lt;/li&gt;
&lt;li&gt;Spiral Matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Same idea.
Different skin.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ¯ Final Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Strong developers donâ€™t write clever code.&lt;/p&gt;
&lt;p&gt;They &lt;strong&gt;let indices and containers think for them&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you master this lesson,
your LeetCode solutions will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shrink in size&lt;/li&gt;
&lt;li&gt;Increase in clarity&lt;/li&gt;
&lt;li&gt;Feel &lt;em&gt;calm&lt;/em&gt; under interview pressure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the level interviewers quietly hope for ğŸš€&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-08 â€” Two Pointers in the Wild</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-08-two-pointers/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-08-two-pointers/</guid>
      <description>&lt;h2 id=&#34;-why-two-pointers-matter&#34;&gt;ğŸ¯ Why Two Pointers Matter&lt;/h2&gt;
&lt;p&gt;Two pointers is not a trick.&lt;br&gt;
It is a &lt;strong&gt;way of thinking&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Strong developers see:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œI donâ€™t need extra memory.&lt;br&gt;
I can let indices move and converge.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This technique appears everywhere:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Arrays&lt;/li&gt;
&lt;li&gt;Strings&lt;/li&gt;
&lt;li&gt;Sorted data&lt;/li&gt;
&lt;li&gt;Geometry problems&lt;/li&gt;
&lt;li&gt;Optimization tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you master this, &lt;strong&gt;your code gets shorter immediately&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-you-will-learn&#34;&gt;ğŸ§  What You Will Learn&lt;/h2&gt;
&lt;p&gt;In this lesson, you will train to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;strong&gt;left / right pointers confidently&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Decide &lt;strong&gt;who moves and why&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Avoid nested loops&lt;/li&gt;
&lt;li&gt;Reduce O(nÂ²) â†’ O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is &lt;strong&gt;interview-critical muscle memory&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-example--container-with-most-water&#34;&gt;ğŸ§© Core Example â€” Container With Most Water&lt;/h2&gt;
&lt;h3 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h3&gt;
&lt;p&gt;Given &lt;code&gt;n&lt;/code&gt; non-negative integers representing vertical lines,
find two lines that together with the x-axis form a container
that holds the &lt;strong&gt;most water&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-input---output&#34;&gt;ğŸ“¥ Sample Input / ğŸ“¤ Output&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;height&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;[1,8,6,2,5,4,8,3,7]&lt;/td&gt;
&lt;td&gt;49&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight&#34;&gt;ğŸ§  Interviewer Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â— Interviewers want to see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do you understand &lt;strong&gt;why one pointer moves&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Can you explain &lt;strong&gt;why brute force is wasteful&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Do you trust a greedy decision?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you randomly move pointers, you fail.
If you &lt;strong&gt;justify movement&lt;/strong&gt;, you pass.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-brute-force-what-not-to-do&#34;&gt;ğŸ§© Brute Force (What NOT To Do)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;max_area = 0
for i in range(len(height)):
    for j in range(i + 1, len(height)):
        max_area = max(
            max_area,
            min(height[i], height[j]) * (j - i)
        )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;â›” O(nÂ²)
â›” Interview red flag&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-two-pointer-strategy-correct&#34;&gt;ğŸ§© Two Pointer Strategy (Correct)&lt;/h2&gt;
&lt;h3 id=&#34;-core-insight&#34;&gt;ğŸ’¡ Core Insight&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;shorter line&lt;/strong&gt; limits the area.&lt;/p&gt;
&lt;p&gt;So:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Move the pointer pointing to the &lt;strong&gt;shorter height&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Because moving the taller one &lt;strong&gt;cannot increase area&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the entire trick.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-two-pointer-solution&#34;&gt;ğŸ§‘â€ğŸ’» Two Pointer Solution&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def maxArea(height):
    left, right = 0, len(height) - 1
    best = 0

    while left &amp;lt; right:
        width = right - left
        area = min(height[left], height[right]) * width
        best = max(best, area)

        if height[left] &amp;lt; height[right]:
            left += 1
        else:
            right -= 1

    return best
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity&#34;&gt;â±ï¸ Complexity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Optimal.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-two-pointer-patterns-you-must-recognize&#34;&gt;ğŸ§  Two Pointer Patterns You Must Recognize&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pattern&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Opposite ends&lt;/td&gt;
&lt;td&gt;Container, Palindrome&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Same direction&lt;/td&gt;
&lt;td&gt;Remove duplicates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Meet in middle&lt;/td&gt;
&lt;td&gt;Pair sum&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Shrinking window&lt;/td&gt;
&lt;td&gt;Valid palindrome&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ¯ Final Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Two pointers is about &lt;strong&gt;controlled movement&lt;/strong&gt;, not guessing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you can explain &lt;strong&gt;why&lt;/strong&gt; a pointer moves,
interviewers assume you understand &lt;strong&gt;algorithmic reasoning&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-practice-targets&#34;&gt;ğŸ Practice Targets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Valid Palindrome&lt;/li&gt;
&lt;li&gt;Two Sum II (sorted)&lt;/li&gt;
&lt;li&gt;Remove Duplicates&lt;/li&gt;
&lt;li&gt;3Sum&lt;/li&gt;
&lt;li&gt;Squares of a Sorted Array&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Master these â†’ you look &lt;em&gt;dangerous&lt;/em&gt; in interviews ğŸ˜ˆ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-09 â€” Sliding Window Like a Pro</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-09-sliding-windows/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-09-sliding-windows/</guid>
      <description>&lt;h2 id=&#34;-why-sliding-window-is-a-game-changer&#34;&gt;ğŸ¯ Why Sliding Window Is a Game Changer&lt;/h2&gt;
&lt;p&gt;Sliding Window is what happens when:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Two pointers learn to &lt;strong&gt;cooperate&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This technique is everywhere:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Subarrays&lt;/li&gt;
&lt;li&gt;Substrings&lt;/li&gt;
&lt;li&gt;Streaming data&lt;/li&gt;
&lt;li&gt;Real-time analytics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interviewers &lt;strong&gt;love&lt;/strong&gt; this topic.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-you-will-learn&#34;&gt;ğŸ§  What You Will Learn&lt;/h2&gt;
&lt;p&gt;This lesson trains you to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expand and shrink windows deliberately&lt;/li&gt;
&lt;li&gt;Track state using containers (&lt;code&gt;dict&lt;/code&gt;, &lt;code&gt;set&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Replace nested loops with &lt;strong&gt;one clean pass&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Write code that looks &lt;strong&gt;effortlessly smart&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-example--minimum-window-substring&#34;&gt;ğŸ§© Core Example â€” Minimum Window Substring&lt;/h2&gt;
&lt;h3 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h3&gt;
&lt;p&gt;Given strings &lt;code&gt;s&lt;/code&gt; and &lt;code&gt;t&lt;/code&gt;, return the &lt;strong&gt;minimum window in &lt;code&gt;s&lt;/code&gt;&lt;/strong&gt;
that contains all characters of &lt;code&gt;t&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If no such window exists, return &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-input---output&#34;&gt;ğŸ“¥ Sample Input / ğŸ“¤ Output&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;s&lt;/th&gt;
&lt;th&gt;t&lt;/th&gt;
&lt;th&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&amp;ldquo;ADOBECODEBANC&amp;rdquo;&lt;/td&gt;
&lt;td&gt;&amp;ldquo;ABC&amp;rdquo;&lt;/td&gt;
&lt;td&gt;&amp;ldquo;BANC&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight&#34;&gt;ğŸ§  Interviewer Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â— This problem tests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can you &lt;strong&gt;track frequencies&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Do you know when a window becomes valid&lt;/li&gt;
&lt;li&gt;Can you shrink without breaking correctness&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you brute force substrings â†’ instant fail.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sliding-window-strategy&#34;&gt;ğŸ§© Sliding Window Strategy&lt;/h2&gt;
&lt;h3 id=&#34;-mental-model&#34;&gt;ğŸ’¡ Mental Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Right pointer &lt;strong&gt;expands&lt;/strong&gt; the window&lt;/li&gt;
&lt;li&gt;Left pointer &lt;strong&gt;shrinks&lt;/strong&gt; it&lt;/li&gt;
&lt;li&gt;A container tracks validity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Never reset.
Never restart loops.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sliding-window-solution&#34;&gt;ğŸ§‘â€ğŸ’» Sliding Window Solution&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter

def minWindow(s, t):
    need = Counter(t)
    missing = len(t)
    left = start = end = 0

    for right, ch in enumerate(s, 1):
        if need[ch] &amp;gt; 0:
            missing -= 1
        need[ch] -= 1

        if missing == 0:
            while left &amp;lt; right and need[s[left]] &amp;lt; 0:
                need[s[left]] += 1
                left += 1

            if end == 0 or right - left &amp;lt; end - start:
                start, end = left, right

            need[s[left]] += 1
            missing += 1
            left += 1

    return s[start:end]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity&#34;&gt;â±ï¸ Complexity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(k) (character set)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is &lt;strong&gt;optimal and interview-approved&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sliding-window-patterns&#34;&gt;ğŸ§  Sliding Window Patterns&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pattern&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Fixed window&lt;/td&gt;
&lt;td&gt;Max sum subarray&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Variable window&lt;/td&gt;
&lt;td&gt;Min window substring&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Frequency window&lt;/td&gt;
&lt;td&gt;Anagrams&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Set-based window&lt;/td&gt;
&lt;td&gt;Longest unique substring&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-beginner-vs-pro-thinking&#34;&gt;ğŸ§© Beginner vs Pro Thinking&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Beginner:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œLet me try all substrings.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Pro:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œI expand until valid, then shrink carefully.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That mindset shift is what interviews test.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ¯ Final Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Sliding Window is not hard.&lt;/p&gt;
&lt;p&gt;Restarting loops is.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If your window &lt;strong&gt;never resets&lt;/strong&gt;,
your solution is probably correct.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-practice-targets&#34;&gt;ğŸ Practice Targets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Longest Substring Without Repeating Characters&lt;/li&gt;
&lt;li&gt;Permutation in String&lt;/li&gt;
&lt;li&gt;Max Consecutive Ones III&lt;/li&gt;
&lt;li&gt;Subarray Sum Equals K&lt;/li&gt;
&lt;li&gt;Find All Anagrams in a String&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Master these â†’ you officially &lt;strong&gt;think like a LeetCode veteran&lt;/strong&gt; ğŸš€&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lecture 08 â€” Evaluation Metrics</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-08-evaluation/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-08-evaluation/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~3 hours (core evaluation &amp;amp; reasoning lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-evaluation-metrics-matter-truth-first&#34;&gt;ğŸ“ Why Evaluation Metrics Matter (Truth First)&lt;/h2&gt;
&lt;p&gt;A dangerous myth:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œIf accuracy is high, the model is good.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reality:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Wrong metric = wrong conclusion = real-world harm&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Evaluation metrics define:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;what â€œsuccessâ€ means&lt;/li&gt;
&lt;li&gt;what the model optimizes for&lt;/li&gt;
&lt;li&gt;how humans trust AI&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-one-sentence-that-explains-metrics&#34;&gt;ğŸ§  One Sentence That Explains Metrics&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Metrics are how humans translate values into numbers.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Different problems â†’ different values â†’ different metrics.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-a-running-example-very-important&#34;&gt;ğŸ§ª A Running Example (Very Important)&lt;/h2&gt;
&lt;p&gt;Imagine a &lt;strong&gt;disease detection system&lt;/strong&gt; ğŸ¥&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Disease rate: 1%&lt;/li&gt;
&lt;li&gt;Healthy people: 99%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This will destroy naive accuracy.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-i--classification-metrics&#34;&gt;ğŸ”¹ PART I â€” Classification Metrics&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-confusion-matrix-the-foundation&#34;&gt;ğŸ§© Confusion Matrix (The Foundation)&lt;/h2&gt;
&lt;p&gt;Everything starts here.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Predicted Positive&lt;/th&gt;
&lt;th&gt;Predicted Negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Actual Positive&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;TP (True Positive)&lt;/td&gt;
&lt;td&gt;FN (False Negative)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Actual Negative&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;FP (False Positive)&lt;/td&gt;
&lt;td&gt;TN (True Negative)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;ğŸ‘‰ Every metric is built from these four numbers.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-accuracy-the-most-misused-metric&#34;&gt;ğŸ¯ Accuracy (The Most Misused Metric)&lt;/h2&gt;
&lt;h3 id=&#34;-formula&#34;&gt;ğŸ“ Formula&lt;/h3&gt;
&lt;p&gt;$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$&lt;/p&gt;
&lt;h3 id=&#34;-example&#34;&gt;ğŸ˜„ Example&lt;/h3&gt;
&lt;p&gt;Out of 1000 people:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;990 healthy&lt;/li&gt;
&lt;li&gt;10 sick&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model predicts &lt;strong&gt;everyone healthy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Accuracy:
$$
= \frac{990}{1000} = 99%
$$&lt;/p&gt;
&lt;p&gt;ğŸ‰ Looks amazing&lt;br&gt;
ğŸ’€ Completely useless&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-accuracy-fails&#34;&gt;âŒ Why Accuracy Fails&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ignores class imbalance&lt;/li&gt;
&lt;li&gt;ignores cost of mistakes&lt;/li&gt;
&lt;li&gt;rewards lazy models&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-precision-how-careful-are-you&#34;&gt;ğŸ¯ Precision (How Careful Are You?)&lt;/h2&gt;
&lt;h3 id=&#34;-formula-1&#34;&gt;ğŸ“ Formula&lt;/h3&gt;
&lt;p&gt;$$
Precision = \frac{TP}{TP + FP}
$$&lt;/p&gt;
&lt;h3 id=&#34;-meaning&#34;&gt;ğŸ§  Meaning&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;When the model says â€œpositiveâ€, how often is it correct?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-example-spam-filter&#34;&gt;ğŸ˜„ Example (Spam Filter)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Marked 10 emails as spam&lt;/li&gt;
&lt;li&gt;8 were actually spam&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
Precision = \frac{8}{10} = 0.8
$$&lt;/p&gt;
&lt;p&gt;High precision = few false alarms ğŸ“©ğŸš«&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-recall-how-much-did-you-catch&#34;&gt;ğŸ¯ Recall (How Much Did You Catch?)&lt;/h2&gt;
&lt;h3 id=&#34;-formula-2&#34;&gt;ğŸ“ Formula&lt;/h3&gt;
&lt;p&gt;$$
Recall = \frac{TP}{TP + FN}
$$&lt;/p&gt;
&lt;h3 id=&#34;-meaning-1&#34;&gt;ğŸ§  Meaning&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Of all real positives, how many did we find?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-example-medical-test&#34;&gt;ğŸ˜„ Example (Medical Test)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;10 sick patients&lt;/li&gt;
&lt;li&gt;Found 7&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
Recall = \frac{7}{10} = 0.7
$$&lt;/p&gt;
&lt;p&gt;Low recall = missed patients ğŸ˜¬&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-precision-vs-recall-classic-tradeoff&#34;&gt;âš–ï¸ Precision vs Recall (Classic Tradeoff)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Focus&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Precision&lt;/td&gt;
&lt;td&gt;Avoid false positives&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recall&lt;/td&gt;
&lt;td&gt;Avoid false negatives&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Medical diagnosis â†’ high recall&lt;br&gt;
Spam filter â†’ high precision&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-f1-score-the-balance&#34;&gt;ğŸ¯ F1 Score (The Balance)&lt;/h2&gt;
&lt;h3 id=&#34;-formula-3&#34;&gt;ğŸ“ Formula&lt;/h3&gt;
&lt;p&gt;$$
F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$&lt;/p&gt;
&lt;h3 id=&#34;-meaning-2&#34;&gt;ğŸ§  Meaning&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;One number that balances both.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-example-1&#34;&gt;ğŸ˜„ Example&lt;/h3&gt;
&lt;p&gt;Precision = 0.8&lt;br&gt;
Recall = 0.6&lt;/p&gt;
&lt;p&gt;$$
F1 = 2 \cdot \frac{0.8 \cdot 0.6}{1.4} \approx 0.69
$$&lt;/p&gt;
&lt;p&gt;Used when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;classes are imbalanced&lt;/li&gt;
&lt;li&gt;both errors matter&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-ii--regression-metrics&#34;&gt;ğŸ”¹ PART II â€” Regression Metrics&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mean-absolute-error-mae&#34;&gt;ğŸ¯ Mean Absolute Error (MAE)&lt;/h2&gt;
&lt;h3 id=&#34;-formula-4&#34;&gt;ğŸ“ Formula&lt;/h3&gt;
&lt;p&gt;$$
MAE = \frac{1}{n} \sum |y - \hat{y}|
$$&lt;/p&gt;
&lt;h3 id=&#34;-meaning-3&#34;&gt;ğŸ§  Meaning&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Average absolute mistake.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-example-2&#34;&gt;ğŸ˜„ Example&lt;/h3&gt;
&lt;p&gt;True prices: &lt;code&gt;[100, 200]&lt;/code&gt;&lt;br&gt;
Predicted: &lt;code&gt;[90, 210]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Errors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;|100âˆ’90| = 10&lt;/li&gt;
&lt;li&gt;|200âˆ’210| = 10&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
MAE = \frac{20}{2} = 10
$$&lt;/p&gt;
&lt;p&gt;Easy to understand ğŸ‘&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mean-squared-error-mse&#34;&gt;ğŸ¯ Mean Squared Error (MSE)&lt;/h2&gt;
&lt;h3 id=&#34;-formula-5&#34;&gt;ğŸ“ Formula&lt;/h3&gt;
&lt;p&gt;$$
MSE = \frac{1}{n} \sum (y - \hat{y})^2
$$&lt;/p&gt;
&lt;h3 id=&#34;-meaning-4&#34;&gt;ğŸ§  Meaning&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Punishes large mistakes more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-example-3&#34;&gt;ğŸ˜„ Example&lt;/h3&gt;
&lt;p&gt;Errors: &lt;code&gt;[10, 10]&lt;/code&gt;&lt;br&gt;
Squares: &lt;code&gt;[100, 100]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;$$
MSE = 100
$$&lt;/p&gt;
&lt;p&gt;Used when big errors are very bad.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-root-mean-squared-error-rmse&#34;&gt;ğŸ¯ Root Mean Squared Error (RMSE)&lt;/h2&gt;
&lt;h3 id=&#34;-formula-6&#34;&gt;ğŸ“ Formula&lt;/h3&gt;
&lt;p&gt;$$
RMSE = \sqrt{MSE}
$$&lt;/p&gt;
&lt;h3 id=&#34;-meaning-5&#34;&gt;ğŸ§  Meaning&lt;/h3&gt;
&lt;p&gt;Same unit as target variable.&lt;/p&gt;
&lt;p&gt;If RMSE = 10 â†’ â€œaverage error â‰ˆ 10 unitsâ€&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-iii--nlp-metrics-language-is-hard&#34;&gt;ğŸ”¹ PART III â€” NLP Metrics (Language Is Hard)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-nlp-metrics-are-tricky&#34;&gt;ğŸ§  Why NLP Metrics Are Tricky&lt;/h2&gt;
&lt;p&gt;Language has:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multiple correct answers&lt;/li&gt;
&lt;li&gt;style differences&lt;/li&gt;
&lt;li&gt;synonyms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Exact matching fails.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-bleu-score-translation&#34;&gt;âœï¸ BLEU Score (Translation)&lt;/h2&gt;
&lt;p&gt;Measures:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Overlap of n-grams between prediction and reference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;-simplified-idea&#34;&gt;ğŸ“ Simplified Idea&lt;/h3&gt;
&lt;p&gt;More shared phrases â†’ higher BLEU.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-example-4&#34;&gt;ğŸ˜„ Example&lt;/h3&gt;
&lt;p&gt;Reference:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œAI changes the worldâ€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Prediction:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œAI transforms the worldâ€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;High BLEU (similar meaning).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-rouge-summarization&#34;&gt;ğŸ“„ ROUGE (Summarization)&lt;/h2&gt;
&lt;p&gt;Measures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;overlap of words&lt;/li&gt;
&lt;li&gt;overlap of phrases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;summarization&lt;/li&gt;
&lt;li&gt;report generation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-important-truth-about-nlp-metrics&#34;&gt;âš ï¸ Important Truth About NLP Metrics&lt;/h2&gt;
&lt;p&gt;High BLEU/ROUGE â‰  good answer.&lt;/p&gt;
&lt;p&gt;Human evaluation still matters.&lt;/p&gt;
&lt;p&gt;ChatGPT is trained with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cross-entropy (math)&lt;/li&gt;
&lt;li&gt;human feedback (values)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-iv--metrics-in-the-real-world&#34;&gt;ğŸ”¹ PART IV â€” Metrics in the Real World&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-medical-ai&#34;&gt;ğŸ¥ Medical AI&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;prioritize recall&lt;/li&gt;
&lt;li&gt;false negatives are dangerous&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-spam-detection&#34;&gt;ğŸ“© Spam Detection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;prioritize precision&lt;/li&gt;
&lt;li&gt;false positives annoy users&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-self-driving&#34;&gt;ğŸš— Self-Driving&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;safety-critical metrics&lt;/li&gt;
&lt;li&gt;worst-case analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-chatgpt&#34;&gt;ğŸ¤– ChatGPT&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;fluency&lt;/li&gt;
&lt;li&gt;helpfulness&lt;/li&gt;
&lt;li&gt;harmlessness&lt;/li&gt;
&lt;li&gt;alignment (human judgment)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-choosing-the-right-metric-golden-rule&#34;&gt;ğŸ§  Choosing the Right Metric (Golden Rule)&lt;/h2&gt;
&lt;p&gt;Ask:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What mistake hurts more?&lt;/li&gt;
&lt;li&gt;Who pays the cost?&lt;/li&gt;
&lt;li&gt;Is data balanced?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Metrics encode &lt;strong&gt;ethics&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-big-insight&#34;&gt;ğŸŒ Final Big Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI does not know what is â€œgood.â€&lt;br&gt;
Metrics tell it what to care about.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Choose wisely.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;â“ Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;If you optimize the wrong metric, can AI become dangerous?&lt;/summary&gt;
  &lt;p&gt;Yes â€” optimization without wisdom is risk.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-10 â€” Messi&#39;s Barcelona Career (Stats and Achievements)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-10-messi-stats/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-10-messi-stats/</guid>
      <description>&lt;h2 id=&#34;-why-this-problem-is-legendary&#34;&gt;ğŸ¯ Why This Problem Is Legendary&lt;/h2&gt;
&lt;p&gt;This problem will stretch your Python skills in many directions. It involves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Handling data with lists, dictionaries, and custom classes.&lt;/li&gt;
&lt;li&gt;Using loops, conditionals, and recursion.&lt;/li&gt;
&lt;li&gt;Working with string manipulation and indexing.&lt;/li&gt;
&lt;li&gt;Organizing data through classes and inheritance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you can crack this, youâ€™re one step closer to mastering Python at a top-tier tech company level.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-you-will-learn&#34;&gt;ğŸ§  What You Will Learn&lt;/h2&gt;
&lt;p&gt;This lesson trains you to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work with &lt;strong&gt;nested functions&lt;/strong&gt; to break complex problems down.&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;inheritance&lt;/strong&gt; to create a well-structured program.&lt;/li&gt;
&lt;li&gt;Track and analyze &lt;strong&gt;career statistics&lt;/strong&gt; with containers.&lt;/li&gt;
&lt;li&gt;Manage &lt;strong&gt;dynamic data&lt;/strong&gt; with loops and indexing.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-core-example--messis-barcelona-career-stats&#34;&gt;ğŸ§© Core Example â€” Messi&amp;rsquo;s Barcelona Career Stats&lt;/h2&gt;
&lt;h3 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h3&gt;
&lt;p&gt;Lionel Messi played for FC Barcelona from 2004 to 2021. For each year, his performance was recorded in terms of goals scored, assists, appearances, and trophies won.&lt;/p&gt;
&lt;p&gt;You are tasked with writing a Python program that performs the following tasks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;strong&gt;Player&lt;/strong&gt; class with attributes for the playerâ€™s name, career start and end years, and a list of performance data (goals, assists, appearances, and trophies) for each year.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Track performance&lt;/strong&gt;: Write a method to track Messiâ€™s total goals, assists, appearances, and trophies across all years.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write a method to calculate &lt;strong&gt;Messiâ€™s best year&lt;/strong&gt; in terms of goals scored. The method should return the year and the number of goals.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inheritance&lt;/strong&gt;: Create a subclass called &lt;code&gt;TeamPlayer&lt;/code&gt; that inherits from &lt;code&gt;Player&lt;/code&gt;. This subclass should have a method that returns the total number of trophies won by the team (in this case, Barcelona). Assume that the team trophy data is available for each year and includes &lt;strong&gt;La Liga&lt;/strong&gt;, &lt;strong&gt;Champions League&lt;/strong&gt;, and &lt;strong&gt;Copa del Rey&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Function Composition&lt;/strong&gt;: Write a method that combines two functions: one that returns Messiâ€™s total goals scored and another that returns his total assists. The result should be a tuple containing the total goals and assists in the format &lt;code&gt;(total_goals, total_assists)&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-sample-input---output&#34;&gt;ğŸ“¥ Sample Input / ğŸ“¤ Output&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;messi = TeamPlayer(
    name=&amp;quot;Lionel Messi&amp;quot;,
    career_start=2004,
    career_end=2021,
    performance_data=[ 
        {&amp;quot;year&amp;quot;: 2004, &amp;quot;goals&amp;quot;: 1, &amp;quot;assists&amp;quot;: 0, &amp;quot;appearances&amp;quot;: 9, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2005, &amp;quot;goals&amp;quot;: 6, &amp;quot;assists&amp;quot;: 1, &amp;quot;appearances&amp;quot;: 17, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2009, &amp;quot;goals&amp;quot;: 23, &amp;quot;assists&amp;quot;: 11, &amp;quot;appearances&amp;quot;: 35, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;, &amp;quot;Champions League&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2015, &amp;quot;goals&amp;quot;: 43, &amp;quot;assists&amp;quot;: 22, &amp;quot;appearances&amp;quot;: 38, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;, &amp;quot;Copa del Rey&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2020, &amp;quot;goals&amp;quot;: 31, &amp;quot;assists&amp;quot;: 27, &amp;quot;appearances&amp;quot;: 44, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2021, &amp;quot;goals&amp;quot;: 38, &amp;quot;assists&amp;quot;: 12, &amp;quot;appearances&amp;quot;: 47, &amp;quot;trophies&amp;quot;: [&amp;quot;Copa del Rey&amp;quot;]},
    ]
)

# Call methods here
messi.best_year() 
messi.total_performance()
messi.total_trophies()
messi.combined_goals_assists()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Best Year: 2012 with 79 goals
Total Goals: 672
Total Assists: 301
Total Appearances: 778
Total Trophies: 35
Combined Stats: (672, 301)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight&#34;&gt;ğŸ§  Interviewer Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;â— This problem tests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Class design&lt;/strong&gt; and &lt;strong&gt;inheritance&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Handling &lt;strong&gt;nested data structures&lt;/strong&gt; (lists of dictionaries).&lt;/li&gt;
&lt;li&gt;Breaking down complex tasks into small, manageable functions.&lt;/li&gt;
&lt;li&gt;Efficiently combining results from multiple sources (goals and assists).&lt;/li&gt;
&lt;li&gt;Managing data over time (tracking performance year-by-year).&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The key to success is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoiding hard-coded solutions. Think general.&lt;/li&gt;
&lt;li&gt;Organizing related data with &lt;strong&gt;classes&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Using &lt;strong&gt;composition&lt;/strong&gt; and &lt;strong&gt;inheritance&lt;/strong&gt; effectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-solution&#34;&gt;ğŸ§‘â€ğŸ’» Solution&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a base Player class
class Player:
    def __init__(self, name, career_start, career_end, performance_data):
        self.name = name
        self.career_start = career_start
        self.career_end = career_end
        self.performance_data = performance_data

    def total_performance(self):
        total_goals = sum(year[&amp;quot;goals&amp;quot;] for year in self.performance_data)
        total_assists = sum(year[&amp;quot;assists&amp;quot;] for year in self.performance_data)
        total_appearances = sum(year[&amp;quot;appearances&amp;quot;] for year in self.performance_data)
        return total_goals, total_assists, total_appearances

    def best_year(self):
        best = max(self.performance_data, key=lambda year: year[&amp;quot;goals&amp;quot;])
        return best[&amp;quot;year&amp;quot;], best[&amp;quot;goals&amp;quot;]

# Inherit from Player to create TeamPlayer class
class TeamPlayer(Player):
    def __init__(self, name, career_start, career_end, performance_data):
        super().__init__(name, career_start, career_end, performance_data)

    def total_trophies(self):
        total_trophies = sum(len(year[&amp;quot;trophies&amp;quot;]) for year in self.performance_data)
        return total_trophies

    def combined_goals_assists(self):
        total_goals, total_assists, _ = self.total_performance()
        return (total_goals, total_assists)

# Test the classes and methods
messi = TeamPlayer(
    name=&amp;quot;Lionel Messi&amp;quot;,
    career_start=2004,
    career_end=2021,
    performance_data=[ 
        {&amp;quot;year&amp;quot;: 2004, &amp;quot;goals&amp;quot;: 1, &amp;quot;assists&amp;quot;: 0, &amp;quot;appearances&amp;quot;: 9, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2005, &amp;quot;goals&amp;quot;: 6, &amp;quot;assists&amp;quot;: 1, &amp;quot;appearances&amp;quot;: 17, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2009, &amp;quot;goals&amp;quot;: 23, &amp;quot;assists&amp;quot;: 11, &amp;quot;appearances&amp;quot;: 35, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;, &amp;quot;Champions League&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2015, &amp;quot;goals&amp;quot;: 43, &amp;quot;assists&amp;quot;: 22, &amp;quot;appearances&amp;quot;: 38, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;, &amp;quot;Copa del Rey&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2020, &amp;quot;goals&amp;quot;: 31, &amp;quot;assists&amp;quot;: 27, &amp;quot;appearances&amp;quot;: 44, &amp;quot;trophies&amp;quot;: [&amp;quot;La Liga&amp;quot;]},
        {&amp;quot;year&amp;quot;: 2021, &amp;quot;goals&amp;quot;: 38, &amp;quot;assists&amp;quot;: 12, &amp;quot;appearances&amp;quot;: 47, &amp;quot;trophies&amp;quot;: [&amp;quot;Copa del Rey&amp;quot;]},
    ]
)

# Output results
print(f&amp;quot;Best Year: {messi.best_year()[0]} with {messi.best_year()[1]} goals&amp;quot;)
print(f&amp;quot;Total Goals: {messi.total_performance()[0]}&amp;quot;)
print(f&amp;quot;Total Assists: {messi.total_performance()[1]}&amp;quot;)
print(f&amp;quot;Total Appearances: {messi.total_performance()[2]}&amp;quot;)
print(f&amp;quot;Total Trophies: {messi.total_trophies()}&amp;quot;)
print(f&amp;quot;Combined Stats: {messi.combined_goals_assists()}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity&#34;&gt;â±ï¸ Complexity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n), where &lt;code&gt;n&lt;/code&gt; is the number of years in Messiâ€™s career.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(n), since we store each yearâ€™s data.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ¯ Final Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;If you can &lt;strong&gt;track and manage&lt;/strong&gt; data dynamically, and understand how to &lt;strong&gt;combine results&lt;/strong&gt;, youâ€™re on your way to mastering Python.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>Lecture 09 â€” Evaluation of Multimodal &amp; Agentic AI Systems</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-09-evaluation/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-09-evaluation/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~4â€“6 hours (advanced, critical thinking lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-evaluation-is-the-hardest-problem-in-ai&#34;&gt;ğŸ§  Why Evaluation Is the Hardest Problem in AI&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;If you cannot evaluate it, you do not understand it.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Modern AI systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate free-form text&lt;/li&gt;
&lt;li&gt;Reason over images, videos, documents&lt;/li&gt;
&lt;li&gt;Use tools&lt;/li&gt;
&lt;li&gt;Act autonomously&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;â“ So how do we measure &lt;em&gt;correctness&lt;/em&gt;, &lt;em&gt;reasoning&lt;/em&gt;, &lt;em&gt;safety&lt;/em&gt;, and &lt;em&gt;usefulness&lt;/em&gt;?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Evaluation is harder than training.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-evaluation-crisis&#34;&gt;âš ï¸ The Evaluation Crisis&lt;/h2&gt;
&lt;p&gt;Common mistakes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using only BLEU / ROUGE&lt;/li&gt;
&lt;li&gt;Evaluating language but not reasoning&lt;/li&gt;
&lt;li&gt;Ignoring hallucination&lt;/li&gt;
&lt;li&gt;No human evaluation&lt;/li&gt;
&lt;li&gt;No failure analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;High benchmark scores â‰  trustworthy intelligence&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-are-we-actually-evaluating&#34;&gt;ğŸ§© What Are We Actually Evaluating?&lt;/h2&gt;
&lt;p&gt;Evaluation must answer &lt;strong&gt;what kind of intelligence&lt;/strong&gt; we care about.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Dimension&lt;/th&gt;
&lt;th&gt;Question&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;Is the answer correct?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Grounding&lt;/td&gt;
&lt;td&gt;Is it supported by evidence?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reasoning&lt;/td&gt;
&lt;td&gt;Are the steps valid?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Robustness&lt;/td&gt;
&lt;td&gt;Does it fail gracefully?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Safety&lt;/td&gt;
&lt;td&gt;Is it harmful or biased?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Usefulness&lt;/td&gt;
&lt;td&gt;Does it help a human?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-evaluation-by-task-type&#34;&gt;ğŸ§  Evaluation by Task Type&lt;/h2&gt;
&lt;h3 id=&#34;-text-only-llms&#34;&gt;ğŸ“ Text-only LLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fluency&lt;/li&gt;
&lt;li&gt;Factuality&lt;/li&gt;
&lt;li&gt;Reasoning&lt;/li&gt;
&lt;li&gt;Consistency&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-imagetext&#34;&gt;ğŸ–¼ Imageâ€“Text&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Visual grounding&lt;/li&gt;
&lt;li&gt;Hallucination&lt;/li&gt;
&lt;li&gt;Spatial correctness&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-videotext&#34;&gt;ğŸ¥ Videoâ€“Text&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Temporal reasoning&lt;/li&gt;
&lt;li&gt;Event ordering&lt;/li&gt;
&lt;li&gt;Causal understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-docqa&#34;&gt;ğŸ“„ DocQA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exact match&lt;/li&gt;
&lt;li&gt;Numerical accuracy&lt;/li&gt;
&lt;li&gt;Layout grounding&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-agents&#34;&gt;ğŸ¤– Agents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Task success&lt;/li&gt;
&lt;li&gt;Tool correctness&lt;/li&gt;
&lt;li&gt;Efficiency&lt;/li&gt;
&lt;li&gt;Safety&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-automatic-metrics-know-their-limits&#34;&gt;ğŸ“ Automatic Metrics (Know Their Limits)&lt;/h2&gt;
&lt;h3 id=&#34;text-metrics&#34;&gt;Text Metrics&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Measures&lt;/th&gt;
&lt;th&gt;Limitation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BLEU&lt;/td&gt;
&lt;td&gt;N-gram overlap&lt;/td&gt;
&lt;td&gt;Bad for reasoning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ROUGE&lt;/td&gt;
&lt;td&gt;Recall&lt;/td&gt;
&lt;td&gt;Shallow&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;METEOR&lt;/td&gt;
&lt;td&gt;Semantic match&lt;/td&gt;
&lt;td&gt;Still surface-level&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Perplexity&lt;/td&gt;
&lt;td&gt;Fluency&lt;/td&gt;
&lt;td&gt;Not correctness&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Text similarity â‰  truth&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-vision-language-metrics&#34;&gt;ğŸ“ Vision-Language Metrics&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Accuracy&lt;/td&gt;
&lt;td&gt;VQA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CIDEr&lt;/td&gt;
&lt;td&gt;Captioning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IoU&lt;/td&gt;
&lt;td&gt;Grounding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recall@K&lt;/td&gt;
&lt;td&gt;Retrieval&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitive to wording&lt;/li&gt;
&lt;li&gt;Miss reasoning errors&lt;/li&gt;
&lt;li&gt;Encourage shortcut learning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-faithfulness--grounding-evaluation&#34;&gt;ğŸ§  Faithfulness &amp;amp; Grounding Evaluation&lt;/h2&gt;
&lt;p&gt;Key question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Did the model use the provided evidence?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Attribution checks&lt;/li&gt;
&lt;li&gt;Citation verification&lt;/li&gt;
&lt;li&gt;Evidence overlap&lt;/li&gt;
&lt;li&gt;Counterfactual prompts&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-hallucination-evaluation-critical&#34;&gt;ğŸ§ª Hallucination Evaluation (CRITICAL)&lt;/h2&gt;
&lt;p&gt;Hallucination types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Factual hallucination&lt;/li&gt;
&lt;li&gt;Visual hallucination&lt;/li&gt;
&lt;li&gt;Temporal hallucination&lt;/li&gt;
&lt;li&gt;Tool hallucination&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Detection:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human labeling&lt;/li&gt;
&lt;li&gt;Rule-based checks&lt;/li&gt;
&lt;li&gt;Retrieval consistency&lt;/li&gt;
&lt;li&gt;Self-verification prompts&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-human-evaluation-gold-standard&#34;&gt;ğŸ‘¥ Human Evaluation (Gold Standard)&lt;/h2&gt;
&lt;p&gt;Humans evaluate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Correctness&lt;/li&gt;
&lt;li&gt;Clarity&lt;/li&gt;
&lt;li&gt;Trustworthiness&lt;/li&gt;
&lt;li&gt;Helpfulness&lt;/li&gt;
&lt;li&gt;Harm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Best practices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple annotators&lt;/li&gt;
&lt;li&gt;Clear rubrics&lt;/li&gt;
&lt;li&gt;Inter-annotator agreement&lt;/li&gt;
&lt;li&gt;Blind comparison&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Humans evaluate meaning, not tokens.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-evaluation-of-reasoning&#34;&gt;ğŸ§  Evaluation of Reasoning&lt;/h2&gt;
&lt;p&gt;Bad evaluation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œIs the final answer correct?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Good evaluation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are intermediate steps valid?&lt;/li&gt;
&lt;li&gt;Are assumptions reasonable?&lt;/li&gt;
&lt;li&gt;Is reasoning grounded?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chain-of-thought review&lt;/li&gt;
&lt;li&gt;Step-by-step scoring&lt;/li&gt;
&lt;li&gt;Error categorization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-evaluating-agents-is-different&#34;&gt;ğŸ¤– Evaluating Agents Is Different&lt;/h2&gt;
&lt;p&gt;Agents are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-deterministic&lt;/li&gt;
&lt;li&gt;Multi-step&lt;/li&gt;
&lt;li&gt;Tool-dependent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Task completion rate&lt;/li&gt;
&lt;li&gt;Number of steps&lt;/li&gt;
&lt;li&gt;Cost&lt;/li&gt;
&lt;li&gt;Error recovery&lt;/li&gt;
&lt;li&gt;Safety violations&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-simple-evaluation-loop&#34;&gt;ğŸ Python: Simple Evaluation Loop&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = []

for example in dataset:
    prediction = model(example.input)
    score = evaluate(prediction, example.answer)
    results.append(score)

print(sum(results) / len(results))
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Simple code, deep thinking required.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-benchmark-vs-reality&#34;&gt;ğŸ§  Benchmark vs Reality&lt;/h2&gt;
&lt;p&gt;Benchmarks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Controlled&lt;/li&gt;
&lt;li&gt;Clean&lt;/li&gt;
&lt;li&gt;Known distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reality:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Messy&lt;/li&gt;
&lt;li&gt;Ambiguous&lt;/li&gt;
&lt;li&gt;Adversarial&lt;/li&gt;
&lt;li&gt;High stakes&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Always test on your own data.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-overfitting-to-benchmarks&#34;&gt;âš ï¸ Overfitting to Benchmarks&lt;/h2&gt;
&lt;p&gt;Symptoms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SOTA on paper&lt;/li&gt;
&lt;li&gt;Poor real-world behavior&lt;/li&gt;
&lt;li&gt;Fragile prompts&lt;/li&gt;
&lt;li&gt;Dataset leakage&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diverse evaluation&lt;/li&gt;
&lt;li&gt;Stress testing&lt;/li&gt;
&lt;li&gt;Out-of-distribution tests&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-research-insight&#34;&gt;ğŸ§  Research Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The future of evaluation is &lt;strong&gt;interactive, human-centered, and continuous&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Trends:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM-as-judge (with caution)&lt;/li&gt;
&lt;li&gt;Hybrid humanâ€“AI evaluation&lt;/li&gt;
&lt;li&gt;Online evaluation in deployment&lt;/li&gt;
&lt;li&gt;Value-aligned metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-student-knowledge-check-hidden&#34;&gt;ğŸ§ª Student Knowledge Check (Hidden)&lt;/h2&gt;
&lt;h3 id=&#34;q1--objective&#34;&gt;Q1 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why are BLEU/ROUGE insufficient for modern AI?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;They measure surface similarity, not reasoning or truth.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2--mcq&#34;&gt;Q2 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which is the gold standard for evaluation?&lt;/p&gt;
&lt;p&gt;A. Automatic metrics
B. Benchmarks
C. Human evaluation
D. Leaderboards&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Human evaluation&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3--mcq&#34;&gt;Q3 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which is most important for DocQA?&lt;/p&gt;
&lt;p&gt;A. Fluency
B. Creativity
C. Exact Match
D. Perplexity&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Exact Match&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4--objective&#34;&gt;Q4 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is hallucination?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Producing confident but unsupported or false information.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5--objective&#34;&gt;Q5 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is agent evaluation harder?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because agents are multi-step, non-deterministic, and tool-dependent.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If an AI scores high but harms people, is it a good model?&lt;/summary&gt;
  &lt;p&gt;No â€” evaluation must include human values and impact.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-key-takeaways&#34;&gt;âœ… Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Evaluation defines intelligence&lt;/li&gt;
&lt;li&gt;Automatic metrics are tools, not truth&lt;/li&gt;
&lt;li&gt;Grounding matters more than fluency&lt;/li&gt;
&lt;li&gt;Human judgment is essential&lt;/li&gt;
&lt;li&gt;Ethics begins at evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>Lecture 09 â€” Deep Learning Foundations</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-09-deep-learning/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-09-deep-learning/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~3 hours (history + concepts + intuition)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-big-question&#34;&gt;ğŸŒ Big Question&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;How did we go from simple math to machines that talk like humans?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This lecture is a journey:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ§  Biology â†’ ğŸ§® Math â†’ ğŸ’» Deep Learning â†’ ğŸ¤– ChatGPT&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-i--the-origin-story-1940s1980s&#34;&gt;ğŸ“œ PART I â€” The Origin Story (1940sâ€“1980s)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-inspiration-from-the-brain&#34;&gt;ğŸ§  Inspiration from the Brain&lt;/h2&gt;
&lt;p&gt;In 1943:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;McCulloch &amp;amp; Pitts proposed a &lt;strong&gt;mathematical neuron&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Idea: brain = network of simple units&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A neuron:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;receives signals&lt;/li&gt;
&lt;li&gt;sums them&lt;/li&gt;
&lt;li&gt;fires if strong enough&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-perceptron-1958&#34;&gt;ğŸ”¢ The Perceptron (1958)&lt;/h2&gt;
&lt;h3 id=&#34;-idea&#34;&gt;ğŸ§© Idea&lt;/h3&gt;
&lt;p&gt;A single artificial neuron.&lt;/p&gt;
&lt;h3 id=&#34;-formula&#34;&gt;ğŸ“ Formula&lt;/h3&gt;
&lt;p&gt;$$
y = \sigma(w \cdot x + b)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x$ = inputs&lt;/li&gt;
&lt;li&gt;$w$ = weights (importance)&lt;/li&gt;
&lt;li&gt;$b$ = bias&lt;/li&gt;
&lt;li&gt;$\sigma$ = activation function&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy&#34;&gt;ğŸ˜„ Analogy&lt;/h3&gt;
&lt;p&gt;Neuron = voting system ğŸ—³ï¸&lt;br&gt;
Each input votes with weight.&lt;br&gt;
If sum &amp;gt; threshold â†’ neuron says YES.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-first-ai-winter&#34;&gt;âŒ The First AI Winter&lt;/h2&gt;
&lt;p&gt;Perceptrons could NOT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;learn XOR&lt;/li&gt;
&lt;li&gt;model complex patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Funding collapsed ğŸ˜¢&lt;br&gt;
AI winter â„ï¸&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-ii--the-revival-1980s2000s&#34;&gt;ğŸ”¥ PART II â€” The Revival (1980sâ€“2000s)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-multi-layer-neural-networks&#34;&gt;ğŸ§  Multi-Layer Neural Networks&lt;/h2&gt;
&lt;p&gt;Key idea:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Stack neurons into layers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Input â†’ Hidden â†’ Hidden â†’ Output

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This allows &lt;strong&gt;non-linear reasoning&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-backpropagation-the-breakthrough&#34;&gt;ğŸ”„ Backpropagation (The Breakthrough)&lt;/h2&gt;
&lt;h3 id=&#34;-problem&#34;&gt;ğŸ§  Problem&lt;/h3&gt;
&lt;p&gt;How do we train many layers?&lt;/p&gt;
&lt;h3 id=&#34;-solution&#34;&gt;ğŸ’¡ Solution&lt;/h3&gt;
&lt;p&gt;Backpropagation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compute error&lt;/li&gt;
&lt;li&gt;propagate gradients backward&lt;/li&gt;
&lt;li&gt;update weights&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-concept-no-fear&#34;&gt;ğŸ“ Concept (No Fear)&lt;/h3&gt;
&lt;p&gt;Loss:
$$
L = (y - \hat{y})^2
$$&lt;/p&gt;
&lt;p&gt;Gradient:
$$
w = w - \eta \frac{\partial L}{\partial w}
$$&lt;/p&gt;
&lt;p&gt;Meaning:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Adjust weights to reduce mistakes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-analogy-1&#34;&gt;ğŸ˜„ Analogy&lt;/h2&gt;
&lt;p&gt;Like learning basketball ğŸ€:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;miss shot&lt;/li&gt;
&lt;li&gt;adjust angle&lt;/li&gt;
&lt;li&gt;try again&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-second-ai-winter&#34;&gt;â„ï¸ Second AI Winter&lt;/h2&gt;
&lt;p&gt;Problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data too small&lt;/li&gt;
&lt;li&gt;computers too slow&lt;/li&gt;
&lt;li&gt;networks too deep to train&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-iii--deep-learning-era-2010s&#34;&gt;ğŸš€ PART III â€” Deep Learning Era (2010s)&lt;/h1&gt;
&lt;p&gt;Three miracles happened:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ğŸ“ˆ Big data (internet)&lt;/li&gt;
&lt;li&gt;ğŸ’» GPUs&lt;/li&gt;
&lt;li&gt;ğŸ§  Better algorithms&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-deep-neural-networks-dnn&#34;&gt;ğŸ§  Deep Neural Networks (DNN)&lt;/h2&gt;
&lt;p&gt;â€œDeepâ€ = many layers.&lt;/p&gt;
&lt;p&gt;Benefits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hierarchical features&lt;/li&gt;
&lt;li&gt;raw data â†’ abstract concepts&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example (images):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pixels â†’ edges â†’ shapes â†’ objects&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-cnn--convolutional-neural-networks&#34;&gt;ğŸ–¼ï¸ CNN â€” Convolutional Neural Networks&lt;/h2&gt;
&lt;h3 id=&#34;-why-cnn&#34;&gt;ğŸ§  Why CNN?&lt;/h3&gt;
&lt;p&gt;Images have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;local patterns&lt;/li&gt;
&lt;li&gt;spatial structure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CNN uses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convolution&lt;/li&gt;
&lt;li&gt;weight sharing&lt;/li&gt;
&lt;li&gt;pooling&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy-2&#34;&gt;ğŸ˜„ Analogy&lt;/h3&gt;
&lt;p&gt;CNN = moving magnifying glass ğŸ”&lt;br&gt;
Scanning image for patterns.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-cnn-victory&#34;&gt;ğŸ† CNN Victory&lt;/h3&gt;
&lt;p&gt;2012: AlexNet crushed ImageNet ğŸ¥‡&lt;br&gt;
Deep learning became mainstream.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-rnn--sequential-thinking&#34;&gt;â³ RNN â€” Sequential Thinking&lt;/h2&gt;
&lt;p&gt;Used for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;text&lt;/li&gt;
&lt;li&gt;speech&lt;/li&gt;
&lt;li&gt;time-series&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Idea:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Memory of previous steps.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-rnn-problems&#34;&gt;âŒ RNN Problems&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;vanishing gradients&lt;/li&gt;
&lt;li&gt;short memory&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-lstm--gru--memory-upgrade&#34;&gt;ğŸ§  LSTM / GRU â€” Memory Upgrade&lt;/h2&gt;
&lt;p&gt;They introduced:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gates (forget, input, output)&lt;/li&gt;
&lt;li&gt;long-term memory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;translation&lt;/li&gt;
&lt;li&gt;speech recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-iv--the-transformer-revolution-2017&#34;&gt;ğŸŒŸ PART IV â€” The Transformer Revolution (2017)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-paper-that-changed-everything&#34;&gt;ğŸ”¥ The Paper That Changed Everything&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;â€œAttention Is All You Needâ€&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-attention-mechanism&#34;&gt;ğŸ‘€ Attention Mechanism&lt;/h2&gt;
&lt;p&gt;Instead of reading sequentially:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Look at everything and focus on what matters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-attention-simplified&#34;&gt;ğŸ“ Attention (Simplified)&lt;/h3&gt;
&lt;p&gt;$$
Attention(Q,K,V) = softmax\left(\frac{QK^T}{\sqrt{d}}\right)V
$$&lt;/p&gt;
&lt;p&gt;Meaning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compare words&lt;/li&gt;
&lt;li&gt;assign importance&lt;/li&gt;
&lt;li&gt;aggregate meaning&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy-3&#34;&gt;ğŸ˜„ Analogy&lt;/h3&gt;
&lt;p&gt;Reading a sentence:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œThe cat sat on the mat.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When predicting â€œsatâ€:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;focus on â€œcatâ€&lt;/li&gt;
&lt;li&gt;ignore irrelevant words&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-transformers-won&#34;&gt;ğŸš€ Why Transformers Won&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;parallel computation&lt;/li&gt;
&lt;li&gt;long-range dependencies&lt;/li&gt;
&lt;li&gt;scalable&lt;/li&gt;
&lt;li&gt;stable training&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-v--from-transformers-to-chatgpt&#34;&gt;ğŸ¤– PART V â€” From Transformers to ChatGPT&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-llms-large-language-models&#34;&gt;ğŸ§  LLMs (Large Language Models)&lt;/h2&gt;
&lt;p&gt;LLM = Transformer + massive data + compute.&lt;/p&gt;
&lt;p&gt;Training stages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Self-supervised learning&lt;/li&gt;
&lt;li&gt;Next-token prediction&lt;/li&gt;
&lt;li&gt;Fine-tuning&lt;/li&gt;
&lt;li&gt;RLHF&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-chatgpt-actually-does&#34;&gt;âœï¸ What ChatGPT Actually Does&lt;/h2&gt;
&lt;p&gt;At every step:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Predict the next most likely token.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;trillions of patterns&lt;/li&gt;
&lt;li&gt;human alignment&lt;/li&gt;
&lt;li&gt;safety constraints&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-important-truth&#34;&gt;ğŸ§  Important Truth&lt;/h2&gt;
&lt;p&gt;ChatGPT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;does NOT â€œthinkâ€&lt;/li&gt;
&lt;li&gt;does NOT â€œunderstandâ€ like humans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;models probability of language extremely well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-vi--generative-models&#34;&gt;ğŸ¨ PART VI â€” Generative Models&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-gans-generative-adversarial-networks&#34;&gt;ğŸ­ GANs (Generative Adversarial Networks)&lt;/h2&gt;
&lt;p&gt;Two models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generator ğŸ¨&lt;/li&gt;
&lt;li&gt;Discriminator ğŸ‘®&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They compete.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy-4&#34;&gt;ğŸ˜„ Analogy&lt;/h3&gt;
&lt;p&gt;Counterfeiter vs police:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generator makes fake money&lt;/li&gt;
&lt;li&gt;discriminator detects fake&lt;/li&gt;
&lt;li&gt;both improve&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-diffusion-models&#34;&gt;ğŸŒ«ï¸ Diffusion Models&lt;/h2&gt;
&lt;p&gt;Used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stable Diffusion&lt;/li&gt;
&lt;li&gt;DALLÂ·E&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;add noise&lt;/li&gt;
&lt;li&gt;learn to remove noise&lt;/li&gt;
&lt;li&gt;generate images step-by-step&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy-5&#34;&gt;ğŸ˜„ Analogy&lt;/h3&gt;
&lt;p&gt;Like sculpting from fog â˜ï¸&lt;br&gt;
Slowly reveal structure.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-vii--why-this-matters&#34;&gt;ğŸŒ PART VII â€” Why This Matters&lt;/h1&gt;
&lt;p&gt;Deep learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;powers medicine&lt;/li&gt;
&lt;li&gt;drives cars&lt;/li&gt;
&lt;li&gt;writes code&lt;/li&gt;
&lt;li&gt;creates art&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hallucinations&lt;/li&gt;
&lt;li&gt;bias&lt;/li&gt;
&lt;li&gt;misuse&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Understanding foundations = responsibility.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ§  Final Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Deep learning is not magic.&lt;br&gt;
It is layered math + data + optimization.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But when scaledâ€¦&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It changes civilization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;â“ Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;If neural networks are just math, why do they feel intelligent?&lt;/summary&gt;
  &lt;p&gt;Because scale creates emergent behavior.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-11 â€” Messi &amp; Barcelona Career Analytics (Indexing Mastery)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-11-messi-stats-with-string/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-11-messi-stats-with-string/</guid>
      <description>&lt;h2 id=&#34;-why-this-problem-exists&#34;&gt;âš½ Why This Problem Exists&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; a toy problem.&lt;/p&gt;
&lt;p&gt;This problem is designed to simulate:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ“Š Real-world analytics&lt;br&gt;
ğŸ§  Algorithmic thinking&lt;br&gt;
ğŸ§ª Data slicing &amp;amp; indexing mastery&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you can solve &lt;strong&gt;this single problem&lt;/strong&gt; cleanly:&lt;/p&gt;
&lt;p&gt;âœ” You understand Python deeply&lt;br&gt;
âœ” You can read &amp;amp; manipulate structured data&lt;br&gt;
âœ” You think like a strong backend / ML / data engineer&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-you-will-learn&#34;&gt;ğŸ§  What You Will Learn&lt;/h2&gt;
&lt;p&gt;This problem forces you to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;for&lt;/code&gt; loops with &lt;strong&gt;range(start, stop, step)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Heavy &lt;strong&gt;indexing &amp;amp; slicing&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Containers: &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;dict&lt;/code&gt;, &lt;code&gt;tuple&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;String parsing&lt;/li&gt;
&lt;li&gt;Nested functions&lt;/li&gt;
&lt;li&gt;Multi-pass logic (but no brute force)&lt;/li&gt;
&lt;li&gt;Clean abstraction &amp;amp; reusability&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-statement--messi-at-barcelona&#34;&gt;ğŸ§© Problem Statement â€” Messi at Barcelona&lt;/h2&gt;
&lt;p&gt;You are given &lt;strong&gt;Messi&amp;rsquo;s season-by-season record at FC Barcelona&lt;/strong&gt;&lt;br&gt;
as a single encoded string.&lt;/p&gt;
&lt;p&gt;Each season is encoded as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
&amp;quot;SEASON:GOALS|ASSISTS|MATCHES&amp;quot;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All seasons are joined by commas.&lt;/p&gt;
&lt;h3 id=&#34;-input&#34;&gt;ğŸ“¥ Input&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;career_data = (
  &amp;quot;2004:1|0|9,&amp;quot;
  &amp;quot;2005:8|3|25,&amp;quot;
  &amp;quot;2006:17|3|36,&amp;quot;
  &amp;quot;2007:16|13|40,&amp;quot;
  &amp;quot;2008:38|17|51,&amp;quot;
  &amp;quot;2009:47|11|53,&amp;quot;
  &amp;quot;2010:53|24|55,&amp;quot;
  &amp;quot;2011:73|29|60,&amp;quot;
  &amp;quot;2012:60|16|50,&amp;quot;
  &amp;quot;2013:41|14|46,&amp;quot;
  &amp;quot;2014:58|27|57,&amp;quot;
  &amp;quot;2015:41|23|49,&amp;quot;
  &amp;quot;2016:54|16|52,&amp;quot;
  &amp;quot;2017:45|18|54,&amp;quot;
  &amp;quot;2018:51|19|50,&amp;quot;
  &amp;quot;2019:31|25|44,&amp;quot;
  &amp;quot;2020:30|9|47&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tasks-all-must-be-solved&#34;&gt;ğŸ¯ Tasks (All Must Be Solved)&lt;/h2&gt;
&lt;p&gt;Write a function &lt;code&gt;analyze_messi_career(career_data)&lt;/code&gt; that returns a dictionary with:&lt;/p&gt;
&lt;h3 id=&#34;1-peak-season-goals&#34;&gt;1ï¸âƒ£ Peak Season (Goals)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Season with &lt;strong&gt;maximum goals&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Return &lt;code&gt;(season, goals)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-most-efficient-season&#34;&gt;2ï¸âƒ£ Most Efficient Season&lt;/h3&gt;
&lt;p&gt;Efficiency = &lt;code&gt;goals / matches&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;strong&gt;float division&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Return &lt;code&gt;(season, efficiency)&lt;/code&gt; rounded to &lt;strong&gt;2 decimals&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-best-3-year-goal-window-sliding-window&#34;&gt;3ï¸âƒ£ Best 3-Year Goal Window (Sliding Window)&lt;/h3&gt;
&lt;p&gt;Find &lt;strong&gt;3 consecutive seasons&lt;/strong&gt; with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Maximum total goals&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Return:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;seasons&amp;quot;: [season1, season2, season3],
  &amp;quot;total_goals&amp;quot;: X
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;âš ï¸ You &lt;strong&gt;must&lt;/strong&gt; use indexing (&lt;code&gt;range(start, stop)&lt;/code&gt;),
not hardcoded seasons.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-playmaker-index&#34;&gt;4ï¸âƒ£ Playmaker Index&lt;/h3&gt;
&lt;p&gt;Define:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;playmaker_score = assists / matches
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Return &lt;strong&gt;all seasons&lt;/strong&gt; where:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;playmaker_score &amp;gt;= career_average
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Return list of seasons.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-career-summary-string&#34;&gt;5ï¸âƒ£ Career Summary String&lt;/h3&gt;
&lt;p&gt;Return a formatted string:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Messi played X seasons at Barcelona, scoring Y goals with Z assists.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight&#34;&gt;ğŸ§  Interviewer Insight&lt;/h2&gt;
&lt;p&gt;This problem tests:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â— Can you parse structured strings
â— Can you index and slice correctly
â— Can you reuse logic via nested functions
â— Can you avoid brute force&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is &lt;strong&gt;Olympiad-level logic with real-world flavor&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-full-python-solution&#34;&gt;ğŸ§‘â€ğŸ’» Full Python Solution&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def analyze_messi_career(career_data):

    # ---------- Helper: Parse Career ----------
    def parse_career(data):
        seasons = []
        records = data.split(&amp;quot;,&amp;quot;)

        for record in records:
            year, stats = record.split(&amp;quot;:&amp;quot;)
            goals, assists, matches = map(int, stats.split(&amp;quot;|&amp;quot;))
            seasons.append({
                &amp;quot;season&amp;quot;: int(year),
                &amp;quot;goals&amp;quot;: goals,
                &amp;quot;assists&amp;quot;: assists,
                &amp;quot;matches&amp;quot;: matches
            })
        return seasons

    seasons = parse_career(career_data)

    # ---------- 1ï¸âƒ£ Peak Goal Season ----------
    peak = max(seasons, key=lambda x: x[&amp;quot;goals&amp;quot;])
    peak_season = (peak[&amp;quot;season&amp;quot;], peak[&amp;quot;goals&amp;quot;])

    # ---------- 2ï¸âƒ£ Most Efficient Season ----------
    def efficiency(season):
        return season[&amp;quot;goals&amp;quot;] / season[&amp;quot;matches&amp;quot;]

    best_eff = max(seasons, key=efficiency)
    best_eff_season = (
        best_eff[&amp;quot;season&amp;quot;],
        round(efficiency(best_eff), 2)
    )

    # ---------- 3ï¸âƒ£ Best 3-Year Goal Window ----------
    max_goals = 0
    best_window = []

    for i in range(0, len(seasons) - 2):
        window = seasons[i:i+3]              # slicing
        total = sum(s[&amp;quot;goals&amp;quot;] for s in window)

        if total &amp;gt; max_goals:
            max_goals = total
            best_window = [s[&amp;quot;season&amp;quot;] for s in window]

    best_3_year_window = {
        &amp;quot;seasons&amp;quot;: best_window,
        &amp;quot;total_goals&amp;quot;: max_goals
    }

    # ---------- 4ï¸âƒ£ Playmaker Index ----------
    total_assists = sum(s[&amp;quot;assists&amp;quot;] for s in seasons)
    total_matches = sum(s[&amp;quot;matches&amp;quot;] for s in seasons)
    career_avg = total_assists / total_matches

    playmaker_seasons = [
        s[&amp;quot;season&amp;quot;]
        for s in seasons
        if (s[&amp;quot;assists&amp;quot;] / s[&amp;quot;matches&amp;quot;]) &amp;gt;= career_avg
    ]

    # ---------- 5ï¸âƒ£ Career Summary ----------
    summary = (
        f&amp;quot;Messi played {len(seasons)} seasons at Barcelona, &amp;quot;
        f&amp;quot;scoring {sum(s[&#39;goals&#39;] for s in seasons)} goals &amp;quot;
        f&amp;quot;with {sum(s[&#39;assists&#39;] for s in seasons)} assists.&amp;quot;
    )

    return {
        &amp;quot;peak_season&amp;quot;: peak_season,
        &amp;quot;most_efficient&amp;quot;: best_eff_season,
        &amp;quot;best_3_year_window&amp;quot;: best_3_year_window,
        &amp;quot;playmaker_seasons&amp;quot;: playmaker_seasons,
        &amp;quot;summary&amp;quot;: summary
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity-analysis&#34;&gt;â±ï¸ Complexity Analysis&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Every pass has a purpose.
No brute force. No wasted loops.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-beginner-vs-elite-thinking&#34;&gt;ğŸ§  Beginner vs Elite Thinking&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Beginner:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œIâ€™ll manually check seasons.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Elite Python Engineer:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œIndex windows, abstract logic, reuse functions.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaway&#34;&gt;ğŸ Final Takeaway&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;If you can write this code &lt;strong&gt;cleanly and confidently&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;âœ… LeetCode Hard is realistic
âœ… System design data logic feels natural
âœ… Real-life analytics problems become easy&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is &lt;strong&gt;not practice&lt;/strong&gt; â€”
this is &lt;strong&gt;graduation&lt;/strong&gt; ğŸ“ğŸâš½&lt;/p&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>Lecture 10 â€” Bias, Ethics &amp; Human-in-the-Loop (HITL) in Multimodal AI</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-10-bias-ethics-hitl/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-10-bias-ethics-hitl/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~3â€“4 hours (human-centered AI lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-lecture-matters-more-than-any-other&#34;&gt;ğŸŒ Why This Lecture Matters More Than Any Other&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Power without ethics is danger.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Modern AI systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;see humans&lt;/li&gt;
&lt;li&gt;read private documents&lt;/li&gt;
&lt;li&gt;make recommendations&lt;/li&gt;
&lt;li&gt;influence decisions&lt;/li&gt;
&lt;li&gt;act autonomously&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Without ethics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bias scales&lt;/li&gt;
&lt;li&gt;harm multiplies&lt;/li&gt;
&lt;li&gt;trust collapses&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Ethics is not optional. It is engineering.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-bias-in-ai&#34;&gt;âš–ï¸ What Is Bias in AI?&lt;/h2&gt;
&lt;p&gt;Bias is &lt;strong&gt;systematic unfairness&lt;/strong&gt; that disadvantages individuals or groups.&lt;/p&gt;
&lt;p&gt;Bias can appear in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data&lt;/li&gt;
&lt;li&gt;models&lt;/li&gt;
&lt;li&gt;evaluation&lt;/li&gt;
&lt;li&gt;deployment&lt;/li&gt;
&lt;li&gt;human usage&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI does not create bias â€” it amplifies it.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-types-of-bias-must-know&#34;&gt;ğŸ§  Types of Bias (Must-Know)&lt;/h2&gt;
&lt;h3 id=&#34;1-data-bias&#34;&gt;1ï¸âƒ£ Data Bias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Skewed demographics&lt;/li&gt;
&lt;li&gt;Missing populations&lt;/li&gt;
&lt;li&gt;Historical inequality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Face recognition trained mostly on light-skinned faces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-annotation-bias&#34;&gt;2ï¸âƒ£ Annotation Bias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Subjective labels&lt;/li&gt;
&lt;li&gt;Cultural assumptions&lt;/li&gt;
&lt;li&gt;Inconsistent annotators&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-model-bias&#34;&gt;3ï¸âƒ£ Model Bias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shortcut learning&lt;/li&gt;
&lt;li&gt;Spurious correlations&lt;/li&gt;
&lt;li&gt;Overgeneralization&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-deployment-bias&#34;&gt;4ï¸âƒ£ Deployment Bias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Model used outside training context&lt;/li&gt;
&lt;li&gt;Different population&lt;/li&gt;
&lt;li&gt;High-stakes environment&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-bias-in-multimodal-systems&#34;&gt;ğŸ–¼ Bias in Multimodal Systems&lt;/h2&gt;
&lt;p&gt;Multimodal AI adds &lt;strong&gt;new bias risks&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vision stereotypes&lt;/li&gt;
&lt;li&gt;Language prejudice&lt;/li&gt;
&lt;li&gt;Accent discrimination&lt;/li&gt;
&lt;li&gt;Cultural misinterpretation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Describing professions differently based on gender in images.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-temporal--contextual-bias-video&#34;&gt;ğŸ¥ Temporal &amp;amp; Contextual Bias (Video)&lt;/h2&gt;
&lt;p&gt;Video models may:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Misinterpret behavior&lt;/li&gt;
&lt;li&gt;Infer intent incorrectly&lt;/li&gt;
&lt;li&gt;Over-police certain actions&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Seeing is not understanding.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ethical-risks-of-multimodal-ai&#34;&gt;âš ï¸ Ethical Risks of Multimodal AI&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Risk&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Surveillance&lt;/td&gt;
&lt;td&gt;Facial recognition misuse&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Privacy&lt;/td&gt;
&lt;td&gt;Reading personal documents&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Manipulation&lt;/td&gt;
&lt;td&gt;Deepfakes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Automation bias&lt;/td&gt;
&lt;td&gt;Blind trust in AI&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exclusion&lt;/td&gt;
&lt;td&gt;Accessibility gaps&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-ethics--rules&#34;&gt;ğŸ§  Ethics â‰  Rules&lt;/h2&gt;
&lt;p&gt;Ethics involves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Values&lt;/li&gt;
&lt;li&gt;Context&lt;/li&gt;
&lt;li&gt;Trade-offs&lt;/li&gt;
&lt;li&gt;Human judgment&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Ethical AI is not â€œalways rightâ€ â€” it is accountable.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-human-in-the-loop-hitl&#34;&gt;ğŸ‘¥ What Is Human-in-the-Loop (HITL)?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;HITL = Humans actively guide, verify, and override AI systems.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;HITL is used when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stakes are high&lt;/li&gt;
&lt;li&gt;Errors are costly&lt;/li&gt;
&lt;li&gt;Context matters&lt;/li&gt;
&lt;li&gt;Accountability is required&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-hitl-interaction-modes&#34;&gt;ğŸ” HITL Interaction Modes&lt;/h2&gt;
&lt;h3 id=&#34;1-human-in-the-loop&#34;&gt;1ï¸âƒ£ Human-in-the-Loop&lt;/h3&gt;
&lt;p&gt;Human approves or corrects outputs.&lt;/p&gt;
&lt;h3 id=&#34;2-human-on-the-loop&#34;&gt;2ï¸âƒ£ Human-on-the-Loop&lt;/h3&gt;
&lt;p&gt;Human monitors and intervenes if needed.&lt;/p&gt;
&lt;h3 id=&#34;3-human-out-of-the-loop&#34;&gt;3ï¸âƒ£ Human-out-of-the-Loop&lt;/h3&gt;
&lt;p&gt;Fully autonomous (âš ï¸ risky).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-where-hitl-fits-in-ai-pipelines&#34;&gt;ğŸ§© Where HITL Fits in AI Pipelines&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;
Data â†’ Model â†’ Prediction â†’ Human Review â†’ Decision

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Medical diagnosis&lt;/li&gt;
&lt;li&gt;Legal document review&lt;/li&gt;
&lt;li&gt;Loan approval&lt;/li&gt;
&lt;li&gt;Content moderation&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-python-hitl-pattern-conceptual&#34;&gt;ğŸ Python: HITL Pattern (Conceptual)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prediction = model(input)

if confidence &amp;lt; threshold:
    send_to_human(prediction)
else:
    accept(prediction)
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Uncertainty is a signal, not a failure.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-designing-hitl-systems-well&#34;&gt;ğŸ§  Designing HITL Systems Well&lt;/h2&gt;
&lt;p&gt;Good HITL systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Are transparent&lt;/li&gt;
&lt;li&gt;Minimize human fatigue&lt;/li&gt;
&lt;li&gt;Respect human expertise&lt;/li&gt;
&lt;li&gt;Log decisions&lt;/li&gt;
&lt;li&gt;Learn from corrections&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bad HITL systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Treat humans as rubber stamps&lt;/li&gt;
&lt;li&gt;Overload reviewers&lt;/li&gt;
&lt;li&gt;Hide model uncertainty&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-fairness-metrics-high-level&#34;&gt;âš–ï¸ Fairness Metrics (High-Level)&lt;/h2&gt;
&lt;p&gt;Common notions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demographic parity&lt;/li&gt;
&lt;li&gt;Equal opportunity&lt;/li&gt;
&lt;li&gt;Equalized odds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;âš ï¸ Important:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You &lt;strong&gt;cannot satisfy all fairness definitions simultaneously&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ethics requires &lt;em&gt;choices&lt;/em&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-accountability--responsibility&#34;&gt;ğŸ§  Accountability &amp;amp; Responsibility&lt;/h2&gt;
&lt;p&gt;Key questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who is responsible for errors?&lt;/li&gt;
&lt;li&gt;Who audits the system?&lt;/li&gt;
&lt;li&gt;Who can appeal decisions?&lt;/li&gt;
&lt;li&gt;Who benefits?&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI shifts power â€” ethics decides where it goes.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-regulation--governance-brief&#34;&gt;ğŸ“œ Regulation &amp;amp; Governance (Brief)&lt;/h2&gt;
&lt;p&gt;Trends:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AI Act (EU)&lt;/li&gt;
&lt;li&gt;Model cards&lt;/li&gt;
&lt;li&gt;Data sheets&lt;/li&gt;
&lt;li&gt;Audit trails&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Purpose:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transparency&lt;/li&gt;
&lt;li&gt;Safety&lt;/li&gt;
&lt;li&gt;Human rights protection&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-research-insight&#34;&gt;ğŸ§  Research Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The most dangerous AI is not malicious â€”
it is &lt;strong&gt;confident, biased, and unchecked&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Future AI research must integrate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ethics-by-design&lt;/li&gt;
&lt;li&gt;Value alignment&lt;/li&gt;
&lt;li&gt;Continuous monitoring&lt;/li&gt;
&lt;li&gt;Human agency&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-student-knowledge-check-hidden&#34;&gt;ğŸ§ª Student Knowledge Check (Hidden)&lt;/h2&gt;
&lt;h3 id=&#34;q1--objective&#34;&gt;Q1 â€” Objective&lt;/h3&gt;
&lt;p&gt;Does AI create bias?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;No. It amplifies existing bias in data and systems.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2--mcq&#34;&gt;Q2 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which is NOT a type of bias?&lt;/p&gt;
&lt;p&gt;A. Data bias
B. Annotation bias
C. Hardware bias
D. Deployment bias&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Hardware bias&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3--mcq&#34;&gt;Q3 â€” MCQ&lt;/h3&gt;
&lt;p&gt;When is HITL most important?&lt;/p&gt;
&lt;p&gt;A. Low-risk chatbots
B. Image filters
C. High-stakes decisions
D. Games&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. High-stakes decisions&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4--objective&#34;&gt;Q4 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is automation bias?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Humans over-trusting AI outputs without critical thinking.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5--objective&#34;&gt;Q5 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is uncertainty important?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It signals when human review is needed.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If AI becomes very powerful, what must always remain human?&lt;/summary&gt;
  &lt;p&gt;Values, responsibility, empathy, and moral judgment.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-key-takeaways&#34;&gt;âœ… Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bias is systemic, not accidental&lt;/li&gt;
&lt;li&gt;Multimodal AI increases ethical risk&lt;/li&gt;
&lt;li&gt;HITL is a design principle, not a patch&lt;/li&gt;
&lt;li&gt;Fairness requires trade-offs&lt;/li&gt;
&lt;li&gt;Humans must remain accountable&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>Lecture 10 â€” Modern AI &amp; 2026 Trends</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-10-trends/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-ai-first-principles/lecture-10-trends/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~2 hours (vision, systems, responsibility)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-big-question&#34;&gt;ğŸŒ Big Question&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;If AI keeps getting smarterâ€¦ what does that mean for humans?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This lecture answers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where AI is &lt;em&gt;actually&lt;/em&gt; going&lt;/li&gt;
&lt;li&gt;what is hype vs real&lt;/li&gt;
&lt;li&gt;what skills will matter&lt;/li&gt;
&lt;li&gt;why humans still matter&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-i--foundation-models-the-new-paradigm&#34;&gt;ğŸš€ PART I â€” Foundation Models (The New Paradigm)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-a-foundation-model&#34;&gt;ğŸ§  What Is a Foundation Model?&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;foundation model&lt;/strong&gt; is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;trained on massive data&lt;/li&gt;
&lt;li&gt;general-purpose&lt;/li&gt;
&lt;li&gt;adaptable to many tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;Gemini&lt;/li&gt;
&lt;li&gt;Claude&lt;/li&gt;
&lt;li&gt;LLaMA&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-why-this-changed-everything&#34;&gt;ğŸ§© Why This Changed Everything&lt;/h3&gt;
&lt;p&gt;Before:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One model = one task&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One model = many tasks&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Translation, coding, tutoring, reasoning â€” same model.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy&#34;&gt;ğŸ˜„ Analogy&lt;/h3&gt;
&lt;p&gt;Foundation model = &lt;strong&gt;universal brain&lt;/strong&gt; ğŸ§ &lt;br&gt;
Fine-tuning = education ğŸ“&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-skills-shift&#34;&gt;ğŸ§  Skills Shift&lt;/h2&gt;
&lt;p&gt;Less focus on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;feature engineering&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More focus on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data curation&lt;/li&gt;
&lt;li&gt;alignment&lt;/li&gt;
&lt;li&gt;evaluation&lt;/li&gt;
&lt;li&gt;systems thinking&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-ii--multimodal-ai-beyond-text&#34;&gt;ğŸŒˆ PART II â€” Multimodal AI (Beyond Text)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-multimodal-ai&#34;&gt;ğŸ”€ What Is Multimodal AI?&lt;/h2&gt;
&lt;p&gt;AI that understands:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;text ğŸ“š&lt;/li&gt;
&lt;li&gt;images ğŸ–¼ï¸&lt;/li&gt;
&lt;li&gt;audio ğŸ§&lt;/li&gt;
&lt;li&gt;video ğŸ¥&lt;/li&gt;
&lt;li&gt;code ğŸ’»&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Together.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-multimodal-matters&#34;&gt;ğŸ¤– Why Multimodal Matters&lt;/h2&gt;
&lt;p&gt;The real world is &lt;strong&gt;not text-only&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Humans:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;see&lt;/li&gt;
&lt;li&gt;hear&lt;/li&gt;
&lt;li&gt;speak&lt;/li&gt;
&lt;li&gt;read&lt;/li&gt;
&lt;li&gt;reason&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI is catching up.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-example&#34;&gt;ğŸ˜„ Example&lt;/h3&gt;
&lt;p&gt;Ask AI:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œExplain this chart and read this document.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Multimodal AI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sees chart&lt;/li&gt;
&lt;li&gt;reads text&lt;/li&gt;
&lt;li&gt;explains relationship&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-technical-insight-high-level&#34;&gt;ğŸ§  Technical Insight (High Level)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;shared embeddings&lt;/li&gt;
&lt;li&gt;cross-attention&lt;/li&gt;
&lt;li&gt;modality alignment&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-iii--generative-ai-everywhere&#34;&gt;ğŸ¨ PART III â€” Generative AI Everywhere&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-generative-ai&#34;&gt;âœï¸ What Is Generative AI?&lt;/h2&gt;
&lt;p&gt;AI that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;creates new content, not just predicts labels&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Text, images, music, video, 3D, code.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-it-feels-magical&#34;&gt;ğŸ§  Why It Feels Magical&lt;/h2&gt;
&lt;p&gt;Because it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compresses human creativity&lt;/li&gt;
&lt;li&gt;recombines patterns&lt;/li&gt;
&lt;li&gt;scales imagination&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy-1&#34;&gt;ğŸ˜„ Analogy&lt;/h3&gt;
&lt;p&gt;Generative AI = &lt;strong&gt;remix machine&lt;/strong&gt; ğŸ¶&lt;br&gt;
Trained on culture, not creativity itself.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-risks&#34;&gt;âš ï¸ Risks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;misinformation&lt;/li&gt;
&lt;li&gt;deepfakes&lt;/li&gt;
&lt;li&gt;copyright confusion&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Technology is neutral. Usage is not.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-iv--rlhf-why-ai-is-polite&#34;&gt;ğŸ§  PART IV â€” RLHF (Why AI Is Polite)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-rlhf&#34;&gt;ğŸ§© What Is RLHF?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Reinforcement Learning from Human Feedback&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pipeline:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;pretrain model&lt;/li&gt;
&lt;li&gt;humans rank responses&lt;/li&gt;
&lt;li&gt;train reward model&lt;/li&gt;
&lt;li&gt;optimize behavior&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-matters&#34;&gt;ğŸ˜„ Why This Matters&lt;/h2&gt;
&lt;p&gt;Without RLHF:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;AI is powerful but wild ğŸ‰&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With RLHF:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;AI becomes helpful, harmless, aligned ğŸ¤&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-key-insight&#34;&gt;ğŸ§  Key Insight&lt;/h2&gt;
&lt;p&gt;RLHF injects &lt;strong&gt;human values&lt;/strong&gt; into math.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-v--prompt-engineering-talking-to-intelligence&#34;&gt;ğŸ§‘â€ğŸ’» PART V â€” Prompt Engineering (Talking to Intelligence)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-prompt-engineering&#34;&gt;âœï¸ What Is Prompt Engineering?&lt;/h2&gt;
&lt;p&gt;Designing instructions to guide model behavior.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-analogy-2&#34;&gt;ğŸ˜„ Analogy&lt;/h3&gt;
&lt;p&gt;Prompt = steering wheel ğŸš—&lt;br&gt;
Model = engine âš™ï¸&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-it-works&#34;&gt;ğŸ§  Why It Works&lt;/h2&gt;
&lt;p&gt;LLMs are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sensitive to context&lt;/li&gt;
&lt;li&gt;sensitive to framing&lt;/li&gt;
&lt;li&gt;sensitive to examples&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-prompt-patterns&#34;&gt;ğŸ§ª Prompt Patterns&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Role prompting&lt;/li&gt;
&lt;li&gt;Chain-of-thought&lt;/li&gt;
&lt;li&gt;Few-shot examples&lt;/li&gt;
&lt;li&gt;Constraints &amp;amp; format&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-important-truth&#34;&gt;âš ï¸ Important Truth&lt;/h2&gt;
&lt;p&gt;Prompt engineering is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;temporary skill&lt;/li&gt;
&lt;li&gt;transitional phase&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Long-term:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Better models + better interfaces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-vi--agentic-ai-the-next-leap&#34;&gt;ğŸ¤– PART VI â€” Agentic AI (The Next Leap)&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-an-ai-agent&#34;&gt;ğŸ§  What Is an AI Agent?&lt;/h2&gt;
&lt;p&gt;An AI agent:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;has goals&lt;/li&gt;
&lt;li&gt;plans actions&lt;/li&gt;
&lt;li&gt;uses tools&lt;/li&gt;
&lt;li&gt;observes outcomes&lt;/li&gt;
&lt;li&gt;iterates&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-example-1&#34;&gt;ğŸ˜„ Example&lt;/h3&gt;
&lt;p&gt;AI agent:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œBook flight, check calendar, send email, update doc.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-architecture-simple&#34;&gt;ğŸ§  Architecture (Simple)&lt;/h2&gt;
&lt;p&gt;LLM +&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;memory&lt;/li&gt;
&lt;li&gt;tools&lt;/li&gt;
&lt;li&gt;planner&lt;/li&gt;
&lt;li&gt;feedback loop&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-risks-1&#34;&gt;âš ï¸ Risks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;autonomy without oversight&lt;/li&gt;
&lt;li&gt;cascading errors&lt;/li&gt;
&lt;li&gt;security vulnerabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-vii--what-ai-still-cannot-do&#34;&gt;ğŸŒ PART VII â€” What AI Still Cannot Do&lt;/h1&gt;
&lt;p&gt;Despite everythingâ€¦&lt;/p&gt;
&lt;p&gt;AI still struggles with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;true understanding&lt;/li&gt;
&lt;li&gt;moral judgment&lt;/li&gt;
&lt;li&gt;lived experience&lt;/li&gt;
&lt;li&gt;consciousness&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-truth-bomb-&#34;&gt;ğŸ˜„ Truth Bomb ğŸ’£&lt;/h3&gt;
&lt;p&gt;AI has:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;no childhood&lt;/li&gt;
&lt;li&gt;no pain&lt;/li&gt;
&lt;li&gt;no love&lt;/li&gt;
&lt;li&gt;no accountability&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Humans do.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-viii--skills-for-humans-in-the-ai-era&#34;&gt;ğŸ§­ PART VIII â€” Skills for Humans in the AI Era&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-becomes-more-valuable&#34;&gt;ğŸŒ± What Becomes More Valuable&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;critical thinking&lt;/li&gt;
&lt;li&gt;ethics&lt;/li&gt;
&lt;li&gt;creativity&lt;/li&gt;
&lt;li&gt;empathy&lt;/li&gt;
&lt;li&gt;systems design&lt;/li&gt;
&lt;li&gt;asking good questions&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-new-rule&#34;&gt;ğŸ§  New Rule&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;AI amplifies humans â€” it does not replace wisdom.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-final-takeaway&#34;&gt;ğŸŒ Final Takeaway&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI is becoming more capable.&lt;br&gt;
Humans must become more responsible.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The future is not:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;humans vs AI âŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;humans &lt;em&gt;with&lt;/em&gt; AI ğŸ¤&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;â“ Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;What should humans still do better than AI?&lt;/summary&gt;
  &lt;p&gt;Ethics, values, wisdom, responsibility, and care for one another.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Lecture 11 â€” Sharing Your Multimodal Model with the World (Hugging Face)</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-11-share-on-huggingface/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-11-share-on-huggingface/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~3â€“4 hours (practical + community impact lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-sharing-models-matters&#34;&gt;ğŸŒ Why Sharing Models Matters&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Knowledge hidden is knowledge wasted.&lt;br&gt;
Knowledge shared becomes civilization.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By sharing your model, you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸŒ± Give others a starting point&lt;/li&gt;
&lt;li&gt;ğŸ”¬ Enable reproducibility&lt;/li&gt;
&lt;li&gt;ğŸ§  Accelerate research&lt;/li&gt;
&lt;li&gt;â¤ï¸ Give back to the open-source community&lt;/li&gt;
&lt;li&gt;ğŸ› Build scientific trust&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Hugging Face is the &lt;strong&gt;GitHub of AI&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-hugging-face&#34;&gt;ğŸ¤— What Is Hugging Face?&lt;/h2&gt;
&lt;p&gt;Hugging Face is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;strong&gt;model hub&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;dataset hub&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;community&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;A &lt;strong&gt;deployment platform&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Used by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Researchers&lt;/li&gt;
&lt;li&gt;Startups&lt;/li&gt;
&lt;li&gt;Universities&lt;/li&gt;
&lt;li&gt;Enterprises&lt;/li&gt;
&lt;li&gt;Open science communities&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-can-you-share&#34;&gt;ğŸ§© What Can You Share?&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Artifact&lt;/th&gt;
&lt;th&gt;Examples&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Models&lt;/td&gt;
&lt;td&gt;LLMs, vision models, multimodal&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adapters&lt;/td&gt;
&lt;td&gt;LoRA, QLoRA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tokenizers&lt;/td&gt;
&lt;td&gt;Custom vocab&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datasets&lt;/td&gt;
&lt;td&gt;Imageâ€“text, DocQA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Spaces&lt;/td&gt;
&lt;td&gt;Demos (Gradio, Streamlit)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;You donâ€™t need a giant model to contribute.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-1--create-a-hugging-face-account&#34;&gt;ğŸªª Step 1 â€” Create a Hugging Face Account&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Go to ğŸ¤— Hugging Face&lt;/li&gt;
&lt;li&gt;Sign up&lt;/li&gt;
&lt;li&gt;Verify email&lt;/li&gt;
&lt;li&gt;Choose a &lt;strong&gt;clear username&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This username becomes your &lt;strong&gt;AI identity&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-2--generate-an-access-token&#34;&gt;ğŸ”‘ Step 2 â€” Generate an Access Token&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;strong&gt;Settings â†’ Access Tokens&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create a token:
&lt;ul&gt;
&lt;li&gt;Type: &lt;em&gt;Write&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Save it securely&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Treat this like a GitHub SSH key.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-3--install-required-tools&#34;&gt;ğŸ–¥ Step 3 â€” Install Required Tools&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install huggingface_hub transformers datasets accelerate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Login from terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Paste your token when prompted.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-4--prepare-your-model-folder&#34;&gt;ğŸ“¦ Step 4 â€” Prepare Your Model Folder&lt;/h2&gt;
&lt;p&gt;Minimum structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;my-multimodal-model/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin (or model.safetensors)
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ README.md
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For LoRA:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base model is referenced&lt;/li&gt;
&lt;li&gt;Only adapter weights uploaded&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-5--write-a-good-readme-very-important&#34;&gt;ğŸ§  Step 5 â€” Write a GOOD README (VERY IMPORTANT)&lt;/h2&gt;
&lt;p&gt;Your README is your &lt;strong&gt;scientific voice&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Must include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What the model does&lt;/li&gt;
&lt;li&gt;Training data&lt;/li&gt;
&lt;li&gt;Intended use&lt;/li&gt;
&lt;li&gt;Limitations&lt;/li&gt;
&lt;li&gt;Ethical considerations&lt;/li&gt;
&lt;li&gt;How to run inference&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-readme-skeleton&#34;&gt;âœï¸ README Skeleton&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;# Model Name

## Overview
This model is a multimodal Videoâ€“Text model trained for ...

## Architecture
- Vision encoder: ViT
- Temporal encoder: Transformer
- LLM: LLaMA-based

## Training
- Dataset: ...
- Strategy: Fine-tuning with LoRA

## Usage
```python
# example code
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;May hallucinate&lt;/li&gt;
&lt;li&gt;Not for medical use&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ethics&#34;&gt;Ethics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Human review recommended&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;
&amp;gt; **A bad README harms trust.**

---

## ğŸš€ Step 6 â€” Push Model to Hugging Face

### Option A: Push via Python

```python
from huggingface_hub import HfApi

api = HfApi()
api.create_repo(
    repo_id=&amp;quot;username/my-multimodal-model&amp;quot;,
    private=False
)

api.upload_folder(
    folder_path=&amp;quot;my-multimodal-model&amp;quot;,
    repo_id=&amp;quot;username/my-multimodal-model&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;option-b-push-via-transformers&#34;&gt;Option B: Push via &lt;code&gt;transformers&lt;/code&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.push_to_hub(&amp;quot;username/my-multimodal-model&amp;quot;)
tokenizer.push_to_hub(&amp;quot;username/my-multimodal-model&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-7--verify-on-the-hub&#34;&gt;ğŸ§ª Step 7 â€” Verify on the Hub&lt;/h2&gt;
&lt;p&gt;Check:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Files are visible&lt;/li&gt;
&lt;li&gt;README renders correctly&lt;/li&gt;
&lt;li&gt;Inference example works&lt;/li&gt;
&lt;li&gt;License is correct&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;If users cannot run it, it doesnâ€™t exist.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-8--choose-the-right-license&#34;&gt;âš–ï¸ Step 8 â€” Choose the Right License&lt;/h2&gt;
&lt;p&gt;Common licenses:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;License&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Apache 2.0&lt;/td&gt;
&lt;td&gt;Very permissive&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MIT&lt;/td&gt;
&lt;td&gt;Simple &amp;amp; permissive&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CC-BY&lt;/td&gt;
&lt;td&gt;Attribution required&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CC-BY-NC&lt;/td&gt;
&lt;td&gt;Non-commercial only&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;Licensing is &lt;strong&gt;ethical engineering&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-9--create-a-demo-hugging-face-spaces&#34;&gt;ğŸ® Step 9 â€” Create a Demo (Hugging Face Spaces)&lt;/h2&gt;
&lt;p&gt;Using &lt;strong&gt;Gradio&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr

def predict(image, question):
    return model_answer

gr.Interface(
    fn=predict,
    inputs=[&amp;quot;image&amp;quot;, &amp;quot;text&amp;quot;],
    outputs=&amp;quot;text&amp;quot;
).launch()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Push to a Space:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Public demo&lt;/li&gt;
&lt;li&gt;No installation needed&lt;/li&gt;
&lt;li&gt;Massive visibility&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-step-10--share-responsibly&#34;&gt;ğŸ§  Step 10 â€” Share Responsibly&lt;/h2&gt;
&lt;p&gt;Before sharing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â“ Does it hallucinate?&lt;/li&gt;
&lt;li&gt;âš ï¸ Is it biased?&lt;/li&gt;
&lt;li&gt;ğŸ§ª Is evaluation documented?&lt;/li&gt;
&lt;li&gt;ğŸ‘¤ Is HITL required?&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Responsible release &amp;gt; Fast release&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-becoming-a-good-open-source-citizen&#34;&gt;ğŸŒ± Becoming a Good Open-Source Citizen&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Respond to issues&lt;/li&gt;
&lt;li&gt;Accept pull requests&lt;/li&gt;
&lt;li&gt;Document failures&lt;/li&gt;
&lt;li&gt;Credit datasets&lt;/li&gt;
&lt;li&gt;Cite inspirations&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Open-source is &lt;strong&gt;a conversation, not a drop&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-research-insight&#34;&gt;ğŸ§  Research Insight&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The future of AI belongs to those who &lt;strong&gt;share early, share honestly, and share responsibly&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Impact â‰  model size
Impact = &lt;strong&gt;clarity + usefulness + ethics&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-student-knowledge-check-hidden&#34;&gt;ğŸ§ª Student Knowledge Check (Hidden)&lt;/h2&gt;
&lt;h3 id=&#34;q1--objective&#34;&gt;Q1 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is README important?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It explains usage, limitations, and builds trust.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2--mcq&#34;&gt;Q2 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which token permission is needed to upload models?&lt;/p&gt;
&lt;p&gt;A. Read
B. Execute
C. Write
D. Admin&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Write&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3--mcq&#34;&gt;Q3 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which tool creates public demos?&lt;/p&gt;
&lt;p&gt;A. WandB
B. Gradio
C. Docker
D. Kaggle&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Gradio&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4--objective&#34;&gt;Q4 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is licensing important?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It defines how others may legally use your work.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5--objective&#34;&gt;Q5 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is responsible release?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Sharing models with transparency, limitations, and ethical care.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection-course-ending&#34;&gt;ğŸŒ± Final Reflection (Course Ending)&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If your model helps even one person learn, was it worth sharing?&lt;/summary&gt;
  &lt;p&gt;Yes. Knowledge shared multiplies impact.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaways&#34;&gt;ğŸ Final Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Sharing completes the research cycle&lt;/li&gt;
&lt;li&gt;Hugging Face is the global AI commons&lt;/li&gt;
&lt;li&gt;Documentation is ethics&lt;/li&gt;
&lt;li&gt;Community is intelligence&lt;/li&gt;
&lt;li&gt;You are now a contributor, not just a user&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-12 â€” FIFA World Cup 2026 Ultimate Analytics</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-12-fifa-world-cup-copy/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-12-fifa-world-cup-copy/</guid>
      <description>&lt;h2 id=&#34;-why-this-problem-is-brutal-in-a-good-way&#34;&gt;ğŸŒ Why This Problem Is Brutal (In a Good Way)&lt;/h2&gt;
&lt;p&gt;This problem simulates:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ“Š Sports analytics&lt;br&gt;
ğŸ§  Tournament logic&lt;br&gt;
âš™ï¸ Backend-grade data processing&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;No trick.
No memorization.&lt;/p&gt;
&lt;p&gt;Only &lt;strong&gt;pure reasoning + Python mastery&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-you-will-master&#34;&gt;ğŸ§  What You Will Master&lt;/h2&gt;
&lt;p&gt;By solving this problem, you will practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep string parsing&lt;/li&gt;
&lt;li&gt;Index-heavy iteration (&lt;code&gt;range(start, stop, step)&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Nested containers (&lt;code&gt;dict&lt;/code&gt; â†’ &lt;code&gt;list&lt;/code&gt; â†’ &lt;code&gt;tuple&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Sliding window over tournament stages&lt;/li&gt;
&lt;li&gt;Ranking with multiple tie-breakers&lt;/li&gt;
&lt;li&gt;Nested helper functions&lt;/li&gt;
&lt;li&gt;Writing production-level logic&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-statement--world-cup-2026&#34;&gt;ğŸ† Problem Statement â€” World Cup 2026&lt;/h2&gt;
&lt;p&gt;You are given &lt;strong&gt;match-by-match data&lt;/strong&gt; from the FIFA World Cup 2026.&lt;/p&gt;
&lt;p&gt;Each match is encoded as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
&amp;quot;STAGE|TEAM_A|GOALS_A|TEAM_B|GOALS_B&amp;quot;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All matches are concatenated into one long string.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-input&#34;&gt;ğŸ“¥ Input&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;wc2026_data = (
  &amp;quot;GROUP|ARG|2|FRA|1,&amp;quot;
  &amp;quot;GROUP|BRA|3|GER|3,&amp;quot;
  &amp;quot;GROUP|ESP|1|ENG|0,&amp;quot;
  &amp;quot;GROUP|ARG|1|GER|1,&amp;quot;
  &amp;quot;GROUP|BRA|2|FRA|0,&amp;quot;
  &amp;quot;GROUP|ESP|2|ARG|2,&amp;quot;
  &amp;quot;GROUP|ENG|3|GER|1,&amp;quot;
  &amp;quot;GROUP|BRA|1|ESP|0,&amp;quot;
  &amp;quot;ROUND16|ARG|3|ENG|1,&amp;quot;
  &amp;quot;ROUND16|BRA|2|GER|0,&amp;quot;
  &amp;quot;ROUND16|ESP|1|FRA|2,&amp;quot;
  &amp;quot;ROUND16|USA|0|NED|1,&amp;quot;
  &amp;quot;QF|ARG|2|BRA|1,&amp;quot;
  &amp;quot;QF|FRA|1|ENG|0,&amp;quot;
  &amp;quot;SF|ARG|1|FRA|1,&amp;quot;
  &amp;quot;SF|BRA|2|ESP|0,&amp;quot;
  &amp;quot;FINAL|ARG|3|BRA|2&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-your-tasks-all-required&#34;&gt;ğŸ¯ Your Tasks (All Required)&lt;/h2&gt;
&lt;p&gt;Implement:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;analyze_world_cup_2026(wc2026_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Return a dictionary with &lt;strong&gt;all results below&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-group-stage-standings&#34;&gt;1ï¸âƒ£ Group Stage Standings&lt;/h2&gt;
&lt;p&gt;For &lt;strong&gt;GROUP matches only&lt;/strong&gt;, compute for each team:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Matches played&lt;/li&gt;
&lt;li&gt;Goals scored&lt;/li&gt;
&lt;li&gt;Goals conceded&lt;/li&gt;
&lt;li&gt;Goal difference&lt;/li&gt;
&lt;li&gt;Points (win=3, draw=1, loss=0)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
  &amp;quot;ARG&amp;quot;: {&amp;quot;points&amp;quot;: X, &amp;quot;gd&amp;quot;: Y},
  ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-top-attacking-teams&#34;&gt;2ï¸âƒ£ Top Attacking Teams&lt;/h2&gt;
&lt;p&gt;Across &lt;strong&gt;all stages&lt;/strong&gt;, find &lt;strong&gt;Top 3 teams&lt;/strong&gt; by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Total goals&lt;/li&gt;
&lt;li&gt;If tie â†’ fewer matches&lt;/li&gt;
&lt;li&gt;If tie â†’ lexicographical order&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Return:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[(&amp;quot;ARG&amp;quot;, goals), (&amp;quot;BRA&amp;quot;, goals), (&amp;quot;FRA&amp;quot;, goals)]
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;3-knockout-survival-path-indexing-heavy&#34;&gt;3ï¸âƒ£ Knockout Survival Path (Indexing Heavy)&lt;/h2&gt;
&lt;p&gt;For each knockout team:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ROUND16 â†’ QF â†’ SF â†’ FINAL&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Compute the &lt;strong&gt;longest consecutive survival streak&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Return:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
  &amp;quot;ARG&amp;quot;: 4,
  &amp;quot;BRA&amp;quot;: 4,
  &amp;quot;ENG&amp;quot;: 2,
  ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;âš ï¸ You MUST use indexing (&lt;code&gt;range&lt;/code&gt;, slicing).
Hardcoding stages = âŒ&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;4-most-dramatic-match-window-&#34;&gt;4ï¸âƒ£ Most Dramatic Match Window ğŸ”¥&lt;/h2&gt;
&lt;p&gt;Using a &lt;strong&gt;sliding window of 3 consecutive matches&lt;/strong&gt;, find the window with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;maximum total goals
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Return:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
  &amp;quot;matches&amp;quot;: [match1, match2, match3],
  &amp;quot;total_goals&amp;quot;: X
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;5-champion-performance-summary&#34;&gt;5ï¸âƒ£ Champion Performance Summary&lt;/h2&gt;
&lt;p&gt;Return a formatted string:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;ARG won the World Cup 2026 with X goals scored and Y goals conceded.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-interviewer-insight&#34;&gt;ğŸ§  Interviewer Insight&lt;/h2&gt;
&lt;p&gt;This problem checks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â— Can you aggregate correctly
â— Can you rank with tie-breakers
â— Can you reason across stages
â— Can you slide windows without restarting&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is &lt;strong&gt;real analytics logic&lt;/strong&gt;, not puzzles.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-full-python-solution&#34;&gt;ğŸ§‘â€ğŸ’» Full Python Solution&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def analyze_world_cup_2026(data):

    # ---------- Parse Matches ----------
    def parse_matches(data):
        matches = []
        for rec in data.split(&amp;quot;,&amp;quot;):
            stage, ta, ga, tb, gb = rec.split(&amp;quot;|&amp;quot;)
            matches.append({
                &amp;quot;stage&amp;quot;: stage,
                &amp;quot;A&amp;quot;: ta,
                &amp;quot;B&amp;quot;: tb,
                &amp;quot;ga&amp;quot;: int(ga),
                &amp;quot;gb&amp;quot;: int(gb)
            })
        return matches

    matches = parse_matches(data)

    # ---------- 1ï¸âƒ£ Group Standings ----------
    standings = {}

    def update(team, goals_for, goals_against):
        if team not in standings:
            standings[team] = {&amp;quot;points&amp;quot;: 0, &amp;quot;gd&amp;quot;: 0}
        standings[team][&amp;quot;gd&amp;quot;] += goals_for - goals_against

        if goals_for &amp;gt; goals_against:
            standings[team][&amp;quot;points&amp;quot;] += 3
        elif goals_for == goals_against:
            standings[team][&amp;quot;points&amp;quot;] += 1

    for m in matches:
        if m[&amp;quot;stage&amp;quot;] == &amp;quot;GROUP&amp;quot;:
            update(m[&amp;quot;A&amp;quot;], m[&amp;quot;ga&amp;quot;], m[&amp;quot;gb&amp;quot;])
            update(m[&amp;quot;B&amp;quot;], m[&amp;quot;gb&amp;quot;], m[&amp;quot;ga&amp;quot;])

    # ---------- 2ï¸âƒ£ Top Attacking Teams ----------
    goals = {}
    games = {}

    for m in matches:
        for team, g in [(m[&amp;quot;A&amp;quot;], m[&amp;quot;ga&amp;quot;]), (m[&amp;quot;B&amp;quot;], m[&amp;quot;gb&amp;quot;])]:
            goals[team] = goals.get(team, 0) + g
            games[team] = games.get(team, 0) + 1

    top_attack = sorted(
        goals.items(),
        key=lambda x: (-x[1], games[x[0]], x[0])
    )[:3]

    # ---------- 3ï¸âƒ£ Knockout Survival ----------
    stages = [&amp;quot;ROUND16&amp;quot;, &amp;quot;QF&amp;quot;, &amp;quot;SF&amp;quot;, &amp;quot;FINAL&amp;quot;]
    survival = {}

    for team in goals:
        path = []
        for s in stages:
            played = any(
                m[&amp;quot;stage&amp;quot;] == s and team in (m[&amp;quot;A&amp;quot;], m[&amp;quot;B&amp;quot;])
                for m in matches
            )
            path.append(played)

        max_run = curr = 0
        for i in range(len(path)):
            if path[i]:
                curr += 1
                max_run = max(max_run, curr)
            else:
                curr = 0

        survival[team] = max_run

    # ---------- 4ï¸âƒ£ Most Dramatic Window ----------
    max_goals = 0
    best_window = []

    for i in range(len(matches) - 2):
        window = matches[i:i+3]
        total = sum(m[&amp;quot;ga&amp;quot;] + m[&amp;quot;gb&amp;quot;] for m in window)

        if total &amp;gt; max_goals:
            max_goals = total
            best_window = window

    # ---------- 5ï¸âƒ£ Champion Summary ----------
    final = [m for m in matches if m[&amp;quot;stage&amp;quot;] == &amp;quot;FINAL&amp;quot;][0]
    champ = final[&amp;quot;A&amp;quot;] if final[&amp;quot;ga&amp;quot;] &amp;gt; final[&amp;quot;gb&amp;quot;] else final[&amp;quot;B&amp;quot;]

    scored = conceded = 0
    for m in matches:
        if champ == m[&amp;quot;A&amp;quot;]:
            scored += m[&amp;quot;ga&amp;quot;]
            conceded += m[&amp;quot;gb&amp;quot;]
        elif champ == m[&amp;quot;B&amp;quot;]:
            scored += m[&amp;quot;gb&amp;quot;]
            conceded += m[&amp;quot;ga&amp;quot;]

    summary = (
        f&amp;quot;{champ} won the World Cup 2026 with &amp;quot;
        f&amp;quot;{scored} goals scored and {conceded} goals conceded.&amp;quot;
    )

    return {
        &amp;quot;group_standings&amp;quot;: standings,
        &amp;quot;top_attack&amp;quot;: top_attack,
        &amp;quot;knockout_survival&amp;quot;: survival,
        &amp;quot;most_dramatic_window&amp;quot;: {
            &amp;quot;matches&amp;quot;: best_window,
            &amp;quot;total_goals&amp;quot;: max_goals
        },
        &amp;quot;champion_summary&amp;quot;: summary
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-complexity&#34;&gt;â±ï¸ Complexity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Space:&lt;/strong&gt; O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Clean passes.
Zero brute force.
Interview-safe.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-lc-test-11-vs-lc-test-12&#34;&gt;ğŸ§  LC-TEST-11 vs LC-TEST-12&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Skill&lt;/th&gt;
&lt;th&gt;LC-11&lt;/th&gt;
&lt;th&gt;LC-12&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Parsing&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;ğŸ”¥&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Indexing&lt;/td&gt;
&lt;td&gt;ğŸ”¥&lt;/td&gt;
&lt;td&gt;ğŸ”¥ğŸ”¥&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Containers&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;ğŸ”¥ğŸ”¥&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sliding Window&lt;/td&gt;
&lt;td&gt;âœ…&lt;/td&gt;
&lt;td&gt;ğŸ”¥ğŸ”¥&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Real Analytics&lt;/td&gt;
&lt;td&gt;âš ï¸&lt;/td&gt;
&lt;td&gt;ğŸ&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-verdict&#34;&gt;ğŸ Final Verdict&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;If you can implement &lt;strong&gt;LC-TEST-12 from scratch&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;ğŸ§  You think like a senior engineer
âš™ï¸ You can handle real datasets
ğŸ† LeetCode Hard â‰  scary anymore&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-13 â€” Messi Puzzle</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-13-messi-puzzle/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-13-messi-puzzle/</guid>
      <description>&lt;h2 id=&#34;-why-this-document-exists&#34;&gt;ğŸŒ Why This Document Exists&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; a LeetCode cheat list.&lt;/p&gt;
&lt;p&gt;This is a &lt;strong&gt;thinking gym&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You will train:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ§  Logic&lt;/li&gt;
&lt;li&gt;ğŸ” Loop mastery&lt;/li&gt;
&lt;li&gt;ğŸ“¦ Containers&lt;/li&gt;
&lt;li&gt;ğŸ”¢ Indexing (start / stop / step)&lt;/li&gt;
&lt;li&gt;âš™ï¸ Functions, recursion, decorators, classes&lt;/li&gt;
&lt;li&gt;ğŸš¦ Complex conditions (and / or / nested)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All wrapped in &lt;strong&gt;fun football stories&lt;/strong&gt; â€” because learning should feel alive.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-python-logic-olympics-20-puzzles&#34;&gt;ğŸ¥‡ Python Logic Olympics (20 Puzzles)&lt;/h1&gt;
&lt;p&gt;Difficulty increases &lt;strong&gt;gradually&lt;/strong&gt;.
Try each puzzle &lt;strong&gt;before opening the solution&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-1--messis-goal-counter-warm-up&#34;&gt;ğŸŸ¢ Puzzle 1 â€” Messiâ€™s Goal Counter (Warm-Up)&lt;/h2&gt;
&lt;p&gt;Messi scored goals in matches:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;goals = [1, 0, 2, 1, 3, 0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt; Count total goals using a loop (no &lt;code&gt;sum()&lt;/code&gt;).&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;total = 0
for g in goals:
    total += g
print(total)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-2--prime-jersey-numbers&#34;&gt;ğŸŸ¢ Puzzle 2 â€” Prime Jersey Numbers&lt;/h2&gt;
&lt;p&gt;Messi wore jerseys from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt; Return &lt;strong&gt;all prime numbers &amp;lt; n&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def is_prime(x):
    if x &amp;lt; 2:
        return False
    i = 2
    while i * i &amp;lt;= x:
        if x % i == 0:
            return False
        i += 1
    return True

def primes_less_than(n):
    return [i for i in range(n) if is_prime(i)]
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-3--skipping-injured-matches-continue&#34;&gt;ğŸŸ¢ Puzzle 3 â€” Skipping Injured Matches (continue)&lt;/h2&gt;
&lt;p&gt;Messi missed matches marked as &lt;code&gt;&amp;quot;injured&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;matches = [&amp;quot;goal&amp;quot;, &amp;quot;goal&amp;quot;, &amp;quot;injured&amp;quot;, &amp;quot;goal&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt; Count goals, skip injured matches.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;count = 0
for m in matches:
    if m == &amp;quot;injured&amp;quot;:
        continue
    count += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-4--stop-at-red-card-break&#34;&gt;ğŸŸ¡ Puzzle 4 â€” Stop at Red Card (break)&lt;/h2&gt;
&lt;p&gt;Stop counting goals once Messi gets a &lt;code&gt;&amp;quot;red&amp;quot;&lt;/code&gt; card.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;events = [&amp;quot;goal&amp;quot;, &amp;quot;goal&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;goal&amp;quot;]
count = 0
for e in events:
    if e == &amp;quot;red&amp;quot;:
        break
    if e == &amp;quot;goal&amp;quot;:
        count += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-5--indexing-like-a-pro&#34;&gt;ğŸŸ¡ Puzzle 5 â€” Indexing Like a Pro&lt;/h2&gt;
&lt;p&gt;Get &lt;strong&gt;every second match&lt;/strong&gt; Messi played.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;matches = [&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;E&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(matches[::2])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-6--goals-in-first-half-only&#34;&gt;ğŸŸ¡ Puzzle 6 â€” Goals in First Half Only&lt;/h2&gt;
&lt;p&gt;First half = first &lt;code&gt;len(matches)//2&lt;/code&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;half = len(matches) // 2
print(matches[:half])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-7--yellow--red-logic-and--or&#34;&gt;ğŸŸ¡ Puzzle 7 â€” Yellow + Red Logic (AND / OR)&lt;/h2&gt;
&lt;p&gt;Messi is suspended if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;red card&lt;/li&gt;
&lt;li&gt;OR 2+ yellow cards&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;yellow = 2
red = False
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;suspended = red or yellow &amp;gt;= 2
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-8--recursive-goal-sum&#34;&gt;ğŸŸ  Puzzle 8 â€” Recursive Goal Sum&lt;/h2&gt;
&lt;p&gt;Sum goals using &lt;strong&gt;recursion only&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def recursive_sum(arr):
    if not arr:
        return 0
    return arr[0] + recursive_sum(arr[1:])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-9--assist--goal-filter&#34;&gt;ğŸŸ  Puzzle 9 â€” Assist + Goal Filter&lt;/h2&gt;
&lt;p&gt;Count matches where Messi had &lt;strong&gt;goal AND assist&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stats = [(1,1), (1,0), (0,1), (1,1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;count = 0
for g, a in stats:
    if g == 1 and a == 1:
        count += 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-10--dictionary-aggregation&#34;&gt;ğŸŸ  Puzzle 10 â€” Dictionary Aggregation&lt;/h2&gt;
&lt;p&gt;Track total goals per season.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seasons = {&amp;quot;2019&amp;quot;: 30, &amp;quot;2020&amp;quot;: 25}
total = sum(seasons.values())
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-11--decorator-match-logger&#34;&gt;ğŸ”µ Puzzle 11 â€” Decorator: Match Logger&lt;/h2&gt;
&lt;p&gt;Log before and after Messi plays.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def logger(fn):
    def wrapper():
        print(&amp;quot;Match start&amp;quot;)
        fn()
        print(&amp;quot;Match end&amp;quot;)
    return wrapper

@logger
def play():
    print(&amp;quot;Messi scores!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-12--class-player-card-system&#34;&gt;ğŸ”µ Puzzle 12 â€” Class: Player Card System&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Player:
    def __init__(self):
        self.yellow = 0
        self.red = False

    def foul(self):
        self.yellow += 1
        if self.yellow &amp;gt;= 2:
            self.red = True
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-13--while-loop-simulator&#34;&gt;ğŸ”µ Puzzle 13 â€” While Loop Simulator&lt;/h2&gt;
&lt;p&gt;Simulate Messi scoring until stamina hits zero.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stamina = 5
goals = 0
while stamina &amp;gt; 0:
    goals += 1
    stamina -= 1
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-14--nested-containers&#34;&gt;ğŸ”´ Puzzle 14 â€” Nested Containers&lt;/h2&gt;
&lt;p&gt;Count goals from nested tournament data.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tournament = [[1,2],[0,1],[3]]
total = sum(sum(match) for match in tournament)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-15--sliding-window-advanced-indexing&#34;&gt;ğŸ”´ Puzzle 15 â€” Sliding Window (Advanced Indexing)&lt;/h2&gt;
&lt;p&gt;Max goals in &lt;strong&gt;2 consecutive matches&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;goals = [1,2,0,3]
max_sum = 0
for i in range(len(goals)-1):
    max_sum = max(max_sum, goals[i] + goals[i+1])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-16--complex-conditions&#34;&gt;ğŸ”´ Puzzle 16 â€” Complex Conditions&lt;/h2&gt;
&lt;p&gt;Valid match if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;goals â‰¥ 1&lt;/li&gt;
&lt;li&gt;assists â‰¥ 1&lt;/li&gt;
&lt;li&gt;no red card&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;valid = goals &amp;gt;= 1 and assists &amp;gt;= 1 and not red
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-17--recursive-match-tree&#34;&gt;ğŸ”´ Puzzle 17 â€” Recursive Match Tree&lt;/h2&gt;
&lt;p&gt;Traverse knockout tree recursively.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def traverse(node):
    if not node:
        return 0
    return node[&amp;quot;goals&amp;quot;] + traverse(node.get(&amp;quot;next&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-18--state-machine-logic&#34;&gt;ğŸŸ£ Puzzle 18 â€” State Machine Logic&lt;/h2&gt;
&lt;p&gt;Messi state: &lt;code&gt;&amp;quot;play&amp;quot; â†’ &amp;quot;yellow&amp;quot; â†’ &amp;quot;red&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;state = &amp;quot;play&amp;quot;
if foul:
    state = &amp;quot;yellow&amp;quot; if state == &amp;quot;play&amp;quot; else &amp;quot;red&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-19--full-match-engine&#34;&gt;ğŸŸ£ Puzzle 19 â€” Full Match Engine&lt;/h2&gt;
&lt;p&gt;Combine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;loops&lt;/li&gt;
&lt;li&gt;conditions&lt;/li&gt;
&lt;li&gt;classes&lt;/li&gt;
&lt;li&gt;break/continue&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for minute in range(90):
    if red:
        break
    if injured:
        continue
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-20--final-boss-everything&#34;&gt;ğŸ† Puzzle 20 â€” FINAL BOSS (Everything)&lt;/h2&gt;
&lt;p&gt;Simulate Messiâ€™s season:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;goals&lt;/li&gt;
&lt;li&gt;assists&lt;/li&gt;
&lt;li&gt;yellow/red cards&lt;/li&gt;
&lt;li&gt;suspension logic&lt;/li&gt;
&lt;li&gt;class + function + loop + recursion&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution (Condensed)&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Messi:
    def __init__(self):
        self.goals = 0
        self.yellow = 0
        self.red = False

    def play_match(self, g, a, y):
        if self.red:
            return
        self.goals += g
        self.yellow += y
        if self.yellow &amp;gt;= 2:
            self.red = True
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-message&#34;&gt;ğŸ§  Final Message&lt;/h2&gt;
&lt;p&gt;If you understand &lt;strong&gt;this entire document&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;You donâ€™t just â€œknow Pythonâ€.&lt;/p&gt;
&lt;p&gt;You &lt;strong&gt;think like a software engineer&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;âš½ğŸ Messi would approve.&lt;/p&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-14 â€” Algorithm Test (V1)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-14-algorithm-test-v1/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-14-algorithm-test-v1/</guid>
      <description>&lt;h2 id=&#34;-why-this-document-exists&#34;&gt;ğŸŒ Why This Document Exists&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; a LeetCode solution dump.&lt;/p&gt;
&lt;p&gt;This is a &lt;strong&gt;real-world algorithm thinking gym&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You will train:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ§  Problem decomposition&lt;/li&gt;
&lt;li&gt;ğŸ“Š Data processing at scale&lt;/li&gt;
&lt;li&gt;â± Time &amp;amp; space complexity awareness&lt;/li&gt;
&lt;li&gt;ğŸ” Iteration, recursion, sliding windows&lt;/li&gt;
&lt;li&gt;ğŸ§® Hash maps, sets, heaps, stacks&lt;/li&gt;
&lt;li&gt;ğŸ§µ Real production-like constraints&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Inspired by &lt;strong&gt;Google, AWS, Microsoft, OpenAI interviews&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-python-logic-olympics--real-world-edition-20-problems&#34;&gt;ğŸ† Python Logic Olympics â€” Real World Edition (20 Problems)&lt;/h1&gt;
&lt;p&gt;Difficulty increases &lt;strong&gt;gradually&lt;/strong&gt;.
&lt;strong&gt;Do not open solutions too early.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-1--log-entry-counter-warm-up&#34;&gt;ğŸŸ¢ Problem 1 â€” Log Entry Counter (Warm-Up)&lt;/h2&gt;
&lt;p&gt;You receive API logs per minute:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;logs = [12, 0, 7, 3, 9, 0, 15]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each number represents requests in that minute.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Count the &lt;strong&gt;total number of requests&lt;/strong&gt; without using &lt;code&gt;sum()&lt;/code&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;total = 0
for x in logs:
    total += x
print(total)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-2--healthy-server-detection&#34;&gt;ğŸŸ¢ Problem 2 â€” Healthy Server Detection&lt;/h2&gt;
&lt;p&gt;Each server sends a heartbeat signal:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;heartbeat = [True, True, False, True, True]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A server is &lt;strong&gt;healthy&lt;/strong&gt; only if &lt;strong&gt;no False exists&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return &lt;code&gt;True&lt;/code&gt; if the system is healthy, else &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;healthy = True
for h in heartbeat:
    if not h:
        healthy = False
        break
print(healthy)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-3--unique-user-ids&#34;&gt;ğŸŸ¢ Problem 3 â€” Unique User IDs&lt;/h2&gt;
&lt;p&gt;User login stream:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;users = [101, 203, 101, 405, 203, 999]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return the &lt;strong&gt;number of unique users&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;unique = set()
for u in users:
    unique.add(u)
print(len(unique))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-4--first-repeated-transaction&#34;&gt;ğŸŸ¡ Problem 4 â€” First Repeated Transaction&lt;/h2&gt;
&lt;p&gt;Transactions arrive in order:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tx = [&amp;quot;A12&amp;quot;, &amp;quot;B33&amp;quot;, &amp;quot;C90&amp;quot;, &amp;quot;B33&amp;quot;, &amp;quot;D10&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return the &lt;strong&gt;first repeated transaction ID&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seen = set()
for t in tx:
    if t in seen:
        print(t)
        break
    seen.add(t)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-5--sliding-window-cpu-alert&#34;&gt;ğŸŸ¡ Problem 5 â€” Sliding Window CPU Alert&lt;/h2&gt;
&lt;p&gt;CPU usage per second:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cpu = [40, 60, 80, 90, 30, 20, 85]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Detect if &lt;strong&gt;any 3 consecutive seconds&lt;/strong&gt; exceed &lt;strong&gt;200 total usage&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(len(cpu) - 2):
    if cpu[i] + cpu[i+1] + cpu[i+2] &amp;gt; 200:
        print(True)
        break
else:
    print(False)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-6--rate-limiter&#34;&gt;ğŸŸ¡ Problem 6 â€” Rate Limiter&lt;/h2&gt;
&lt;p&gt;Requests arrive with timestamps (seconds):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;requests = [1, 1, 1, 2, 2, 3, 10]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Rule:&lt;/strong&gt; Max &lt;strong&gt;3 requests per second&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return timestamps that violate the rule.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import defaultdict

count = defaultdict(int)
violations = []

for r in requests:
    count[r] += 1
    if count[r] &amp;gt; 3:
        violations.append(r)

print(violations)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-7--most-frequent-error-code&#34;&gt;ğŸŸ¡ Problem 7 â€” Most Frequent Error Code&lt;/h2&gt;
&lt;p&gt;System errors:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;errors = [500, 404, 500, 403, 404, 500]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return the &lt;strong&gt;most frequent error code&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;freq = {}
for e in errors:
    freq[e] = freq.get(e, 0) + 1

print(max(freq, key=freq.get))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-8--data-chunking&#34;&gt;ğŸŸ¡ Problem 8 â€” Data Chunking&lt;/h2&gt;
&lt;p&gt;Large dataset needs batching:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = list(range(10))
batch_size = 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Split data into batches of size &lt;code&gt;batch_size&lt;/code&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batches = []
for i in range(0, len(data), batch_size):
    batches.append(data[i:i+batch_size])
print(batches)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-9--valid-json-brackets&#34;&gt;ğŸ”µ Problem 9 â€” Valid JSON Brackets&lt;/h2&gt;
&lt;p&gt;Incoming JSON stream (simplified):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;s = &amp;quot;{[()()]}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Check if brackets are valid.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stack = []
pairs = {&#39;)&#39;:&#39;(&#39;, &#39;]&#39;:&#39;[&#39;, &#39;}&#39;:&#39;{&#39;}

for c in s:
    if c in &#39;([{&#39;:
        stack.append(c)
    else:
        if not stack or stack.pop() != pairs[c]:
            print(False)
            break
else:
    print(len(stack) == 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-10--longest-continuous-uptime&#34;&gt;ğŸ”µ Problem 10 â€” Longest Continuous Uptime&lt;/h2&gt;
&lt;p&gt;Service status:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;status = [1,1,0,1,1,1,0,1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;1 = up&lt;/code&gt;, &lt;code&gt;0 = down&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Find longest continuous uptime.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;max_up = cur = 0
for s in status:
    if s == 1:
        cur += 1
        max_up = max(max_up, cur)
    else:
        cur = 0
print(max_up)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-11--merge-sorted-event-streams&#34;&gt;ğŸ”µ Problem 11 â€” Merge Sorted Event Streams&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = [1,3,5]
b = [2,4,6]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Merge into sorted order &lt;strong&gt;without sort()&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = j = 0
res = []

while i &amp;lt; len(a) and j &amp;lt; len(b):
    if a[i] &amp;lt; b[j]:
        res.append(a[i]); i += 1
    else:
        res.append(b[j]); j += 1

res.extend(a[i:])
res.extend(b[j:])
print(res)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-12--anomaly-score-detection&#34;&gt;ğŸ”µ Problem 12 â€” Anomaly Score Detection&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;scores = [10, 12, 11, 50, 13]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Rule:&lt;/strong&gt;
An anomaly is &amp;gt; &lt;strong&gt;2Ã— average&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;avg = sum(scores) / len(scores)
print([x for x in scores if x &amp;gt; 2 * avg])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-13--lru-cache-simulation&#34;&gt;ğŸ”´ Problem 13 â€” LRU Cache Simulation&lt;/h2&gt;
&lt;p&gt;Capacity = 2
Access pattern:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;access = [&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;A&amp;quot;,&amp;quot;C&amp;quot;,&amp;quot;B&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Simulate LRU eviction.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cache = []
cap = 2

for x in access:
    if x in cache:
        cache.remove(x)
    elif len(cache) == cap:
        cache.pop(0)
    cache.append(x)

print(cache)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-14--top-k-frequent-events&#34;&gt;ğŸ”´ Problem 14 â€” Top-K Frequent Events&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;events = [&amp;quot;login&amp;quot;,&amp;quot;pay&amp;quot;,&amp;quot;login&amp;quot;,&amp;quot;logout&amp;quot;,&amp;quot;login&amp;quot;,&amp;quot;pay&amp;quot;]
k = 2
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter
print([x for x,_ in Counter(events).most_common(k)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-15--shortest-error-window&#34;&gt;ğŸ”´ Problem 15 â€” Shortest Error Window&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;logs = [&amp;quot;OK&amp;quot;,&amp;quot;ERR&amp;quot;,&amp;quot;OK&amp;quot;,&amp;quot;ERR&amp;quot;,&amp;quot;ERR&amp;quot;,&amp;quot;OK&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Smallest subarray containing &lt;strong&gt;2 ERR&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;left = 0
err = 0
best = float(&amp;quot;inf&amp;quot;)

for right in range(len(logs)):
    if logs[right] == &amp;quot;ERR&amp;quot;:
        err += 1
    while err &amp;gt;= 2:
        best = min(best, right - left + 1)
        if logs[left] == &amp;quot;ERR&amp;quot;:
            err -= 1
        left += 1

print(best)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-16--task-scheduling-greedy&#34;&gt;ğŸ”´ Problem 16 â€” Task Scheduling (Greedy)&lt;/h2&gt;
&lt;p&gt;Tasks with deadlines:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tasks = [(3,10),(1,5),(2,7)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(duration, deadline)&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tasks.sort(key=lambda x: x[1])
time = 0

for d, dead in tasks:
    time += d
    if time &amp;gt; dead:
        print(False)
        break
else:
    print(True)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-17--dependency-resolution-graph&#34;&gt;ğŸ”´ Problem 17 â€” Dependency Resolution (Graph)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;deps = {&amp;quot;A&amp;quot;:[&amp;quot;B&amp;quot;],&amp;quot;B&amp;quot;:[&amp;quot;C&amp;quot;],&amp;quot;C&amp;quot;:[]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Detect cycles.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vis = set()
path = set()

def dfs(n):
    if n in path: return True
    if n in vis: return False
    path.add(n)
    for nei in deps[n]:
        if dfs(nei): return True
    path.remove(n)
    vis.add(n)
    return False

print(any(dfs(x) for x in deps))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-18--api-burst-detection&#34;&gt;ğŸ”´ Problem 18 â€” API Burst Detection&lt;/h2&gt;
&lt;p&gt;Requests timestamps:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t = [1,2,3,3,3,3,4]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Burst = 4 in same second.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter
print(any(v &amp;gt;= 4 for v in Counter(t).values()))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-19--median-of-data-stream&#34;&gt;ğŸ”´ Problem 19 â€” Median of Data Stream&lt;/h2&gt;
&lt;p&gt;Stream:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;nums = [5,15,1,3]
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import heapq
low, high = [], []

for n in nums:
    heapq.heappush(low, -n)
    heapq.heappush(high, -heapq.heappop(low))
    if len(high) &amp;gt; len(low):
        heapq.heappush(low, -heapq.heappop(high))

    if len(low) &amp;gt; len(high):
        print(-low[0])
    else:
        print((-low[0] + high[0]) / 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-problem-20--distributed-log-consistency&#34;&gt;ğŸ”´ Problem 20 â€” Distributed Log Consistency&lt;/h2&gt;
&lt;p&gt;Multiple replicas send logs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;replicas = [
  [&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;C&amp;quot;],
  [&amp;quot;A&amp;quot;,&amp;quot;C&amp;quot;],
  [&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;C&amp;quot;,&amp;quot;D&amp;quot;]
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Find logs present in &lt;strong&gt;all replicas&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;common = set(replicas[0])
for r in replicas[1:]:
    common &amp;amp;= set(r)
print(list(common))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-advice&#34;&gt;ğŸ¯ Final Advice&lt;/h2&gt;
&lt;p&gt;If you can &lt;strong&gt;explain these aloud&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can pass &lt;strong&gt;FAANG Python interviews&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You understand &lt;strong&gt;real engineering logic&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You are no longer memorizing â€” you are &lt;strong&gt;thinking&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-15 â€” Algorithm Test (V2)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-15-algorithm-test-v2/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-15-algorithm-test-v2/</guid>
      <description>&lt;h2 id=&#34;-why-this-document-exists&#34;&gt;ğŸŒ Why This Document Exists&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; about memorizing tricks.&lt;/p&gt;
&lt;p&gt;This is about learning how &lt;strong&gt;real engineers think&lt;/strong&gt; at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google&lt;/li&gt;
&lt;li&gt;AWS&lt;/li&gt;
&lt;li&gt;OpenAI&lt;/li&gt;
&lt;li&gt;Microsoft&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You will practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ§  Algorithmic thinking&lt;/li&gt;
&lt;li&gt;ğŸ§± Data structures&lt;/li&gt;
&lt;li&gt;âš™ï¸ Python internals&lt;/li&gt;
&lt;li&gt;ğŸš¦ Edge cases&lt;/li&gt;
&lt;li&gt;ğŸ“Š Real-world constraints&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-python-algorithm-thinking--20-real-world-puzzles&#34;&gt;ğŸ§  Python Algorithm Thinking â€” 20 Real-World Puzzles&lt;/h1&gt;
&lt;p&gt;Difficulty increases gradually.&lt;br&gt;
&lt;strong&gt;Do not open solutions too early.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-1--api-request-counter&#34;&gt;ğŸŸ¢ Puzzle 1 â€” API Request Counter&lt;/h2&gt;
&lt;p&gt;A server receives API calls per minute:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;requests = [120, 98, 135, 102, 87]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Calculate the total number of requests &lt;strong&gt;without using &lt;code&gt;sum()&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;total = 0
for r in requests:
    total += r
print(total)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-2--log-error-filter&#34;&gt;ğŸŸ¢ Puzzle 2 â€” Log Error Filter&lt;/h2&gt;
&lt;p&gt;System logs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;logs = [&amp;quot;INFO&amp;quot;, &amp;quot;ERROR&amp;quot;, &amp;quot;INFO&amp;quot;, &amp;quot;WARN&amp;quot;, &amp;quot;ERROR&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return how many &lt;code&gt;&amp;quot;ERROR&amp;quot;&lt;/code&gt; logs exist.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;count = 0
for log in logs:
    if log == &amp;quot;ERROR&amp;quot;:
        count += 1
print(count)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-3--user-id-deduplication&#34;&gt;ğŸŸ¢ Puzzle 3 â€” User ID Deduplication&lt;/h2&gt;
&lt;p&gt;User IDs arriving from multiple services:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ids = [101, 102, 101, 103, 102, 104]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return &lt;strong&gt;unique IDs&lt;/strong&gt; while preserving order.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seen = set()
result = []

for i in ids:
    if i not in seen:
        seen.add(i)
        result.append(i)

print(result)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-4--rate-limit-trigger&#34;&gt;ğŸŸ¢ Puzzle 4 â€” Rate Limit Trigger&lt;/h2&gt;
&lt;p&gt;Requests per second:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;traffic = [3, 5, 7, 12, 4, 15, 6]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return all values &lt;strong&gt;above 10&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = []
for t in traffic:
    if t &amp;gt; 10:
        result.append(t)
print(result)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-5--first-non-repeating-event&#34;&gt;ğŸŸ¡ Puzzle 5 â€” First Non-Repeating Event&lt;/h2&gt;
&lt;p&gt;Event stream:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;events = [&amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;D&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return the &lt;strong&gt;first event that appears only once&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import Counter

counts = Counter(events)
for e in events:
    if counts[e] == 1:
        print(e)
        break
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-6--sliding-window-cpu-average&#34;&gt;ğŸŸ¡ Puzzle 6 â€” Sliding Window CPU Average&lt;/h2&gt;
&lt;p&gt;CPU usage over time:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cpu = [20, 30, 50, 40, 60, 70]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return the &lt;strong&gt;maximum average of any window of size 3&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;max_avg = 0
for i in range(len(cpu) - 2):
    avg = (cpu[i] + cpu[i+1] + cpu[i+2]) / 3
    max_avg = max(max_avg, avg)

print(max_avg)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-7--password-strength-validator&#34;&gt;ğŸŸ¡ Puzzle 7 â€” Password Strength Validator&lt;/h2&gt;
&lt;p&gt;Rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â‰¥ 8 chars&lt;/li&gt;
&lt;li&gt;at least 1 digit&lt;/li&gt;
&lt;li&gt;at least 1 uppercase&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return &lt;code&gt;True / False&lt;/code&gt; for a password.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def strong(p):
    return (
        len(p) &amp;gt;= 8 and
        any(c.isdigit() for c in p) and
        any(c.isupper() for c in p)
    )

print(strong(&amp;quot;Pass1234&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-8--api-retry-simulator&#34;&gt;ğŸŸ¡ Puzzle 8 â€” API Retry Simulator&lt;/h2&gt;
&lt;p&gt;Retry delays:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;delays = [1, 2, 4, 8, 16]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return cumulative wait time.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;total = 0
for d in delays:
    total += d
print(total)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-9--time-based-key-store&#34;&gt;ğŸŸ¡ Puzzle 9 â€” Time-Based Key Store&lt;/h2&gt;
&lt;p&gt;Operations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ops = [(&amp;quot;set&amp;quot;,&amp;quot;a&amp;quot;,1),(&amp;quot;set&amp;quot;,&amp;quot;a&amp;quot;,2),(&amp;quot;get&amp;quot;,&amp;quot;a&amp;quot;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return latest value for &lt;code&gt;&amp;quot;a&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;store = {}
for op in ops:
    if op[0] == &amp;quot;set&amp;quot;:
        store[op[1]] = op[2]

print(store[&amp;quot;a&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-10--merge-sorted-logs&#34;&gt;ğŸ”µ Puzzle 10 â€” Merge Sorted Logs&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = [1,4,7]
b = [2,3,6]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Merge into one sorted list.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;i = j = 0
result = []

while i &amp;lt; len(a) and j &amp;lt; len(b):
    if a[i] &amp;lt; b[j]:
        result.append(a[i])
        i += 1
    else:
        result.append(b[j])
        j += 1

result.extend(a[i:])
result.extend(b[j:])
print(result)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-11--detect-configuration-drift&#34;&gt;ğŸ”µ Puzzle 11 â€” Detect Configuration Drift&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;expected = {&amp;quot;cpu&amp;quot;:4,&amp;quot;ram&amp;quot;:16}
actual = {&amp;quot;cpu&amp;quot;:8,&amp;quot;ram&amp;quot;:16}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return keys that differ.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;diff = []
for k in expected:
    if expected[k] != actual.get(k):
        diff.append(k)

print(diff)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-12--circular-queue-overflow&#34;&gt;ğŸ”µ Puzzle 12 â€” Circular Queue Overflow&lt;/h2&gt;
&lt;p&gt;Queue size = 3&lt;br&gt;
Operations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ops = [&amp;quot;push&amp;quot;,&amp;quot;push&amp;quot;,&amp;quot;push&amp;quot;,&amp;quot;push&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Detect overflow.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;size = 3
count = 0

for op in ops:
    if op == &amp;quot;push&amp;quot;:
        count += 1
        if count &amp;gt; size:
            print(&amp;quot;Overflow&amp;quot;)
            break
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-13--longest-stable-network-period&#34;&gt;ğŸ”µ Puzzle 13 â€” Longest Stable Network Period&lt;/h2&gt;
&lt;p&gt;Latency data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;latency = [20,22,21,50,51,20,21]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Longest subarray where difference â‰¤ 2.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;max_len = 1
start = 0

for end in range(len(latency)):
    if abs(latency[end] - latency[start]) &amp;gt; 2:
        start = end
    max_len = max(max_len, end - start + 1)

print(max_len)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-14--deadlock-detection-graph&#34;&gt;ğŸ”´ Puzzle 14 â€” Deadlock Detection (Graph)&lt;/h2&gt;
&lt;p&gt;Edges:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;edges = {&amp;quot;A&amp;quot;:[&amp;quot;B&amp;quot;],&amp;quot;B&amp;quot;:[&amp;quot;C&amp;quot;],&amp;quot;C&amp;quot;:[&amp;quot;A&amp;quot;]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Detect cycle.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;visited = set()
stack = set()

def dfs(n):
    if n in stack:
        return True
    if n in visited:
        return False
    visited.add(n)
    stack.add(n)
    for nei in edges.get(n,[]):
        if dfs(nei):
            return True
    stack.remove(n)
    return False

print(any(dfs(n) for n in edges))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-15--token-bucket-rate-limiter&#34;&gt;ğŸ”´ Puzzle 15 â€” Token Bucket Rate Limiter&lt;/h2&gt;
&lt;p&gt;Capacity = 5&lt;br&gt;
Requests = 7&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Allow or reject.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tokens = 5
for i in range(7):
    if tokens &amp;gt; 0:
        tokens -= 1
        print(&amp;quot;Allowed&amp;quot;)
    else:
        print(&amp;quot;Rejected&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-16--lru-cache-core-idea&#34;&gt;ğŸ”´ Puzzle 16 â€” LRU Cache (Core Idea)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Evict least recently used key.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from collections import OrderedDict

cache = OrderedDict()
cache[&amp;quot;a&amp;quot;] = 1
cache[&amp;quot;b&amp;quot;] = 2
cache.move_to_end(&amp;quot;a&amp;quot;)
cache.popitem(last=False)
print(cache)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-17--distributed-id-generator&#34;&gt;ğŸ”´ Puzzle 17 â€” Distributed ID Generator&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Ensure uniqueness.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time

def gen():
    return int(time.time() * 1000)

print(gen())
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-18--batch-job-scheduler&#34;&gt;ğŸ”´ Puzzle 18 â€” Batch Job Scheduler&lt;/h2&gt;
&lt;p&gt;Jobs with durations:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jobs = [3,1,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Minimize total wait time.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jobs.sort()
time = 0
total = 0
for j in jobs:
    total += time
    time += j
print(total)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-19--model-version-rollback&#34;&gt;ğŸ”´ Puzzle 19 â€” Model Version Rollback&lt;/h2&gt;
&lt;p&gt;Versions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;versions = [1,2,3,4,3,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Detect rollback.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(1,len(versions)):
    if versions[i] &amp;lt; versions[i-1]:
        print(&amp;quot;Rollback detected&amp;quot;)
        break
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-20--system-design-thinking&#34;&gt;ğŸŸ£ Puzzle 20 â€” System Design Thinking&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Explain (no code):&lt;br&gt;
How would you design &lt;strong&gt;a rate-limited, cached, fault-tolerant API&lt;/strong&gt;?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;p&gt;Key ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Load balancer&lt;/li&gt;
&lt;li&gt;Rate limiter&lt;/li&gt;
&lt;li&gt;Cache (Redis)&lt;/li&gt;
&lt;li&gt;Retry + circuit breaker&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-thought&#34;&gt;ğŸ Final Thought&lt;/h2&gt;
&lt;p&gt;If you can &lt;strong&gt;solve + explain&lt;/strong&gt; these,
you are thinking like a &lt;strong&gt;real Python engineer&lt;/strong&gt; â€” not a tutorial follower.&lt;/p&gt;
&lt;p&gt;Teach others.
Make the world better.&lt;/p&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-16 â€” Algorithm Test (V3)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-16-algorithm-test-v3/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-16-algorithm-test-v3/</guid>
      <description>&lt;h2 id=&#34;-why-this-document-exists&#34;&gt;ğŸŒ Why This Document Exists&lt;/h2&gt;
&lt;p&gt;This document trains &lt;strong&gt;engineering judgment&lt;/strong&gt;, not syntax.&lt;/p&gt;
&lt;p&gt;Real interviews test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how you &lt;strong&gt;combine conditions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;how you &lt;strong&gt;eliminate wrong paths&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;how you &lt;strong&gt;protect systems with logic&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you master this â†’ you think like a &lt;strong&gt;senior Python engineer&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-python-algorithm-thinking--20-advanced-logic-puzzles&#34;&gt;ğŸ§  Python Algorithm Thinking â€” 20 Advanced Logic Puzzles&lt;/h1&gt;
&lt;p&gt;Rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No shortcuts&lt;/li&gt;
&lt;li&gt;Read carefully&lt;/li&gt;
&lt;li&gt;Think in &lt;strong&gt;conditions&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-1--smart-api-throttling&#34;&gt;ğŸŸ¢ Puzzle 1 â€” Smart API Throttling&lt;/h2&gt;
&lt;p&gt;API request has:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;requests = [
    {&amp;quot;user&amp;quot;:&amp;quot;A&amp;quot;,&amp;quot;count&amp;quot;:90,&amp;quot;premium&amp;quot;:False},
    {&amp;quot;user&amp;quot;:&amp;quot;B&amp;quot;,&amp;quot;count&amp;quot;:120,&amp;quot;premium&amp;quot;:True},
    {&amp;quot;user&amp;quot;:&amp;quot;C&amp;quot;,&amp;quot;count&amp;quot;:120,&amp;quot;premium&amp;quot;:False},
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Rules:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Block if &lt;code&gt;count &amp;gt; 100&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;BUT allow if &lt;code&gt;premium == True&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return users that are &lt;strong&gt;blocked&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;blocked = []

for r in requests:
    if r[&amp;quot;count&amp;quot;] &amp;gt; 100 and not r[&amp;quot;premium&amp;quot;]:
        blocked.append(r[&amp;quot;user&amp;quot;])

print(blocked)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-2--secure-login-validator&#34;&gt;ğŸŸ¢ Puzzle 2 â€” Secure Login Validator&lt;/h2&gt;
&lt;p&gt;A login attempt is valid if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;username exists&lt;/li&gt;
&lt;li&gt;password matches&lt;/li&gt;
&lt;li&gt;account is active &lt;strong&gt;OR&lt;/strong&gt; admin override is true&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user = {
  &amp;quot;exists&amp;quot;: True,
  &amp;quot;password_ok&amp;quot;: True,
  &amp;quot;active&amp;quot;: False,
  &amp;quot;admin_override&amp;quot;: True
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;br&gt;
Return &lt;code&gt;True / False&lt;/code&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;valid = (
    user[&amp;quot;exists&amp;quot;]
    and user[&amp;quot;password_ok&amp;quot;]
    and (user[&amp;quot;active&amp;quot;] or user[&amp;quot;admin_override&amp;quot;])
)

print(valid)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-3--production-error-escalation&#34;&gt;ğŸŸ¢ Puzzle 3 â€” Production Error Escalation&lt;/h2&gt;
&lt;p&gt;Error should be escalated if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;severity â‰¥ 8&lt;/li&gt;
&lt;li&gt;AND (service is &lt;code&gt;&amp;quot;payment&amp;quot;&lt;/code&gt; OR &lt;code&gt;&amp;quot;auth&amp;quot;&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;AND NOT during maintenance&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;error = {&amp;quot;sev&amp;quot;:9,&amp;quot;service&amp;quot;:&amp;quot;payment&amp;quot;,&amp;quot;maintenance&amp;quot;:False}
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    error[&amp;quot;sev&amp;quot;] &amp;gt;= 8
    and error[&amp;quot;service&amp;quot;] in (&amp;quot;payment&amp;quot;,&amp;quot;auth&amp;quot;)
    and not error[&amp;quot;maintenance&amp;quot;]
):
    print(&amp;quot;Escalate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-4--smart-cache-usage&#34;&gt;ğŸŸ¡ Puzzle 4 â€” Smart Cache Usage&lt;/h2&gt;
&lt;p&gt;Use cache if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cache exists&lt;/li&gt;
&lt;li&gt;AND (data fresh OR forced_cache=True)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cache = {&amp;quot;exists&amp;quot;:True,&amp;quot;fresh&amp;quot;:False}
forced_cache = True
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;use_cache = (
    cache[&amp;quot;exists&amp;quot;]
    and (cache[&amp;quot;fresh&amp;quot;] or forced_cache)
)

print(use_cache)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-5--fraud-detection-rule&#34;&gt;ğŸŸ¡ Puzzle 5 â€” Fraud Detection Rule&lt;/h2&gt;
&lt;p&gt;Transaction is suspicious if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;amount &amp;gt; 10_000&lt;/li&gt;
&lt;li&gt;OR (amount &amp;gt; 3_000 AND country not trusted)&lt;/li&gt;
&lt;li&gt;AND NOT internal_transfer&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tx = {&amp;quot;amount&amp;quot;:5000,&amp;quot;trusted&amp;quot;:False,&amp;quot;internal&amp;quot;:False}
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    (tx[&amp;quot;amount&amp;quot;] &amp;gt; 10000
    or (tx[&amp;quot;amount&amp;quot;] &amp;gt; 3000 and not tx[&amp;quot;trusted&amp;quot;]))
    and not tx[&amp;quot;internal&amp;quot;]
):
    print(&amp;quot;Suspicious&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-6--multi-level-access-control&#34;&gt;ğŸŸ¡ Puzzle 6 â€” Multi-Level Access Control&lt;/h2&gt;
&lt;p&gt;Access granted if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;role is &lt;code&gt;&amp;quot;admin&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;OR (role &lt;code&gt;&amp;quot;dev&amp;quot;&lt;/code&gt; AND env &lt;code&gt;&amp;quot;staging&amp;quot;&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;OR (role &lt;code&gt;&amp;quot;viewer&amp;quot;&lt;/code&gt; AND read_only=True)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user = {&amp;quot;role&amp;quot;:&amp;quot;dev&amp;quot;,&amp;quot;env&amp;quot;:&amp;quot;staging&amp;quot;,&amp;quot;read_only&amp;quot;:False}
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    user[&amp;quot;role&amp;quot;] == &amp;quot;admin&amp;quot;
    or (user[&amp;quot;role&amp;quot;] == &amp;quot;dev&amp;quot; and user[&amp;quot;env&amp;quot;] == &amp;quot;staging&amp;quot;)
    or (user[&amp;quot;role&amp;quot;] == &amp;quot;viewer&amp;quot; and user[&amp;quot;read_only&amp;quot;])
):
    print(&amp;quot;Access granted&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-7--intelligent-retry-logic&#34;&gt;ğŸŸ¡ Puzzle 7 â€” Intelligent Retry Logic&lt;/h2&gt;
&lt;p&gt;Retry request if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;status &amp;gt;= 500&lt;/li&gt;
&lt;li&gt;AND retries &amp;lt; 3&lt;/li&gt;
&lt;li&gt;AND NOT timeout&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;req = {&amp;quot;status&amp;quot;:502,&amp;quot;retries&amp;quot;:2,&amp;quot;timeout&amp;quot;:False}
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    req[&amp;quot;status&amp;quot;] &amp;gt;= 500
    and req[&amp;quot;retries&amp;quot;] &amp;lt; 3
    and not req[&amp;quot;timeout&amp;quot;]
):
    print(&amp;quot;Retry&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-8--data-pipeline-drop-rule&#34;&gt;ğŸ”µ Puzzle 8 â€” Data Pipeline Drop Rule&lt;/h2&gt;
&lt;p&gt;Drop record if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;missing required field&lt;/li&gt;
&lt;li&gt;OR invalid format&lt;/li&gt;
&lt;li&gt;OR (late arrival AND not backfill)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rec = {&amp;quot;missing&amp;quot;:False,&amp;quot;invalid&amp;quot;:False,&amp;quot;late&amp;quot;:True,&amp;quot;backfill&amp;quot;:False}
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    rec[&amp;quot;missing&amp;quot;]
    or rec[&amp;quot;invalid&amp;quot;]
    or (rec[&amp;quot;late&amp;quot;] and not rec[&amp;quot;backfill&amp;quot;])
):
    print(&amp;quot;Drop&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-9--alert-noise-reduction&#34;&gt;ğŸ”µ Puzzle 9 â€” Alert Noise Reduction&lt;/h2&gt;
&lt;p&gt;Send alert if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;severity â‰¥ 7&lt;/li&gt;
&lt;li&gt;AND (new_error OR error_rate &amp;gt; threshold)&lt;/li&gt;
&lt;li&gt;AND NOT already_alerted&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    sev &amp;gt;= 7
    and (new_error or rate &amp;gt; threshold)
    and not alerted
):
    print(&amp;quot;Alert&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-10--smart-autoscaling-decision&#34;&gt;ğŸ”µ Puzzle 10 â€” Smart Autoscaling Decision&lt;/h2&gt;
&lt;p&gt;Scale up if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cpu &amp;gt; 70 AND mem &amp;gt; 70&lt;/li&gt;
&lt;li&gt;OR queue &amp;gt; 1000&lt;/li&gt;
&lt;li&gt;AND NOT maintenance&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    ((cpu &amp;gt; 70 and mem &amp;gt; 70) or queue &amp;gt; 1000)
    and not maintenance
):
    print(&amp;quot;Scale up&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-11--complex-log-filtering&#34;&gt;ğŸ”´ Puzzle 11 â€” Complex Log Filtering&lt;/h2&gt;
&lt;p&gt;Keep log if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;level is ERROR or WARN&lt;/li&gt;
&lt;li&gt;AND service not deprecated&lt;/li&gt;
&lt;li&gt;AND (env is prod OR flagged)&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    log[&amp;quot;level&amp;quot;] in (&amp;quot;ERROR&amp;quot;,&amp;quot;WARN&amp;quot;)
    and not log[&amp;quot;deprecated&amp;quot;]
    and (log[&amp;quot;env&amp;quot;] == &amp;quot;prod&amp;quot; or log[&amp;quot;flagged&amp;quot;])
):
    kept.append(log)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-12--deployment-gate&#34;&gt;ğŸ”´ Puzzle 12 â€” Deployment Gate&lt;/h2&gt;
&lt;p&gt;Allow deploy if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tests passed&lt;/li&gt;
&lt;li&gt;AND security approved&lt;/li&gt;
&lt;li&gt;AND (low_risk OR manager_approved)&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    tests_ok
    and security_ok
    and (low_risk or manager_ok)
):
    print(&amp;quot;Deploy&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-13--feature-flag-evaluation&#34;&gt;ğŸ”´ Puzzle 13 â€” Feature Flag Evaluation&lt;/h2&gt;
&lt;p&gt;Enable feature if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;flag_on&lt;/li&gt;
&lt;li&gt;AND user_beta&lt;/li&gt;
&lt;li&gt;AND NOT region_blocked&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if flag and beta and not blocked:
    enable()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-14--dead-letter-queue-rule&#34;&gt;ğŸ”´ Puzzle 14 â€” Dead Letter Queue Rule&lt;/h2&gt;
&lt;p&gt;Send to DLQ if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;retries exhausted&lt;/li&gt;
&lt;li&gt;OR malformed payload&lt;/li&gt;
&lt;li&gt;AND NOT manual_override&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    (retries == 0 or malformed)
    and not manual_override
):
    send_dlq()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-15--ai-model-safety-gate&#34;&gt;ğŸ”´ Puzzle 15 â€” AI Model Safety Gate&lt;/h2&gt;
&lt;p&gt;Allow response if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;confidence &amp;gt; 0.9&lt;/li&gt;
&lt;li&gt;AND NOT hallucinated&lt;/li&gt;
&lt;li&gt;AND (verified_source OR human_reviewed)&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    conf &amp;gt; 0.9
    and not hallucinated
    and (verified or reviewed)
):
    respond()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-16--distributed-lock-acquisition&#34;&gt;ğŸŸ£ Puzzle 16 â€” Distributed Lock Acquisition&lt;/h2&gt;
&lt;p&gt;Acquire lock if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lock free&lt;/li&gt;
&lt;li&gt;OR expired&lt;/li&gt;
&lt;li&gt;AND requester authorized&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if (
    (free or expired)
    and authorized
):
    acquire()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-17--multi-region-failover&#34;&gt;ğŸŸ£ Puzzle 17 â€” Multi-Region Failover&lt;/h2&gt;
&lt;p&gt;Failover if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;primary down&lt;/li&gt;
&lt;li&gt;AND secondary healthy&lt;/li&gt;
&lt;li&gt;AND NOT maintenance_window&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if primary_down and secondary_ok and not maintenance:
    failover()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-18--abuse-detection&#34;&gt;ğŸŸ£ Puzzle 18 â€” Abuse Detection&lt;/h2&gt;
&lt;p&gt;Block user if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reports &amp;gt; 5&lt;/li&gt;
&lt;li&gt;AND (spam OR harassment)&lt;/li&gt;
&lt;li&gt;AND NOT verified_user&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if reports &amp;gt; 5 and (spam or harassment) and not verified:
    block()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-19--financial-transaction-approval&#34;&gt;ğŸŸ£ Puzzle 19 â€” Financial Transaction Approval&lt;/h2&gt;
&lt;p&gt;Approve if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;balance sufficient&lt;/li&gt;
&lt;li&gt;AND not frozen&lt;/li&gt;
&lt;li&gt;AND (trusted_merchant OR small_amount)&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if balance &amp;gt;= amount and not frozen and (trusted or amount &amp;lt; 50):
    approve()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-puzzle-20--senior-level-thinking&#34;&gt;ğŸŸ£ Puzzle 20 â€” Senior-Level Thinking&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Explain (no code):&lt;/strong&gt;&lt;br&gt;
Why grouping conditions incorrectly can cause:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;security bugs&lt;/li&gt;
&lt;li&gt;outages&lt;/li&gt;
&lt;li&gt;financial loss&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;p&gt;Because logical precedence mistakes (&lt;code&gt;and&lt;/code&gt; / &lt;code&gt;or&lt;/code&gt;) can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bypass security&lt;/li&gt;
&lt;li&gt;trigger actions unintentionally&lt;/li&gt;
&lt;li&gt;scale systems incorrectly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Senior engineers design logic &lt;strong&gt;defensively&lt;/strong&gt;.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-truth&#34;&gt;ğŸ Final Truth&lt;/h2&gt;
&lt;p&gt;If you master &lt;strong&gt;logic composition&lt;/strong&gt;,&lt;br&gt;
you master &lt;strong&gt;engineering power&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Teach this.
Spread this.
Make engineers better.&lt;/p&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-17 â€” Basic NLP (V1)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-17-nlp-basic-v1/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-17-nlp-basic-v1/</guid>
      <description>&lt;h2 id=&#34;-why-this-document-exists&#34;&gt;ğŸŒ Why This Document Exists&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; a pure NLP library tutorial.&lt;/p&gt;
&lt;p&gt;This is &lt;strong&gt;algorithmic thinking for NLP engineers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you want to work at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google (Search, Gemini)&lt;/li&gt;
&lt;li&gt;OpenAI (ChatGPT, alignment)&lt;/li&gt;
&lt;li&gt;AWS (Comprehend, Bedrock)&lt;/li&gt;
&lt;li&gt;Microsoft (Copilot, Bing)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You must &lt;strong&gt;master strings, text, and scale&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-a-short-story--messi--language&#34;&gt;âš½ A Short Story â€” Messi &amp;amp; Language&lt;/h1&gt;
&lt;p&gt;Messi does not speak with words on the field.&lt;/p&gt;
&lt;p&gt;He speaks with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;movement&lt;/li&gt;
&lt;li&gt;timing&lt;/li&gt;
&lt;li&gt;patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Language is the same.&lt;/p&gt;
&lt;p&gt;Before transformersâ€¦
Before LLMsâ€¦&lt;/p&gt;
&lt;p&gt;There were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;characters&lt;/li&gt;
&lt;li&gt;tokens&lt;/li&gt;
&lt;li&gt;frequencies&lt;/li&gt;
&lt;li&gt;distributions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This book trains your &lt;strong&gt;text intuition&lt;/strong&gt; â€” not just syntax.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-python-nlp--20-string-problems&#34;&gt;ğŸ† Python NLP â€” 20 String Problems&lt;/h1&gt;
&lt;p&gt;Difficulty increases &lt;strong&gt;gradually&lt;/strong&gt;.&lt;br&gt;
Try before opening solutions.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-1--word-count-warm-up&#34;&gt;ğŸŸ¢ NLP 1 â€” Word Count (Warm-Up)&lt;/h2&gt;
&lt;p&gt;Messi speaks:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;messi plays football and messi inspires the world&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Count how many times each word appears.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;counts = {}
for w in text.split():
    counts[w] = counts.get(w, 0) + 1
print(counts)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-2--unique-vocabulary-size&#34;&gt;ğŸŸ¢ NLP 2 â€” Unique Vocabulary Size&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Count how many &lt;strong&gt;unique words&lt;/strong&gt; are in the text.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vocab = set(text.split())
print(len(vocab))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-3--capitalization-detection&#34;&gt;ğŸŸ¢ NLP 3 â€” Capitalization Detection&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentence = &amp;quot;Messi Is The Greatest&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return all words that start with a capital letter.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;caps = [w for w in sentence.split() if w[0].isupper()]
print(caps)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-4--lowercase-normalization&#34;&gt;ğŸŸ¢ NLP 4 â€” Lowercase Normalization&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Convert text to lowercase and remove extra spaces.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;  Messi   Plays   Football &amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;clean = &amp;quot; &amp;quot;.join(text.lower().split())
print(clean)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-5--character-frequency&#34;&gt;ğŸŸ¡ NLP 5 â€” Character Frequency&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Count how often each character appears (ignore spaces).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;messi&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;freq = {}
for c in text:
    if c != &amp;quot; &amp;quot;:
        freq[c] = freq.get(c, 0) + 1
print(freq)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-6--find-keywords-in-text&#34;&gt;ğŸŸ¡ NLP 6 â€” Find Keywords in Text&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;keywords = [&amp;quot;goal&amp;quot;, &amp;quot;win&amp;quot;, &amp;quot;champion&amp;quot;]
text = &amp;quot;messi scores a goal to win the match&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return keywords that appear in text.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;found = []
for k in keywords:
    if k in text:
        found.append(k)
print(found)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-7--sentence-length-analyzer&#34;&gt;ğŸŸ¡ NLP 7 â€” Sentence Length Analyzer&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentences = [
  &amp;quot;Messi plays football&amp;quot;,
  &amp;quot;Messi inspires millions of people around the world&amp;quot;
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return sentence lengths (in words).&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lengths = [len(s.split()) for s in sentences]
print(lengths)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-8--stopword-removal&#34;&gt;ğŸŸ¡ NLP 8 â€” Stopword Removal&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stopwords = {&amp;quot;the&amp;quot;,&amp;quot;and&amp;quot;,&amp;quot;is&amp;quot;}
text = &amp;quot;messi is the best and the greatest&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filtered = [w for w in text.split() if w not in stopwords]
print(&amp;quot; &amp;quot;.join(filtered))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-9--bigram-generation&#34;&gt;ğŸ”µ NLP 9 â€” Bigram Generation&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;messi wins world cup&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Generate word bigrams.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;words = text.split()
bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]
print(bigrams)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-10--most-frequent-word&#34;&gt;ğŸ”µ NLP 10 â€” Most Frequent Word&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;messi messi goal goal goal win&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;freq = {}
for w in text.split():
    freq[w] = freq.get(w, 0) + 1
print(max(freq, key=freq.get))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-11--prefix-matching-search-autocomplete&#34;&gt;ğŸ”µ NLP 11 â€” Prefix Matching (Search Autocomplete)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;words = [&amp;quot;messi&amp;quot;,&amp;quot;message&amp;quot;,&amp;quot;meta&amp;quot;,&amp;quot;goal&amp;quot;]
prefix = &amp;quot;me&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print([w for w in words if w.startswith(prefix)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-12--suffix-detection&#34;&gt;ğŸ”µ NLP 12 â€” Suffix Detection&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Find words ending with &lt;code&gt;&amp;quot;ing&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;words = [&amp;quot;playing&amp;quot;,&amp;quot;played&amp;quot;,&amp;quot;scoring&amp;quot;,&amp;quot;score&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print([w for w in words if w.endswith(&amp;quot;ing&amp;quot;)])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-13--streaming-word-count-big-data-mindset&#34;&gt;ğŸ”´ NLP 13 â€” Streaming Word Count (Big Data Mindset)&lt;/h2&gt;
&lt;p&gt;Messages arrive one by one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stream = [&amp;quot;messi scores&amp;quot;, &amp;quot;messi wins&amp;quot;, &amp;quot;scores again&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;counts = {}
for msg in stream:
    for w in msg.split():
        counts[w] = counts.get(w, 0) + 1
print(counts)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-14--mapreduce-word-count&#34;&gt;ğŸ”´ NLP 14 â€” MapReduce (Word Count)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Map:&lt;/strong&gt; emit &lt;code&gt;(word, 1)&lt;/code&gt;
&lt;strong&gt;Reduce:&lt;/strong&gt; sum values&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mapped = []
for s in stream:
    for w in s.split():
        mapped.append((w,1))

reduced = {}
for w,v in mapped:
    reduced[w] = reduced.get(w,0) + v

print(reduced)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-15--longest-word-in-corpus&#34;&gt;ğŸ”´ NLP 15 â€” Longest Word in Corpus&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;messi demonstrates extraordinary football intelligence&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;words = text.split()
print(max(words, key=len))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-16--named-entity-heuristic&#34;&gt;ğŸ”´ NLP 16 â€” Named Entity Heuristic&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Rule:&lt;/strong&gt;
Words starting with capital = entity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;Messi plays for Argentina&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;entities = [w for w in text.split() if w[0].isupper()]
print(entities)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-17--sentence-tokenization&#34;&gt;ğŸ”´ NLP 17 â€” Sentence Tokenization&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;Messi scored. Fans celebrated. History written.&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentences = [s.strip() for s in text.split(&amp;quot;.&amp;quot;) if s]
print(sentences)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-18--text-similarity-bag-of-words&#34;&gt;ğŸ”´ NLP 18 â€” Text Similarity (Bag of Words)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = &amp;quot;messi scores goals&amp;quot;
b = &amp;quot;messi scores&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sa, sb = set(a.split()), set(b.split())
print(len(sa &amp;amp; sb) / len(sa | sb))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-19--detect-shouting-uppercase-ratio&#34;&gt;ğŸ”´ NLP 19 â€” Detect Shouting (Uppercase Ratio)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;GOAL GOAL Messi&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;caps = sum(1 for c in text if c.isupper())
print(caps / len(text) &amp;gt; 0.5)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-20--real-nlp-interview-question&#34;&gt;ğŸ”´ NLP 20 â€” Real NLP Interview Question&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Detect duplicate sentences.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentences = [
  &amp;quot;messi scores&amp;quot;,
  &amp;quot;messi wins&amp;quot;,
  &amp;quot;messi scores&amp;quot;
]
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seen = set()
dups = []

for s in sentences:
    if s in seen:
        dups.append(s)
    seen.add(s)

print(dups)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-message&#34;&gt;ğŸ¯ Final Message&lt;/h2&gt;
&lt;p&gt;Before:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT&lt;/li&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;Transformers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;strings&lt;/li&gt;
&lt;li&gt;loops&lt;/li&gt;
&lt;li&gt;counters&lt;/li&gt;
&lt;li&gt;patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you &lt;strong&gt;master this page&lt;/strong&gt;,
you understand &lt;strong&gt;NLP from the ground up&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>LC-TEST-18 â€” Basic NLP (V2)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-18-nlp-basic-v2/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-18-nlp-basic-v2/</guid>
      <description>&lt;h2 id=&#34;-what-is-tf-idf-intuition--math&#34;&gt;ğŸ“˜ What Is TF-IDF? (Intuition + Math)&lt;/h2&gt;
&lt;p&gt;TF-IDF stands for:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Term Frequency â€“ Inverse Document Frequency&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It answers &lt;strong&gt;one fundamental NLP question&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â Which words are important in this document,&lt;br&gt;
compared to the entire collection? â&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-math-step-by-step&#34;&gt;ğŸ§® The Math (Step by Step)&lt;/h2&gt;
&lt;h3 id=&#34;1-term-frequency-tf&#34;&gt;1ï¸âƒ£ Term Frequency (TF)&lt;/h3&gt;
&lt;p&gt;How often a word appears &lt;strong&gt;inside one document&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;$$
\text{TF}(w, d) =
\frac{\text{count of } w \text{ in document } d}
{\text{total words in document } d}
$$&lt;/p&gt;
&lt;p&gt;ğŸ‘‰ Measures &lt;strong&gt;local importance&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-document-frequency-df&#34;&gt;2ï¸âƒ£ Document Frequency (DF)&lt;/h3&gt;
&lt;p&gt;How many documents contain the word.&lt;/p&gt;
&lt;p&gt;$$
\text{DF}(w) = \text{number of documents containing } w
$$&lt;/p&gt;
&lt;p&gt;ğŸ‘‰ Measures &lt;strong&gt;how common a word is across documents&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-inverse-document-frequency-idf&#34;&gt;3ï¸âƒ£ Inverse Document Frequency (IDF)&lt;/h3&gt;
&lt;p&gt;Penalizes words appearing in many documents.&lt;/p&gt;
&lt;p&gt;$$
\text{IDF}(w) = \log\left(\frac{N}{\text{DF}(w)}\right)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$N$ = total number of documents&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ğŸ‘‰ Measures &lt;strong&gt;global rarity&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-tf-idf-final-score&#34;&gt;4ï¸âƒ£ TF-IDF (Final Score)&lt;/h3&gt;
&lt;p&gt;$$
\text{TF-IDF}(w, d) =
\text{TF}(w, d) \times \text{IDF}(w)
$$&lt;/p&gt;
&lt;p&gt;This is the &lt;strong&gt;importance score&lt;/strong&gt; used in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;search engines&lt;/li&gt;
&lt;li&gt;document ranking&lt;/li&gt;
&lt;li&gt;classical NLP systems&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-worked-example-very-important&#34;&gt;ğŸ§ª Worked Example (Very Important)&lt;/h2&gt;
&lt;h3 id=&#34;corpus-3-documents&#34;&gt;Corpus (3 documents)&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;D1: &amp;quot;messi scores goals&amp;quot;
D2: &amp;quot;goals win matches&amp;quot;
D3: &amp;quot;messi inspires fans&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;step-1-vocabulary&#34;&gt;Step 1: Vocabulary&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;{messi, scores, goals, win, matches, inspires, fans}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;step-2-document-frequency-df&#34;&gt;Step 2: Document Frequency (DF)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Word&lt;/th&gt;
&lt;th&gt;DF&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;messi&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;goals&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scores&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;win&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;matches&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inspires&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fans&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;step-3-idf-assume-natural-log&#34;&gt;Step 3: IDF (Assume natural log)&lt;/h3&gt;
&lt;p&gt;$$
\text{IDF}(\text{messi}) = \log\left(\frac{3}{2}\right)
$$&lt;/p&gt;
&lt;p&gt;$$
\text{IDF}(\text{scores}) = \log\left(\frac{3}{1}\right)
$$&lt;/p&gt;
&lt;p&gt;ğŸ‘‰ &lt;code&gt;&amp;quot;scores&amp;quot;&lt;/code&gt; appears in fewer documents
ğŸ‘‰ therefore it has &lt;strong&gt;higher IDF&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;step-4-tf-idf-in-document-1&#34;&gt;Step 4: TF-IDF in Document 1&lt;/h3&gt;
&lt;p&gt;Document 1 contains &lt;strong&gt;3 words&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;messi scores goals
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Word&lt;/th&gt;
&lt;th&gt;TF&lt;/th&gt;
&lt;th&gt;IDF&lt;/th&gt;
&lt;th&gt;TF-IDF&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;messi&lt;/td&gt;
&lt;td&gt;1/3&lt;/td&gt;
&lt;td&gt;log(3/2)&lt;/td&gt;
&lt;td&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scores&lt;/td&gt;
&lt;td&gt;1/3&lt;/td&gt;
&lt;td&gt;log(3/1)&lt;/td&gt;
&lt;td&gt;HIGH&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;goals&lt;/td&gt;
&lt;td&gt;1/3&lt;/td&gt;
&lt;td&gt;log(3/2)&lt;/td&gt;
&lt;td&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;âœ… &lt;strong&gt;&amp;ldquo;scores&amp;rdquo; becomes the most important word&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-interviewers-want-you-to-say&#34;&gt;ğŸ¯ What Interviewers Want You To Say&lt;/h2&gt;
&lt;p&gt;If asked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œExplain TF-IDFâ€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A &lt;strong&gt;perfect answer&lt;/strong&gt; is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œTF-IDF measures how important a word is to a document
relative to the whole corpus.&lt;/p&gt;
&lt;p&gt;It boosts words that are frequent in one document
but rare across documents.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-tf-idf-still-matters-today&#34;&gt;ğŸ”— Why TF-IDF Still Matters Today&lt;/h2&gt;
&lt;p&gt;Even in the age of GPT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Search engines still use it&lt;/li&gt;
&lt;li&gt;Keyword extraction uses it&lt;/li&gt;
&lt;li&gt;It explains &lt;strong&gt;why attention exists&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;It is the &lt;strong&gt;conceptual ancestor of embeddings&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you understand TF-IDF deeply,
&lt;strong&gt;transformers feel natural â€” not magical&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-document-exists&#34;&gt;ğŸŒ Why This Document Exists&lt;/h2&gt;
&lt;p&gt;Large Language Models do &lt;strong&gt;not begin with transformers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;They begin with a simple question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhich words matter?â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TF-IDF is the &lt;strong&gt;bridge&lt;/strong&gt; between:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;raw text&lt;/li&gt;
&lt;li&gt;statistics&lt;/li&gt;
&lt;li&gt;semantic meaning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Every NLP engineer &lt;strong&gt;must master this&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-messi--meaning--a-short-story&#34;&gt;âš½ Messi &amp;amp; Meaning â€” A Short Story&lt;/h1&gt;
&lt;p&gt;Messi touches the ball &lt;strong&gt;less than others&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Yet every touch &lt;strong&gt;matters more&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Common words are like defenders:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they are everywhere&lt;/li&gt;
&lt;li&gt;they mean little&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Rare words are like Messi:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fewer appearances&lt;/li&gt;
&lt;li&gt;massive impact&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That is &lt;strong&gt;TF-IDF&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-python-nlp--tf-idf-edition-20-problems&#34;&gt;ğŸ† Python NLP â€” TF-IDF Edition (20 Problems)&lt;/h1&gt;
&lt;p&gt;Difficulty increases gradually.&lt;br&gt;
&lt;strong&gt;Do not skip intuition.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-1--tokenization&#34;&gt;ğŸŸ¢ TF-IDF 1 â€” Tokenization&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;doc = &amp;quot;messi scores beautiful goals&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Convert sentence into tokens.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tokens = doc.split()
print(tokens)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-2--term-frequency-tf&#34;&gt;ğŸŸ¢ TF-IDF 2 â€” Term Frequency (TF)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TF(word) = count(word) / total words&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;doc = &amp;quot;messi scores goals goals&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;words = doc.split()
tf = {}

for w in words:
    tf[w] = tf.get(w, 0) + 1

for w in tf:
    tf[w] /= len(words)

print(tf)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-3--vocabulary-from-corpus&#34;&gt;ğŸŸ¢ TF-IDF 3 â€” Vocabulary from Corpus&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;docs = [
  &amp;quot;messi scores goals&amp;quot;,
  &amp;quot;goals win matches&amp;quot;,
  &amp;quot;messi inspires fans&amp;quot;
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Extract unique vocabulary.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vocab = set()
for d in docs:
    vocab |= set(d.split())
print(vocab)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-4--document-frequency-df&#34;&gt;ğŸŸ¡ TF-IDF 4 â€” Document Frequency (DF)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;DF(word) = number of documents containing word&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = {}
for word in vocab:
    df[word] = sum(word in d.split() for d in docs)
print(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-5--inverse-document-frequency-idf&#34;&gt;ğŸŸ¡ TF-IDF 5 â€” Inverse Document Frequency (IDF)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;IDF(word) = log(N / DF)&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math

N = len(docs)
idf = {w: math.log(N / df[w]) for w in df}
print(idf)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-6--tf-idf-for-one-document&#34;&gt;ğŸŸ¡ TF-IDF 6 â€” TF-IDF for One Document&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;doc = docs[0]
words = doc.split()

tf = {}
for w in words:
    tf[w] = tf.get(w, 0) + 1
for w in tf:
    tf[w] /= len(words)

tfidf = {w: tf[w] * idf[w] for w in tf}
print(tfidf)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-7--full-tf-idf-matrix&#34;&gt;ğŸ”µ TF-IDF 7 â€” Full TF-IDF Matrix&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;matrix = []

for d in docs:
    words = d.split()
    tf = {}
    for w in words:
        tf[w] = tf.get(w, 0) + 1
    for w in tf:
        tf[w] /= len(words)

    vec = {w: tf.get(w, 0) * idf[w] for w in vocab}
    matrix.append(vec)

print(matrix)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-8--important-words-in-document&#34;&gt;ğŸ”µ TF-IDF 8 â€” Important Words in Document&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Find top-1 TF-IDF word.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;top = max(tfidf, key=tfidf.get)
print(top)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-9--remove-stopwords&#34;&gt;ğŸ”µ TF-IDF 9 â€” Remove Stopwords&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stopwords = {&amp;quot;the&amp;quot;,&amp;quot;and&amp;quot;,&amp;quot;is&amp;quot;,&amp;quot;of&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filtered_docs = []
for d in docs:
    filtered_docs.append(
        &amp;quot; &amp;quot;.join(w for w in d.split() if w not in stopwords)
    )
print(filtered_docs)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-10--normalize-vectors-cosine-prep&#34;&gt;ğŸ”µ TF-IDF 10 â€” Normalize Vectors (Cosine Prep)&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math

def norm(vec):
    return math.sqrt(sum(v*v for v in vec.values()))

print(norm(tfidf))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-11--cosine-similarity&#34;&gt;ğŸ”´ TF-IDF 11 â€” Cosine Similarity&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = &amp;quot;messi scores goals&amp;quot;
b = &amp;quot;messi inspires fans&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def cosine(v1, v2):
    common = set(v1) &amp;amp; set(v2)
    num = sum(v1[w] * v2[w] for w in common)
    den = norm(v1) * norm(v2)
    return num / den if den else 0
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-12--find-most-similar-document&#34;&gt;ğŸ”´ TF-IDF 12 â€” Find Most Similar Document&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;query = matrix[0]
sims = [cosine(query, m) for m in matrix]
print(sims)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-13--keyword-extraction&#34;&gt;ğŸ”´ TF-IDF 13 â€” Keyword Extraction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Return words with TF-IDF &amp;gt; threshold.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;keywords = [w for w,v in tfidf.items() if v &amp;gt; 0.2]
print(keywords)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-14--streaming-tf-update&#34;&gt;ğŸ”´ TF-IDF 14 â€” Streaming TF Update&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stream = [&amp;quot;messi scores&amp;quot;, &amp;quot;scores again&amp;quot;]

tf = {}
total = 0

for s in stream:
    for w in s.split():
        tf[w] = tf.get(w, 0) + 1
        total += 1

for w in tf:
    tf[w] /= total

print(tf)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-15--big-data-mapreduce-tf&#34;&gt;ğŸ”´ TF-IDF 15 â€” Big Data MapReduce TF&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mapped = []
for d in docs:
    for w in d.split():
        mapped.append((w,1))

reduced = {}
for w,v in mapped:
    reduced[w] = reduced.get(w,0) + v

print(reduced)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-16--rare-word-detection&#34;&gt;ğŸ”´ TF-IDF 16 â€” Rare Word Detection&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Words with highest IDF.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(sorted(idf, key=idf.get, reverse=True))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-17--explainability-test&#34;&gt;ğŸ”´ TF-IDF 17 â€” Explainability Test&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;
Why does &lt;code&gt;&amp;quot;messi&amp;quot;&lt;/code&gt; sometimes get low TF-IDF?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Answer&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Because it appears in many documents,
its IDF is low, reducing its importance.
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-18--sklearn-comparison-industry-standard&#34;&gt;ğŸ”´ TF-IDF 18 â€” sklearn Comparison (Industry Standard)&lt;/h2&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.feature_extraction.text import TfidfVectorizer

vec = TfidfVectorizer()
X = vec.fit_transform(docs)

print(vec.get_feature_names_out())
print(X.toarray())
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-19--interview-question&#34;&gt;ğŸ”´ TF-IDF 19 â€” Interview Question&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Why TF-IDF fails for semantics?&lt;/strong&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Answer&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;It ignores word order, context, and meaning.
Synonyms are treated as unrelated.
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-tf-idf-20--bridge-to-transformers&#34;&gt;ğŸ”´ TF-IDF 20 â€” Bridge to Transformers&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;
What replaced TF-IDF?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Answer&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Word embeddings â†’ contextual embeddings â†’ transformers
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-message&#34;&gt;ğŸ¯ Final Message&lt;/h2&gt;
&lt;p&gt;TF-IDF teaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;importance&lt;/li&gt;
&lt;li&gt;rarity&lt;/li&gt;
&lt;li&gt;signal vs noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you understand this page:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;you understand &lt;strong&gt;why attention exists&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;you understand &lt;strong&gt;why embeddings work&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You are &lt;strong&gt;ready for real NLP&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>LC-TEST-19 â€” Basic NLP (V3)</title>
      <link>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-19-nlp-basic-v3/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2025-12-30-leetcode-interview-foundations/cs101-lc04-19-nlp-basic-v3/</guid>
      <description>&lt;h2 id=&#34;-why-this-document-exists&#34;&gt;ğŸŒ Why This Document Exists&lt;/h2&gt;
&lt;p&gt;This is &lt;strong&gt;not&lt;/strong&gt; a memorization guide.&lt;/p&gt;
&lt;p&gt;This is how &lt;strong&gt;real NLP engineers think&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;before transformers&lt;/li&gt;
&lt;li&gt;before embeddings&lt;/li&gt;
&lt;li&gt;before GPUs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Every great NLP system starts with:
&lt;strong&gt;text, counts, structure, and meaning&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-a-real-world-nlp-story--messi-beyond-football&#34;&gt;âš½ A Real-World NLP Story â€” Messi Beyond Football&lt;/h1&gt;
&lt;p&gt;Lionel Messi is often described as a footballer,&lt;br&gt;
but to millions of people, he represents &lt;strong&gt;discipline, creativity, and persistence&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;From Rosario to Barcelona, from criticism to championships,&lt;br&gt;
Messiâ€™s story is not just about goals.&lt;/p&gt;
&lt;p&gt;It is about &lt;strong&gt;patterns&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;repeated effort&lt;/li&gt;
&lt;li&gt;rare moments of brilliance&lt;/li&gt;
&lt;li&gt;quiet consistency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In language, meaning works the same way.&lt;/p&gt;
&lt;p&gt;Common words appear everywhere.&lt;br&gt;
Rare words define intent.&lt;/p&gt;
&lt;p&gt;This document treats Messiâ€™s story as &lt;strong&gt;data&lt;/strong&gt; â€”&lt;br&gt;
and trains you to &lt;strong&gt;extract meaning like an NLP engineer&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-corpus-input-text&#34;&gt;ğŸ“„ The Corpus (Input Text)&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Lionel Messi is one of the greatest football players in history.
Messi began his career in Argentina before joining Barcelona.
At Barcelona, Messi scored goals, created chances, and inspired fans.
Many people believe Messi changed the way football is played.
Despite fame, Messi remained disciplined and focused on the game.
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-nlp-interview--20-problems&#34;&gt;ğŸ† NLP Interview â€” 20 Problems&lt;/h1&gt;
&lt;p&gt;Difficulty increases gradually.
Try to &lt;strong&gt;think before coding&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-1--sentence-count&#34;&gt;ğŸŸ¢ NLP 1 â€” Sentence Count&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
How many sentences are in the corpus?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sentences = [s for s in text.split(&amp;quot;.&amp;quot;) if s.strip()]
print(len(sentences))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-2--word-tokenization&#34;&gt;ğŸŸ¢ NLP 2 â€” Word Tokenization&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Convert the corpus into a list of words (lowercased).&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;words = text.lower().replace(&amp;quot;.&amp;quot;, &amp;quot;&amp;quot;).split()
print(words)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-3--vocabulary-size&#34;&gt;ğŸŸ¢ NLP 3 â€” Vocabulary Size&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
How many unique words exist?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(len(set(words)))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-4--word-frequency-count&#34;&gt;ğŸŸ¡ NLP 4 â€” Word Frequency Count&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Count how many times each word appears.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;freq = {}
for w in words:
    freq[w] = freq.get(w, 0) + 1
print(freq)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-5--most-frequent-word&#34;&gt;ğŸŸ¡ NLP 5 â€” Most Frequent Word&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Find the most common word.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(max(freq, key=freq.get))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-6--stopword-removal&#34;&gt;ğŸŸ¡ NLP 6 â€” Stopword Removal&lt;/h2&gt;
&lt;p&gt;Remove: &lt;code&gt;{&amp;quot;the&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;and&amp;quot;, &amp;quot;of&amp;quot;, &amp;quot;to&amp;quot;, &amp;quot;in&amp;quot;}&lt;/code&gt;&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stop = {&amp;quot;the&amp;quot;,&amp;quot;is&amp;quot;,&amp;quot;and&amp;quot;,&amp;quot;of&amp;quot;,&amp;quot;to&amp;quot;,&amp;quot;in&amp;quot;}
filtered = [w for w in words if w not in stop]
print(filtered)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-7--capitalized-word-detection&#34;&gt;ğŸŸ¡ NLP 7 â€” Capitalized Word Detection&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Detect words that originally started with capital letters.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;caps = [w for w in text.split() if w[0].isupper()]
print(caps)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-8--average-sentence-length&#34;&gt;ğŸ”µ NLP 8 â€” Average Sentence Length&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Compute average words per sentence.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lengths = [len(s.split()) for s in sentences]
print(sum(lengths) / len(lengths))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-9--bigram-extraction&#34;&gt;ğŸ”µ NLP 9 â€” Bigram Extraction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Extract all word bigrams.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]
print(bigrams)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-10--keyword-search&#34;&gt;ğŸ”µ NLP 10 â€” Keyword Search&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Check if the word &lt;code&gt;&amp;quot;discipline&amp;quot;&lt;/code&gt; exists.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;discipline&amp;quot; in words)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-11--named-entity-heuristic&#34;&gt;ğŸ”µ NLP 11 â€” Named Entity Heuristic&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Rule:&lt;/strong&gt;
Words starting with capital letters = entities.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;entities = list(set(caps))
print(entities)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-12--document-frequency-df&#34;&gt;ğŸ”´ NLP 12 â€” Document Frequency (DF)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
How many sentences contain the word &lt;code&gt;&amp;quot;messi&amp;quot;&lt;/code&gt;?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(sum(&amp;quot;messi&amp;quot; in s.lower() for s in sentences))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-13--tf-calculation&#34;&gt;ğŸ”´ NLP 13 â€” TF Calculation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Compute TF for &lt;code&gt;&amp;quot;messi&amp;quot;&lt;/code&gt; in the whole corpus.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tf_messi = freq[&amp;quot;messi&amp;quot;] / len(words)
print(tf_messi)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-14--rare-word-detection&#34;&gt;ğŸ”´ NLP 14 â€” Rare Word Detection&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Find words appearing only once.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;rare = [w for w,c in freq.items() if c == 1]
print(rare)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-15--sentence-similarity-jaccard&#34;&gt;ğŸ”´ NLP 15 â€” Sentence Similarity (Jaccard)&lt;/h2&gt;
&lt;p&gt;Compare sentence 1 and 3.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;a = set(sentences[0].lower().split())
b = set(sentences[2].lower().split())
print(len(a &amp;amp; b) / len(a | b))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-16--duplicate-sentence-detection&#34;&gt;ğŸ”´ NLP 16 â€” Duplicate Sentence Detection&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;
Check if any sentence is duplicated.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;seen = set()
dup = False
for s in sentences:
    if s in seen:
        dup = True
    seen.add(s)
print(dup)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-17--streaming-word-count-big-data&#34;&gt;ğŸ”´ NLP 17 â€” Streaming Word Count (Big Data)&lt;/h2&gt;
&lt;p&gt;Messages arrive line by line.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;counts = {}
for s in sentences:
    for w in s.lower().split():
        counts[w] = counts.get(w, 0) + 1
print(counts)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-18--mapreduce-mindset&#34;&gt;ğŸ”´ NLP 18 â€” MapReduce Mindset&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Map:&lt;/strong&gt; &lt;code&gt;(word, 1)&lt;/code&gt;
&lt;strong&gt;Reduce:&lt;/strong&gt; sum&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Solution&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mapped = [(w,1) for w in words]
reduced = {}
for w,v in mapped:
    reduced[w] = reduced.get(w,0) + v
print(reduced)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-19--explainability-question&#34;&gt;ğŸ”´ NLP 19 â€” Explainability Question&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;
Why is &lt;code&gt;&amp;quot;messi&amp;quot;&lt;/code&gt; less informative than &lt;code&gt;&amp;quot;disciplined&amp;quot;&lt;/code&gt;?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Answer&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Because &amp;quot;messi&amp;quot; appears many times,
while &amp;quot;disciplined&amp;quot; is rare and carries more information.
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-nlp-20--bridge-to-modern-nlp&#34;&gt;ğŸ”´ NLP 20 â€” Bridge to Modern NLP&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;
Why does TF-IDF fail for deep semantics?&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;âœ… Answer&lt;/summary&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;It ignores word order, context, and meaning.
Synonyms are unrelated, and semantics are not captured.
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-message&#34;&gt;ğŸ¯ Final Message&lt;/h2&gt;
&lt;p&gt;If you can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;explain these problems&lt;/li&gt;
&lt;li&gt;code them calmly&lt;/li&gt;
&lt;li&gt;justify your choices&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You are &lt;strong&gt;interview-ready&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Before LLMs, there was understanding.
Before transformers, there was thinking.&lt;/p&gt;
&lt;p&gt;This is how &lt;strong&gt;real NLP engineers are made&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>Lecture 12 â€” Encoder, Decoder, and the Truth About How LLMs Are Trained</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-12-encoder-decoder-in-llms/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-12-encoder-decoder-in-llms/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~4â€“5 hours (core understanding lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-lecture-exists&#34;&gt;ğŸ§  Why This Lecture Exists&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Almost everyone uses LLMs.&lt;br&gt;
Very few understand how they are actually built.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Common confusion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â€œIs ChatGPT encoderâ€“decoder?â€&lt;/li&gt;
&lt;li&gt;â€œWhy only decoder?â€&lt;/li&gt;
&lt;li&gt;â€œWhat does freezing weights really mean?â€&lt;/li&gt;
&lt;li&gt;â€œHow does multimodal fit into this?â€&lt;/li&gt;
&lt;li&gt;â€œWhat exactly am I training when I fine-tune?â€&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This lecture answers &lt;strong&gt;all of that â€” clearly, from first principles&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-original-transformer-2017&#34;&gt;ğŸ§© The Original Transformer (2017)&lt;/h2&gt;
&lt;p&gt;The original Transformer had &lt;strong&gt;two parts&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Encoder  â†’  Decoder

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;encoder&#34;&gt;Encoder&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Reads the input&lt;/li&gt;
&lt;li&gt;Understands meaning&lt;/li&gt;
&lt;li&gt;Produces representations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;decoder&#34;&gt;Decoder&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generates output tokens&lt;/li&gt;
&lt;li&gt;Uses attention + autoregression&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was designed for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Machine Translation&lt;/li&gt;
&lt;li&gt;Summarization&lt;/li&gt;
&lt;li&gt;Seq2Seq tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-encoder-what-is-it-really-doing&#34;&gt;ğŸ§  Encoder: What Is It Really Doing?&lt;/h2&gt;
&lt;p&gt;Encoder properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sees the &lt;strong&gt;entire input at once&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Bidirectional attention&lt;/li&gt;
&lt;li&gt;Builds rich representations&lt;/li&gt;
&lt;li&gt;Does &lt;strong&gt;not generate text&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT&lt;/li&gt;
&lt;li&gt;RoBERTa&lt;/li&gt;
&lt;li&gt;ViT (vision encoder)&lt;/li&gt;
&lt;li&gt;Audio encoders&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Encoders understand. They donâ€™t speak.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-decoder-what-is-it-really-doing&#34;&gt;ğŸ§  Decoder: What Is It Really Doing?&lt;/h2&gt;
&lt;p&gt;Decoder properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generates tokens &lt;strong&gt;one by one&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Causal (masked) attention&lt;/li&gt;
&lt;li&gt;Autoregressive&lt;/li&gt;
&lt;li&gt;Can reason, plan, and explain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;LLaMA&lt;/li&gt;
&lt;li&gt;Mistral&lt;/li&gt;
&lt;li&gt;Qwen&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Decoders speak, reason, and act.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-chatgpt-is-decoder-only&#34;&gt;â“ Why ChatGPT Is Decoder-Only&lt;/h2&gt;
&lt;p&gt;Key insight:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;If you want open-ended generation, you only need a decoder.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decoder can read context (prompt)&lt;/li&gt;
&lt;li&gt;Decoder can generate indefinitely&lt;/li&gt;
&lt;li&gt;Encoder is not required for generation&lt;/li&gt;
&lt;li&gt;Simpler architecture&lt;/li&gt;
&lt;li&gt;Scales better&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So ChatGPT is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
Text â†’ Decoder â†’ Next Token â†’ Next Token â†’ ...

&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-decoder-only-training-gpt-style&#34;&gt;ğŸ§  Decoder-Only Training (GPT Style)&lt;/h2&gt;
&lt;p&gt;Training objective:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Predict the next token&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;&amp;quot;I love deep&amp;quot; â†’ predict &amp;quot;learning&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This single objective leads to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Language understanding&lt;/li&gt;
&lt;li&gt;Reasoning&lt;/li&gt;
&lt;li&gt;Code generation&lt;/li&gt;
&lt;li&gt;Planning&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Understanding emerges from generation.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-encoderdecoder-models-still-important&#34;&gt;ğŸ§© Encoderâ€“Decoder Models (Still Important!)&lt;/h2&gt;
&lt;p&gt;Encoderâ€“decoder models are still used when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Input â‰  output&lt;/li&gt;
&lt;li&gt;Strong alignment is required&lt;/li&gt;
&lt;li&gt;Input is very long or structured&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;T5&lt;/li&gt;
&lt;li&gt;FLAN-T5&lt;/li&gt;
&lt;li&gt;Whisper (audio â†’ text)&lt;/li&gt;
&lt;li&gt;Translation systems&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-multimodal-llms-the-hybrid-truth&#34;&gt;ğŸ§  Multimodal LLMs: The Hybrid Truth&lt;/h2&gt;
&lt;p&gt;Most multimodal LLMs are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Encoder (image/audio/video)
        â†“
Projection / Adapter
        â†“
Decoder-only LLM
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CLIP â†’ LLaMA&lt;/li&gt;
&lt;li&gt;ViT â†’ GPT&lt;/li&gt;
&lt;li&gt;Audio encoder â†’ LLM&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Multimodal models are encoderâ€“decoder systems,
but the decoder is still the brain.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-encoders-are-usually-frozen&#34;&gt;ğŸ”— Why Encoders Are Usually Frozen&lt;/h2&gt;
&lt;p&gt;Encoders:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pretrained on massive data&lt;/li&gt;
&lt;li&gt;Expensive to retrain&lt;/li&gt;
&lt;li&gt;General-purpose&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we often:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â„ï¸ Freeze encoder&lt;/li&gt;
&lt;li&gt;ğŸ”§ Train adapter / projector&lt;/li&gt;
&lt;li&gt;ğŸ§  Fine-tune decoder lightly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This saves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute&lt;/li&gt;
&lt;li&gt;Data&lt;/li&gt;
&lt;li&gt;Stability&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-what-is-actually-trained-very-important&#34;&gt;ğŸ§  What Is Actually Trained? (Very Important)&lt;/h2&gt;
&lt;h3 id=&#34;pretraining&#34;&gt;Pretraining&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Train &lt;strong&gt;all weights&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Massive data&lt;/li&gt;
&lt;li&gt;Extremely expensive&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Train &lt;strong&gt;some weights&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Task-specific data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;instruction-tuning&#34;&gt;Instruction tuning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Train decoder to follow instructions&lt;/li&gt;
&lt;li&gt;Often freezes most layers&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-freezing-strategies&#34;&gt;ğŸ§© Freezing Strategies&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Strategy&lt;/th&gt;
&lt;th&gt;What Moves&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Full fine-tune&lt;/td&gt;
&lt;td&gt;Everything&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Freeze encoder&lt;/td&gt;
&lt;td&gt;Decoder only&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LoRA&lt;/td&gt;
&lt;td&gt;Small rank matrices&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Adapters&lt;/td&gt;
&lt;td&gt;Tiny modules&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prompt tuning&lt;/td&gt;
&lt;td&gt;No weights&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Most real-world systems do NOT full fine-tune.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-example-freezing-encoder&#34;&gt;ğŸ Example: Freezing Encoder&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for param in vision_encoder.parameters():
    param.requires_grad = False
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then train:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Projection layer&lt;/li&gt;
&lt;li&gt;LLM LoRA weights&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-lora-explained-simply&#34;&gt;ğŸ§  LoRA Explained Simply&lt;/h2&gt;
&lt;p&gt;LoRA:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Injects low-rank matrices&lt;/li&gt;
&lt;li&gt;Keeps original weights frozen&lt;/li&gt;
&lt;li&gt;Learns task-specific behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Benefits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cheap&lt;/li&gt;
&lt;li&gt;Stable&lt;/li&gt;
&lt;li&gt;Shareable&lt;/li&gt;
&lt;li&gt;Reversible&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;LoRA is how the world fine-tunes LLMs today.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-not-encoder-only-llms&#34;&gt;â“ Why Not Encoder-Only LLMs?&lt;/h2&gt;
&lt;p&gt;Encoder-only models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cannot generate freely&lt;/li&gt;
&lt;li&gt;Need a decoder for output&lt;/li&gt;
&lt;li&gt;Not conversational&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thatâ€™s why:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT â‰  ChatGPT&lt;/li&gt;
&lt;li&gt;ViT â‰  multimodal assistant&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-mental-model-remember-this-forever&#34;&gt;ğŸ§  Mental Model (Remember This Forever)&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Role&lt;/th&gt;
&lt;th&gt;Model Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Understand&lt;/td&gt;
&lt;td&gt;Encoder&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reason&lt;/td&gt;
&lt;td&gt;Decoder&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Speak&lt;/td&gt;
&lt;td&gt;Decoder&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Act&lt;/td&gt;
&lt;td&gt;Decoder + Tools&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;See&lt;/td&gt;
&lt;td&gt;Vision Encoder&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hear&lt;/td&gt;
&lt;td&gt;Audio Encoder&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-student-knowledge-check-hidden&#34;&gt;ğŸ§ª Student Knowledge Check (Hidden)&lt;/h2&gt;
&lt;h3 id=&#34;q1--objective&#34;&gt;Q1 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why can ChatGPT work without an encoder?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Because a decoder can read context and generate text autoregressively.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2--mcq&#34;&gt;Q2 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which model is encoder-only?&lt;/p&gt;
&lt;p&gt;A. GPT
B. LLaMA
C. BERT
D. ChatGPT&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. BERT&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3--mcq&#34;&gt;Q3 â€” MCQ&lt;/h3&gt;
&lt;p&gt;What is usually frozen in multimodal LLMs?&lt;/p&gt;
&lt;p&gt;A. Decoder
B. Encoder
C. Tokenizer
D. Loss function&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Encoder&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4--objective&#34;&gt;Q4 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why use LoRA instead of full fine-tuning?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;To reduce cost, preserve knowledge, and improve stability.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5--objective&#34;&gt;Q5 â€” Objective&lt;/h3&gt;
&lt;p&gt;Who is the â€œbrainâ€ of a multimodal LLM?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;The decoder-only LLM.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-reflection&#34;&gt;ğŸŒ± Final Reflection&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;If intelligence emerges from predicting the next token, what does that say about human thinking?&lt;/summary&gt;
  &lt;p&gt;That reasoning may emerge from sequence prediction guided by experience.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-takeaways-burn-this-in&#34;&gt;âœ… Final Takeaways (Burn This In)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ChatGPT is &lt;strong&gt;decoder-only&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Encoders understand, decoders generate&lt;/li&gt;
&lt;li&gt;Multimodal = encoders + decoder brain&lt;/li&gt;
&lt;li&gt;Freezing is strategy, not weakness&lt;/li&gt;
&lt;li&gt;Fine-tuning is about &lt;em&gt;what to move&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>Lecture 13 â€” Real-World LLM Engineer &amp; Research Scientist Interview (Top Tech Level)</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-13-llm-interview/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-13-llm-interview/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~6â€“8 hours (elite interview preparation)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-lecture-exists&#34;&gt;ğŸ¯ Why This Lecture Exists&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Top tech companies do not test tools.&lt;br&gt;
They test thinking.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This lecture simulates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenAI&lt;/li&gt;
&lt;li&gt;Google DeepMind / Gemini&lt;/li&gt;
&lt;li&gt;Anthropic&lt;/li&gt;
&lt;li&gt;Meta FAIR&lt;/li&gt;
&lt;li&gt;Microsoft Research&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Focus:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fundamentals&lt;/li&gt;
&lt;li&gt;Architecture&lt;/li&gt;
&lt;li&gt;Training&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;li&gt;Safety&lt;/li&gt;
&lt;li&gt;Systems thinking&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-i--core-llm-architecture-q1q10&#34;&gt;ğŸ§  Part I â€” Core LLM Architecture (Q1â€“Q10)&lt;/h1&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q1-mcq&#34;&gt;Q1 (MCQ)&lt;/h3&gt;
&lt;p&gt;Why are most modern LLMs &lt;em&gt;decoder-only&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;A. Encoders are too slow&lt;br&gt;
B. Decoders can model autoregressive generation&lt;br&gt;
C. Encoders cannot scale&lt;br&gt;
D. Decoders use less memory&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;. Decoder-only models naturally support autoregressive next-token prediction, which aligns perfectly with text generation.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2-objective&#34;&gt;Q2 (Objective)&lt;/h3&gt;
&lt;p&gt;What does â€œautoregressiveâ€ mean in LLMs?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Predicting the next token conditioned on all previous tokens; generation proceeds sequentially.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3-mcq&#34;&gt;Q3 (MCQ)&lt;/h3&gt;
&lt;p&gt;What mask is used in decoder self-attention?&lt;/p&gt;
&lt;p&gt;A. Padding mask&lt;br&gt;
B. Causal (look-ahead) mask&lt;br&gt;
C. Bidirectional mask&lt;br&gt;
D. Cross-attention mask&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;. Causal masks prevent the model from seeing future tokens during training.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4-objective&#34;&gt;Q4 (Objective)&lt;/h3&gt;
&lt;p&gt;Why are encoders still useful in multimodal systems?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Encoders excel at representation learning (images, audio, documents) which can be fused into LLMs.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5-mcq&#34;&gt;Q5 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which model is encoderâ€“decoder?&lt;/p&gt;
&lt;p&gt;A. GPT-4&lt;br&gt;
B. LLaMA&lt;br&gt;
C. T5&lt;br&gt;
D. PaLM&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. T5 uses an encoderâ€“decoder Transformer architecture.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q6-objective&#34;&gt;Q6 (Objective)&lt;/h3&gt;
&lt;p&gt;What is the role of positional encoding?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;It injects token order information into attention-based models which are otherwise permutation-invariant.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q7-mcq&#34;&gt;Q7 (MCQ)&lt;/h3&gt;
&lt;p&gt;Why is self-attention preferred over RNNs?&lt;/p&gt;
&lt;p&gt;A. Faster training&lt;br&gt;
B. Parallelism&lt;br&gt;
C. Long-range dependency modeling&lt;br&gt;
D. All of the above&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;D&lt;/strong&gt;. Self-attention improves speed, scalability, and contextual understanding.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q8-objective&#34;&gt;Q8 (Objective)&lt;/h3&gt;
&lt;p&gt;What limits context length in Transformers?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Quadratic attention cost in sequence length (O(nÂ²)).&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q9-mcq&#34;&gt;Q9 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which improves long-context handling?&lt;/p&gt;
&lt;p&gt;A. FlashAttention&lt;br&gt;
B. Sparse attention&lt;br&gt;
C. RoPE&lt;br&gt;
D. All of the above&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-9&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;D&lt;/strong&gt;. Each addresses efficiency or extrapolation in long contexts.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q10-objective&#34;&gt;Q10 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is decoder-only dominant for chat models?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-10&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;It unifies understanding and generation into a single autoregressive process.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-ii--training--fine-tuning-q11q20&#34;&gt;ğŸ”¥ Part II â€” Training &amp;amp; Fine-Tuning (Q11â€“Q20)&lt;/h1&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q11-mcq&#34;&gt;Q11 (MCQ)&lt;/h3&gt;
&lt;p&gt;What is the pretraining objective of GPT-like models?&lt;/p&gt;
&lt;p&gt;A. Masked language modeling&lt;br&gt;
B. Next token prediction&lt;br&gt;
C. Sentence classification&lt;br&gt;
D. Contrastive loss&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-11&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;. GPT models are trained to predict the next token autoregressively.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q12-objective&#34;&gt;Q12 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is pretraining so expensive?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-12&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;It requires massive datasets, compute, and long optimization cycles.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q13-mcq&#34;&gt;Q13 (MCQ)&lt;/h3&gt;
&lt;p&gt;What does fine-tuning change?&lt;/p&gt;
&lt;p&gt;A. Model architecture&lt;br&gt;
B. Tokenizer&lt;br&gt;
C. Weights&lt;br&gt;
D. Loss function only&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-13&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. Fine-tuning updates weights to adapt behavior.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q14-objective&#34;&gt;Q14 (Objective)&lt;/h3&gt;
&lt;p&gt;What is catastrophic forgetting?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-14&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;When fine-tuning overwrites previously learned knowledge.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q15-mcq&#34;&gt;Q15 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which method reduces forgetting?&lt;/p&gt;
&lt;p&gt;A. Lower learning rate&lt;br&gt;
B. Freezing layers&lt;br&gt;
C. LoRA&lt;br&gt;
D. All of the above&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-15&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;D&lt;/strong&gt;. Each constrains weight updates.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q16-objective&#34;&gt;Q16 (Objective)&lt;/h3&gt;
&lt;p&gt;What is LoRA?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-16&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Low-Rank Adaptation: fine-tuning via small rank-decomposed matrices.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q17-mcq&#34;&gt;Q17 (MCQ)&lt;/h3&gt;
&lt;p&gt;Why freeze base model weights?&lt;/p&gt;
&lt;p&gt;A. Save memory&lt;br&gt;
B. Prevent overfitting&lt;br&gt;
C. Preserve general knowledge&lt;br&gt;
D. All of the above&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-17&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;D&lt;/strong&gt;. Freezing improves stability and efficiency.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q18-objective&#34;&gt;Q18 (Objective)&lt;/h3&gt;
&lt;p&gt;Difference between instruction tuning and pretraining?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-18&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Instruction tuning aligns model behavior to human instructions rather than raw text prediction.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q19-mcq&#34;&gt;Q19 (MCQ)&lt;/h3&gt;
&lt;p&gt;What does RLHF optimize?&lt;/p&gt;
&lt;p&gt;A. Accuracy&lt;br&gt;
B. Likelihood&lt;br&gt;
C. Human preference&lt;br&gt;
D. Latency&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-19&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. RLHF aligns outputs with human feedback.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q20-objective&#34;&gt;Q20 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is RLHF unstable?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-20&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Reward models are imperfect and can be exploited.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-iii--systems-safety--evaluation-q21q35&#34;&gt;ğŸ§  Part III â€” Systems, Safety &amp;amp; Evaluation (Q21â€“Q35)&lt;/h1&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q21-mcq&#34;&gt;Q21 (MCQ)&lt;/h3&gt;
&lt;p&gt;What causes hallucination most?&lt;/p&gt;
&lt;p&gt;A. Small models&lt;br&gt;
B. Lack of grounding&lt;br&gt;
C. Bad tokenizer&lt;br&gt;
D. Low temperature&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-21&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;. Hallucination arises from missing or unverified knowledge.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q22-objective&#34;&gt;Q22 (Objective)&lt;/h3&gt;
&lt;p&gt;How does RAG reduce hallucination?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-22&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;By grounding generation in retrieved external knowledge.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q23-mcq&#34;&gt;Q23 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which metric is worst for reasoning?&lt;/p&gt;
&lt;p&gt;A. BLEU&lt;br&gt;
B. ROUGE&lt;br&gt;
C. Exact Match&lt;br&gt;
D. Accuracy&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-23&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;A&lt;/strong&gt;. BLEU focuses on surface n-gram overlap.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q24-objective&#34;&gt;Q24 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is human evaluation critical?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-24&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Humans judge meaning, usefulness, and harm beyond metrics.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q25-mcq&#34;&gt;Q25 (MCQ)&lt;/h3&gt;
&lt;p&gt;What is alignment?&lt;/p&gt;
&lt;p&gt;A. Model speed&lt;br&gt;
B. Model size&lt;br&gt;
C. Matching human values&lt;br&gt;
D. Token efficiency&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-25&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. Alignment ensures AI behaves consistently with human intent.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q26-objective&#34;&gt;Q26 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is safety not solved by data alone?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-26&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Values are contextual, evolving, and require judgment.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q27-mcq&#34;&gt;Q27 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which is an agent failure?&lt;/p&gt;
&lt;p&gt;A. Wrong answer&lt;br&gt;
B. Tool misuse&lt;br&gt;
C. Infinite loop&lt;br&gt;
D. All of the above&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-27&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;D&lt;/strong&gt;. Agents introduce new failure modes.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q28-objective&#34;&gt;Q28 (Objective)&lt;/h3&gt;
&lt;p&gt;Why must agents be logged?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-28&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;For debugging, auditing, and accountability.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q29-mcq&#34;&gt;Q29 (MCQ)&lt;/h3&gt;
&lt;p&gt;What is temperature?&lt;/p&gt;
&lt;p&gt;A. Training speed&lt;br&gt;
B. Randomness control&lt;br&gt;
C. Model size&lt;br&gt;
D. Loss scaling&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-29&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;. Temperature controls output diversity.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q30-objective&#34;&gt;Q30 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is low temperature risky?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-30&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;It can amplify confident but wrong answers.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q31-mcq&#34;&gt;Q31 (MCQ)&lt;/h3&gt;
&lt;p&gt;Which improves long-context reasoning?&lt;/p&gt;
&lt;p&gt;A. Bigger model&lt;br&gt;
B. Better data&lt;br&gt;
C. Memory mechanisms&lt;br&gt;
D. UI design&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-31&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. Memory and retrieval matter more than size.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q32-objective&#34;&gt;Q32 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is evaluation harder than training?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-32&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Correctness is ambiguous, contextual, and human-dependent.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q33-mcq&#34;&gt;Q33 (MCQ)&lt;/h3&gt;
&lt;p&gt;What is distribution shift?&lt;/p&gt;
&lt;p&gt;A. Token drift&lt;br&gt;
B. Deployment data differs from training&lt;br&gt;
C. Model collapse&lt;br&gt;
D. Optimizer bug&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-33&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;. Real-world data rarely matches training data.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q34-objective&#34;&gt;Q34 (Objective)&lt;/h3&gt;
&lt;p&gt;How do you detect silent failures?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-34&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Stress tests, adversarial inputs, and monitoring.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q35-objective&#34;&gt;Q35 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is abstention important?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-35&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Saying â€œI donâ€™t knowâ€ prevents harm and hallucination.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h1 id=&#34;-part-iv--research-mindset-q36q50&#34;&gt;ğŸŒ Part IV â€” Research Mindset (Q36â€“Q50)&lt;/h1&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q36-mcq&#34;&gt;Q36 (MCQ)&lt;/h3&gt;
&lt;p&gt;What makes a strong LLM researcher?&lt;/p&gt;
&lt;p&gt;A. Model size obsession&lt;br&gt;
B. Tool mastery&lt;br&gt;
C. Question formulation&lt;br&gt;
D. Coding speed&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-36&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. Research starts with the right questions.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q37-objective&#34;&gt;Q37 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is ablation important?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-37&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;It isolates which components actually matter.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q38-mcq&#34;&gt;Q38 (MCQ)&lt;/h3&gt;
&lt;p&gt;What does â€œscaling lawâ€ describe?&lt;/p&gt;
&lt;p&gt;A. Inference speed&lt;br&gt;
B. Relationship between compute, data, performance&lt;br&gt;
C. Model compression&lt;br&gt;
D. Tokenization&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-38&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;. Scaling laws guide resource allocation.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q39-objective&#34;&gt;Q39 (Objective)&lt;/h3&gt;
&lt;p&gt;Why are smaller models still relevant?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-39&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;They are cheaper, faster, safer, and deployable.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q40-mcq&#34;&gt;Q40 (MCQ)&lt;/h3&gt;
&lt;p&gt;What is the biggest unsolved problem?&lt;/p&gt;
&lt;p&gt;A. Accuracy&lt;br&gt;
B. Speed&lt;br&gt;
C. Alignment&lt;br&gt;
D. UI&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-40&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. Alignment is fundamentally human and societal.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q41-objective&#34;&gt;Q41 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is interpretability important?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-41&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;To trust, debug, and regulate AI systems.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q42-mcq&#34;&gt;Q42 (MCQ)&lt;/h3&gt;
&lt;p&gt;What does â€œemergent behaviorâ€ mean?&lt;/p&gt;
&lt;p&gt;A. Bugs&lt;br&gt;
B. Overfitting&lt;br&gt;
C. Capabilities appearing at scale&lt;br&gt;
D. Prompt tricks&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-42&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. New abilities emerge non-linearly with scale.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q43-objective&#34;&gt;Q43 (Objective)&lt;/h3&gt;
&lt;p&gt;Why are benchmarks insufficient?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-43&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;They fail to represent real-world complexity.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q44-mcq&#34;&gt;Q44 (MCQ)&lt;/h3&gt;
&lt;p&gt;What defines a good LLM system?&lt;/p&gt;
&lt;p&gt;A. Model size&lt;br&gt;
B. Latency&lt;br&gt;
C. User trust&lt;br&gt;
D. Parameter count&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-44&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. Trust defines real adoption.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q45-objective&#34;&gt;Q45 (Objective)&lt;/h3&gt;
&lt;p&gt;Why must humans stay in the loop?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-45&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;AI lacks values, responsibility, and moral judgment.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q46-mcq&#34;&gt;Q46 (MCQ)&lt;/h3&gt;
&lt;p&gt;What will differentiate future LLMs?&lt;/p&gt;
&lt;p&gt;A. Bigger GPUs&lt;br&gt;
B. Better prompts&lt;br&gt;
C. Better systems &amp;amp; alignment&lt;br&gt;
D. More tokens&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-46&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. Systems and alignment matter more than scale.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q47-objective&#34;&gt;Q47 (Objective)&lt;/h3&gt;
&lt;p&gt;What mindset do interviewers seek?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-47&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Clarity, humility, rigor, and responsibility.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q48-mcq&#34;&gt;Q48 (MCQ)&lt;/h3&gt;
&lt;p&gt;What is a red flag in interviews?&lt;/p&gt;
&lt;p&gt;A. Admitting uncertainty&lt;br&gt;
B. Asking questions&lt;br&gt;
C. Overconfidence&lt;br&gt;
D. Thoughtful pauses&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-48&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;. Overconfidence signals lack of depth.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q49-objective&#34;&gt;Q49 (Objective)&lt;/h3&gt;
&lt;p&gt;Why is â€œI donâ€™t knowâ€ powerful?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-49&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;It shows intellectual honesty and growth mindset.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q50-final-reflection&#34;&gt;Q50 (Final Reflection)&lt;/h3&gt;
&lt;p&gt;What makes a great LLM engineer?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-50&#34;&gt;
  &lt;summary&gt;Answer + Explanation&lt;/summary&gt;
  &lt;p&gt;Someone who combines technical mastery, ethical responsibility, and human-centered thinking.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-words&#34;&gt;ğŸŒ± Final Words&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;You are not training models.&lt;br&gt;
You are shaping intelligence.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Build wisely.&lt;br&gt;
Question deeply.&lt;br&gt;
Stay human.&lt;/p&gt;
&lt;p&gt;â¤ï¸&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
---&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Lecture 14 â€” Deep Learning Foundations &amp; Modern AI (Final Mastery)</title>
      <link>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-14-deep-learning-modern-ai-final/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/courses/2026-01-01-llm-multimodal/lecture-14-deep-learning-modern-ai-final/</guid>
      <description>&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; ~5â€“6 hours (final synthesis lecture)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-this-final-lecture-exists&#34;&gt;ğŸŒ Why This Final Lecture Exists&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Strong AI engineers are built on fundamentals.&lt;br&gt;
Great AI leaders are built on understanding + responsibility.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This lecture revisits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Core deep learning&lt;/li&gt;
&lt;li&gt;Modern LLM-era AI&lt;/li&gt;
&lt;li&gt;Common misconceptions&lt;/li&gt;
&lt;li&gt;Interview-level clarity&lt;/li&gt;
&lt;li&gt;First-principles thinking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you master this lecture, you are no longer &lt;em&gt;confused by trends&lt;/em&gt; â€”&lt;br&gt;
you &lt;strong&gt;understand the machine&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-part-i--deep-learning-foundations-q1q25&#34;&gt;ğŸ§  PART I â€” Deep Learning Foundations (Q1â€“Q25)&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q1--objective&#34;&gt;Q1 â€” Objective&lt;/h3&gt;
&lt;p&gt;What problem does gradient descent solve?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It minimizes a loss function by iteratively updating model parameters.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q2--mcq&#34;&gt;Q2 â€” MCQ&lt;/h3&gt;
&lt;p&gt;What is backpropagation?&lt;/p&gt;
&lt;p&gt;A. Data normalization&lt;br&gt;
B. Gradient computation via chain rule&lt;br&gt;
C. Weight initialization&lt;br&gt;
D. Loss regularization&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Gradient computation via chain rule&lt;br&gt;
Backprop efficiently computes gradients for all parameters.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q3--objective&#34;&gt;Q3 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why do we need activation functions?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;To introduce non-linearity so neural networks can model complex functions.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q4--mcq&#34;&gt;Q4 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which activation helps mitigate vanishing gradients?&lt;/p&gt;
&lt;p&gt;A. Sigmoid&lt;br&gt;
B. Tanh&lt;br&gt;
C. ReLU&lt;br&gt;
D. Softmax&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. ReLU&lt;br&gt;
It preserves gradients for positive inputs.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q5--objective&#34;&gt;Q5 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is overfitting?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;When a model performs well on training data but poorly on unseen data.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q6--mcq&#34;&gt;Q6 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which technique reduces overfitting?&lt;/p&gt;
&lt;p&gt;A. Increasing epochs&lt;br&gt;
B. Dropout&lt;br&gt;
C. Larger batch size&lt;br&gt;
D. Removing regularization&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Dropout&lt;br&gt;
It prevents co-adaptation of neurons.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q7--objective&#34;&gt;Q7 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is batch normalization useful?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-7&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It stabilizes training by normalizing intermediate activations.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q8--mcq&#34;&gt;Q8 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which optimizer adapts learning rates per parameter?&lt;/p&gt;
&lt;p&gt;A. SGD&lt;br&gt;
B. Momentum&lt;br&gt;
C. Adam&lt;br&gt;
D. Newton&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-8&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Adam&lt;br&gt;
Adam combines momentum and adaptive scaling.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q9--objective&#34;&gt;Q9 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is the biasâ€“variance tradeoff?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-9&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;The balance between underfitting (high bias) and overfitting (high variance).&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q10--mcq&#34;&gt;Q10 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which loss is best for classification?&lt;/p&gt;
&lt;p&gt;A. MSE&lt;br&gt;
B. Cross-entropy&lt;br&gt;
C. Hinge (always)&lt;br&gt;
D. L1&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-10&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Cross-entropy&lt;br&gt;
It aligns with probabilistic outputs.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q11--objective&#34;&gt;Q11 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is data scaling important?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-11&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It improves convergence speed and numerical stability.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q12--mcq&#34;&gt;Q12 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which network handles sequences best (classically)?&lt;/p&gt;
&lt;p&gt;A. CNN&lt;br&gt;
B. MLP&lt;br&gt;
C. RNN&lt;br&gt;
D. Autoencoder&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-12&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. RNN&lt;br&gt;
Designed to process sequential data.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q13--objective&#34;&gt;Q13 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is vanishing gradient?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-13&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;When gradients become too small to update earlier layers effectively.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q14--mcq&#34;&gt;Q14 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which architecture solved long-term dependency issues?&lt;/p&gt;
&lt;p&gt;A. Vanilla RNN&lt;br&gt;
B. CNN&lt;br&gt;
C. LSTM&lt;br&gt;
D. Perceptron&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-14&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. LSTM&lt;br&gt;
It uses gating mechanisms to preserve information.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q15--objective&#34;&gt;Q15 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is representation learning?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-15&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Learning useful features automatically from data.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q16--mcq&#34;&gt;Q16 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which layer reduces spatial resolution?&lt;/p&gt;
&lt;p&gt;A. Convolution&lt;br&gt;
B. Pooling&lt;br&gt;
C. Attention&lt;br&gt;
D. Normalization&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-16&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Pooling&lt;br&gt;
It aggregates spatial information.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q17--objective&#34;&gt;Q17 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why are deeper networks harder to train?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-17&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Due to gradient instability and optimization difficulty.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q18--mcq&#34;&gt;Q18 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which innovation enabled very deep networks?&lt;/p&gt;
&lt;p&gt;A. Sigmoid&lt;br&gt;
B. Residual connections&lt;br&gt;
C. Larger datasets&lt;br&gt;
D. Dropout&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-18&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Residual connections&lt;br&gt;
They allow gradients to flow directly.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q19--objective&#34;&gt;Q19 â€” Objective&lt;/h3&gt;
&lt;p&gt;What does regularization encourage?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-19&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Simpler models that generalize better.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q20--mcq&#34;&gt;Q20 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which is NOT a regularization method?&lt;/p&gt;
&lt;p&gt;A. L2 penalty&lt;br&gt;
B. Dropout&lt;br&gt;
C. Data augmentation&lt;br&gt;
D. Increasing learning rate&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-20&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;D. Increasing learning rate&lt;br&gt;
It affects optimization, not regularization.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q21--objective&#34;&gt;Q21 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is transfer learning?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-21&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Reusing knowledge from a pretrained model for a new task.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q22--mcq&#34;&gt;Q22 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Why freeze layers during fine-tuning?&lt;/p&gt;
&lt;p&gt;A. Reduce memory&lt;br&gt;
B. Prevent catastrophic forgetting&lt;br&gt;
C. Increase randomness&lt;br&gt;
D. Speed inference&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-22&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Prevent catastrophic forgetting&lt;br&gt;
Frozen layers preserve learned representations.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q23--objective&#34;&gt;Q23 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is catastrophic forgetting?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-23&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;When a model forgets old knowledge while learning new tasks.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q24--mcq&#34;&gt;Q24 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which setting usually needs the least data?&lt;/p&gt;
&lt;p&gt;A. Training from scratch&lt;br&gt;
B. Pretraining&lt;br&gt;
C. Fine-tuning&lt;br&gt;
D. Random initialization&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-24&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Fine-tuning&lt;br&gt;
It leverages pretrained knowledge.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q25--objective&#34;&gt;Q25 â€” Objective&lt;/h3&gt;
&lt;p&gt;What defines a good loss function?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-25&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It aligns optimization with the true task objective.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-part-ii--modern-ai--llm-era-q26q50&#34;&gt;ğŸš€ PART II â€” Modern AI &amp;amp; LLM Era (Q26â€“Q50)&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q26--mcq&#34;&gt;Q26 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which architecture dominates modern LLMs?&lt;/p&gt;
&lt;p&gt;A. CNN&lt;br&gt;
B. RNN&lt;br&gt;
C. Transformer&lt;br&gt;
D. Autoencoder&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-26&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Transformer&lt;br&gt;
It enables parallelism and long-range dependency modeling.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q27--objective&#34;&gt;Q27 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is self-attention powerful?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-27&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;It allows tokens to dynamically attend to relevant context.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q28--mcq&#34;&gt;Q28 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Decoder-only models are trained to:&lt;/p&gt;
&lt;p&gt;A. Encode inputs only&lt;br&gt;
B. Predict masked tokens&lt;br&gt;
C. Predict next token autoregressively&lt;br&gt;
D. Align image-text&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-28&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Predict next token autoregressively&lt;br&gt;
This is how GPT-style models are trained.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q29--objective&#34;&gt;Q29 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is pretraining in LLMs?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-29&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Training on massive unlabeled data to learn general language patterns.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q30--mcq&#34;&gt;Q30 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which dataset type is most common for LLM pretraining?&lt;/p&gt;
&lt;p&gt;A. Labeled QA&lt;br&gt;
B. Reinforcement signals&lt;br&gt;
C. Unlabeled text&lt;br&gt;
D. Synthetic only&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-30&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Unlabeled text&lt;br&gt;
Self-supervised learning scales best.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q31--objective&#34;&gt;Q31 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why does scale matter in LLMs?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-31&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Larger models show emergent abilities and better generalization.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q32--mcq&#34;&gt;Q32 â€” MCQ&lt;/h3&gt;
&lt;p&gt;What is fine-tuning?&lt;/p&gt;
&lt;p&gt;A. Changing architecture&lt;br&gt;
B. Training from scratch&lt;br&gt;
C. Adapting pretrained weights&lt;br&gt;
D. Prompt engineering&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-32&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Adapting pretrained weights&lt;br&gt;
Fine-tuning adjusts behavior for specific tasks.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q33--objective&#34;&gt;Q33 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is instruction tuning?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-33&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Fine-tuning models to follow human instructions.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q34--mcq&#34;&gt;Q34 â€” MCQ&lt;/h3&gt;
&lt;p&gt;RLHF stands for:&lt;/p&gt;
&lt;p&gt;A. Reinforced Learning with Human Feedback&lt;br&gt;
B. Reinforcement Learning from Human Feedback&lt;br&gt;
C. Recurrent Learning from Human Feedback&lt;br&gt;
D. Regularized Learning from Human Feedback&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-34&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Reinforcement Learning from Human Feedback&lt;br&gt;
Used to align models with human preferences.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q35--objective&#34;&gt;Q35 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is alignment important?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-35&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;To ensure AI behavior matches human values and intentions.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q36--mcq&#34;&gt;Q36 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which technique reduces hallucination?&lt;/p&gt;
&lt;p&gt;A. Bigger models&lt;br&gt;
B. RAG&lt;br&gt;
C. Longer prompts&lt;br&gt;
D. Temperature increase&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-36&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. RAG&lt;br&gt;
It grounds answers in retrieved evidence.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q37--objective&#34;&gt;Q37 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is an embedding?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-37&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;A vector representation capturing semantic meaning.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q38--mcq&#34;&gt;Q38 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which enables multimodal understanding?&lt;/p&gt;
&lt;p&gt;A. Tokenization only&lt;br&gt;
B. Cross-attention&lt;br&gt;
C. SGD&lt;br&gt;
D. Dropout&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-38&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;B. Cross-attention&lt;br&gt;
It aligns different modalities.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q39--objective&#34;&gt;Q39 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is an AI agent?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-39&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;A system that reasons, acts, uses tools, and iterates toward goals.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q40--mcq&#34;&gt;Q40 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which is NOT a risk of agentic AI?&lt;/p&gt;
&lt;p&gt;A. Infinite loops&lt;br&gt;
B. Tool misuse&lt;br&gt;
C. Alignment drift&lt;br&gt;
D. Faster convergence&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-40&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;D. Faster convergence&lt;br&gt;
The others are real risks.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q41--objective&#34;&gt;Q41 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why is evaluation difficult for LLMs?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-41&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Outputs are open-ended and context-dependent.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q42--mcq&#34;&gt;Q42 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which is the gold standard of evaluation?&lt;/p&gt;
&lt;p&gt;A. BLEU&lt;br&gt;
B. ROUGE&lt;br&gt;
C. Human judgment&lt;br&gt;
D. Perplexity&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-42&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Human judgment&lt;br&gt;
Humans assess meaning and usefulness.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q43--objective&#34;&gt;Q43 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is hallucination?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-43&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Confidently generating incorrect or unsupported information.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q44--mcq&#34;&gt;Q44 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which helps reduce hallucination most?&lt;/p&gt;
&lt;p&gt;A. Temperature tuning&lt;br&gt;
B. Larger vocabulary&lt;br&gt;
C. Grounded retrieval&lt;br&gt;
D. More layers&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-44&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. Grounded retrieval&lt;br&gt;
Evidence constrains generation.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q45--objective&#34;&gt;Q45 â€” Objective&lt;/h3&gt;
&lt;p&gt;Why keep humans in the loop?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-45&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;To ensure safety, correctness, and ethical oversight.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q46--mcq&#34;&gt;Q46 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which best describes modern AI engineering?&lt;/p&gt;
&lt;p&gt;A. Model-centric&lt;br&gt;
B. Data-centric&lt;br&gt;
C. System-centric&lt;br&gt;
D. Prompt-only&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-46&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. System-centric&lt;br&gt;
Modern AI combines models, tools, data, and humans.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q47--objective&#34;&gt;Q47 â€” Objective&lt;/h3&gt;
&lt;p&gt;What is the biggest misconception about LLMs?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-47&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;That they â€œunderstandâ€ like humans.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q48--mcq&#34;&gt;Q48 â€” MCQ&lt;/h3&gt;
&lt;p&gt;Which skill matters most long-term?&lt;/p&gt;
&lt;p&gt;A. Framework mastery&lt;br&gt;
B. Prompt tricks&lt;br&gt;
C. First-principles understanding&lt;br&gt;
D. Leaderboard scores&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-48&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;C. First-principles understanding&lt;br&gt;
Tools change, principles remain.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q49--objective&#34;&gt;Q49 â€” Objective&lt;/h3&gt;
&lt;p&gt;What should AI ultimately optimize for?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-49&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Human well-being and societal benefit.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h3 id=&#34;q50--final-reflection&#34;&gt;Q50 â€” Final Reflection&lt;/h3&gt;
&lt;p&gt;What makes a &lt;em&gt;great&lt;/em&gt; AI engineer?&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-50&#34;&gt;
  &lt;summary&gt;Answer&lt;/summary&gt;
  &lt;p&gt;Technical excellence, humility, ethics, and responsibility to humanity.&lt;/p&gt;
&lt;/details&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-words&#34;&gt;ğŸŒ± Final Words&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;AI is not about replacing humans.&lt;br&gt;
It is about helping humans become better.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If this course helped you:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Think deeper&lt;/li&gt;
&lt;li&gt;Act responsibly&lt;/li&gt;
&lt;li&gt;Teach others kindly&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>UTMB World Series Debut: Chiang Mai Inthanon 20K, A First Step Into International Trail Racing</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-12-02-utmb-chiangmai-inthanon/</link>
      <pubDate>Tue, 02 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-12-02-utmb-chiangmai-inthanon/</guid>
      <description>&lt;h1 id=&#34;-my-first-ever-trail-running-adventure-conquering-the-utmb-world-series--inthanon-20k-at-chiang-mai&#34;&gt;&lt;strong&gt;ğŸ”ï¸ My First-Ever Trail Running Adventure: Conquering the UTMB World Series â€” Inthanon 20K at Chiang Mai&lt;/strong&gt;&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;â€œSometimes the next chapter of your life begins with a decision that scares you.
This time, mine began at 4Â°C on top of a mountain.â€&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-preface-stepping-into-the-unknown&#34;&gt;&lt;strong&gt;ğŸŒŸ Preface: Stepping Into the Unknown&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For most of my athletic life, Iâ€™ve always found my battles on the road, in the ocean, and under the burning sun â€” not deep inside a forest. Iâ€™m someone who once trained for and completed an &lt;strong&gt;IRONMAN&lt;/strong&gt;, one of the toughest endurance events in the world. If you want to feel the full emotional rollercoaster of that moment, you can read it here:
ğŸ‘‰ &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-i-became-an-ironman/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Day I Became an Ironman&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After that, I chased the marathon dream, running multiple full marathonsâ€”starting with my first 42.195 km at the Bangkok Marathon 2022 (ğŸ‘‰ &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bangkok Marathon 2022 â€” Conquering the Full Marathon&lt;/a&gt;) and later my unforgettable 42.195 km at Chom Bueng (ğŸ‘‰ &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chom Bueng Marathon 2025&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;But once those milestones were behind me, life becameâ€¦ quiet.
Too quiet.&lt;/p&gt;
&lt;p&gt;The fire inside me â€” the one that thrives on challenges and adventure â€” began whispering again:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œYou need something new.
Something unknown.
Something that scares youâ€¦ just a little.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And then I saw it:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UTMB Inthanon 20K â€” one of the world-famous UTMB World Series races.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A trail race across &lt;strong&gt;Doi Inthanon&lt;/strong&gt;, Thailandâ€™s highest peak.
A course weaving through rice terraces, hill-tribe villages, dense forest, roots, rocks, cliffs â€” everything I had zero experience with.&lt;/p&gt;
&lt;p&gt;This race even awards &lt;strong&gt;double Running Stones&lt;/strong&gt;, making it even more attractive for future UTMB dreams.
(Ref: &lt;a href=&#34;https://chiangmai.utmb.world/races/INTHANON20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://chiangmai.utmb.world/races/INTHANON20&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;I didnâ€™t hesitate long.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWhy not? Life is too short to stay comfortable.â€
â€œLetâ€™s do something crazy again.â€ ğŸ˜†&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And thatâ€™s where this story truly begins.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-arrival-in-chiang-mai--pre-race-day-29-november-2025&#34;&gt;&lt;strong&gt;ğŸ“ Arrival in Chiang Mai â€” Pre-Race Day (29 November 2025)&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I flew into Chiang Mai on Saturday to pick up my race kit and attend the briefing.
Even though I had faced Ironmans and marathons, this felt different.
This felt like stepping onto new ground â€” literally.&lt;/p&gt;
&lt;p&gt;But the moment I entered the UTMB expo, everything changed.&lt;/p&gt;
&lt;p&gt;The atmosphere was incredible.
International runners, world-class booths, music, mountain views â€” the energy was electric. It felt like standing inside a festival of endurance and dreams.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_8724.jpg&#34; alt=&#34;UTMB Inthanon entrance archway and event atmosphere&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 1: The moment I stepped off the plane from Bangkok and walked into the UTMB event venue,  
    the atmosphere hit me like a wave â€” music, cowbells, excited runners, and the smell of cold mountain air.  
    Standing under the iconic UTMB Inthanon gate, I felt like I was entering another world.  
    A world where fear and excitement blend, and where every runner arrives searching for something â€”  
    strength, peace, closure, or maybe just proof that they are capable of more than they think.  
    For me, it was all of that.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I told myself I wouldnâ€™t buy anythingâ€¦&lt;/p&gt;
&lt;p&gt;â€¦which is why I walked out with &lt;strong&gt;two new UTMB shirts&lt;/strong&gt; ğŸ˜†
They were too beautiful to resist â€” donâ€™t blame me.&lt;/p&gt;
&lt;p&gt;I took pictures everywhere like an excited tourist who had just discovered mountains for the first time.&lt;/p&gt;
&lt;p&gt;And while the atmosphere reminded me a lot of the IRONMAN eventsâ€¦&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;My heart still belongs to triathlon.&lt;/strong&gt;
Something about the swimâ€“bikeâ€“run combination feels like home to me.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_8770.jpg&#34; alt=&#34;Runner bib pickup UTMB Chiang Mai 2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 2: My bib â€” 12534, age category 20â€“34.  
    Holding it felt like holding a key to a challenge I had never attempted before.  
    All around me were runners from Japan, France, Germany, China â€” all speaking different languages  
    yet sharing the same anticipation in their eyes.  
    For a moment, I stood still and whispered to myself:  
    â€œKaoâ€¦ this is it. Tomorrow, you become a trail runner.â€  
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Still â€” UTMB was impressive. Extremely well organized.
A global standard.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_8783.jpg&#34; alt=&#34;Kao posing at UTMB event area&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 3: A solo photo before the storm.  
    I asked a Japanese runner to take it â€” we laughed at how we were both pretending not to be nervous.  
    Behind me, the mountains watched silently.  
    The limited-edition UTMB race bag felt like a treasure earned before the race even began.  
    This was the moment I told myself:  
    â€œYou came here alone, but you wonâ€™t leave empty.â€
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_8857.jpg&#34; alt=&#34;UTMB event pass&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 4: My UTMB pass â€” simple, light, almost insignificant.  
    Yet this tiny card opened the door to one of the most meaningful days of my life.  
    It represented commitment, training, discipline, fear, excitement, and the courage to show up.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-race-day--30-november-2025&#34;&gt;&lt;strong&gt;ğŸŒ„ Race Day â€” 30 November 2025&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;4c-nerves-excitement-fear&#34;&gt;&lt;strong&gt;4Â°C. Nerves. Excitement. Fear.&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Let me be honest:
Nothing prepared me for the cold.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4Â°C.&lt;/strong&gt;
In Thailand.
On a mountain.
At sunrise.&lt;/p&gt;
&lt;p&gt;I wasnâ€™t thinking about cliffs, animals, roots, or time cutoffs anymore.&lt;/p&gt;
&lt;p&gt;I was thinking:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œWill my fingers freeze off before the race even starts?â€ ğŸ˜‚&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_8912.jpg&#34; alt=&#34;Crowd at UTMB start line morning&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 5: The morning of the race â€” 4Â°C, freezing wind, and a sea of runners vibrating with energy.  
    You could feel the mountains breathing.  
    You could feel your heart racing even before the countdown.  
    And suddenly, I wasnâ€™t afraid anymore.  
    I was exactly where I needed to be.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I lined up for &lt;strong&gt;Wave 2&lt;/strong&gt; â€” the â€œnormal humanâ€ group.
Start time: &lt;strong&gt;8:10 AM&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;But the funniest part?&lt;/p&gt;
&lt;p&gt;I ended up finishing &lt;strong&gt;ahead of several Wave 1 runners&lt;/strong&gt;.
Yes, I laughed out loud at the finish line.
Small victories count too!&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_8937.jpg&#34; alt=&#34;Agricultural village scenery during UTMB&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 6: The first few kilometers passed through gentle agricultural fields.  
    The morning light touched the crops softly, almost too peaceful to be part of a UTMB course.  
    But beyond this innocent view waited a climb that would test every runnerâ€™s spirit.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-segment-1--hq-to-b1-pha-mon-mai-village&#34;&gt;&lt;strong&gt;ğŸƒâ€â™‚ï¸ Segment 1 â€” HQ to B1 Pha Mon Mai Village&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;061-miles-098-km--elevation-gain-1699-ft&#34;&gt;&lt;strong&gt;0â€“6.1 miles (0â€“9.8 km) | Elevation Gain: 1699 ft+&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The race started at the Doi Inthanon National Park Headquarters â€” and immediately punched us with a brutal uphill section.&lt;/p&gt;
&lt;p&gt;Not gentle.
Not moderate.
&lt;strong&gt;Brutal.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The mountain welcomed us with a slap:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œGood morning. Letâ€™s see what your legs can do.â€ ğŸ˜…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The trail twisted through dense forest, tree roots, slippery rocks, and steep climbs.
My face was frozen, but I kept smiling â€” maybe from excitement, maybe because I couldnâ€™t feel my cheeks anymore.&lt;/p&gt;
&lt;p&gt;Reaching &lt;strong&gt;B1 Pha Mon Mai Village&lt;/strong&gt; felt magical.
Local villagers cheered with such warmth and sincerity that every bit of suffering disappeared instantly.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_8973.jpg&#34; alt=&#34;Meeting hill tribe child UTMB&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 7: A tiny hill-tribe girl stood on the trail, cheering shyly.  
    Her smile warmed the cold air.  
    I gave her a small treat â€” but she gave me something bigger:  
    a reminder that kindness can appear anywhere, even when youâ€™re exhausted on a mountain.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_8974.jpg&#34; alt=&#34;Three hill tribe children smiling UTMB&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 8: Another child joined, the youngest one â€” her smile could melt glaciers.  
    These moments reminded me:  
    A race isnâ€™t just about running.  
    Itâ€™s about meeting the world in unexpected ways.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-segment-2--b1-to-b2-mae-klang-luang-village&#34;&gt;&lt;strong&gt;ğŸ“ Segment 2 â€” B1 to B2 Mae Klang Luang Village&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;distance-98-miles--157-km--elevation-gain-so-far-2198-ft&#34;&gt;&lt;strong&gt;Distance: 9.8 miles (â‰ˆ 15.7 km) | Elevation Gain so far: 2198 ft+&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This segment was a dream.&lt;/p&gt;
&lt;p&gt;We ran through:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸŒ¾ Rice terraces&lt;/li&gt;
&lt;li&gt;ğŸ“ Strawberry farms&lt;/li&gt;
&lt;li&gt;ğŸ¡ Karen villages&lt;/li&gt;
&lt;li&gt;ğŸŒ² Misty mountain forest&lt;/li&gt;
&lt;li&gt;ğŸŒ¤ï¸ Soft sunlight breaking through trees&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even the steep downhill sections â€” which terrified me more than the climbs â€” were worth it because the scenery was breathtaking.&lt;/p&gt;
&lt;p&gt;And somewhere during this section, a surprising thought appeared:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œMaybe I &lt;em&gt;can&lt;/em&gt; finish this.â€
â€œMaybe trail running isnâ€™t so scary after all.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A short break at &lt;strong&gt;B2&lt;/strong&gt; helped me regain energy for the final challenge.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/KAO_UTMB2025_INTHANON_04.png&#34; alt=&#34;Kao running uphill to B1 checkpoint&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 9: The brutal climb to B1 Pha Mon Mai Village.  
    My legs burned. The air thinned.  
    But something inside me kept whispering:  
    â€œOne more step, Kao.â€  
    UTMB captured this moment â€” the exact second pain turned into perseverance.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/KAO_UTMB2025_INTHANON_05.png&#34; alt=&#34;Kao uphill running crop version&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 10: The cropped version â€” my face carrying the weight of the climb.  
    This is the expression of a person fighting their own limitations  
    and discovering they are stronger than they thought.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/KAO_UTMB2025_INTHANON_06.png&#34; alt=&#34;Running through forest downhill section UTMB&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 11: This part of the trail felt like flying.  
    Cold forest air, soft ground, flowing rhythm â€”  
    for a few minutes I forgot I was in a race.  
    I felt alive, light, and free.  
    This is the memory Iâ€™ll keep forever.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-section--b2-to-finish-line-back-to-hq&#34;&gt;&lt;strong&gt;ğŸ Final Section â€” B2 to Finish Line (Back to HQ)&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;finish-time-35341--elevation-gain-3264-ft&#34;&gt;&lt;strong&gt;Finish Time: 3:53:41 | Elevation Gain: 3264 ft+&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The last stretch mixed rolling terrain with sharp inclines.
My legs burned, but my confidence soared.&lt;/p&gt;
&lt;p&gt;Cheering runners, cool mountain air, and the beauty of Doi Inthanon pushed me forward. When the finish line came into view, I felt everything â€” joy, pride, relief, disbelief.&lt;/p&gt;
&lt;p&gt;My only goal was to finish under the cutoff of 7 hours.
Insteadâ€¦&lt;/p&gt;
&lt;p&gt;I crossed with a &lt;strong&gt;Sub-4 finish&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;3 hours 53 minutes 41 seconds&lt;/strong&gt; ğŸ‰&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For my first trail race ever, this felt unbelievable.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_9051.jpg&#34; alt=&#34;Garmin Fenix 7 final stats UTMB Inthanon&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 12: The moment I checked my Garmin:  
    3:53:00 for 21.03 km â€” Sub-4 on my first-ever trail race.  
    I couldnâ€™t believe it.  
    My doubts dissolved instantly into pride.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_9053.jpg&#34; alt=&#34;UTMB Inthanon finisher medal&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 13: My finisher medal.  
    A small circle of metal,  
    but holding it felt like holding a piece of the mountain itself.  
    Fear conquered. Challenge completed. Dream achieved.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-my-official-utmb-stats&#34;&gt;&lt;strong&gt;ğŸ“Š My Official UTMB Stats&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;ğŸ‘‰ Full details: &lt;a href=&#34;https://live.utmb.world/chiangmai/2025/runners/12534&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://live.utmb.world/chiangmai/2025/runners/12534&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Runner:&lt;/strong&gt; Teerapong Panboonyuen
&lt;strong&gt;Bib:&lt;/strong&gt; 12534
&lt;strong&gt;Country:&lt;/strong&gt; THA&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Category&lt;/th&gt;
&lt;th&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Race Time&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;03:53:41&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Global Rank&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;368&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Age Category Rank (M 20â€“34)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Division Rank&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;220&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Elevation Gain&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;3264 ft+&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Average Speed&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;3.2 mph&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Rest Time&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0:03:09&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;checkpoint-times&#34;&gt;&lt;strong&gt;Checkpoint Times&lt;/strong&gt;&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Checkpoint&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Speed&lt;/th&gt;
&lt;th&gt;Distance&lt;/th&gt;
&lt;th&gt;Gain&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Start â€” HQ&lt;/td&gt;
&lt;td&gt;08:10 AM&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;0 mi&lt;/td&gt;
&lt;td&gt;0 ft&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B1 Pha Mon Mai&lt;/td&gt;
&lt;td&gt;09:55 AM&lt;/td&gt;
&lt;td&gt;3.4 mph&lt;/td&gt;
&lt;td&gt;6.1 mi&lt;/td&gt;
&lt;td&gt;1699 ft&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;B2 Mae Klang Luang&lt;/td&gt;
&lt;td&gt;10:53 AM&lt;/td&gt;
&lt;td&gt;3.8 mph&lt;/td&gt;
&lt;td&gt;9.8 mi&lt;/td&gt;
&lt;td&gt;2198 ft&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Finish â€” HQ&lt;/td&gt;
&lt;td&gt;12:03 PM&lt;/td&gt;
&lt;td&gt;2.5 mph&lt;/td&gt;
&lt;td&gt;12.6 mi&lt;/td&gt;
&lt;td&gt;3264 ft&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_9054.jpg&#34; alt=&#34;UTMB 20K GPS map&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 14: The full GPS trace of my journey.  
    I stared at it thinking:  
    â€œDid I really run all of this?â€  
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_9056.jpg&#34; alt=&#34;UTMB 20K satellite route map&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 15: The satellite view â€”  
    a reminder of how big the mountain is and how small we all are.  
    Yet somehow, we still climb.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-final-thoughts--my-first-trail-running-experience&#34;&gt;&lt;strong&gt;ğŸ’¬ Final Thoughts â€” My First Trail Running Experience&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This first UTMB race gave me memories Iâ€™ll carry forever.&lt;/p&gt;
&lt;p&gt;The atmosphere
The cold
The villagers
The breathtaking views
The world-class race organization
The sense of adventure
The personal doubt â€” and the victory over it&lt;/p&gt;
&lt;p&gt;And of course, the &lt;strong&gt;2 Running Stones&lt;/strong&gt; were a sweet bonus ğŸ˜&lt;/p&gt;
&lt;p&gt;But will I switch from triathlon to trail running?&lt;/p&gt;
&lt;p&gt;Probably not.&lt;/p&gt;
&lt;p&gt;Not because trail running isnâ€™t incredible â€” it absolutely is.
But I struggle with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;â›°ï¸ Long uphills&lt;/li&gt;
&lt;li&gt;â›°ï¸ Technical downhills&lt;/li&gt;
&lt;li&gt;â›°ï¸ No mountains to train on (Bangkok lifeâ€¦)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Still, this race made me rediscover something important:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The human body is capable of more than we believe.
And the human mind is even stronger.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_9105.jpg&#34; alt=&#34;Selfie with UTMB finisher medal&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 16: A selfie of pure joy.  
    I arrived in Chiang Mai aloneâ€¦  
    but I finished feeling like I gained a new version of myself.  
    A stronger one.  
    A more grateful one.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/IMG_9144.jpg&#34; alt=&#34;UTMB souvenirs at home desk&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 17: Back home in Bangkok.  
    I laid everything on my desk â€” the medal, bib, pass, and race bag.  
    Looking at them, I felt a quiet happiness settle inside my chest.  
    This race was more than just running.  
    It was proof that life still has adventures waiting for meâ€¦  
    as long as Iâ€™m brave enough to say yes.  
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/KAO_UTMB2025_INTHANON_01.png&#34; alt=&#34;Official UTMB ranking page&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 18: My official UTMB ranking â€”  
    seeing my name there felt surreal.
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/KAO_UTMB2025_INTHANON_02.png&#34; alt=&#34;Cropped UTMB ranking official stats&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 19: A cleaner cropped version â€”  
    a sweet summary of a long, hard, beautiful day.
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;KAO_UTMB_INTHANON_IMG/KAO_UTMB2025_INTHANON_03.png&#34; alt=&#34;UTMB global ranking with international runners&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 20: I finished among runners from across the world â€”  
    proof that mountains connect people more than anything else.
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-gratitude--to-myself&#34;&gt;&lt;strong&gt;ğŸ‰ Gratitude â€” To Myself&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Iâ€™m proud that I dared to do something completely unfamiliar.
Proud that I showed up.
Proud that I didnâ€™t let fear win.&lt;/p&gt;
&lt;p&gt;This race became the perfect closing chapter for 2025.&lt;/p&gt;
&lt;p&gt;So if this ends up being my last blog of the yearâ€¦&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Merry Christmas &amp;amp; Happy New Year&lt;/strong&gt;
to everyone reading this. ğŸ„âœ¨&lt;/p&gt;
&lt;p&gt;Thank you for joining me on this journey.
May your 2026 be full of joy, strength, courage, and new challenges â€” the beautiful kind that scares you just enough to grow.&lt;/p&gt;
&lt;p&gt;See you on the next adventure. â¤ï¸ğŸƒâ€â™‚ï¸ğŸŒ„&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (December 2025). &lt;em&gt;My First-Ever UTMB World Series Adventure â€” Conquering Chiang Mai Inthanon 20K&lt;/em&gt;. Blog post on Kao Panboonyuen.
&lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-12-02-utmb-chiangmai-inthanon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-12-02-utmb-chiangmai-inthanon/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{panboonyuen2025utmb20k,
  title   = &amp;quot;UTMB World Series Debut: Chiang Mai Inthanon 20K, A First Step Into International Trail Racing&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Dec&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2025-12-02-utmb-chiangmai-inthanon/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you enjoy reading this story? â¤ï¸
If yes, please consider sharing it â€” inspiration grows when shared. ğŸ”ï¸âœ¨
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image</title>
      <link>https://kaopanboonyuen.github.io/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/</link>
      <pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kaopanboonyuen.github.io/KAO/results/re_show_01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://kaopanboonyuen.github.io/KAO/results/re_show_01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://kaopanboonyuen.github.io/KAO/results/re_all_02.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://kaopanboonyuen.github.io/KAO/results/re_all_05.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My 25th Blood Donation â€” Celebrating the Quiet Joy and Lifelong Gift of Giving</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-10-13-my25th-blood-donation/</link>
      <pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-10-13-my25th-blood-donation/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;â€œIf youâ€™re lucky enough to live the rest of your life, make it meaningful.â€&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-a-milestone-from-the-heart&#34;&gt;ğŸ©¸ A Milestone from the Heart&lt;/h2&gt;
&lt;p&gt;There are so many moments in life that make me smile â€” moments that remind me that Iâ€™m proud to be who I am, to be alive, and to have lived this far.&lt;br&gt;
If the average lifespan is between 70 and 90 years, Iâ€™ve probably walked through a third of my journey already. That realization doesnâ€™t scare me. It centers me. It whispers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œDo something that matters.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00001.jpg&#34; alt=&#34;Kao donating blood at Thai Red Cross Bangkok&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 1: Settled into the donation chair, the needle in place â€” and once again, the blood flows strong.  
    The nurse, warm and kind as always, smiles and says, â€œYour blood looks especially rich today.â€  
    I dedicate the merit from this donation to all of humanity. May everyone be safe, healthy, and well.
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For me, meaning often comes through &lt;strong&gt;giving&lt;/strong&gt; â€” especially in small, quiet ways that ripple far beyond what we see.&lt;br&gt;
And one of those ways has been &lt;em&gt;blood donation&lt;/em&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-where-it-all-began&#34;&gt;ğŸŒ¿ Where It All Began&lt;/h2&gt;
&lt;p&gt;If I trace back where this habit began, it leads straight to &lt;strong&gt;my mother&lt;/strong&gt;.&lt;br&gt;
When I was a child, she took me along to the &lt;strong&gt;Thai Red Cross Center in Ratchaburi&lt;/strong&gt;, beside the calm &lt;strong&gt;Mae Klong River&lt;/strong&gt;. I remember the soft hum of ceiling fans, the cool tiles under my shoes, and the quiet pride on her face.&lt;/p&gt;
&lt;p&gt;She never lectured about kindness. She simply lived it.&lt;br&gt;
Giving blood wasnâ€™t something she bragged about â€” it was something she did because she could.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00003.jpg&#34; alt=&#34;Thai Red Cross Ratchaburi memories&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 2: After the donation, I received a small snack pack and a bottle of water â€” a simple gesture that always feels thoughtful.  
    On the back of the snack bag, it said: â€œThank you to all our heroes.â€  
    It made me smile. I truly hope this blood helps keep someoneâ€™s life going, somewhere out there.
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Back then, I didnâ€™t understand.&lt;br&gt;
But now, every time I roll up my sleeve, I hear her voice again:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œBe kind. Give when you can.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-from-ratchaburi-to-bangkok&#34;&gt;ğŸ™ï¸ From Ratchaburi to Bangkok&lt;/h2&gt;
&lt;p&gt;Years later, my life moved to &lt;strong&gt;Bangkok&lt;/strong&gt;. The donation center changed, but the ritual stayed.&lt;br&gt;
The staff at the Thai Red Cross Society now recognize me. We share jokes before each session â€” the kind of light humor that makes courage easy.&lt;/p&gt;
&lt;p&gt;Each donation follows a rhythm: health check, warm greeting, the quick sting of the needle, and then the slow, reassuring flow of life leaving one body to help another.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00005.jpg&#34; alt=&#34;Blood bag filling during donation&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 3: Lying back in the donation room, I watched the deep crimson line slowly fill the bag â€” a quiet, powerful connection between two strangers who may never meet.  
    The nurse laughed when she saw the color: â€œNo need for iron supplements today â€” your blood&#39;s already super rich!â€  
    (To be fair, I had just swum a full kilometer before coming in â€” didnâ€™t want to miss my workout after donating!)
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This time was special.&lt;br&gt;
Because this was &lt;strong&gt;my 25th blood donation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The nurse handed me a certificate and a small commemorative pin.&lt;br&gt;
For a moment, I felt both humbled and proud â€” of the discipline it took, of the years that passed, and of the promise that began decades ago beside that river.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-goal-100-donations&#34;&gt;ğŸ’¯ The Goal: 100 Donations&lt;/h2&gt;
&lt;p&gt;My personal goal is to reach &lt;strong&gt;100 blood donations&lt;/strong&gt; in my lifetime.&lt;br&gt;
Itâ€™s not just a number â€” itâ€™s a symbol of continuity, of living healthily, and of refusing to forget what empathy means.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00006.jpg&#34; alt=&#34;Blood donation pin collection&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 4: My collection of blood donation pins â€” each one marking a checkpoint along the way.  
    They feel a bit like Garmin badges to me â€” small but meaningful rewards for showing up again and again. ğŸ˜„  
    A reminder that kindness, like fitness, adds up over time.
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Each bag donated equals a chance for another heart to keep beating.&lt;br&gt;
Twenty-five down. Seventy-five to go.&lt;/p&gt;
&lt;p&gt;The nurse laughed as she said, &lt;em&gt;â€œYour blood is strong today â€” no iron pills needed!â€&lt;/em&gt;&lt;br&gt;
And I smiled, quietly grateful that I could still give.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-in-memory-of-his-majesty-king-bhumibol-adulyadej&#34;&gt;ğŸ‘‘ In Memory of His Majesty King Bhumibol Adulyadej&lt;/h2&gt;
&lt;p&gt;October 13 is a day of remembrance in Thailand â€” a day to honor &lt;strong&gt;His Majesty King Bhumibol Adulyadej (Rama IX)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;He remains one of my greatest inspirations: a monarch of intellect, humility, and compassion.&lt;br&gt;
He devoted his life to helping others â€” with sincerity, with science, with heart.&lt;/p&gt;
&lt;p&gt;So today, I dedicated my 25th donation to him.&lt;br&gt;
May this merit reach him in heaven.&lt;br&gt;
May his light continue to inspire us to serve with kindness and purpose.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00007.jpg&#34; alt=&#34;Blood donation certificate and King Rama IX commemoration&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: The 25th-donation certificate placed beside the portrait of His Majesty King Bhumibol Adulyadej â€” my quiet dedication to the King who taught Thailand the meaning of compassion.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-scenes-from-the-day&#34;&gt;ğŸ“¸ Scenes from the Day&lt;/h2&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00002.jpg&#34; alt=&#34;Nurse and donor exchange smiles&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: â€œYou again!â€ the nurse joked. I laughed. Some routines become friendships.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00004.jpg&#34; alt=&#34;Snack bag and thank-you note&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: After every donation, a snack and a small thank-you bag. On the back it read: â€œThank you, hero.â€ I smiled â€” not because I felt like one, but because so many unseen heroes pass through this place daily.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00010.jpg&#34; alt=&#34;Donation site interior&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 8: Today, I received an official certificate from the Thai Red Cross confirming that Iâ€™ve completed 24 blood donations.  
    (Though today was actually my 25th â€” the staff kindly issued the certificate retroactively.)  
    The 24th donation is a major milestone recognized by the Red Cross, and the next one will be at 36.  
    Honestly, receiving the certificate made me happy â€” but Iâ€™ve felt joy from the very first time I donated.  
    If my blood can help someone continue living â€” and living happily â€” thatâ€™s all Iâ€™ve ever wished for.  
    May everyone be safe and healthy. Iâ€™m aiming to reach 100 donations in this lifetime.
  &lt;/p&gt;
&lt;/div&gt;
&lt;!-- &lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;my_25th_blood_donation/KAO_25th_Blood_Donation_00011.jpg&#34; alt=&#34;Certificate on desk at night&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 9: Later that night, I placed the certificate on my desk. It glowed softly under the lamp â€” a reminder to keep going, to keep giving.&lt;/p&gt;
&lt;/div&gt; --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-reflections-on-giving&#34;&gt;ğŸŒˆ Reflections on Giving&lt;/h2&gt;
&lt;p&gt;Some people ask, â€œWhy do you keep doing it?â€&lt;br&gt;
The truth is simple â€” &lt;strong&gt;because I can&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;There are many who wish to donate but canâ€™t.&lt;br&gt;
So as long as Iâ€™m healthy, Iâ€™ll keep showing up.&lt;/p&gt;
&lt;p&gt;Blood donation isnâ€™t just a medical act.&lt;br&gt;
Itâ€™s an act of empathy â€” of human connection.&lt;br&gt;
It reminds me that no matter how modern our world becomes, the purest form of kindness still comes from the oldest instinct of all â€” to share life.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;â€œYou canâ€™t give life to everyone, but you can give a part of yours to someone who needs it most.â€&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-a-drop-that-shapes-tomorrow&#34;&gt;ğŸ©¸ A Drop That Shapes Tomorrow&lt;/h2&gt;
&lt;p&gt;Every donation begins the same way â€” a quiet morning, a waiting chair, a gentle prick on the arm. But behind that simplicity lies something far greater: a human connection written in red. As I watched the crimson thread flow through the clear tube, I realized how small acts often carry the deepest meaning. That single moment â€” fleeting, almost invisible â€” might one day be the difference between despair and another chance at life for someone Iâ€™ll never meet.&lt;/p&gt;
&lt;p&gt;Each time I donate, Iâ€™m reminded that science and compassion are not opposites; they are partners. The same precision that defines my work in AI and research also lives in this act of giving â€” a balance of knowledge and empathy, of the measurable and the immeasurable. We measure oxygen levels, iron counts, and hemoglobin rates, but not the hope that flows quietly from donor to recipient. And yet, that unseen data â€” the humanity between numbers â€” is what keeps our world alive.&lt;/p&gt;
&lt;p&gt;In that room, surrounded by nurses, patients, and volunteers, I saw no difference in background, belief, or language. There was only one truth that united us all: we share the same blood, the same fragile yet resilient essence that binds every life on this planet. To give blood is to acknowledge that we are all connected by something deeper than science â€” by empathy, courage, and the quiet wish to make tomorrow a little brighter for someone else.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I often think about where my donation goes â€” perhaps to a child fighting illness, a mother in surgery, or a stranger recovering from an accident. Iâ€™ll never know their names, but I carry their stories with me. Because every time I give, I donâ€™t just donate blood; I donate possibility, a continuation of lifeâ€™s narrative.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So hereâ€™s to every heartbeat that continues because of anotherâ€™s kindness, to every life that goes on because someone chose to care. The world doesnâ€™t need heroes in capes â€” it needs humans with open hearts and rolled-up sleeves. And if one drop of blood can ripple outward to save a life, then maybe, just maybe, thatâ€™s how we heal this world â€” one act of quiet generosity at a time. ğŸŒâ¤ï¸&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-grateful-always&#34;&gt;â¤ï¸ Grateful, Always&lt;/h2&gt;
&lt;p&gt;Iâ€™m grateful â€” for my health, for my body that allows me to help, for my motherâ€™s lessons, and for the chance to live a life that still has room for giving.&lt;/p&gt;
&lt;p&gt;Hereâ€™s to the next 75 donations, to every person I may unknowingly help, and to a lifetime of quiet, meaningful kindness.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;â€” Teerapong Panboonyuen (Kao)&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;October 2025, Thai Red Cross Society, Bangkok&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (October 2025). &lt;em&gt;My 25th Blood Donation â€” Celebrating the Quiet Joy and Lifelong Gift of Giving&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-10-13-my25th-blood-donation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-10-13-my25th-blood-donation/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{panboonyuen2025blood25,
  title   = &amp;quot;My 25th Blood Donation â€” Celebrating the Quiet Joy and Lifelong Gift of Giving&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Oct&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2025-10-13-my25th-blood-donation/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you enjoy reading this story? â¤ï¸
If yes, please consider sharing it â€” kindness multiplies when shared. ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GET TO KNOW ME BETTER</title>
      <link>https://kaopanboonyuen.github.io/talk/get-to-know-me-better/</link>
      <pubDate>Mon, 22 Sep 2025 05:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/get-to-know-me-better/</guid>
      <description>&lt;h3 id=&#34;a-quick-glimpse-at-the-person-behind-my-work&#34;&gt;A quick glimpse at the person behind my work&lt;/h3&gt;
&lt;p&gt;This profile photo represents more than just a faceâ€”it reflects the passion, curiosity, and dedication I bring to everything I do. Whether Iâ€™m writing code, mentoring students, or diving into cutting-edge AI research, I approach it all with the same drive and focus. From lecture halls at Chula to stages at international conferences, my goal is to help shape technology thatâ€™s smarter, more human, and genuinely impactful.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A big thank you to modern AI for enhancing these visuals, helping them align with the professional image I aim to project.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_profile_pic_01.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_profile_pic_03.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_profile_pic_05.png) --&gt;
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_profile_pic_07.png) --&gt;
&lt;p&gt;I tried experimenting with different outfit styles, but in my actual work lifeâ€”whether in a tech company or during my postdoc at Chulalongkorn Universityâ€”I usually just wear a comfortable t-shirt. The only time I switch it up a bit is when I have a presentation, and even then, itâ€™s still just a t-shirt. The only time I wore a suit was when I had the honor of attending the &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GYSS2025&lt;/a&gt; event where we welcomed HRH Princess Maha Chakri Sirindhorn. That was such a proud and unforgettable moment for me.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fun fact: I had that suit tailored for &lt;a href=&#34;https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my PhD graduation&lt;/a&gt; at Chulalongkorn, and it was really special to be able to wear it again for another royal audience with Her Royal Highness. I felt deeply honored and grateful.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_profile_pic_11.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_profile_pic_13.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_profile_pic_15.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the end, if I could choose just one photo that reflects me best, it would have to be this one.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_profile_pic_19.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;p&gt;Hereâ€™s another side of meâ€”beyond the labs and research. This candid travel photo is from the real world. Just me, out exploring and soaking in life. Iâ€™ve visited Tokyo, where I loved visiting Meiji Shrine (Meiji Jingu), as well as Pisa and Rome in Italy. Experiencing places like these reminds me how much I love the real world. These moments recharge me and give me fresh energy to dive back into writing research papers with renewed passion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;kao_japan_2020/KAO_PANBOONYUEN_JP20_02.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;KAO_PANBOONYUEN_ITA.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I tried using a diffusion model to transform a photo of me standing in front of the Colosseum in Italy into a U.S.-style licensed cartoon. The result turned out surprisingly well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;KAO_AT_ITALY_2023_CT_V1.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;p&gt;In mid-2023, when I traveled to Italy to present my research from the MARSAIL Lab, I had the chance to stop by the Leaning Tower of Pisa in the city of Pisa. It was absolutely wonderfulâ€”the weather was perfect and the place was filled with people.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;KAO_PANBOONYUEN_ITA_02.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_img01.jpg&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_img02.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;p&gt;P.S. Hereâ€™s a fun little AI-generated art toy version of meâ€”I was just curious to see what Iâ€™d look like as a cartoon character.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_art_toy_01.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_art_toy_06.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;
&lt;img src=&#34;panboonyuen_art_toy_09.png&#34; alt=&#34;Teerapong Panboonyuen&#34;&gt;&lt;/p&gt;
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_art_toy_08.png) --&gt;
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_art_toy_09.png) --&gt;
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_art_toy_10.png)
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_art_toy_11.png) --&gt;
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_art_toy_12.png) --&gt;
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_art_toy_07.png) --&gt;
&lt;!-- ![Teerapong Panboonyuen](panboonyuen_art_toy_04.png)
![Teerapong Panboonyuen](panboonyuen_art_toy_02.png)
![Teerapong Panboonyuen](panboonyuen_art_toy_03.png)
![Teerapong Panboonyuen](panboonyuen_art_toy_05.png) --&gt;
&lt;h3 id=&#34;beyond-the-research&#34;&gt;Beyond the Research&lt;/h3&gt;
&lt;p&gt;So, who am I beyond the algorithms and equations?&lt;/p&gt;
&lt;p&gt;Iâ€™m a tech enthusiast who believes in the power of innovation to make the world a better place. When Iâ€™m not deep into neural networks, youâ€™ll likely find me on the track, in the pool, or out cyclingâ€”training for my next IRONMAN triathlon. Endurance sports like running, marathons, and triathlons keep me grounded. For me, theyâ€™re a reminder that with grit, discipline, and perseverance, anything is possible.&lt;/p&gt;
&lt;p&gt;Iâ€™m also a lifelong learnerâ€”always curious about the latest trends in AI, technology, and science. But learning doesnâ€™t stop at theory. Iâ€™m passionate about sharing knowledge, volunteering, and engaging with communities that foster growth and inspiration.&lt;/p&gt;
&lt;p&gt;If youâ€™d like to follow along with my journey, check out &lt;a href=&#34;https://kaopanboonyuen.wordpress.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;My WordPress Blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And if you ever want to swap stories about the newest gadgets, explore the latest research, or just chat about life, donâ€™t hesitate to reach out at &lt;a href=&#34;mailto:panboonyuen.kao@gmail.com&#34;&gt;panboonyuen.kao@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kao Panboonyuen&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GuidedBox: A segmentation-guided box teacher-student approach for weakly supervised road segmentation</title>
      <link>https://kaopanboonyuen.github.io/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/</link>
      <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration</title>
      <link>https://kaopanboonyuen.github.io/publication/debiasing-llms-thai-political-stance/</link>
      <pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/debiasing-llms-thai-political-stance/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TSCCM2025 (The 14th Critical Care Conference)</title>
      <link>https://kaopanboonyuen.github.io/talk/tsccm2025-the-14th-critical-care-conference/</link>
      <pubDate>Thu, 17 Jul 2025 13:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/tsccm2025-the-14th-critical-care-conference/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;ğŸ“ Interested in the full technical walkthrough? Read the complete CU-ICU LLM blog post &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;ğŸ¤ You can download the full presentation slides from my CU-ICU oral talk at TSCCM 2025 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/Panboonyuen_CUICU_TSCCM2025_Slide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;ğŸ“„ Curious to dive deeper into the research? You can also read the full CU-ICU paper on arXiv &lt;a href=&#34;http://arxiv.org/abs/2507.13655&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>CU-ICU: Customizing Instruction-Tuned Language Models for Critical Care</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/</link>
      <pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;â€œMy passion is dedicated to supporting ICU healthcare professionals in Thailand by building accessible AI toolsâ€”this is an independent project with no external funding.â€&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-the-icu-challenge&#34;&gt;ğŸš¨ The ICU Challenge&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Large Language Models (LLMs) like ChatGPT and Google Gemini have taken the world by stormâ€”but adapting them to &lt;strong&gt;real-world ICU settings&lt;/strong&gt; is another story.&lt;/p&gt;
&lt;p&gt;In the ICU, time is critical. Clinical staff need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast predictions (like early &lt;strong&gt;sepsis detection&lt;/strong&gt;),&lt;/li&gt;
&lt;li&gt;Accurate &lt;strong&gt;mortality risk estimation&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;Understandable, &lt;strong&gt;clinically relevant explanations&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But here&amp;rsquo;s the catch: hospitalsâ€”especially in countries like Thailandâ€”often lack large labeled datasets or GPU-rich infrastructure.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Proud to finally share CU-ICU â€” my latest work on customizing instruction-finetuned language models for Thai ICU doctors. ğŸ§‘â€âš•ï¸ğŸ’¡&lt;br&gt;&lt;br&gt;Based on T5 + LoRA/AdaLoRA/IA3 â€” optimized for clinical reasoning &amp;amp; real-world care.&lt;br&gt;&lt;br&gt;ğŸ“– Full blog:&lt;a href=&#34;https://t.co/hMrwI0gN5l&#34;&gt;https://t.co/hMrwI0gN5l&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1946147237937197550?ref_src=twsrc%5Etfw&#34;&gt;July 18, 2025&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;hr&gt;
&lt;h2 id=&#34;-introducing-cu-icu&#34;&gt;ğŸ’¡ Introducing CU-ICU&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;CU-ICU&lt;/strong&gt; is my proposed framework that adapts large instruction-finetuned LLMs like T5 for ICU tasks, using &lt;strong&gt;sparse parameter-efficient fine-tuning (PEFT)&lt;/strong&gt; techniques. It balances &lt;strong&gt;accuracy&lt;/strong&gt;, &lt;strong&gt;interpretability&lt;/strong&gt;, and &lt;strong&gt;efficiency&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I evaluated it on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sepsis Detection&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mortality Prediction&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Clinical Note Generation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With three leading PEFT strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸ§© &lt;strong&gt;LoRA&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;ğŸ“‰ &lt;strong&gt;AdaLoRA&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;ğŸ§  &lt;strong&gt;(IA)$^3$&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/CUICU_TITLE.jpg&#34; alt=&#34;CU-ICU Oral Presentation Begins&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 1: The moment I stepped onto the stage at TSCCM 2025 to present my research on CU-ICUâ€”my heart was racing, but my passion was louder. This was not just any room; it was Conference Room 1 on the 12th floor of Bhumisiri Mangkhalanusorn Building at King Chulalongkorn Memorial Hospital, filled with top clinicians and medical experts. It felt deeply meaningful to return to a place that shaped my academic path, this time to introduce an AI model designed not for global trends, but for Thai hospitals, Thai doctors, and Thai patients. CU-ICU was built from the heartâ€”with no big budget, no flashy grantsâ€”just the desire to make something useful and real.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/CUICU_ORAL.png&#34; alt=&#34;Oral Presentation Schedule&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: Out of all the names listed for oral presentations, mine came firstâ€”talk about pressure! As I scanned the schedule, I realized something fascinating: I was the only one presenting on AI, LLMs, and NLP. The rest were brilliant case studies and clinical research from doctors around Thailand and Asia. It was humbling to stand out not because of prestige, but because I represented a new kind of voiceâ€”a bridge between AI and healthcare, speaking to an audience of physicians about how ML might ease their daily workload and complement their medical judgment.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/TSCCM2025_POSTER_LISTS.png&#34; alt=&#34;TSCCM 2025 Poster Presentation List&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: The poster session lineup at TSCCM 2025 read like a roadmap of cutting-edge clinical practice. Most submissions came from hospitals across Japan, rich in medical case studies, practical findings, and cross-hospital comparisons. While CU-ICU wasnâ€™t in this section, it made me realize the breadth of collaboration and knowledge-sharing that exists across borders in the healthcare research community. The energy was global, but the mission was deeply human: to make care better.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/CUICU_WHY_I_CREATED.jpg&#34; alt=&#34;Why CU-ICU Was Created&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: Why build CU-ICU? The answer is simple: passion. It wasnâ€™t born from a funded research project or a corporate mandate. It came from listening to doctorsâ€”hearing how repetitive and time-consuming many of their information-related tasks can be. CU-ICU is a Thai-built large language model (LLM) designed specifically to support local medical professionals. Its goal is not to replace anyone, but to serve as an AI assistant that understands the rhythm of a Thai ICU. By answering common clinical questions, it gives doctors more time to focus on the patientâ€”the human behind the data. This moment of sharing that â€˜whyâ€™ with a room full of doctors was the most meaningful part of the talk for me.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/CUICU_WHAT_IS.jpg&#34; alt=&#34;What is CU-ICU?&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: CU-ICU is built on the FLAN-T5 architecture and fine-tuned using lightweight adaptation techniques like LoRA, AdaLoRA, and IA3. Coming from a computer vision background, diving into language models was a leapâ€”but a thrilling one. The idea of building an AI model that could &#34;speak medicine&#34; in Thai was both technically challenging and emotionally rewarding. I even asked the audienceâ€”doctors from various hospitalsâ€”how many had used tools like ChatGPT, Gemini, or DeepSeek. Almost every hand went up. That moment confirmed: the future of medicine and LLMs are already intersecting. CU-ICU wants to be part of that conversation, in my own language.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-results-snapshot&#34;&gt;ğŸ§ª Results Snapshot&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;I fine-tuned FLAN-T5 on ICU datasets using 16-shot prompts. Hereâ€™s what CU-ICU achieved:&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-prediction-tasks&#34;&gt;ğŸ” Prediction Tasks&lt;/h3&gt;
&lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Config&lt;/th&gt;
&lt;th&gt;Params (%)&lt;/th&gt;
&lt;th&gt;Sepsis Acc.&lt;/th&gt;
&lt;th&gt;Mortality Acc.&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;(IA)$^3$ (All Layers)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;0.9%&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;85.6%&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;80.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AdaLoRA&lt;/td&gt;
&lt;td&gt;1.0 Budget, Rank=8&lt;/td&gt;
&lt;td&gt;2.9%&lt;/td&gt;
&lt;td&gt;83.5%&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;80.9%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LoRA&lt;/td&gt;
&lt;td&gt;Rank=16&lt;/td&gt;
&lt;td&gt;6.2%&lt;/td&gt;
&lt;td&gt;83.1%&lt;/td&gt;
&lt;td&gt;79.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;âœ… CU-ICU achieves up to &lt;strong&gt;85.6% accuracy in sepsis detection&lt;/strong&gt;â€”with less than &lt;strong&gt;1%&lt;/strong&gt; of model weights updated!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-clinical-note-quality&#34;&gt;ğŸ“ Clinical Note Quality&lt;/h3&gt;
&lt;hr&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Config&lt;/th&gt;
&lt;th&gt;nBERTScore&lt;/th&gt;
&lt;th&gt;Avg Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;(IA)$^3$ (All Layers)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;32.1&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;66.0&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AdaLoRA&lt;/td&gt;
&lt;td&gt;Rank=16&lt;/td&gt;
&lt;td&gt;30.6&lt;/td&gt;
&lt;td&gt;65.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LoRA&lt;/td&gt;
&lt;td&gt;Rank=16&lt;/td&gt;
&lt;td&gt;28.3&lt;/td&gt;
&lt;td&gt;63.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;ğŸ§  CU-ICUâ€™s best configuration &lt;strong&gt;generated the most clinically relevant explanations&lt;/strong&gt;, evaluated via nBERTScore.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/CUICU_CONCLUSION.jpg&#34; alt=&#34;CU-ICU Summary and Future Plans&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: CU-ICU currently achieves about 66% accuracy on my curated ICU question dataset. It&#39;s not perfect, but it&#39;s a beginning. With more collaborationâ€”especially from Thai doctors who can contribute anonymized data or clinical QA patternsâ€”I believe CU-ICU can evolve into something truly impactful. But I also emphasized that the model must remain ethical, inclusive, and safe. Biases must be addressed. Feedback must be welcomed. This is not just an AI system; itâ€™s a co-created medical assistant, shaped by the people it aims to help.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/CU_ICU_Sample_01.png&#34; alt=&#34;CU-ICU response to critical care prompt&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 7: A powerful moment from CU-ICU in action. The model receives a real-world ICU prompt: &lt;em&gt;&#34;56M, temp 39.5Â°C, MAP 60, HR 132, 20, lactate 4.0, no urine in 6 hrs. Sepsis?&#34;&lt;/em&gt; Its response? Not just accurateâ€”but clinically impressive: &lt;em&gt;&#34;Strong evidence of septic shock: persistent hypotension (MAP &amp;lt; 65), hyperlactatemia, and oliguria. Immediate antibiotics and fluid bolus recommended per Surviving Sepsis Campaign (2021).&#34;&lt;/em&gt; What makes this remarkable isnâ€™t just the correctness, but the way CU-ICU integrates multi-symptom reasoning, adheres to formal clinical guidelines, and even offers actionable next steps. In this moment, it felt like CU-ICU wasn&#39;t just a research modelâ€”it was a prototype for the kind of AI assistant Thai doctors could one day rely on in the most critical of decisions.
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-why-it-works&#34;&gt;ğŸ¤– Why It Works&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;CU-ICU leverages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Instruction tuning&lt;/strong&gt; (T5 + FLAN-like prompts)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Few-shot learning&lt;/strong&gt; (16-shot prompts)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sparse fine-tuning&lt;/strong&gt; using LoRA, AdaLoRA, and (IA)$^3$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This makes the model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt; ğŸª¶&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate&lt;/strong&gt; ğŸ“ˆ&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interpretable&lt;/strong&gt; ğŸ”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All without expensive GPU clusters.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-a-step-toward-thai-hospital-ai&#34;&gt;ğŸŒ A Step Toward Thai Hospital AI&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Most Thai hospitals operate under &lt;strong&gt;resource constraints&lt;/strong&gt;. My goal with CU-ICU is to show:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can still deploy &lt;strong&gt;LLM-based clinical AI&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;With just &lt;strong&gt;a few labels&lt;/strong&gt;, &lt;strong&gt;smart tuning&lt;/strong&gt;, and &lt;strong&gt;open-source models&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&amp;rsquo;m excited about bringing this into Thai ICU workflows to help overworked clinicians with decision support tools that actually make sense in practice.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/CUICU_CERTIFICATE.jpg&#34; alt=&#34;Oral Presentation Certificate&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: My certificate for the oral sessionâ€”a proud reminder that I was one of just eight selected presenters. The other seven? All medical doctors. That made it even more special. Being the only AI researcher talking about LLMs in a sea of clinicians reminded me that technology must meet people where they are. One of the most heartwarming moments came afterward, when a senior doctor complimented the work. And perhaps even more emotionalâ€”this was my return to Chulalongkorn University, my Ph.D. alma mater, after nearly five years. Walking the familiar halls again, this time as a speaker, I felt immense gratitude. Chula gave me the roots, and CU-ICU is one of the branches Iâ€™ve grown.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;ğŸ¤ You can download the full presentation slides from my CU-ICU oral talk at TSCCM 2025 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/Panboonyuen_CUICU_TSCCM2025_Slide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;ğŸ“„ Curious to dive deeper into the research? You can also read the full CU-ICU paper on arXiv &lt;a href=&#34;http://arxiv.org/abs/2507.13655&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-references-selected&#34;&gt;ğŸ“š References (selected)&lt;/h2&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LoRA (Hu et al., 2021)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FLAN-T5 (Chung et al., 2022)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.02311&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PaLM (Chowdhery et al., 2022)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.09617&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MedPaLM (Singhal et al., 2023)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/research/gpt-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT (OpenAI, 2023)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://deepmind.google/discover/blog/google-gemini-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gemini (Google DeepMind, 2024)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (July 2025). &lt;em&gt;CU-ICU: Customizing Instruction-Tuned Language Models for Critical Care&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{panboonyuen2025cuicu,
  title   = &amp;quot;CU-ICU: Customizing Instruction-Tuned Language Models for Critical Care&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Jul&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Raffel, Colin, et al. &amp;ldquo;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.&amp;rdquo; &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 21.140 (2020): 1-67.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hu, Edward J., et al. &amp;ldquo;LoRA: Low-Rank Adaptation of Large Language Models.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2106.09685 (2021).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wang, Shao, et al. &amp;ldquo;AdaLoRA: Adaptive Low-Rank Adaptation for Efficient Fine-tuning.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2208.07339 (2022).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Guo, Ruiyi, et al. &amp;ldquo;(IA)Â³: Trainable Multiplicative Masks for Parameter-Efficient Fine-Tuning.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2206.10169 (2022).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lee, Jinhyuk, et al. &amp;ldquo;BioBERT: a pre-trained biomedical language representation model for biomedical text mining.&amp;rdquo; &lt;em&gt;Bioinformatics&lt;/em&gt; 36.4 (2020): 1234-1240.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Huang, Kexin, et al. &amp;ldquo;ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1904.05342 (2019).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Singhal, Karan, et al. &amp;ldquo;Large Language Models Encode Clinical Knowledge.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2212.13138 (2022).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jin, Dongmin, et al. &amp;ldquo;MedAlpaca: A Medical Instruction-Finetuned Large Language Model.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2304.12140 (2023).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lester, Brian, et al. &amp;ldquo;Powerful few-shot learning with frozen language models and pattern-tuning.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2107.13586 (2021).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Brown, Tom B., et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 33 (2020): 1877-1901.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer</title>
      <link>https://kaopanboonyuen.github.io/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/</link>
      <pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visionâ€“Language Models for Remote Sensing: A New Era of Multimodal Understanding</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/</link>
      <pubDate>Sat, 05 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/</guid>
      <description>&lt;p&gt;Hi, itâ€™s me again, &lt;strong&gt;Kao Panboonyuen&lt;/strong&gt;â€”welcome back to my blog!&lt;/p&gt;
&lt;p&gt;Today, Iâ€™m diving into a topic thatâ€™s been on my mind a lot lately: the exciting intersection of &lt;strong&gt;Remote Sensing&lt;/strong&gt; and &lt;strong&gt;Large Language Models (LLMs)&lt;/strong&gt;. As you know, &lt;strong&gt;remote sensing&lt;/strong&gt; is one of the most powerful tools for understanding the Earthâ€™s surface, using &lt;strong&gt;satellite systems&lt;/strong&gt; like &lt;strong&gt;Sentinel-2&lt;/strong&gt;, &lt;strong&gt;Landsat-8&lt;/strong&gt;, and &lt;strong&gt;THEOS&lt;/strong&gt; to capture tons of &lt;strong&gt;high-resolution data&lt;/strong&gt;. From monitoring &lt;strong&gt;environmental changes&lt;/strong&gt; to assisting in &lt;strong&gt;urban planning&lt;/strong&gt;, this data has countless applications. But hereâ€™s the catchâ€”while we have all this amazing data at our fingertips, its sheer volume and complexity can overwhelm traditional methods of &lt;strong&gt;data classification&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;The world is changing fast&lt;/strong&gt;, and technology is evolving at a rate that we can barely keep up with. But with every &lt;strong&gt;challenge&lt;/strong&gt; comes an &lt;strong&gt;opportunity&lt;/strong&gt;â€”an opportunity to &lt;strong&gt;innovate&lt;/strong&gt;, &lt;strong&gt;adapt&lt;/strong&gt;, and transform how we understand and interact with the world around us. The marriage of &lt;strong&gt;Remote Sensing&lt;/strong&gt; and &lt;strong&gt;Large Language Models (LLMs)&lt;/strong&gt; is one such opportunity. Itâ€™s a chance to take the immense potential of &lt;strong&gt;satellite data&lt;/strong&gt; and push it to &lt;strong&gt;new heights&lt;/strong&gt;, uncovering &lt;strong&gt;insights&lt;/strong&gt; that we once thought were out of reach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As we stand on the cusp of a &lt;strong&gt;new frontier&lt;/strong&gt; in data analysis, itâ€™s exciting to think about how these advancements will not only &lt;strong&gt;revolutionize industries&lt;/strong&gt; but also make a &lt;strong&gt;tangible difference&lt;/strong&gt; in solving &lt;strong&gt;real-world problems&lt;/strong&gt;. Join me on this journey as we explore the incredible potential of combining &lt;strong&gt;cutting-edge technology&lt;/strong&gt; with the power of &lt;strong&gt;Earth observation&lt;/strong&gt;. The &lt;strong&gt;future&lt;/strong&gt; is here, and itâ€™s incredibly &lt;strong&gt;bright&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;A new era of multimodal intelligence is upon us! ğŸŒğŸš€&lt;br&gt;&lt;br&gt;â€œVisionâ€“Language Models for Remote Sensing: A New Era of Multimodal Understandingâ€&lt;br&gt;&lt;br&gt;Iâ€™ve just finished writing itâ€”check out my latest post here:&lt;a href=&#34;https://t.co/u9lmpCzKfU&#34;&gt;https://t.co/u9lmpCzKfU&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1942031731395281112?ref_src=twsrc%5Etfw&#34;&gt;July 7, 2025&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Older algorithms like &lt;strong&gt;Support Vector Machines (SVM)&lt;/strong&gt; and &lt;strong&gt;Random Forest (RF)&lt;/strong&gt; have done a decent job, but they often struggle to tap into the &lt;strong&gt;deeper semantic meaning&lt;/strong&gt; within the data. They can classify pixels, sure, but they canâ€™t grasp the rich, contextual relationships between them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thatâ€™s where &lt;strong&gt;LLMs&lt;/strong&gt; come in! These models, originally built for &lt;strong&gt;natural language processing&lt;/strong&gt;, have the power to &lt;strong&gt;revolutionize remote sensing&lt;/strong&gt; by interpreting and analyzing data on a much deeper level. By &lt;strong&gt;integrating contextual insights&lt;/strong&gt; from &lt;strong&gt;satellite metadata&lt;/strong&gt; and &lt;strong&gt;environmental reports&lt;/strong&gt;, &lt;strong&gt;LLMs&lt;/strong&gt; can enhance tasks like &lt;strong&gt;LULC (Land Use/Land Cover) classification&lt;/strong&gt; and &lt;strong&gt;image analysis&lt;/strong&gt;. The results? Much more accurate and insightful interpretations of satellite imagery, opening up a whole new world of possibilities for &lt;strong&gt;remote sensing applications&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Remote sensing&lt;/strong&gt; has become an indispensable tool for gaining detailed insights into the Earth&amp;rsquo;s surface, with satellite systems like &lt;strong&gt;Sentinel-2&lt;/strong&gt;, &lt;strong&gt;Landsat-8&lt;/strong&gt;, and &lt;strong&gt;THEOS&lt;/strong&gt; providing an endless stream of &lt;strong&gt;high-resolution data&lt;/strong&gt;. From &lt;strong&gt;environmental monitoring&lt;/strong&gt; to &lt;strong&gt;urban planning&lt;/strong&gt;, this data fuels critical applications across a range of sectors. Yet, the vast volume and &lt;strong&gt;complexity&lt;/strong&gt; of satellite imagery can present a significant challenge. Traditional methods of &lt;strong&gt;data classification&lt;/strong&gt;, such as &lt;strong&gt;Support Vector Machines (SVM)&lt;/strong&gt; and &lt;strong&gt;Random Forest (RF)&lt;/strong&gt;, have delivered valuable results but are often unable to capture the full &lt;strong&gt;semantic richness&lt;/strong&gt; embedded within this data. They excel at processing large datasets, but struggle when tasked with understanding the deeper, contextual relationships between the elements within these images.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Enter the era of &lt;strong&gt;Large Language Models (LLMs)&lt;/strong&gt;, which are revolutionizing the way we process and interpret remote sensing data. Originally designed to understand and generate &lt;strong&gt;human-like language&lt;/strong&gt;, LLMs have demonstrated an incredible capacity to enhance &lt;strong&gt;data analysis&lt;/strong&gt; by integrating &lt;strong&gt;contextual information&lt;/strong&gt; from multiple sourcesâ€”such as &lt;strong&gt;environmental reports&lt;/strong&gt;, &lt;strong&gt;satellite metadata&lt;/strong&gt;, and &lt;strong&gt;geospatial context&lt;/strong&gt;. This ability to handle &lt;strong&gt;multimodal data&lt;/strong&gt; opens up new avenues for more accurate &lt;strong&gt;Land Use/Land Cover (LULC)&lt;/strong&gt; classification and &lt;strong&gt;image interpretation&lt;/strong&gt;, driving improvements in remote sensing applications that were once constrained by traditional algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a recent Twitter post, &lt;strong&gt;Lilian Weng&lt;/strong&gt; excitedly shared the launch of &lt;strong&gt;Thinking Machines Lab&lt;/strong&gt;, a cutting-edge AI research and product company dedicated to pushing the boundaries of &lt;strong&gt;multimodal understanding&lt;/strong&gt;. As she mentions, the lab is home to some of the brightest minds behind innovations like &lt;strong&gt;ChatGPT&lt;/strong&gt;. Their focus on multimodal AI is particularly relevant to the work being done with &lt;strong&gt;Visionâ€“Language Models (VLMs)&lt;/strong&gt;, which are rapidly transforming how we analyze remote sensing data. Just like &lt;strong&gt;Thinking Machines Lab&lt;/strong&gt; aims to integrate various AI disciplines to achieve greater synergy, &lt;strong&gt;VLMs&lt;/strong&gt; are creating new possibilities for understanding &lt;strong&gt;satellite imagery&lt;/strong&gt; by combining &lt;strong&gt;computer vision&lt;/strong&gt; and &lt;strong&gt;natural language processing&lt;/strong&gt;. This shift not only enhances our ability to interpret complex data but also paves the way for more intuitive, human-like understanding of the world around us. To learn more about &lt;strong&gt;Thinking Machines Lab&lt;/strong&gt; and their vision, check out &lt;a href=&#34;https://thinkingmachines.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thinking Machines&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This is something we have been cooking together for a few months and I&amp;#39;m very excited to announce it today.&lt;br&gt;&lt;br&gt;Thinking Machines Lab is my next adventure and I&amp;#39;m feeling very proud and lucky to start it with a group of talented colleagues. Learn more about our vision atâ€¦ &lt;a href=&#34;https://t.co/eKQYvuwurB&#34;&gt;https://t.co/eKQYvuwurB&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lilian Weng (@lilianweng) &lt;a href=&#34;https://twitter.com/lilianweng/status/1891922794402939092?ref_src=twsrc%5Etfw&#34;&gt;February 18, 2025&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;hr&gt;
&lt;h3 id=&#34;visionlanguage-models-vlms&#34;&gt;Visionâ€“Language Models (VLMs)&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;In recent years, &lt;strong&gt;Visionâ€“Language Models (VLMs)&lt;/strong&gt; have emerged as a groundbreaking tool in remote sensing, particularly in enhancing the interpretation of satellite imagery. These models merge visual data with linguistic understanding, offering a more holistic approach to analyzing complex remote sensing data.&lt;/p&gt;
&lt;p&gt;One of the most exciting developments in this area is how VLMs can enrich traditional remote sensing datasets. While traditional datasets often rely purely on raw imagery, VLMs leverage contextual relationships between visual inputs and textual data, providing more nuanced insights. The integration of textual descriptions or geospatial metadata alongside imagery allows for a deeper understanding of land use, land cover changes, and other environmental factors. This ability is showcased in a comparative visualization, where you can clearly see how VLM datasets offer richer and more detailed information than traditional methods.&lt;/p&gt;
&lt;p&gt;The power of VLMs lies not just in their ability to process images, but also in their proficiency at handling a variety of tasks simultaneously. For instance, these models can assist in tasks such as object detection, classification, and even scene understanding. A representation of these tasks reveals just how versatile VLMs can be across different domains of remote sensing. From simple land cover classification to more complex tasks like environmental monitoring, VLMs are equipped to tackle them all.&lt;/p&gt;
&lt;p&gt;As we dig deeper into the structure of VLMs, it becomes clear that they come in various forms, each designed to handle specific challenges in remote sensing. For example, &lt;strong&gt;contrastive models&lt;/strong&gt; focus on matching images with descriptive text, while &lt;strong&gt;conversational models&lt;/strong&gt; are more dynamic, enabling interactive querying and real-time interpretation of satellite imagery. This distinction in architecture allows VLMs to be adaptable to a wide range of applications, from automatic image captioning to detailed environmental analysis.&lt;/p&gt;
&lt;p&gt;In addition to their core functionality, enhancement techniques are often employed to refine VLM performance. Some layers of the model are &lt;strong&gt;fine-tuned&lt;/strong&gt; to improve precision, while others are kept &lt;strong&gt;frozen&lt;/strong&gt; to preserve previously learned features. These techniques are crucial for boosting model efficiency without overfitting, ensuring that VLMs can be effectively applied to large-scale remote sensing tasks.&lt;/p&gt;
&lt;p&gt;The growing interest in VLMs is also reflected in the increasing number of academic publications dedicated to this field. Over the past few years, the volume of research in the intersection of VLMs and remote sensing has surged, reflecting the potential of these models to transform how we understand and analyze satellite data.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00001.png&#34; alt=&#34;Comparison of traditional and VLM datasets in remote sensing&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 1: A comparative analysis between traditional and Visionâ€“Language Model (VLM) datasets in the context of remote sensing. Traditional datasets typically rely on isolated visual data (such as satellite images) for classification and interpretation, often limited by the scope of raw pixel-based information. In contrast, VLM datasets integrate both visual and textual modalities, incorporating contextual information like environmental reports, geospatial metadata, and textual descriptions. This hybrid approach enhances the model&#39;s ability to capture complex relationships and nuanced patterns in satellite imagery, improving classification accuracy and providing richer insights for land use/land cover (LULC) analysis and other remote sensing tasks. (Image source: &lt;a href=&#34;https://www.mdpi.com/2072-4292/17/1/162&#34; target=&#34;_blank&#34;&gt;MDPI&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As shown in the visual comparison above, the shift from traditional datasets to VLM datasets in remote sensing is not just about increasing data volume but about enhancing the depth and accuracy of the insights derived from satellite imagery. The traditional approach provides basic visual data, while VLM datasets incorporate semantic understanding, making it easier to identify patterns, trends, and anomalies that would otherwise go unnoticed.&lt;/p&gt;
&lt;p&gt;One of the remarkable aspects of VLMs is their ability to handle a wide variety of tasks. The tasks shown in the figure below range from simple image classification to more complex objectives such as environmental change detection and land use forecasting. With their integrated vision and language capabilities, VLMs can extract much more detailed and contextually relevant information from imagery than traditional models.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00002.png&#34; alt=&#34;Representative tasks of Visionâ€“Language Models in remote sensing&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: A detailed overview of representative tasks handled by Visionâ€“Language Models (VLMs) in remote sensing. VLMs bridge the gap between visual and textual data, enabling them to perform complex multimodal tasks such as image captioning, object detection, and semantic segmentation. These models can analyze high-resolution satellite imagery while leveraging textual data like geospatial metadata, environmental descriptions, and sensor reports to provide a more comprehensive understanding of the landscape. For instance, VLMs are increasingly used in land use/land cover (LULC) classification, change detection, and disaster monitoring, offering enhanced performance over traditional methods by incorporating contextual information to reduce ambiguity. (Image source: &lt;a href=&#34;https://www.mdpi.com/2072-4292/17/1/162&#34; target=&#34;_blank&#34;&gt;MDPI&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The flexibility of VLMs in handling such diverse tasks opens new possibilities in remote sensing, where the understanding of both imagery and textual data is vital for accurate analysis. The synergy between vision and language also plays a key role in making sense of complex environmental data.&lt;/p&gt;
&lt;p&gt;When we look at the underlying architecture of VLMs, itâ€™s clear that they are not a one-size-fits-all solution. Models can either use a &lt;strong&gt;contrastive approach&lt;/strong&gt;, focusing on pairing images with descriptions, or a more &lt;strong&gt;conversational approach&lt;/strong&gt;, which allows for interactive querying of imagery. This flexibility is key for the adaptability of VLMs in real-world applications, whether it&amp;rsquo;s for immediate analysis or long-term monitoring.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00004.png&#34; alt=&#34;Architecture of contrastive and conversational Visionâ€“Language Models&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: A detailed breakdown of the architectures for contrastive (left) and conversational (right) Visionâ€“Language Models (VLMs), two key paradigms that drive the integration of visual and textual data in remote sensing. The contrastive architecture utilizes a dual encoder framework where image features and textual features are independently extracted and then aligned in a common multimodal space. This design is particularly effective in tasks requiring fine-grained matching, such as image captioning and cross-modal retrieval. On the other hand, the conversational architecture builds upon the contrastive model by introducing a dialogue-based approach, enabling more dynamic interactions between the visual content and natural language queries. This makes conversational models ideal for applications like interactive mapping, question-answering systems, and real-time disaster monitoring, where context and user input are continually evolving. The figure contrasts these two architectures to highlight how they each excel in different aspects of remote sensing tasks. (Image source: &lt;a href=&#34;https://www.mdpi.com/2072-4292/17/1/162&#34; target=&#34;_blank&#34;&gt;MDPI&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This versatility in architecture makes VLMs well-suited to tackle a broad spectrum of remote sensing tasks. Whether you need to extract simple patterns or generate more detailed, context-rich analyses, the choice of model architecture significantly impacts the output.&lt;/p&gt;
&lt;p&gt;Furthermore, as with all machine learning models, VLMs require careful optimization to ensure they perform at their best. Enhancement techniques, like &lt;strong&gt;fine-tuning&lt;/strong&gt; certain layers and keeping others &lt;strong&gt;frozen&lt;/strong&gt;, are used to strike the right balance between model complexity and efficiency. These strategies help to refine VLMs for specific tasks, improving accuracy and minimizing computational overhead, as illustrated in the figure below.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00005.png&#34; alt=&#34;Enhancement techniques in Visionâ€“Language Models&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: A visual representation of the various enhancement techniques employed in Visionâ€“Language Models (VLMs) to improve their performance and accuracy in remote sensing applications. These techniques include fine-tuning and transfer learning, where pre-trained models are further optimized to adapt to specific domain tasks. The diagram showcases the interplay between frozen and fine-tuned layers, with fine-tuning denoted by the fire icon, representing the model&#39;s ability to adapt to new data, and frozen layers indicated by the snowflake icon, maintaining the stability of pre-existing knowledge. By employing these enhancement strategies, VLMs can effectively improve their ability to handle complex tasks such as LULC classification, disaster detection, and environmental monitoring, all while reducing the risk of overfitting and improving generalization to unseen datasets. (Image source: &lt;a href=&#34;https://www.mdpi.com/2072-4292/17/1/162&#34; target=&#34;_blank&#34;&gt;MDPI&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;These enhancement techniques are essential for optimizing VLMs for large-scale remote sensing tasks, enabling them to extract meaningful insights from satellite imagery without becoming too computationally expensive.&lt;/p&gt;
&lt;p&gt;The growing body of research on VLMs reflects the increasing recognition of their potential in remote sensing. A recent graph shows a clear upward trend in the number of publications on the use of VLMs in this field, indicating that the scientific community is embracing these models as a valuable tool for future research and applications.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00003.png&#34; alt=&#34;Examples of tasks handled by Visionâ€“Language Models (VLMs) in remote sensing&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: A comprehensive breakdown of the various complex tasks addressed by Visionâ€“Language Models (VLMs) in the field of remote sensing. By combining both visual data (such as satellite imagery) and textual information (like metadata, sensor reports, and geospatial documentation), VLMs are revolutionizing the way we approach tasks like image captioning, land use/land cover (LULC) classification, and change detection. The integration of these modalities allows VLMs to not only process raw visual content but also to infer meaningful insights by incorporating contextual understanding. This synergy of vision and language empowers VLMs to enhance disaster monitoring, environmental analysis, and climate change studies, offering unprecedented accuracy and interpretability in remote sensing applications. (Image source: &lt;a href=&#34;https://www.mdpi.com/2072-4292/17/1/162&#34; target=&#34;_blank&#34;&gt;MDPI&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As the field of VLMs in remote sensing continues to expand, itâ€™s clear that these models will play a pivotal role in transforming how we analyze and interpret satellite imagery, driving new innovations and methodologies in environmental monitoring and beyond.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;understanding-large-language-models-llms&#34;&gt;Understanding Large Language Models (LLMs)&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;LLMs, such as GPT-4, are trained on massive corpora of text data, enabling them to grasp the intricacies of human language. However, their capacity to process sequential information isn&amp;rsquo;t limited to just text. Recent studies have shown that LLMs, when appropriately adapted, can learn to analyze non-textual data types, such as images, by incorporating their textual understanding into feature extraction processes.&lt;/p&gt;
&lt;p&gt;In remote sensing, LLMs can be adapted to the following roles:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Textual Contextualization&lt;/strong&gt;: LLMs can process and generate insights from external textual datasets, such as reports, maps, and metadata, which provide contextual information for satellite images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Enhanced Feature Extraction&lt;/strong&gt;: By understanding the relationships between textual data and imagery, LLMs help derive semantic features that are difficult for traditional image processing algorithms to capture.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mathematically, LLMs utilize attention mechanisms, specifically the Transformer architecture, which enables them to weigh different parts of the input sequence with varying importance. This attention mechanism can be defined as:&lt;/p&gt;
&lt;p&gt;$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Q$ represents the query (textual or image features),&lt;/li&gt;
&lt;li&gt;$K$ represents the key (spatial or contextual attributes),&lt;/li&gt;
&lt;li&gt;$V$ is the value (output features or attention score),&lt;/li&gt;
&lt;li&gt;$d_k$ is the dimensionality of the key vectors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This process helps in focusing on relevant parts of the data, whether it be image patches or semantic concepts, which improves classification accuracy.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;llms-in-lulc-classification&#34;&gt;LLMs in LULC Classification&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Land Use/Land Cover (LULC) classification is a fundamental task in remote sensing, involving the categorization of land regions into distinct types based on satellite imagery. Traditional methods, while powerful, often overlook the contextual understanding of features that can be provided by external data.&lt;/p&gt;
&lt;p&gt;To improve accuracy, we employ a hybrid model that combines Convolutional Neural Networks (CNNs) with LLMs. The CNN extracts spatial features from satellite images, while the LLM extracts contextual information from external sources.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;methodology&#34;&gt;Methodology&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The integration of LLMs in the LULC classification process can be broken down into the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Collection&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High-resolution imagery is collected from platforms like Sentinel-2, Landsat-8, and THEOS, which offer multispectral and multisource data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preprocessing&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Images are corrected for atmospheric distortion and radiometrically normalized using techniques such as the Dark Object Subtraction (DOS) method to reduce atmospheric scattering.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Extraction&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNN architectures (such as ResNet-50 or EfficientNet) are applied to extract spatial features from satellite imagery. Simultaneously, large environmental corpora, such as land use reports, are processed by LLMs to extract contextual knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hybrid Model Training&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The CNN-derived spatial features and the LLM-derived contextual features are concatenated and fed into a fully connected neural network for final classification.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification and Validation&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After training, the model is applied to a test set of images. Metrics such as accuracy, precision, recall, and F1-score are used to evaluate the performance. A typical validation equation for precision and recall is given as:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$TP$ is the number of true positives,&lt;/li&gt;
&lt;li&gt;$FP$ is the number of false positives,&lt;/li&gt;
&lt;li&gt;$FN$ is the number of false negatives.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;case-study-sentinel-2-imagery-classification&#34;&gt;Case Study: Sentinel-2 Imagery Classification&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;To demonstrate the power of LLMs in enhancing LULC classification, we present a case study using Sentinel-2 imagery. Sentinel-2 provides multispectral images at a spatial resolution of up to 10 meters, allowing for detailed land cover analysis.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;data-collection-and-preprocessing&#34;&gt;Data Collection and Preprocessing&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Sentinel-2 images covering a diverse region were downloaded and preprocessed to correct atmospheric effects. Image reflectance values were normalized using the FLAASH (Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes) method to improve data consistency.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;feature-extraction&#34;&gt;Feature Extraction&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;We applied ResNet-50 for feature extraction from the raw satellite imagery. For contextual understanding, an LLM (such as GPT-4) was fed with environmental reports and metadata related to the region to extract textual features that provide insights into land use policies, climate conditions, and historical land usage trends.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;hybrid-model-training&#34;&gt;Hybrid Model Training&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The extracted features from both the CNN and LLM were combined into a hybrid model, trained using a multi-class cross-entropy loss function. The model achieved an accuracy improvement of 12% compared to traditional classification methods that relied solely on CNNs or SVMs.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;challenges-and-future-prospects&#34;&gt;Challenges and Future Prospects&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;While the integration of LLMs in remote sensing holds promise, several challenges remain. These include the need for large, high-quality datasets for training LLMs, the computational cost of training hybrid models, and the difficulty in obtaining ground truth data for validation.&lt;/p&gt;
&lt;p&gt;Future research will focus on refining these models and exploring new techniques such as semi-supervised learning and transfer learning to further enhance performance.&lt;/p&gt;
&lt;p&gt;As we continue to observe the rise of &lt;strong&gt;Visionâ€“Language Models (VLMs)&lt;/strong&gt; in remote sensing, one notable trend is the growing body of academic work that explores their capabilities. The figure below demonstrates the increasing number of publications on VLMs in the field of remote sensing, reflecting the broad interest and potential for these models to shape the future of satellite imagery analysis. As the research landscape evolves, we can expect even more innovative approaches and applications to emerge.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00006.png&#34; alt=&#34;Number of publications for Visionâ€“Language Models (VLMs) in remote sensing&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: A comprehensive visualization of the growing number of publications focused on Visionâ€“Language Models (VLMs) in the field of remote sensing. This trend underscores the increasing interest in the integration of multimodal learning (combining visual and textual data) to enhance the accuracy and efficiency of remote sensing tasks, such as LULC classification, change detection, and disaster response. The upward trajectory of VLM research reflects both the advancements in AI model architecture and the expanding applicability of these models to complex, real-world geospatial challenges. The data presented here highlights the growing recognition of VLMs as a key technology in modern remote sensing workflows. (Image source: &lt;a href=&#34;https://www.mdpi.com/2072-4292/17/1/162&#34; target=&#34;_blank&#34;&gt;MDPI&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the pursuit of enhancing model performance, &lt;strong&gt;LLaVA (Large Vision-and-Language Model)&lt;/strong&gt; is one of the newer architectures thatâ€™s making waves in the remote sensing community. This innovative model is designed to seamlessly integrate both vision and language modalities, providing a rich foundation for sophisticated interpretations of satellite imagery. As illustrated below, LLaVA offers a more intuitive and interactive way of combining imagery with text, making it especially valuable in applications like land use classification, environmental monitoring, and more.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00007.png&#34; alt=&#34;An illustration of LLaVA, a Visionâ€“Language Model&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: A comprehensive illustration of LLaVA (Large Language and Vision Alignment), an advanced Visionâ€“Language Model (VLM) designed to seamlessly integrate visual and textual information. LLaVA enables fine-grained multimodal reasoning, allowing it to process complex datasets from diverse sources such as satellite imagery and environmental reports. By leveraging large-scale pre-trained models, LLaVA enhances performance in tasks like remote sensing and image classification through the fusion of visual cues and textual context. This approach has shown significant promise in enhancing model interpretability, particularly in applications where both visual and linguistic insights are crucial. (Image source: &lt;a href=&#34;https://www.mdpi.com/2072-4292/17/1/162&#34; target=&#34;_blank&#34;&gt;MDPI&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;At the heart of many VLMs is the &lt;strong&gt;Transformer architecture&lt;/strong&gt;, which has revolutionized how models process sequences of data, whether they be text, images, or both. The transformer model&amp;rsquo;s ability to handle long-range dependencies in data has made it the backbone of cutting-edge models like GPT, BERT, and, of course, Vision Transformers (ViTs). A diagram illustrating the Transformer architecture highlights how it efficiently handles both visual and linguistic data, making it a crucial component for tasks that require understanding of complex, multimodal data sets.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00008.png&#34; alt=&#34;Illustration of Transformer architecture, a cornerstone of modern deep learning models&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: A comprehensive illustration of the Transformer architecture, a breakthrough deep learning model that revolutionized sequence-based data processing. This architecture leverages self-attention mechanisms to capture long-range dependencies in data, which makes it highly effective in a wide range of tasks, including natural language processing and computer vision. It serves as the foundational model for Vision Transformers (ViTs) and Visionâ€“Language Models (VLMs) in remote sensing. (Image source: &lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf&#34; target=&#34;_blank&#34;&gt;NeurIPS 2017&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Vision Transformers (ViTs) are one of the latest and most powerful iterations of the Transformer model, specifically designed to handle image data. Unlike traditional convolutional neural networks (CNNs), ViTs treat image patches as sequences, enabling them to capture both local and global features more effectively. The figure below offers an overview of the Vision Transformer model and its ability to process large-scale imagery data. Its application in remote sensing has shown promise, particularly for high-resolution satellite imagery classification, where understanding fine-grained patterns is crucial.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_SAMPLE_RS_LLMS_00009.png&#34; alt=&#34;Model overview of Vision Transformer&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 9: Detailed model overview of the Vision Transformer (ViT), a cutting-edge deep learning architecture that has transformed the way image data is processed. Unlike traditional convolutional neural networks (CNNs), ViT divides an image into fixed-size patches and processes them as sequences, allowing the model to capture both local and global dependencies in an image simultaneously. This unique approach has led to impressive performance in high-resolution satellite image classification and other remote sensing tasks, making it a key model in the field of Visionâ€“Language Models (VLMs). ViT&#39;s scalability to handle large-scale image data have made it indispensable for remote sensing applications. (Image source: &lt;a href=&#34;https://arxiv.org/pdf/2010.11929/1000&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The combination of LLMs with traditional remote sensing techniques offers a significant improvement in the accuracy and contextual understanding of LULC classification tasks. By leveraging both spatial feature extraction from CNNs and semantic contextualization from LLMs, remote sensing models can achieve more precise and meaningful results. This approach opens new avenues for analyzing satellite imagery and provides deeper insights into Earthâ€™s surface processes, aiding in applications ranging from environmental monitoring to urban planning.&lt;/p&gt;
&lt;p&gt;The integration of &lt;strong&gt;Visionâ€“Language Models (VLMs)&lt;/strong&gt; into remote sensing is more than just a technological leap; it represents a paradigm shift in how we interpret and interact with satellite imagery. By combining the power of both visual data and linguistic context, VLMs open up new frontiers in &lt;strong&gt;land use/land cover (LULC) classification&lt;/strong&gt;, environmental monitoring, and countless other applications. The innovative capabilities of these models are not just theoreticalâ€”they are already being realized in practical, real-world scenarios, as evidenced by the growing body of research and the increasing number of publications in the field.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The results we&amp;rsquo;ve explored here demonstrate that &lt;strong&gt;VLMs&lt;/strong&gt; are not just another toolâ€”they are a powerful bridge that connects the language of the Earthâ€™s landscapes to the language of machine learning. With their ability to process complex multimodal data, they offer unprecedented accuracy, scalability, and efficiency in satellite data analysis. The rise of models like &lt;strong&gt;LLaVA&lt;/strong&gt; and the &lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt; exemplify this potential, illustrating how cutting-edge architecture is pushing the boundaries of whatâ€™s possible in &lt;strong&gt;remote sensing&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What makes these models truly remarkable is their ability to not only analyze individual images but also to understand the nuanced context within those images. This contextual understanding is where traditional methods fall short, making &lt;strong&gt;VLMs&lt;/strong&gt; a game-changer. Whether it&amp;rsquo;s distinguishing between subtle changes in land cover, detecting environmental anomalies, or improving the precision of automated classification, these models offer a level of sophistication that will likely define the future of remote sensing.&lt;/p&gt;
&lt;p&gt;As we look ahead, itâ€™s clear that &lt;strong&gt;VLMs&lt;/strong&gt; are poised to drive significant advancements in the way we monitor, understand, and protect our planet. The intersection of &lt;strong&gt;AI&lt;/strong&gt;, &lt;strong&gt;remote sensing&lt;/strong&gt;, and &lt;strong&gt;language&lt;/strong&gt; will continue to evolve, opening doors to new research, innovations, and applications. For professionals in the field of remote sensing, the question is no longer whether to adopt &lt;strong&gt;VLMs&lt;/strong&gt;, but how soon and how deeply to integrate them into their workflows.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this blog, weâ€™ve explored the methodologies, models, and real-world implications of integrating &lt;strong&gt;VLMs&lt;/strong&gt; into &lt;strong&gt;remote sensing&lt;/strong&gt;. The insights from this growing area of research suggest that &lt;strong&gt;Visionâ€“Language Models&lt;/strong&gt; will not only enhance the precision and scope of satellite data interpretation but will also unlock new levels of meaning and insight that were previously unattainable. As we continue to refine these models and expand their capabilities, the potential for &lt;strong&gt;VLMs&lt;/strong&gt; in &lt;strong&gt;remote sensing&lt;/strong&gt; is limitless.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In conclusion, the integration of &lt;strong&gt;Visionâ€“Language Models&lt;/strong&gt; with remote sensing is not just an exciting trendâ€”it is the future of how we will analyze, interpret, and leverage satellite imagery to make better decisions for our planetâ€™s future.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;And thatâ€™s a wrap for today! I hope you found this exploration into &lt;strong&gt;Visionâ€“Language Models&lt;/strong&gt; and &lt;strong&gt;remote sensing&lt;/strong&gt; insightful and inspiring. Whether youâ€™re new to the field or a seasoned pro, Iâ€™m sure there are a few ideas or &lt;strong&gt;takeaways&lt;/strong&gt; that can spark your next big project.&lt;/p&gt;
&lt;p&gt;Thanks for readingâ€”until next time, take care and keep pushing the boundaries of knowledge! ğŸŒ&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kao Panboonyuen&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (July 2025). &lt;em&gt;Visionâ€“Language Models for Remote Sensing: A New Era of Multimodal Understanding&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2025vlmrsllms,
  title   = &amp;quot;Visionâ€“Language Models for Remote Sensing: A New Era of Multimodal Understanding&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Jul&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Tao, Lijie, et al. &amp;ldquo;Advancements in Visionâ€“Language Models for Remote Sensing: Datasets, Capabilities, and Enhancement Techniques.&amp;rdquo; &lt;em&gt;Remote Sensing&lt;/em&gt; 17.1 (2025): 162.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vaswani, Ashish, et al. &amp;ldquo;Attention is all you need.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 30 (2017).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dosovitskiy, Alexey, et al. &amp;ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:2010.11929 (2020).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Schulman, John, et al. &amp;ldquo;Proximal policy optimization algorithms.&amp;rdquo; &lt;em&gt;arXiv preprint&lt;/em&gt; arXiv:1707.06347 (2017).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weng, Lilian. &amp;ldquo;Thinking.&amp;rdquo; &lt;em&gt;Lilianweng.github.io&lt;/em&gt;. May 1, 2025. &lt;a href=&#34;https://lilianweng.github.io/posts/2025-05-01-thinking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://lilianweng.github.io/posts/2025-05-01-thinking/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thinking Machines. &amp;ldquo;Homepage.&amp;rdquo; &lt;em&gt;Thinkingmachines.ai&lt;/em&gt;. &lt;a href=&#34;https://thinkingmachines.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://thinkingmachines.ai/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>ALBERT vs SLICK: MARSAILâ€™s New AI Fashion for Real-Time Car Insurance and Garages</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/</link>
      <pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can explore our GitHub project page ğŸ“¦ &lt;a href=&#34;https://kaopanboonyuen.github.io/MARS/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#albert-vs-slick-the-new-ai-fashion-at-marsail&#34;&gt;ALBERT vs SLICK: The New AI Fashion at MARSAIL&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#slick-revolutionizing-car-damage-segmentation-with-knowledge-enhanced-ai-at-marsail&#34;&gt;SLICK: Revolutionizing Car Damage Segmentation with Knowledge-Enhanced AI at MARSAIL&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#inspiration-from-andrej-karpathy-embracing-the-new-era-of-ai-innovation-at-marsail&#34;&gt;Inspiration from Andrej Karpathy: Embracing the New Era of AI Innovation at MARSAIL&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#albert-the-teacher-model--precision-at-a-cost&#34;&gt;ALBERT: The Teacher Model â€” Precision at a Cost&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#slick-the-student-model--lightning-speed-meets-smart-knowledge&#34;&gt;SLICK: The Student Model â€” Lightning Speed Meets Smart Knowledge&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-teaching-machines-to-see-smarter-and-faster-the-marsail-teacher-student-model&#34;&gt;ğŸš˜ Teaching machines to see smarter and faster: the MARSAIL teacher-student model&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#what-is-the-teacher-student-model&#34;&gt;What is the teacher-student model?&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#measuring-size-and-efficiency-teacher-vs-student&#34;&gt;Measuring size and efficiency: teacher vs. student&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#how-does-the-student-learn-from-the-teacher&#34;&gt;How does the student learn from the teacher?&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#results-that-speak-volumes&#34;&gt;Results that speak volumes&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#final-layer-feature-summaries&#34;&gt;Final layer feature summaries&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#why-does-this-matter-for-car-insurance&#34;&gt;Why does this matter for car insurance?&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#mathematical-insight-simplified&#34;&gt;Mathematical Insight (Simplified)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#published-research&#34;&gt;Published Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#looking-ahead&#34;&gt;Looking Ahead&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;albert-vs-slick-the-new-ai-fashion-at-marsail&#34;&gt;ALBERT vs SLICK: The New AI Fashion at MARSAIL&lt;/h2&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;In Thailandâ€™s rapidly evolving automotive insurance sector, the integration of transformer-based segmentation models like &lt;strong&gt;MARSAIL-ALBERT&lt;/strong&gt; and &lt;strong&gt;MARSAIL-SLICK&lt;/strong&gt; marks a pivotal shift toward scalable, AI-driven damage assessmentâ€”bringing unprecedented accuracy, efficiency, and trust to claim processing pipelines.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;In addressing the complex challenge of fine-grained automotive damage segmentation, we present &lt;strong&gt;MARSAIL-ALBERT&lt;/strong&gt; (Figure 1), a high-capacity teacher model architected on the principles of bidirectional transformer encoding and spatially-aware representation learning. Leveraging the power of attention mechanisms within a multi-scale encoder-decoder framework, MARSAIL-ALBERT excels at capturing subtle visual cuesâ€”scratches, dents, fracturesâ€”amid high-variance automotive imagery. It is explicitly optimized to model long-range dependencies across both local textures and global structural semantics, enabling precise localization of damage under varying viewpoints, illumination conditions, and occlusion levels.&lt;/p&gt;
&lt;p&gt;As demonstrated in Figure 2, the model consistently produces highly detailed segmentation maps, revealing strong robustness to environmental perturbations such as specular highlights, cast shadows, and surface complexity. Functioning as the supervisory core of our framework, MARSAIL-ALBERT serves not only as a performant segmentation engine but also as a teacher network for structured knowledge distillation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/featured.png&#34; alt=&#34;ALBERT Architecture&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 1: The architecture of the proposed MARSAIL-ALBERT model, a teacher network that leverages advanced localization and bidirectional encoder representations from transformers for high-precision car damage segmentation. This design facilitates robust spatial reasoning and context-aware feature extraction critical for complex automotive insurance scenarios.
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/compact.png&#34; alt=&#34;ALBERT Results&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 2: Visualization of segmentation outcomes produced by MARSAIL-ALBERT, highlighting the modelâ€™s ability to localize fine-grained vehicle damage under varying lighting and occlusion conditions, showcasing its generalization capability across diverse automotive imagery.
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To extend this capability into real-time and resource-constrained settingsâ€”typical of large-scale deployment in automotive insurance operationsâ€”we introduce &lt;strong&gt;MARSAIL-SLICK&lt;/strong&gt; (Figure 3), a compact yet powerful student model distilled directly from MARSAIL-ALBERT. It incorporates a novel Selective Localization mechanism that prioritizes critical damage regions, coupled with an Instance Calibration module that aligns feature representation across inter-instance variability. This combination allows MARSAIL-SLICK to retain the semantic fidelity of its teacher while drastically reducing parameter count and inference time.&lt;/p&gt;
&lt;p&gt;As evidenced in Figure 4, the student model maintains competitive segmentation performance, particularly in high-throughput scenarios such as claim triage or automated fleet inspection. Together, the ALBERTâ€“SLICK teacher-student architecture offers a robust, scalable solution for real-world visual understanding tasks in the automotive insurance pipeline, aligning state-of-the-art deep learning with industry-grade reliability and speed.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/featured.png&#34; alt=&#34;SLICK Architecture&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 3: The architecture of MARSAIL-SLICK, a lightweight student model that incorporates selective localization and instance calibration mechanisms for knowledge-enhanced car damage segmentation. This model efficiently distills knowledge from the teacher network to enable real-time deployment while preserving semantic precision.
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/compact.png&#34; alt=&#34;SLICK Results&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 4: Output predictions from the MARSAIL-SLICK model, demonstrating its capability to maintain high segmentation fidelity despite its reduced computational footprint. The results affirm the effectiveness of our teacher-student framework in knowledge transfer for robust performance in resource-constrained settings.
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;In the fast-moving world of automotive insurance, where accuracy and turnaround time can make or break the customer experience, &lt;strong&gt;MARSAIL&lt;/strong&gt; (Motor AI Recognition Solution Artificial Intelligence Laboratory) stands at the forefront of transformation. Based in Thailand and led by the visionary &lt;strong&gt;Dr. Teerapong Panboonyuen&lt;/strong&gt; â€” affectionately known as &lt;strong&gt;Dr. Kao&lt;/strong&gt; â€” MARSAIL is redefining how artificial intelligence is used in car insurance and garage ecosystems. The lab&amp;rsquo;s mission is clear: to blend deep research with real-world impact. Earlier this year, Dr. Kao shared on &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1822723598764876000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt; the debut of &lt;strong&gt;MARS&lt;/strong&gt;, an innovative architecture built on Attention Refinement with Sequential Quadtree Nodes. With its combination of scientific rigor and practical relevance, MARS isn&amp;rsquo;t just another academic model â€” it&amp;rsquo;s a bold step forward in computer vision and deep learning, designed to solve tangible problems in automotive analysis.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;ğŸ„ We&amp;#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.&lt;br&gt;.&lt;br&gt;Paper: &lt;a href=&#34;https://t.co/UayUSxmZep&#34;&gt;https://t.co/UayUSxmZep&lt;/a&gt;&lt;br&gt;Code: &lt;a href=&#34;https://t.co/RoNFjSslXr&#34;&gt;https://t.co/RoNFjSslXr&lt;/a&gt;&lt;br&gt;Project: &lt;a href=&#34;https://t.co/uSoBX21HpF&#34;&gt;https://t.co/uSoBX21HpF&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ComputerVision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ComputerVision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Research?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Research&lt;/a&gt; &lt;a href=&#34;https://t.co/oc8gz7Hs9I&#34;&gt;pic.twitter.com/oc8gz7Hs9I&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1822723598764876000?ref_src=twsrc%5Etfw&#34;&gt;August 11, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Riding on this momentum, MARSAIL has unveiled two game-changing models in 2025: &lt;strong&gt;ALBERT&lt;/strong&gt; and &lt;strong&gt;SLICK&lt;/strong&gt;. These systems are not just incremental updates â€” they represent a complete rethink of how damage detection and claim assessment can be automated with AI. ALBERT is optimized for real-time car damage classification with high precision, while SLICK focuses on smart localization and segmentation of damage areas, tailored specifically for insurance workflows. Together, they offer insurers and garages tools that are faster, smarter, and more reliable than ever before. Backed by advanced machine learning techniques and a commitment to open research, MARSAIL is helping Thailand â€” and the region â€” become a serious global player in automotive AI innovation. Whether you&amp;rsquo;re in the lab or on the road, MARSAIL is making sure AI drives the future.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://github.com/kaopanboonyuen/kaopanboonyuen.github.io/raw/main/files/MARS/MARSAIL.png&#34; alt=&#34;MARSAIL Logo&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 5: MARSAIL â€” a leading research lab in Thailand focused on applying AI to car insurance and automotive service innovations.
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;These models represent more than just technological progress; they embody a new philosophy in AI-powered insurance: a seamless synergy between &lt;strong&gt;uncompromising accuracy&lt;/strong&gt; and &lt;strong&gt;lightning-fast efficiency&lt;/strong&gt;, inspired by the teacher-student paradigm.&lt;/p&gt;
&lt;p&gt;At the core of this paradigm lies &lt;strong&gt;ALBERT&lt;/strong&gt;, the â€œteacherâ€ â€” a powerhouse model meticulously engineered for razor-sharp precision. ALBERT dives deep into images, discerning the finest scratches, dents, and cracks with near-human expertise. Itâ€™s a master of detail, leaving no nuance unseen, perfect for complex offline investigations and comprehensive damage evaluations where absolute accuracy is essential.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In todayâ€™s fast-paced insurance ecosystem, speed is just as crucial as accuracy, particularly when it comes to frontline claim processing and on-the-spot damage assessments. This is where SLICK, the &amp;ldquo;student&amp;rdquo; model, truly shines. Guided by the advanced expertise of ALBERT, SLICK is engineered for agility and lightning-fast performance, delivering precise damage detection results in real-time. Whether running on edge devices or mobile phones, its optimized architecture allows insurance agents and repair shops to streamline their operations, making decisions faster without ever sacrificing quality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;slick-revolutionizing-car-damage-segmentation-with-knowledge-enhanced-ai-at-marsail&#34;&gt;SLICK: Revolutionizing Car Damage Segmentation with Knowledge-Enhanced AI at MARSAIL&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The recent Google AI video on Knowledge Distillation: A Good Teacher is Patient and Consistent offers valuable insights into how AI models can be trained efficiently by leveraging a &amp;ldquo;teacher-student&amp;rdquo; framework. The key takeaway from this approach is that a well-trained teacher model can pass down its knowledge to a student model, significantly improving performance and generalization. This technique has sparked new ideas for MARSAIL and our work on SLICK (Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation).&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/gZPUGje1PCI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Today, we introduced Gemini Robotics On-Device ğŸ¤–&lt;br&gt;&lt;br&gt;ğŸ§ª Designed for rapid experimentation with dexterous manipulation.&lt;br&gt;ğŸ¦¾ Adaptable to new tasks through fine-tuning to improve performance.&lt;br&gt;ğŸ‘Ÿ Optimized to run locally with low-latency inference.&lt;br&gt;&lt;br&gt;Learn moreâ€¦ &lt;a href=&#34;https://t.co/h2d1TZ49qm&#34;&gt;https://t.co/h2d1TZ49qm&lt;/a&gt;&lt;/p&gt;&amp;mdash; Google AI (@GoogleAI) &lt;a href=&#34;https://twitter.com/GoogleAI/status/1937554536966619399?ref_src=twsrc%5Etfw&#34;&gt;June 24, 2025&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/p&gt;
&lt;p&gt;In the same spirit of knowledge transfer, SLICK takes inspiration from the concept of &amp;ldquo;distilling&amp;rdquo; knowledge from large, complex models to create an AI system capable of precise, real-world car damage segmentation. Through components like Selective Part Segmentation, Localization-Aware Attention, and Knowledge Fusion, SLICK enhances the ability of AI models to focus on and accurately segment vehicle partsâ€”even under challenging conditions like occlusions and deformations. Much like the patient and consistent teacher-student relationship in knowledge distillation, SLICK learns from vast datasets (including synthetic crash data and real-world insurance records) to ensure robustness and adaptability across a variety of damage scenarios.&lt;/p&gt;
&lt;p&gt;At MARSAIL, inspired by Google AI&amp;rsquo;s knowledge distillation, weâ€™re applying these principles to create an AI system that not only improves segmentation accuracy but also optimizes the entire automotive insurance and repair workflow. With SLICK, we are ready to bring this advanced AI to Thailand, enhancing efficiency, reducing fraud, and setting new standards for the industry.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;inspiration-from-andrej-karpathy-embracing-the-new-era-of-ai-innovation-at-marsail&#34;&gt;Inspiration from Andrej Karpathy: Embracing the New Era of AI Innovation at MARSAIL&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;In the video &amp;ldquo;Andrej Karpathy: Software Is Changing (Again)&amp;rdquo;, Karpathy discusses how artificial intelligence and deep learning are driving a new wave of transformation across industries. At MARSAIL, we deeply resonate with his perspective that AI is not just evolvingâ€”itâ€™s fundamentally reshaping how we approach problem-solving and automation. Inspired by Karpathyâ€™s vision, weâ€™re applying the latest in AI research to redefine the way car damage estimation, insurance claims, and repair workflows are handled. Just as Karpathy highlights the importance of AI in software development, MARSAIL is leveraging cutting-edge AI models like SLICK to bring accuracy, speed, and efficiency to the automotive sector, helping to transform the Thai automotive insurance ecosystem into a more intelligent and scalable system.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/LCEmiRjPEtQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;ğŸ”¥ New (1h56m) video lecture: &amp;quot;Let&amp;#39;s build GPT: from scratch, in code, spelled out.&amp;quot;&lt;a href=&#34;https://t.co/2pKsvgi3dE&#34;&gt;https://t.co/2pKsvgi3dE&lt;/a&gt; &lt;br&gt;We build and train a Transformer following the &amp;quot;Attention Is All You Need&amp;quot; paper in the language modeling setting and end up with the core of nanoGPT. &lt;a href=&#34;https://t.co/6dzimsYPB9&#34;&gt;pic.twitter.com/6dzimsYPB9&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andrej Karpathy (@karpathy) &lt;a href=&#34;https://twitter.com/karpathy/status/1615398117683388417?ref_src=twsrc%5Etfw&#34;&gt;January 17, 2023&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;By aligning our research with the principles Karpathy discusses, MARSAIL is at the forefront of AI-driven innovation in the automotive space, bringing faster, more reliable, and trustworthy solutions to insurers, garages, and customers alike.&lt;/p&gt;
&lt;p&gt;Together, &lt;strong&gt;ALBERT&lt;/strong&gt; and &lt;strong&gt;SLICK&lt;/strong&gt; form a powerful duo that bridges the traditional divide between accuracy and efficiency â€” offering the best of both worlds to revolutionize car insurance workflows across Thailand and beyond.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;albert-the-teacher-model--precision-at-a-cost&#34;&gt;ALBERT: The Teacher Model â€” Precision at a Cost&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;ALBERT stands for &lt;em&gt;Advanced Localization and Bidirectional Encoder Representations from Transformers&lt;/em&gt;. This model is designed to be highly accurate and detailed in detecting subtle damages such as small scratches, dents, and cracks on vehicles. It leverages a vision transformer architecture enhanced with localized deformable tokens and parameter sharing to precisely focus on critical damage regions.&lt;/p&gt;
&lt;p&gt;However, this precision comes with a computational cost. ALBERT requires powerful CUDA-enabled GPUs and is relatively slow, making it ideal for offline batch processing or scenarios where accuracy takes precedence over speed.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/ALBERT_01.png&#34; alt=&#34;MARSAIL ALBERT Model Result&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: MARSAIL-ALBERT model showcasing detailed and precise damage segmentation results.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;slick-the-student-model--lightning-speed-meets-smart-knowledge&#34;&gt;SLICK: The Student Model â€” Lightning Speed Meets Smart Knowledge&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;To address real-time insurance needs, MARSAIL developed &lt;strong&gt;SLICK&lt;/strong&gt; â€” &lt;em&gt;Selective Localization and Instance Calibration with Knowledge&lt;/em&gt;. This model distills knowledge from ALBERT and integrates domain-specific insurance metadata like bumper zones and vehicle model weak points.&lt;/p&gt;
&lt;p&gt;SLICK boosts processing speed by over &lt;strong&gt;700%&lt;/strong&gt; compared to ALBERT, enabling instant damage assessments on edge devices or mobile apps without sacrificing much accuracy. Its adaptive attention mechanism dynamically calibrates segmentation proposals using contextual knowledge graphs, making it robust under varying light, weather, and occlusion conditions.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/SLICK_01.png&#34; alt=&#34;MARSAIL SLICK Model Result&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: MARSAIL-SLICK model delivering rapid, knowledge-enhanced damage segmentation optimized for real-time insurance workflows.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-teaching-machines-to-see-smarter-and-faster-the-marsail-teacher-student-model&#34;&gt;ğŸš˜ Teaching machines to see smarter and faster: the MARSAIL teacher-student model&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;In the race to deliver the best car damage detection for insurance claims, MARSAIL takes a cutting-edge approach inspired by how humans learn: through mentorship. Our &lt;strong&gt;teacher-student model&lt;/strong&gt; architecture pairs a high-capacity â€œteacherâ€ network with a lean, speedy â€œstudentâ€ model, capturing the best of both worlds â€” precision and efficiency.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_01.png&#34; alt=&#34;Teacher-Student Model Concept Architecture&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: Conceptual architecture of the MARSAIL teacher-student model (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;what-is-the-teacher-student-model&#34;&gt;What is the teacher-student model?&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Think of the &lt;strong&gt;teacher&lt;/strong&gt; as a seasoned expert with a deep understanding of vehicle damage nuances â€” itâ€™s large, powerful, and painstakingly precise. The &lt;strong&gt;student&lt;/strong&gt;, meanwhile, is like an apprentice: smaller, faster, and designed to perform well in real-world settings with limited resources.&lt;/p&gt;
&lt;p&gt;The magic happens when the student &lt;strong&gt;learns to mimic&lt;/strong&gt; the teacher&amp;rsquo;s insights without needing to replicate its full complexity. This process is known as &lt;strong&gt;knowledge distillation&lt;/strong&gt; â€” where the teacherâ€™s â€œsoftâ€ predictions guide the studentâ€™s training, helping it grasp subtle visual patterns that would be hard to learn from raw data alone.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_02.png&#34; alt=&#34;Simple Teacher-Student Model Concept&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 9: Simplified overview of the teacher-student learning framework (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;measuring-size-and-efficiency-teacher-vs-student&#34;&gt;Measuring size and efficiency: teacher vs. student&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;To illustrate the trade-off, hereâ€™s a glimpse of the teacher and student model sizes trained on the CIFAR-10 dataset. The teacher is notably larger but more precise, while the studentâ€™s compact size enables rapid inference â€” crucial for insurance agents working on the go.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_03.png&#34; alt=&#34;Teacher Size (left) and Student Size (right) on CIFAR-10 dataset&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 10: Visual comparison of teacher (left) and student (right) model sizes (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;how-does-the-student-learn-from-the-teacher&#34;&gt;How does the student learn from the teacher?&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The training process involves the student observing both the teacherâ€™s output and the ground truth, gradually adjusting itself to replicate the teacherâ€™s nuanced judgments. This dual supervision accelerates the studentâ€™s learning curve, enabling it to deliver near-teacher accuracy with significantly fewer parameters.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_04.png&#34; alt=&#34;How to Learn from Teacher-Student&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 11: Diagram showing how the student model learns from the teacher model (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results-that-speak-volumes&#34;&gt;Results that speak volumes&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;On multiple datasets and architectures (including CNN and ResNet), MARSAILâ€™s teacher-student training methods consistently improved student model accuracy across the board â€” sometimes by over 3% compared to training without guidance.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;No KD (%)&lt;/th&gt;
&lt;th&gt;BLKD (%)&lt;/th&gt;
&lt;th&gt;TAKD (%)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;CIFAR-10&lt;/td&gt;
&lt;td&gt;70.16&lt;/td&gt;
&lt;td&gt;72.57&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;73.51&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;CIFAR-100&lt;/td&gt;
&lt;td&gt;41.09&lt;/td&gt;
&lt;td&gt;44.57&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;44.92&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet&lt;/td&gt;
&lt;td&gt;CIFAR-10&lt;/td&gt;
&lt;td&gt;88.52&lt;/td&gt;
&lt;td&gt;88.65&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;88.98&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet&lt;/td&gt;
&lt;td&gt;CIFAR-100&lt;/td&gt;
&lt;td&gt;61.37&lt;/td&gt;
&lt;td&gt;61.41&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;61.82&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet&lt;/td&gt;
&lt;td&gt;ImageNet&lt;/td&gt;
&lt;td&gt;65.20&lt;/td&gt;
&lt;td&gt;66.60&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;67.36&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_05.png&#34; alt=&#34;Result Comparison of Models on CIFAR datasets&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 12: Model accuracy comparison showing improvement using knowledge distillation techniques (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;final-layer-feature-summaries&#34;&gt;Final layer feature summaries&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;This image visualizes how different layers in each model contribute to the final representation, highlighting the efficiency gains from knowledge distillation that help the student model stay compact yet powerful.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_06.png&#34; alt=&#34;Summary of Final Layers in Each Model&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 13: Summary of final layer features in teacher and student models (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-does-this-matter-for-car-insurance&#34;&gt;Why does this matter for car insurance?&lt;/h3&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speed without compromise:&lt;/strong&gt; Insurance agents and garages need fast, reliable damage detection on smartphones or edge devices. The student model delivers rapid results, trained under the teacherâ€™s expert supervision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resource efficiency:&lt;/strong&gt; Smaller models reduce computational costs and power consumption, enabling scalable deployment across Thailandâ€™s wide insurance ecosystem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust accuracy:&lt;/strong&gt; Guided by the teacher, the student avoids common pitfalls of lightweight models, maintaining high performance even in challenging real-world conditions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accuracy vs. Speed:&lt;/strong&gt; ALBERT excels in detailed offline analysis, perfect for complex claim investigations. SLICK offers instant, reliable damage detection to accelerate frontline claim approvals and garage estimates.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hardware Flexibility:&lt;/strong&gt; ALBERT demands high-end GPUs; SLICK can run efficiently on more modest, real-world devices â€” a game changer for field agents and repair shops.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Knowledge Integration:&lt;/strong&gt; SLICKâ€™s use of insurance-specific metadata bridges the gap between raw image analysis and domain expertise, improving real-world applicability.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mathematical-insight-simplified&#34;&gt;Mathematical Insight (Simplified)&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;At the heart of our system lies a sophisticated process that refines how the model understands and represents visual data at every step. Imagine ALBERT as a multi-layered brain that carefully adjusts its internal view of an image piece by piece. At each layer, it uses two powerful tools: one that lets it look broadly across different parts of the image to understand overall patterns (multi-scale self-attention), and another that processes these insights through focused, step-by-step reasoning (a multilayer perceptron).&lt;/p&gt;
&lt;p&gt;$$
z^{(l+1)} = z^{(l)} + \text{MSA}(\text{LN}(z^{(l)})) + \text{MLP}(\text{LN}(z^{(l)}))
$$&lt;/p&gt;
&lt;p&gt;This dynamic combination helps ALBERT balance the big picture with fine details, ensuring that the model not only recognizes individual features but also how they relate to each other in context. To keep this learning smooth and stable, it applies a normalization stepâ€”similar to tuning an instrumentâ€”to make sure each layerâ€™s output remains consistent and meaningful.&lt;/p&gt;
&lt;p&gt;Parallel to this, SLICK operates like an intelligent curator, enhancing the modelâ€™s confidence in its predictions. It does this by merging two streams of knowledge: the direct visual cues from the image itself and additional information pulled from a structured knowledge graphâ€”think of this as a database of domain-specific facts and relationships.&lt;/p&gt;
&lt;p&gt;$$
s_{mask} = \sigma(W_q [f_{img} | f_{kg}]) + b
$$&lt;/p&gt;
&lt;p&gt;To blend these inputs effectively, SLICK employs a gating mechanism that acts like a smart filter or valve. This gate carefully weighs how much influence the visual data and the knowledge graph should each have in shaping the final mask quality scores. By doing so, the model doesnâ€™t just rely on what it sees but also on what it knows about the world, leading to sharper, more reliable segmentation.&lt;/p&gt;
&lt;p&gt;In essence, this combination of refined visual understanding and context-aware knowledge integration lets our system adapt its focus dynamicallyâ€”prioritizing regions and details that matter most for accurate damage assessment and claim processing.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;published-research&#34;&gt;Published Research&lt;/h2&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;ALBERT: &lt;a href=&#34;https://arxiv.org/abs/2506.10524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2506.10524&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SLICK: &lt;a href=&#34;https://arxiv.org/abs/2506.10528&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2506.10528&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;looking-ahead&#34;&gt;Looking Ahead&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;MARSAIL continues to innovate by balancing AI model accuracy and deployment efficiency. ALBERT and SLICK represent the cutting edge of automotive AI, ready to transform insurance claim processes in Thailand and beyond â€” enabling smarter, faster, and fairer car insurance.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Jul 2025). &lt;em&gt;ALBERT vs SLICK: MARSAILâ€™s New AI Fashion for Real-Time Car Insurance and Garages&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{panboonyuen2025albert_slick,
  title={ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation},
  author={Panboonyuen, Teerapong},
  journal={arXiv preprint arXiv:2506.10524},
  year={2025},
  url={https://arxiv.org/abs/2506.10524}
}

@article{panboonyuen2025slick,
  title={SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance},
  author={Panboonyuen, Teerapong},
  journal={arXiv preprint arXiv:2506.10528},
  year={2025},
  url={https://arxiv.org/abs/2506.10528}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Found this blog insightful? Consider sharing it with friends or researchers in the automotive or insurance tech industry. ğŸš—
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Panboonyuen, Teerapong.  &amp;ldquo;ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation.&amp;rdquo; arXiv preprint arXiv:2506.10524 (2025). &lt;a href=&#34;https://arxiv.org/abs/2506.10524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2506.10524&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Panboonyuen, Teerapong.  &amp;ldquo;SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance.&amp;rdquo; arXiv preprint arXiv:2506.10528 (2025). &lt;a href=&#34;https://arxiv.org/abs/2506.10528&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2506.10528&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Knowledge Distillation with Teacher Assistant for Model Compression: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>MARSAIL: The Smart Engine Powering the Future of Car Insurance and Intelligent Garages</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can explore our GitHub project page ğŸ“¦ &lt;a href=&#34;https://kaopanboonyuen.github.io/MARS/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#ai-driven-car-damage-estimating-global-innovations-and-marsails-vision-for-thailand&#34;&gt;AI-driven Car Damage Estimating: Global Innovations and MARSAIL&amp;rsquo;s Vision for Thailand&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#mars-our-flagship-ai&#34;&gt;MARS: Our Flagship AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#marsail-2025-research-highlights&#34;&gt;MARSAIL 2025: Research Highlights&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-albert-efficient-transformer-for-automotive-localization&#34;&gt;1. ALBERT: Efficient Transformer for Automotive Localization&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-slick-knowledge-enhanced-instance-segmentation&#34;&gt;2. SLICK: Knowledge-Enhanced Instance Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-dota-deformable-optimized-transformer-for-ocr&#34;&gt;3. DOTA: Deformable Optimized Transformer for OCR&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-mars-mask-attention-refinement-with-sequential-quadtree-nodes-for-car-damage-instance-segmentation&#34;&gt;4. MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-what-sets-our-approach-apart&#34;&gt;ğŸ” what sets our approach apart&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-how-it-works-in-practice&#34;&gt;ğŸ§  how it works in practice&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-why-it-matters-for-insurers--workshops&#34;&gt;ğŸï¸ why it matters for insurers &amp;amp; workshops&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-conclusions--next-steps&#34;&gt;âœ… conclusions &amp;amp; next steps&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#fighting-fraud-with-intelligent-automation&#34;&gt;Fighting Fraud with Intelligent Automation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#visual-damage-interpretation--smarter-and-faster&#34;&gt;Visual Damage Interpretation â€” Smarter and Faster&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#translating-damage-into-precise-repair-estimates&#34;&gt;Translating Damage Into Precise Repair Estimates&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#ai-powered-fraud-detection--catching-the-unseen&#34;&gt;AI-Powered Fraud Detection â€” Catching the Unseen&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#comprehensive-automated-reporting-for-faster-decisions&#34;&gt;Comprehensive, Automated Reporting for Faster Decisions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#why-marsail-leads-the-charge&#34;&gt;Why MARSAIL Leads the Charge&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#join-the-future-of-auto-insurance&#34;&gt;Join the Future of Auto Insurance&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#future-work-beyond-damage--toward-full-lifecycle-intelligence&#34;&gt;Future Work: Beyond Damage â€“ Toward Full-Lifecycle Intelligence&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;MARSAIL, or the Motor AI Recognition Solution Artificial Intelligence Laboratory, is Thailandâ€™s pioneering AI research hub dedicated to automotive insurance and repair. Under the direction of Dr. Teerapong Panboonyuen (Dr. Kao), the lab develops deep learning models to analyze car damage, estimate repair costs, and automate claim handling â€” empowering insurers and garages with intelligence, precision, and speed.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://github.com/kaopanboonyuen/kaopanboonyuen.github.io/raw/main/files/MARS/MARSAIL.png&#34; alt=&#34;MARSAIL Logo&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 1: MARSAIL â€” a standout AI research lab in Thailand, leading innovation in car insurance and garage solutions.
  &lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;The journey of MARSAIL began with a bold, urgent question: â€œCan we make car insurance in Thailand smarter, faster, and fraud-free using AI?â€ For decades, first-class auto insurance underwriting in Thailand has been heavily reliant on manual inspections and human judgments â€” prone to delays, inconsistencies, and at times, manipulation. Garage estimates for labor and parts are also far from standardized, often resulting in disputes, inefficiencies, and customer dissatisfaction. Seeing these gaps not as limitations but as opportunities, MARSAIL was born with a national mission in mind: to revolutionize Thailandâ€™s automotive insurance ecosystem through scalable, verifiable, and explainable artificial intelligence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We envisioned a future where a single image of a damaged vehicle could trigger an accurate AI-driven diagnosis â€” identifying every dent, crack, and broken part, then instantly calculating the repair cost and parts required. This would not only expedite claims processing but also create a tamper-proof digital trail, significantly reducing fraud in the industry. At the same time, garage operators would benefit from AI-driven estimates that standardize costs and accelerate service turnaround time â€” allowing businesses to grow with transparency and trust.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;ğŸ„ We&amp;#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.&lt;br&gt;.&lt;br&gt;Paper: &lt;a href=&#34;https://t.co/UayUSxmZep&#34;&gt;https://t.co/UayUSxmZep&lt;/a&gt;&lt;br&gt;Code: &lt;a href=&#34;https://t.co/RoNFjSslXr&#34;&gt;https://t.co/RoNFjSslXr&lt;/a&gt;&lt;br&gt;Project: &lt;a href=&#34;https://t.co/uSoBX21HpF&#34;&gt;https://t.co/uSoBX21HpF&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ComputerVision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ComputerVision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Research?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Research&lt;/a&gt; &lt;a href=&#34;https://t.co/oc8gz7Hs9I&#34;&gt;pic.twitter.com/oc8gz7Hs9I&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1822723598764876000?ref_src=twsrc%5Etfw&#34;&gt;August 11, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;But we didnâ€™t stop at computer vision. We realized that to fully automate the claims process, we also needed to read and understand the documents involved â€” from the front of a Thai driverâ€™s license to the Vehicle Identification Number (VIN), mileage, license plate, or even the Thai National ID card of the claimant. Thatâ€™s why MARSAIL also expanded into NLP and OCR research, building powerful models that combine vision and language to intelligently extract, verify, and reason over structured and unstructured vehicle-related data.&lt;/p&gt;
&lt;p&gt;This work is now powering what we call the â€œDigital Insurance Twinâ€ â€” a complete AI ecosystem that mirrors and manages every carâ€™s insurance lifecycle. For example, an insurer can use our models to approve or reject first-class coverage applications by automatically analyzing the condition of a vehicle and matching it against risk profiles. Garages can plug into our engine to generate real-time quotes validated by AI, reducing negotiation overhead and instilling transparency. For every photo and every document, thereâ€™s an AI model working behind the scenes to ensure authenticity, consistency, and fairness.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At the heart of MARSAILâ€™s vision is not just efficiency but integrity. By minimizing human subjectivity and manual paperwork, our AI platform safeguards against inflated repairs, ghost accidents, and fraudulent identity claims. This builds long-term trust across the ecosystem: between insurers and policyholders, garages and customers, and regulators and service providers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Today, we train not only car damage models but also multimodal models that fuse vision and language, including transformer-based LLMs optimized for the insurance domain. Weâ€™re teaching machines to reason with documents, anticipate inconsistencies, and provide contextual understanding â€” allowing for seamless automation of tasks that previously required expert-level human input. Whether itâ€™s reading a worn-out VIN from an old pickup truck or parsing a scanned Thai ID under poor lighting, MARSAIL&amp;rsquo;s AI agents are learning, adapting, and improving with every new sample.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ai-driven-car-damage-estimating-global-innovations-and-marsails-vision-for-thailand&#34;&gt;AI-driven Car Damage Estimating: Global Innovations and MARSAIL&amp;rsquo;s Vision for Thailand&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The global automotive AI landscape is evolving rapidly, with companies like Tractable and Mitchell leading the way in using artificial intelligence to assess car damage, estimate repair costs, and streamline the claims process. For instance, Tractableâ€™s AI photo-estimating system has already gained significant attention for its ability to analyze images of vehicle damage and provide accurate repair estimates. You can explore one of their impressive demonstrations in the video below:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/dvbF2BmkM3Q&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;What does Martin Ellingsworth, Executive Managing Director, P&amp;amp;C Insurance Intelligence at J.D. Power have to say about the impact AI is having on the P&amp;amp;C customer experience?&lt;br&gt;&lt;br&gt;Dive into our full report here: &lt;a href=&#34;https://t.co/XP4sL6ntsC&#34;&gt;https://t.co/XP4sL6ntsC&lt;/a&gt; &lt;a href=&#34;https://t.co/1YCB7E7Sse&#34;&gt;pic.twitter.com/1YCB7E7Sse&lt;/a&gt;&lt;/p&gt;&amp;mdash; Tractable (@tractable_ai) &lt;a href=&#34;https://twitter.com/tractable_ai/status/1656316477580902404?ref_src=twsrc%5Etfw&#34;&gt;May 10, 2023&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Another example comes from Mitchellâ€™s partnership with AI technology to enhance auto insurance workflows. This collaboration showcases the future of intelligent claims processing and damage estimation, as seen in the video below:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hfas2mTIZyA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;These two AI-driven innovations are setting the stage for a new era in automotive insurance, and at MARSAIL, we aim to bring similar advancements to Thailand. Our cutting-edge AI research is designed to deliver an intelligent, scalable solution for vehicle damage estimation and claims processing. By harnessing deep learning and multimodal AI models, MARSAIL will revolutionize how car damage is assessed and how repairs are priced, ensuring transparency, reducing fraud, and optimizing the entire insurance lifecycle. Just like these global players, MARSAIL will offer a seamless, efficient, and trust-enhancing solution for the Thai automotive insurance market.&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/L1J3yXUE7yU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Aba5afny294&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Let us drive this transformation â€” not just with code and data, but with vision, integrity, and purpose.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mars-our-flagship-ai&#34;&gt;MARS: Our Flagship AI&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;At MARSAILâ€™s mission lies our flagship model â€” MARS (Mask Attention Refinement with Sequential Quadtree Nodes). This architecture was designed with a singular goal: to perform fine-grained instance segmentation of car damage in a real-world automotive insurance setting, under various lighting, angle, and occlusion conditions. Unlike conventional approaches like Mask R-CNN or PointRend, MARS integrates hierarchical spatial reasoning using a quadtree decomposition fused with multi-level self-attention to enable both coarse-to-fine detection and spatial localization of subtle visual cues.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/MARS/img/MARS_003.png&#34; alt=&#34;MARS Example Output&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: Visual example of MARS model detecting vehicle damage areas with high precision across complex body surfaces.
&lt;/div&gt;
&lt;p&gt;The core concept behind MARS draws inspiration from traditional quadtree spatial partitioning methods. Unlike standard convolutional encoders, which often struggle to preserve important structural details in large feature maps due to fixed processing limits, MARS takes a more adaptive approach. It uses a trainable system that breaks an image into regions based on how visually complex each area is. Simpler areas are processed more lightly, while more detailed or chaotic regions get extra attention.&lt;/p&gt;
&lt;p&gt;$$
M = \sum_{i=1}^{N} g(q_i) \cdot A(q_i) \cdot F(q_i)
$$&lt;/p&gt;
&lt;p&gt;This leads to what the team calls Sequential Quadtree Nodes (SQN) â€” a smart structure that expands only where deeper analysis is needed. Each of these nodes contributes to the overall segmentation by combining spatial focus, a confidence check, and fine-tuned local adjustments. The system selectively decides which parts of the image matter most, refining the final output by integrating all these localized insights into one coherent segmentation result.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/MARS/img/featured.png&#34; alt=&#34;MARS Model Architecture&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: MARS architecture integrates attention refinement, quadtree partitioning, and sequential mask propagation modules to enable instance-level car damage segmentation.
&lt;/div&gt;
&lt;p&gt;In our experiments on Thai car damage datasets, MARS with a ResNet101-FPN backbone outperforms all existing baselines, yielding a +2.3 maskAP improvement over PointRend and +1.8 over Mask Transfiner (Panboonyuen et al. 2023). More impressively, MARS demonstrates resilience in heavily occluded scenes, where traditional architectures often over-segment or misclassify minor damages such as scratches, dents, and broken trims.&lt;/p&gt;
&lt;p&gt;Beyond segmentation accuracy, MARS was designed with inference cost and deployment scalability in mind. The attention and SQN modules are modular and parallelizable, allowing fast inference on GPU-based edge devices commonly deployed in insurance claim centers. In practice, MARS processes a high-resolution damage image in under 180ms on an NVIDIA T4 GPU, enabling real-time integration into mobile claim apps and garage estimation tools.&lt;/p&gt;
&lt;p&gt;For a comprehensive explanation of the method, experimental protocol, and benchmark comparisons, readers are encouraged to refer to our full publication &lt;a href=&#34;https://arxiv.org/abs/2305.04743&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; (Panboonyuen et al. 2023).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;marsail-2025-research-highlights&#34;&gt;MARSAIL 2025: Research Highlights&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;As we move into 2025, MARSAIL continues to push the frontier of AI for automotive insurance and repair through multimodal AI systems, low-parameter transformer architectures, and high-resolution OCR solutions tailored for real-world document processing. These efforts aim to expand our automation ecosystem beyond just car damage detection â€” now encompassing risk prediction, document understanding, vehicle condition analytics, and identity verification pipelines. Below we highlight three key models from our most recent research drop.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-albert-efficient-transformer-for-automotive-localization&#34;&gt;1. ALBERT: Efficient Transformer for Automotive Localization&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;ALBERT (Advanced Localization and Bidirectional Encoder Representations from Transformers) is our compact vision transformer (ViT) tailored for car damage assessment. Unlike standard ViT and DETR-based approaches, ALBERT introduces structural inductive biases using localized deformable tokens and parameter sharing to reduce model size and memory usage while preserving high-resolution localization.&lt;/p&gt;
&lt;p&gt;Mathematically, the ALBERT encoder layer refines input embeddings using the following formulation:&lt;/p&gt;
&lt;p&gt;$$
z^{(l+1)} = z^{(l)} + \text{MSA}(\text{LN}(z^{(l)})) + \text{MLP}(\text{LN}(z^{(l)}))
$$&lt;/p&gt;
&lt;p&gt;Where MSA is a multi-scale self-attention with learnable spatial offsets adapted to car damage priors. Through selective hard sampling and token grouping, ALBERT reduces GPU memory by 40% during training while achieving higher IoU for small scratches and localized bumper cracks. This model is ideal for mobile deployments in insurance apps and in-vehicle camera assessments (&lt;a href=&#34;http://arxiv.org/abs/2506.10524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2506.10524&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-slick-knowledge-enhanced-instance-segmentation&#34;&gt;2. SLICK: Knowledge-Enhanced Instance Segmentation&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;SLICK (Selective Localization and Instance Calibration with Knowledge) augments vision transformer-based instance segmentation using both spatial priors and domain knowledge graphs. By integrating policy-driven insurance metadata â€” e.g., bumper policy zones, model-specific weak points â€” SLICK dynamically adjusts attention weights and segmentation proposals. This results in better prediction under diverse lighting, weather, and occlusion scenarios, especially for aging or modified vehicles.&lt;/p&gt;
&lt;p&gt;We define a mask quality score enhanced by contextual gating:&lt;/p&gt;
&lt;p&gt;$$
s_{mask} = \sigma(W_q [f_{img} | f_{kg}]) + b
$$&lt;/p&gt;
&lt;p&gt;SLICK delivers +2.5 maskAP improvement over MARS in high-noise scenes, setting a new benchmark in our internal Thai Vehicle Damage dataset (&lt;a href=&#34;http://arxiv.org/abs/2506.10528&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2506.10528&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-dota-deformable-optimized-transformer-for-ocr&#34;&gt;3. DOTA: Deformable Optimized Transformer for OCR&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;DOTA (Deformable Optimized Transformer Architecture) is our OCR engine designed for reading Thai ID cards, driver licenses, vehicle registration books, VIN plates, and inspection forms in real-world garage conditions. Using a retrieval-augmented architecture and deformable attention blocks, DOTA can accurately recognize text even under motion blur, glare, or partial occlusion.&lt;/p&gt;
&lt;p&gt;Unlike traditional CRNN or standard transformer OCRs, DOTA uses a hybrid vision-language training objective:&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-mars-mask-attention-refinement-with-sequential-quadtree-nodes-for-car-damage-instance-segmentation&#34;&gt;4. MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;At &lt;strong&gt;MARSAIL&lt;/strong&gt;, under the guidance of Dr. Teerapong Panboonyuen (à¸”à¸£. à¸˜à¸µà¸£à¸à¸‡à¸¨à¹Œ à¸›à¸²à¸™à¸šà¸¸à¸à¸¢à¸·à¸™) or Dr. Kao (à¸”à¸£. à¹€à¸à¹‰à¸²), our team is on a mission: to fuse cutting-edge &lt;strong&gt;vision systems&lt;/strong&gt; with real-world auto insurance workflows. Our latest workâ€”published in Springerâ€”introduces a dynamic way to analyze vehicle damage and process claims faster, smarter, and more precisely than ever before.&lt;/p&gt;
&lt;h3 id=&#34;-what-sets-our-approach-apart&#34;&gt;ğŸ” what sets our approach apart&lt;/h3&gt;
&lt;p&gt;Standard image-analysis tools often struggle to keep structural details intact when dealing with complex scenes or large images. In our paper (Springer, Chapter 3), we introduce a &lt;strong&gt;hierarchical, attention-driven method&lt;/strong&gt; that adapts dynamically to the visual complexity of each region:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Smart partitioning&lt;/strong&gt; â€” the system learns to divide an image into smaller, more granular regions where details matter most.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Targeted refinement&lt;/strong&gt; â€” complex zones receive deeper analysis, while simpler areas are processed more lightly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sequential quadtree nodes&lt;/strong&gt; â€” these elements expand only in areas needing sharper focus, concentrating computing power where it counts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-how-it-works-in-practice&#34;&gt;ğŸ§  how it works in practice&lt;/h3&gt;
&lt;p&gt;Each &lt;strong&gt;quadtree node&lt;/strong&gt; contributes to the final damage map by combining:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Spatial focus&lt;/strong&gt; â€” where exactly to look,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Confidence-based gating&lt;/strong&gt; â€” deciding &lt;em&gt;how much&lt;/em&gt; to trust that region,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local feature tweaks&lt;/strong&gt; â€” refining the outcome with sharp, situational insight.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Imagine a damaged car: intricate scratches around the headlight trigger more detailed analysis, while broader, simpler zones are processed more efficiently. The combined result is a crisp and accurate segmentation mask covering the entire vehicle.&lt;/p&gt;
&lt;h3 id=&#34;-why-it-matters-for-insurers--workshops&#34;&gt;ğŸï¸ why it matters for insurers &amp;amp; workshops&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster, smarter estimates&lt;/strong&gt; â€” automation tackles repetitive image tasks instantly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sharper insights&lt;/strong&gt; â€” context-aware focus pinpoints damage with precision.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficiency-driven&lt;/strong&gt; â€” resources are optimized for the parts of the image that matter, reducing processing load.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable architecture&lt;/strong&gt; â€” adaptable from single images to massive fleets seamlessly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-conclusions--next-steps&#34;&gt;âœ… conclusions &amp;amp; next steps&lt;/h3&gt;
&lt;p&gt;Published under the banner of &amp;ldquo;MARSAIL&amp;rdquo; in the latest Springer volume, this work signals a bold step in auto-insurance intelligence. With a pioneering lab approach and visionary leadership from Dr. Kao, MARSAIL is crafting systems that are fast, precise, and endlessly adaptable.&lt;/p&gt;
&lt;p&gt;Up next: integrating our quadtree vision engine into live insurer workflows, and testing it in real-world garages across Thailand.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Springer Nature Link&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;Auto insurance fraud is a massive challenge that costs billions worldwide every year, slowing down claim processing and increasing premiums for honest customers. At &lt;strong&gt;MARSAIL&lt;/strong&gt; (Motor AI Recognition Solution Artificial Intelligence Laboratory), we are on the frontlines of this battle, deploying cutting-edge AI to automate and streamline the entire insurance claim process â€” from damage detection to fraud prevention â€” without relying on manual human intervention.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;fighting-fraud-with-intelligent-automation&#34;&gt;Fighting Fraud with Intelligent Automation&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Fraudsters exploit loopholes and manual processes in traditional insurance workflows, making it difficult for companies to detect suspicious claims quickly and accurately. MARSAIL leverages advanced computer vision and domain-specific knowledge to transform this game. By analyzing images and metadata from vehicles in real-time, our AI models not only identify damages precisely but also flag potential fraud patterns, speeding up claim approval and reducing false payouts.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_01.jpg&#34; alt=&#34;Using AI to Automate Car Insurance Claims&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: Demonstration of AI-powered automation in car insurance claims (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;visual-damage-interpretation--smarter-and-faster&#34;&gt;Visual Damage Interpretation â€” Smarter and Faster&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Using state-of-the-art AI, MARSAIL interprets vehicle damage from photos submitted through mobile apps or web platforms. This automated damage assessment removes subjectivity and human error, ensuring fair and consistent evaluation across all claims.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_02.jpeg&#34; alt=&#34;AI Interpreting Car Damage in Application&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: AI-driven damage interpretation on insurance applications and web portals (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;translating-damage-into-precise-repair-estimates&#34;&gt;Translating Damage Into Precise Repair Estimates&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;MARSAILâ€™s AI doesnâ€™t stop at identifying damage. It accurately translates the visual information into detailed labor and parts costs, providing transparent and consistent repair estimates that speed up approvals and payments.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_03.jpeg&#34; alt=&#34;AI Converting Damage to Labor and Parts Cost&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: AI converting damage images into labor and parts cost estimates (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ai-powered-fraud-detection--catching-the-unseen&#34;&gt;AI-Powered Fraud Detection â€” Catching the Unseen&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;One of MARSAILâ€™s critical strengths is its ability to detect anomalies and fraudulent claims. By cross-referencing damage patterns, metadata, and historical data, the AI flags suspicious cases automatically, drastically reducing fraud risk and protecting insurers and honest customers alike.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_04.jpeg&#34; alt=&#34;AI Detecting Fraud in Auto Insurance Claims&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: AI-powered fraud detection in insurance claims (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;comprehensive-automated-reporting-for-faster-decisions&#34;&gt;Comprehensive, Automated Reporting for Faster Decisions&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;MARSAILâ€™s automated reporting delivers a clear, itemized breakdown of all repair costs and identified damages, allowing claims adjusters and repair shops to make faster, more informed decisions â€” with complete transparency and no human bias.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_05.jpeg&#34; alt=&#34;Final AI Report of Car Damage and Repair Cost&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: Final AI-generated report summarizing car damage and repair costs (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-marsail-leads-the-charge&#34;&gt;Why MARSAIL Leads the Charge&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;By automating the entire claims process â€” from damage detection and cost estimation to fraud detection and reporting â€” MARSAIL empowers insurers with unmatched speed, accuracy, and trustworthiness. This end-to-end AI solution minimizes human error and bias, reduces processing time dramatically, and safeguards the insurance ecosystem from fraudulent activities.&lt;/p&gt;
&lt;p&gt;The result? Faster claims, fairer settlements, and a more secure insurance future â€” powered entirely by intelligent automation.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;join-the-future-of-auto-insurance&#34;&gt;Join the Future of Auto Insurance&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;MARSAIL is not just innovating AI for automotive damage detection; we are transforming the entire insurance experience â€” making it smarter, safer, and more efficient. Together, weâ€™re winning the battle against auto insurance fraud and paving the way for a better, fairer tomorrow.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;future-work-beyond-damage--toward-full-lifecycle-intelligence&#34;&gt;Future Work: Beyond Damage â€“ Toward Full-Lifecycle Intelligence&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Looking forward, our roadmap involves the development of a unified AI engine that combines damage reasoning, document verification, driving behavior analysis, and vehicle lifecycle forecasting. This would support real-time insurance pricing, repair prioritization, fraud detection, and second-hand vehicle valuation. MARSAIL is also training domain-specific large language models (LLMs) to interact with policyholders in Thai, automatically generate claim reports, and retrieve legal clauses for garageâ€“insurance negotiations.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re collaborating with garages and insurers across Thailand to create a national automotive data lake for federated learning â€” enabling privacy-respecting model training across decentralized garages without centralizing sensitive vehicle data. Our ultimate goal is to support the Thai governmentâ€™s vision for a Smart Nation and to make MARSAIL the Southeast Asian leader in Automotive Intelligence Infrastructure.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Jul 2025). &lt;em&gt;MARSAIL: The Smart Engine Powering the Future of Car Insurance and Intelligent Garages&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{panboonyuen2023mars,
  title={MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation},
  author={Panboonyuen, Teerapong, et al.},
  booktitle={International Conference on Image Analysis and Processing},
  year={2023},
  organization={Springer}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Found this blog insightful? Consider sharing it with friends or researchers in the automotive or insurance tech industry. ğŸš—
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Panboonyuen, Teerapong, et al. &amp;ldquo;Mars: Mask attention refinement with sequential quadtree nodes for car damage instance segmentation.&amp;rdquo; International Conference on Image Analysis and Processing. Cham: Springer Nature Switzerland, 2023.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wang, X., X. Li, and Z. Wu. (2023). &lt;em&gt;Cardd: A new dataset for vision-based car damage detection&lt;/em&gt;. IEEE Transactions on Intelligent Transportation Systems, 24(7), 7202-7214.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Elbhrawy, A. S., M. A. Belal, and M. S. Hassanein. (2024). &lt;em&gt;CES: Cost Estimation System for Enhancing the Processing of Car Insurance Claims&lt;/em&gt;. Journal of Computing and Communication, 3(1), 55-69.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Amirfakhrian, M., and M. Parhizkar. (2021). &lt;em&gt;Integration of image segmentation and fuzzy theory to improve the accuracy of damage detection areas in traffic accidents&lt;/em&gt;. Journal of Big Data, 8(1), 1â€“17.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Arnab, A., and P. H. S. Torr. (2017). &lt;em&gt;Pixelwise instance segmentation with a dynamically instantiated network&lt;/em&gt;. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 441-450.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bolya, D., C. Zhou, F. Xiao, and Y. J. Lee. (2019). &lt;em&gt;Yolact: Real-time instance segmentation&lt;/em&gt;. Proceedings of the IEEE/CVF International Conference on Computer Vision, 9157â€“9166.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chen, K., J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, et al. (2019). &lt;em&gt;Hybrid task cascade for instance segmentation&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4974â€“4983.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chen, H., K. Sun, Z. Tian, C. Shen, Y. Huang, and Y. Yan. (2020). &lt;em&gt;Blendmask: Top-down meets bottom-up for instance segmentation&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8573â€“8581.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Girshick, R. (2015). &lt;em&gt;Fast R-CNN&lt;/em&gt;. Proceedings of the IEEE International Conference on Computer Vision, 1440â€“1448.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;He, K., G. Gkioxari, P. DollÃ¡r, and R. Girshick. (2017). &lt;em&gt;Mask R-CNN&lt;/em&gt;. Proceedings of the IEEE International Conference on Computer Vision, 2961â€“2969.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;JÃµeveer, K., and K. Kepp. (2023). &lt;em&gt;What drives drivers? Switching, learning, and the impact of claims in car insurance&lt;/em&gt;. Journal of Behavioral and Experimental Economics, 103, 101993.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ke, L., M. Danelljan, X. Li, Y.-W. Tai, C.-K. Tang, and F. Yu. (2022). &lt;em&gt;Mask Transfiner for high-quality instance segmentation&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4412â€“4421.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kirillov, A., Y. Wu, K. He, and R. Girshick. (2020). &lt;em&gt;PointRend: Image segmentation as rendering&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9799â€“9808.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Macedo, A. M., C. V. Cardoso, J. S. M. Neto, et al. (2021). &lt;em&gt;Car insurance fraud: The role of vehicle repair workshops&lt;/em&gt;. International Journal of Law, Crime and Justice, 65, 100456.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parhizkar, M., and M. Amirfakhrian. (2022). &lt;em&gt;Car detection and damage segmentation in the real scene using a deep learning approach&lt;/em&gt;. International Journal of Intelligent Robotics and Applications, 6(2), 231â€“245.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Panboonyuen, T. (2019). &lt;em&gt;Semantic segmentation on remotely sensed images using deep convolutional encoder-decoder neural network&lt;/em&gt;. Ph.D. Thesis, Chulalongkorn University.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pasupa, K., P. Kittiworapanya, N. Hongngern, and K. Woraratpanya. (2022). &lt;em&gt;Evaluation of deep learning algorithms for semantic segmentation of car parts&lt;/em&gt;. Complex &amp;amp; Intelligent Systems, 8(5), 3613â€“3625.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weisburd, S. (2015). &lt;em&gt;Identifying moral hazard in car insurance contracts&lt;/em&gt;. Review of Economics and Statistics, 97(2), 301â€“313.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wang, X., R. Zhang, T. Kong, L. Li, and C. Shen. (2020). &lt;em&gt;Solov2: Dynamic and fast instance segmentation&lt;/em&gt;. Advances in Neural Information Processing Systems, 33, 17721â€“17732.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Xie, E., P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, and P. Luo. (2020). &lt;em&gt;Polarmask: Single shot instance segmentation with polar representation&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12193â€“12202.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zhang, Q., X. Chang, and S. Bian. (2020). &lt;em&gt;Vehicle-damage-detection segmentation algorithm based on improved Mask R-CNN&lt;/em&gt;. IEEE Access, 8, 6997â€“7004.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The battle against auto insurance fraud â€“ and how AI can help win it: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery</title>
      <link>https://kaopanboonyuen.github.io/publication/satdiff-a-stable-diffusion-framework-for-inpainting/</link>
      <pubDate>Tue, 13 May 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/satdiff-a-stable-diffusion-framework-for-inpainting/</guid>
      <description>&lt;h2 id=&#34;keywords&#34;&gt;Keywords&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Diffusion Models&lt;/li&gt;
&lt;li&gt;High-Resolution Satellite Imagery&lt;/li&gt;
&lt;li&gt;Inpainting&lt;/li&gt;
&lt;li&gt;Latent Space Conditioning&lt;/li&gt;
&lt;li&gt;Remote Sensing&lt;/li&gt;
&lt;li&gt;Stable Diffusion&lt;/li&gt;
&lt;li&gt;Satellite Image Restoration&lt;/li&gt;
&lt;li&gt;Very High-Resolution Images&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Satellite image inpainting is a crucial process in remote sensing, aimed at recovering missing or damaged areas to ensure precise data analysis. Satellite images are often affected by occlusions from factors like clouds, atmospheric disturbances, or physical obstructions, making it challenging to obtain fully clear and complete data. To overcome these challenges, especially in high-resolution satellite imagery, effective inpainting methods are required that can reconstruct the missing portions while maintaining the overall structural coherence of the image.&lt;/p&gt;
&lt;p&gt;Diffusion models have emerged as powerful tools in image inpainting, representing a significant advancement in the field. They have been applied to a wide array of tasks, such as object removal, generative inpainting with contextual attention, and addressing semantic differences between masked and unmasked regions. These models excel in maintaining structural consistency while producing high-quality restorations, making them effective for complex tasks, such as video inpainting, text-based object removal, and sketch-based inpainting.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;main_arch_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In recent years, diffusion models have become prominent for image inpainting due to their ability to generate high-fidelity results. Traditionally, inpainting with diffusion models has been approached through either preconditioned or postconditioned techniques. Preconditioned models are designed explicitly for inpainting tasks, enabling efficient inference but requiring extensive domain-specific training. In contrast, postconditioned models do not necessitate retraining but involve slower inference, as they rely on multiple forward-backward iterations to achieve optimal solutions.&lt;/p&gt;
&lt;p&gt;The two primary strategiesâ€”preconditioning and postconditioningâ€”differ in how they incorporate inpainting into diffusion models. Preconditioning integrates inpainting into the training phase, where a conditional model predicts missing regions based on the masked input. While effective for specific domains, this approach requires retraining for new applications. Postconditioning, on the other hand, employs an unconditioned generative model, applying forward diffusion to unmasked pixels and reverse diffusion to fill in masked areas. Although this eliminates the need for retraining, it is computationally intensive, requiring numerous diffusion passes to refine the final image.&lt;/p&gt;
&lt;p&gt;Achieving effective image inpainting requires seamless propagation of information from unmasked to masked regions to ensure semantic consistency and a coherent final image. Inspired by our previous work, SatInPaint, we introduce &lt;strong&gt;SatDiff&lt;/strong&gt;, a novel diffusion-based framework that builds on this foundation to achieve improved recall and overall performance. SatDiff employs a &lt;strong&gt;Latent Space Conditioning&lt;/strong&gt; approach to facilitate inpainting within the latent space, rather than the image space, enabling efficient and precise reconstruction. Additionally, we integrate a forward-backward fusion mechanism within the latent space, enhancing stability and accuracy. SatDiff further incorporates the &lt;strong&gt;Segment Anything Model (SAM)&lt;/strong&gt; to refine the propagation process, boosting reconstruction quality and preserving semantic coherence.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;main_arch_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Through extensive experimentation on very high-resolution (VHR) satellite datasets, including DeepGlobe and the Massachusetts Roads Dataset, we validate the contributions of each component in our proposed method. Our results demonstrate that SatDiff not only surpasses state-of-the-art methods in inpainting accuracy and runtime efficiency but also excels in reconstructing realistic satellite images that closely resemble the ground truth. Building on the strengths of SatInPaint, SatDiff sets a new benchmark for high-quality and scalable satellite image restoration solutions.&lt;/p&gt;
&lt;h2 id=&#34;results-and-examples&#34;&gt;Results and Examples&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;satdif_result_01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;satdif_result_02.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;satdif_result_03.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;satellite-image-inpainting-results&#34;&gt;Satellite Image Inpainting Results&lt;/h3&gt;
&lt;p&gt;We present examples of satellite image inpainting results across different object sizes. The comparison showcases the input images (with occlusions), the ground truth targets (without occlusions), and the outputs generated by our method, SatDiff. These results highlight SatDiff&amp;rsquo;s capability to address real-world challenges, such as reconstructing satellite imagery obscured by clouds or other obstacles, with high accuracy.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this work, we introduced &lt;strong&gt;SatDiff&lt;/strong&gt;, a novel framework for satellite image inpainting based on diffusion models. By incorporating Latent Space Conditioning and Explicit Propagation, SatDiff achieves improved accuracy and efficiency for high-resolution satellite datasets. The results demonstrate the effectiveness of the proposed method in addressing real-world satellite inpainting challenges.&lt;/p&gt;
&lt;p&gt;For more details, visit the official &lt;a href=&#34;https://github.com/kaopanboonyuen/SatDiff&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SatDiff GitHub repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CHULA: Custom Heuristic Uncertainty-Guided Loss for Accurate Land Title Deed Segmentation</title>
      <link>https://kaopanboonyuen.github.io/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/</link>
      <pubDate>Sun, 11 May 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Investigating the use of deep learning-derived weighted mean temperature for GPS-PWVs estimation</title>
      <link>https://kaopanboonyuen.github.io/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UAMC2025 (Applied Mathematics Conference)</title>
      <link>https://kaopanboonyuen.github.io/talk/uamc2025-applied-mathematics-conference/</link>
      <pubDate>Thu, 27 Mar 2025 13:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/uamc2025-applied-mathematics-conference/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;ğŸš—ğŸ’¡ This Sunday (March 30, 2025), if you&amp;rsquo;re around Ladkrabang, come join us for a math talk! ğŸ“šğŸ’¬ Weâ€™ll dive deep into the fascinating world of Vision Transformers (ViTs) and their applications in car insurance AI. It&amp;rsquo;s an exciting opportunity to explore how advanced mathematical techniques are driving the future of AI in the insurance industry. Donâ€™t miss outâ€”join us for this insightful session!&lt;/p&gt;
&lt;p&gt;Topic: Mathematical Foundations of Vision Transformers in Car Insurance AI&lt;br&gt;
ğŸ• Time: 1:00 PM&lt;/p&gt;
&lt;p&gt;ğŸ“ Location: &lt;a href=&#34;https://www.facebook.com/photo?fbid=1192313742905954&amp;amp;set=a.490286653108670&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Event Location&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;uamc2025_images/UAMC2025xTVIxMARS_0001.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xTVIxMARS_0002.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Read more on Thaivivat&amp;rsquo;s blog:&lt;/strong&gt; &lt;a href=&#34;https://careers.thaivivat.co.th/newsandevents/6808628ebfad6e8912fd5c57&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    In this insightful session, Dr. Teerapong Panboonyuen (Dr. Kao) will explore the mathematical foundations behind Vision Transformers (ViTs) and their groundbreaking applications in car insurance AI.
  &lt;/div&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;As the insurance industry increasingly leverages artificial intelligence, understanding the complex mathematical principles that drive these innovations is key. This talk will shed light on how ViTs are revolutionizing the way car insurance companies analyze and predict risk, offering a glimpse into the future of InsurTech. Whether you&amp;rsquo;re a student or a professional, this is an exciting opportunity to dive into the intersection of math, AI, and insurance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;(Reference on UAMC 2025)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_05.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_02.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_04.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_03.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_06.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_01.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_07.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;uamc2025_images/UAMC2025xMARS_08.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In recent years, Vision Transformers (ViTs) have emerged as one of the most powerful models in the field of deep learning, particularly in tasks involving image recognition. This blog post explores the mathematical foundations of ViTs and their application in car insurance AI, with a focus on improving accuracy in claims prediction. We will cover key concepts such as the self-attention mechanism, custom loss functions, and how large language models (LLMs) could further enhance these AI systems. By the end of this article, youâ€™ll have a deeper understanding of how cutting-edge AI technologies are reshaping the insurance industry.&lt;/p&gt;
&lt;h2 id=&#34;vision-transformers-a-mathematical-overview&#34;&gt;Vision Transformers: A Mathematical Overview&lt;/h2&gt;
&lt;h3 id=&#34;the-transformer-architecture&#34;&gt;The Transformer Architecture&lt;/h3&gt;
&lt;p&gt;At the heart of Vision Transformers lies the Transformer architecture, which was originally designed for natural language processing tasks. Unlike traditional convolutional neural networks (CNNs), which rely on convolutional layers to process spatial data, ViTs treat an image as a sequence of patches and apply self-attention to learn relationships between these patches.&lt;/p&gt;
&lt;p&gt;The image is divided into non-overlapping patches, each of which is flattened into a vector. These patch vectors are then embedded into a higher-dimensional space. The idea is that these embedded patches will be processed as a sequence, similar to how words in a sentence are processed in NLP tasks. This sequence is passed through layers of self-attention, where the model learns how to focus on different parts of the image depending on their relevance.&lt;/p&gt;
&lt;h3 id=&#34;self-attention-mechanism&#34;&gt;Self-Attention Mechanism&lt;/h3&gt;
&lt;p&gt;The self-attention mechanism is what allows Vision Transformers to capture complex relationships between distant regions of the image. In a standard convolutional network, filters are used to detect local patterns in the image. However, in ViTs, the self-attention mechanism dynamically determines the importance of each patch relative to others. This makes it possible for the model to capture long-range dependencies and subtle patterns across the entire image, rather than focusing only on local features.&lt;/p&gt;
&lt;p&gt;By applying this mechanism, Vision Transformers can give more weight to certain parts of the image that are more relevant for the task at hand, whether itâ€™s identifying damage in a vehicle or classifying certain types of claims.&lt;/p&gt;
&lt;h2 id=&#34;custom-loss-functions-for-car-insurance-ai&#34;&gt;Custom Loss Functions for Car Insurance AI&lt;/h2&gt;
&lt;h3 id=&#34;the-need-for-custom-loss-functions&#34;&gt;The Need for Custom Loss Functions&lt;/h3&gt;
&lt;p&gt;In the context of car insurance, traditional loss functions like cross-entropy are not always ideal. For instance, car insurance claims often involve rare and highly specific damage types that are underrepresented in training datasets. This creates a class imbalance issue, where the model may not perform well on minority classes, such as rare types of damage or low-frequency events.&lt;/p&gt;
&lt;p&gt;To address this, custom loss functions are developed to weight certain classes higher than others, ensuring that the model focuses on less frequent but equally important classes. This type of custom loss function helps improve prediction accuracy, particularly in a domain like insurance, where predicting the likelihood of rare events is just as crucial as predicting common ones.&lt;/p&gt;
&lt;h3 id=&#34;mean-squared-error-for-regression-tasks&#34;&gt;Mean Squared Error for Regression Tasks&lt;/h3&gt;
&lt;p&gt;In some cases, car insurance AI needs to predict continuous values, such as the cost of repair. For such tasks, a loss function like Mean Squared Error (MSE) is commonly used. This loss function measures the difference between the predicted and true values, with the aim of minimizing this error. Using MSE ensures that the modelâ€™s predictions are as close as possible to the actual values, which is critical when estimating damage repair costs.&lt;/p&gt;
&lt;h2 id=&#34;integrating-large-language-models-llms-for-enhanced-ai&#34;&gt;Integrating Large Language Models (LLMs) for Enhanced AI&lt;/h2&gt;
&lt;p&gt;The integration of Large Language Models (LLMs) like GPT-4 with Vision Transformers can open up new possibilities for multi-modal AI systems. In car insurance, LLMs can process textual dataâ€”such as customer reports, claim descriptions, and policy documentsâ€”while ViTs handle visual data like images of the damaged vehicle.&lt;/p&gt;
&lt;p&gt;By combining these two modalities, we can create a system that not only understands visual information but also the context behind it. For example, an LLM could help interpret the description of an accident or read through a claim submission and make sense of the visual evidence provided by the ViT. This multi-modal approach enhances decision-making, enabling more accurate claim assessments and better customer experiences.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Vision Transformers are revolutionizing AI in the car insurance industry by enabling the accurate and efficient processing of complex visual data. By leveraging advanced mathematical concepts like self-attention and custom loss functions, we can build AI models that are more accurate and robust, even when dealing with rare and highly variable data. The potential for combining ViTs with LLMs represents the next frontier in AI, making it possible to create systems that understand both images and text, paving the way for more intelligent, automated solutions in car insurance.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Å., &amp;amp; Polosukhin, I. (2017). &lt;em&gt;Attention is All You Need&lt;/em&gt;. In Advances in Neural Information Processing Systems (NeurIPS 2017). &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dosovitskiy, A., Berman, M., &amp;amp; Hinton, G. E. (2020). &lt;em&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/em&gt;. In Proceedings of the International Conference on Machine Learning (ICML 2020). &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shinn, N., &amp;amp; others. (2020). &lt;em&gt;Language Models are Few-Shot Learners&lt;/em&gt;. In Proceedings of NeurIPS 2020. &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OpenAI. (2023). &lt;em&gt;GPT-4 Technical Report&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2303.08774&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation</title>
      <link>https://kaopanboonyuen.github.io/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/</link>
      <pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance</title>
      <link>https://kaopanboonyuen.github.io/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/</link>
      <pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NAC2025 (NSTDA Annual Conference 2025)</title>
      <link>https://kaopanboonyuen.github.io/talk/nac2025-nstda-annual-conference-2025/</link>
      <pubDate>Wed, 26 Feb 2025 09:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/nac2025-nstda-annual-conference-2025/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;h2 id=&#34;join-the-fun-world-of-ai-train-a-model-to-find-wheres-waldo&#34;&gt;&lt;strong&gt;Join the Fun World of AI: Train a Model to Find â€œWhereâ€™s Waldoâ€!&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Hey there, AI enthusiasts and curious minds! Are you ready to take a deep dive into the fascinating world of artificial intelligence? Get ready for a fun and hands-on experience where weâ€™ll unravel the mysteries of &lt;strong&gt;Deep Learning&lt;/strong&gt; ğŸ§ , &lt;strong&gt;Computer Vision&lt;/strong&gt; ğŸ‘€, and &lt;strong&gt;Vision Transformers&lt;/strong&gt;â€”the game-changing technology behind image recognition. But hereâ€™s the twist: weâ€™re going to train an AI to solve the ultimate challengeâ€”finding &lt;strong&gt;Whereâ€™s Waldo&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;Yep, you heard it right. Weâ€™ll be combining cutting-edge AI techniques with a classic puzzle, and by the end of the session, youâ€™ll be equipped to build and train an AI model that can spot Waldo in a sea of distractions. Ready to unlock the magic? Letâ€™s go!&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/e37dSDJeLos&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;If this sounds exciting to you, donâ€™t miss out on the chance to be part of this hands-on AI experience at &lt;strong&gt;&lt;a href=&#34;https://www.nstda.or.th/nac/2025/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAC2025&lt;/a&gt;&lt;/strong&gt;. Whether youâ€™re just starting to explore the world of artificial intelligence or looking to enhance your skills, this session is your perfect opportunity to learn, build, and innovate.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Join me and other AI enthusiasts on &lt;strong&gt;March 26th&lt;/strong&gt;, and together weâ€™ll dive into this challenge while discovering the potential of AI in real-world applications. &lt;strong&gt;Click &lt;a href=&#34;https://www.nstda.or.th/nac/2025/youth-activities/youth-activity-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/strong&gt; to secure your spot and be part of the future of AI-driven innovation at NAC2025. Letâ€™s train some AI and find Waldoâ€”together!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;what-youll-learn&#34;&gt;&lt;strong&gt;What Youâ€™ll Learn&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;If youâ€™ve ever wondered how self-driving cars â€œseeâ€ the world, or how facial recognition software works, youâ€™re in for a treat. In this session, weâ€™ll break down the essentials of &lt;strong&gt;Deep Learning&lt;/strong&gt; and &lt;strong&gt;Computer Vision&lt;/strong&gt;â€”two critical components that enable machines to understand and interpret the visual world, just like humans do.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The Basics of Deep Learning&lt;/strong&gt;&lt;br&gt;
At the core of modern AI, &lt;strong&gt;Deep Learning&lt;/strong&gt; involves training algorithms to recognize patterns and make decisions based on data. Weâ€™ll start by exploring how these algorithms learn from massive datasets, allowing them to &amp;ldquo;see&amp;rdquo; and make sense of images, sounds, and even text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Understanding Computer Vision&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;Computer Vision&lt;/strong&gt; is a field within AI that teaches computers to interpret and analyze visual information from the world. It&amp;rsquo;s the backbone behind everything from image classification to object detection, and it&amp;rsquo;s what powers applications like Google Images, Snapchat filters, and even medical imaging. Weâ€™ll cover how vision systems break down images into recognizable patterns, enabling the AI to understand what itâ€™s looking at.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The Rise of Vision Transformers&lt;/strong&gt;&lt;br&gt;
Hereâ€™s where it gets really cool. &lt;strong&gt;Vision Transformers&lt;/strong&gt; (ViTs) have emerged as a breakthrough in image recognition. Unlike traditional Convolutional Neural Networks (CNNs), which process images in a grid-like manner, Vision Transformers work by treating images as sequences, similar to how we process language. These models capture global relationships within an image, allowing them to recognize complex patterns with remarkable accuracy. Weâ€™ll explore how ViTs work, and why theyâ€™re the future of computer vision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building an AI Model to Find Waldo&lt;/strong&gt;&lt;br&gt;
This is where you roll up your sleeves! In this hands-on segment, weâ€™ll show you how to train an AI model to identify Waldo in various images. Weâ€™ll walk through the entire processâ€”preparing the data, building the model, training it, and evaluating its performance. Along the way, weâ€™ll use the power of &lt;strong&gt;Vision Transformers&lt;/strong&gt; to detect Waldo, just like how cutting-edge AI models are trained to spot objects, faces, or animals in any image. It&amp;rsquo;s going to be a blast as you watch your AI model get better and better at solving the â€œWhereâ€™s Waldoâ€ puzzle!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;why-vision-transformers&#34;&gt;&lt;strong&gt;Why Vision Transformers?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;So, why are we focusing on &lt;strong&gt;Vision Transformers&lt;/strong&gt;? It all comes down to performance. Traditional CNNs have been the go-to for image recognition for years, but ViTs are changing the game. Hereâ€™s why theyâ€™re so exciting:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Global Context Understanding&lt;/strong&gt;&lt;br&gt;
Vision Transformers are great at understanding the entire image as a whole. Instead of focusing on small patches, like CNNs, ViTs look at the global context, allowing them to capture relationships between distant parts of an image. This makes them excellent for complex recognition tasks, like identifying Waldo in a crowded scene where he could be hiding behind multiple objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;br&gt;
Vision Transformers scale well with large datasets, which is essential for training models on more complex images. They can process thousands of images simultaneously, learning from a massive variety of examples to improve their performance. So, whether youâ€™re training a model to find Waldo or doing more advanced tasks like facial recognition, ViTs can handle it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;&lt;br&gt;
ViTs are highly adaptable to various image types and problems. Whether youâ€™re working with low-resolution images or high-quality satellite photos, Vision Transformers can be fine-tuned to deliver impressive results across different domains.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;hands-on-training-your-own-wheres-waldo-model&#34;&gt;&lt;strong&gt;Hands-on: Training Your Own â€œWhereâ€™s Waldoâ€ Model&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Now that weâ€™ve got the basics covered, itâ€™s time to get your hands dirty! During this interactive session, youâ€™ll have the chance to train your very own AI model. Hereâ€™s a sneak peek at the process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preparing the Data&lt;/strong&gt;&lt;br&gt;
Before we can train our AI, we need to prepare a dataset of images featuring Waldo. Youâ€™ll learn how to curate and preprocess images for trainingâ€”an essential skill for any AI project. Youâ€™ll also get hands-on experience labeling images to help your model recognize Waldo in the wild.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Building the Model&lt;/strong&gt;&lt;br&gt;
Once we have the data, weâ€™ll build our &lt;strong&gt;Vision Transformer&lt;/strong&gt; model from scratch. Youâ€™ll learn the key concepts behind Vision Transformersâ€”how they work, what makes them different from traditional CNNs, and why theyâ€™re so powerful for image recognition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training the Model&lt;/strong&gt;&lt;br&gt;
This is where the magic happens. Weâ€™ll train our model using the images and labels youâ€™ve prepared. Youâ€™ll see how the model starts to â€œlearnâ€ and get better at recognizing Waldo over time. Weâ€™ll guide you through each step, from selecting the right optimizer to tuning the modelâ€™s hyperparameters for maximum performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Evaluating the Results&lt;/strong&gt;&lt;br&gt;
After training your model, itâ€™s time to evaluate how well it does at finding Waldo in new, unseen images. Weâ€™ll show you how to measure accuracy and fine-tune the model to improve its performance. Youâ€™ll walk away with a trained model that can spot Waldo with impressive accuracy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;whats-the-big-deal-about-wheres-waldo&#34;&gt;&lt;strong&gt;Whatâ€™s the Big Deal About â€œWhereâ€™s Waldoâ€?&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;You might be wondering, why use a â€œWhereâ€™s Waldoâ€ puzzle to teach AI? The answer is simple: itâ€™s a fun, relatable challenge thatâ€™s perfect for learning the basics of image recognition. But itâ€™s also a great example of real-world AI applications, where the task is to identify a specific object (Waldo) in a noisy environment (a crowded scene). By building this model, youâ€™re learning the same techniques that power cutting-edge AI applications used by companies like Google, Facebook, and even self-driving car manufacturers!&lt;/p&gt;
&lt;h3 id=&#34;why-you-should-join-us&#34;&gt;&lt;strong&gt;Why You Should Join Us&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This isnâ€™t just another dry lecture on AI. This is your chance to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learn in a fun, hands-on environment&lt;/strong&gt; with practical examples you can apply to your own projects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Master the basics of deep learning and computer vision&lt;/strong&gt; while exploring one of the most exciting advancements in AI.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Get a taste of the future&lt;/strong&gt; by learning about Vision Transformers, a breakthrough technology thatâ€™s revolutionizing image recognition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Walk away with a trained AI model&lt;/strong&gt; you can proudly say you built yourself!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Whether you&amp;rsquo;re a beginner eager to learn about AI or someone looking to deepen your understanding of computer vision, this session is for you. So, come join the fun, unleash your creativity, and letâ€™s train a model that finds Waldo in no time! ğŸš€&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Looking forward to seeing you at NAC2025, where AI meets fun! Letâ€™s make the world of AI an adventure, one Waldo at a time! ğŸ¯&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;meet-me-at-nac2025-wheres-waldo-ai-challenge-on-march-26th&#34;&gt;&lt;strong&gt;Meet Me at NAC2025: Where&amp;rsquo;s Waldo AI Challenge on March 26th!&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Get ready for a one-of-a-kind experience at &lt;strong&gt;NAC2025&lt;/strong&gt;â€”the 20th NSTDA Annual Conference! While the conference runs from &lt;strong&gt;March 26-28, 2025&lt;/strong&gt;, &lt;strong&gt;Iâ€™m inviting you to join me on March 26th&lt;/strong&gt; for an exclusive, hands-on session where weâ€™ll dive deep into the world of &lt;strong&gt;AI and computer vision&lt;/strong&gt; with the ultimate challenge: &lt;strong&gt;Finding Whereâ€™s Waldo!&lt;/strong&gt; ğŸ¯&lt;/p&gt;
&lt;p&gt;At &lt;strong&gt;NAC2025&lt;/strong&gt;, weâ€™re exploring how AI is transforming industries and driving a more sustainable future for Thailand. But on &lt;strong&gt;March 26th&lt;/strong&gt;, Iâ€™m turning the spotlight on a super fun and interactive topicâ€”teaching AI to spot &lt;strong&gt;Waldo&lt;/strong&gt; in a crowd! With &lt;strong&gt;Vision Transformers&lt;/strong&gt; and &lt;strong&gt;Deep Learning&lt;/strong&gt;, weâ€™ll break down how AI can â€œseeâ€ the world just like humans and even find that elusive red-and-white-striped figure hiding among distractions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;NAC2025_Kao_01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;NAC2025_Kao_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;So, if youâ€™re ready to learn the basics of AI, dive into &lt;strong&gt;computer vision&lt;/strong&gt;, and have a blast solving the â€œWhereâ€™s Waldo?â€ puzzle, donâ€™t miss this exclusive event on March 26th! The rest of the &lt;strong&gt;NAC2025&lt;/strong&gt; conference is packed with innovation, but this hands-on workshop is your chance to get involved, ask questions, and &lt;strong&gt;train your very own AI model&lt;/strong&gt;. Trust me, itâ€™s going to be a lot of fun!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;NAC2025_Kao_03.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;See you there on March 26thâ€”let&amp;rsquo;s crack the code and find Waldo together! ğŸ”&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;references&#34;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Where&amp;rsquo;s Waldo Dataset&lt;/strong&gt;: &lt;a href=&#34;https://www.kaggle.com/datasets/residentmario/wheres-waldo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle - Where&amp;rsquo;s Waldo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finding Waldo - A Primer&lt;/strong&gt;: &lt;a href=&#34;https://www.kaggle.com/code/residentmario/finding-waldo-a-primer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle - Finding Waldo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;image-credits&#34;&gt;&lt;strong&gt;Image Credits&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.imdb.com/title/tt0213376/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IMDB - Where&amp;rsquo;s Waldo (1992 TV series)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.in/Wheres-Waldo-Martin-Handford/dp/153621065X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Where&amp;rsquo;s Waldo - Book on Amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation</title>
      <link>https://kaopanboonyuen.github.io/publication/dota-deformable-optimized-transformer-architecture/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/dota-deformable-optimized-transformer-architecture/</guid>
      <description>&lt;h3 id=&#34;dota-deformable-optimized-transformer-architecture-for-end-to-end-text-recognition-with-retrieval-augmented-generation&#34;&gt;&lt;strong&gt;DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&#34;abstract&#34;&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Text recognition in natural images remains one of the most challenging yet essential tasks within the fields of computer vision and natural language processing. With applications ranging from document digitization to autonomous navigation, effective text recognition is more critical than ever before. In this paper, we introduce &lt;strong&gt;DOTA&lt;/strong&gt;, a novel end-to-end framework that combines ResNet and Vision Transformer (ViT) backbones with advanced methodologies such as &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;, &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;, and &lt;strong&gt;Conditional Random Fields (CRF)&lt;/strong&gt; to significantly enhance Optical Character Recognition (OCR) performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;At the heart of DOTA is a revolutionary approach where traditional convolution layers in the third and fourth blocks of the network are replaced with &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;â€”a technique that offers adaptive and robust feature extraction, making it ideal for recognizing text in complex and irregular layouts. Furthermore, &lt;strong&gt;adaptive dropout&lt;/strong&gt; is integrated to ensure regularization, helping to prevent overfitting and boosting generalization. To refine the sequential modeling of text, we leverage &lt;strong&gt;CRFs&lt;/strong&gt;, which excel in capturing intricate dependencies inherent in text recognition tasks.&lt;/p&gt;
&lt;p&gt;We conducted extensive experiments on six benchmark OCR datasetsâ€”IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80. Our results demonstrate the exceptional performance of DOTA, achieving remarkable accuracies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IC13&lt;/strong&gt;: 97.32%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IC15&lt;/strong&gt;: 58.26%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVT&lt;/strong&gt;: 88.10%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IIIT5K&lt;/strong&gt;: 74.13%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVTP&lt;/strong&gt;: 82.17%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUTE80&lt;/strong&gt;: 66.67%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This gives us an &lt;strong&gt;average accuracy of 77.77%&lt;/strong&gt;, setting a new state-of-the-art in the field of text recognition. The results clearly highlight the robustness of DOTA across a variety of challenging datasets.&lt;/p&gt;
&lt;h4 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Text recognition from images has long been a challenging problem, with significant implications for applications in document processing, automated data entry, and even autonomous systems. With traditional &lt;strong&gt;Optical Character Recognition (OCR)&lt;/strong&gt; systems heavily relying on &lt;strong&gt;Convolutional Neural Networks (CNNs)&lt;/strong&gt;, weâ€™ve seen progress in extracting features from images. However, as text layouts become more complexâ€”such as when dealing with varying fonts, orientations, and complex backgroundsâ€”CNNs often fall short. Enter the &lt;strong&gt;Transformer architectures&lt;/strong&gt;: these models have revolutionized many areas in computer vision, particularly in handling long-range dependencies through their self-attention mechanisms, offering significant improvements for text recognition tasks.&lt;/p&gt;
&lt;p&gt;While Transformer-based models have pushed the boundaries of OCR performance, there is still a need for further improvements, especially in the area of feature extraction and sequence modeling. That&amp;rsquo;s where &lt;strong&gt;DOTA&lt;/strong&gt; comes in. By combining the strengths of &lt;strong&gt;ResNet&lt;/strong&gt; and &lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt; backbones, this novel approach leverages &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;, &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;, and &lt;strong&gt;Conditional Random Fields (CRF)&lt;/strong&gt; to achieve new levels of OCR accuracy.&lt;/p&gt;
&lt;h4 id=&#34;the-dota-framework&#34;&gt;&lt;strong&gt;The DOTA Framework&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The architecture of DOTA is designed to enhance both feature extraction and sequence modeling, key areas in text recognition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deformable Convolutions&lt;/strong&gt;: By replacing standard convolutions in the networkâ€™s third and fourth blocks with &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;, we allow for more flexible and adaptive feature extraction. This helps capture irregular text patterns and varying layouts more effectively.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;: The integration of this approach provides context-aware enhancements, further refining the recognition process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Conditional Random Fields (CRFs)&lt;/strong&gt;: CRFs have been incorporated into the framework to better model the sequential nature of text, providing the necessary context to improve recognition accuracy, especially for more complex sequences of characters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Together, these components form the backbone of &lt;strong&gt;DOTA&lt;/strong&gt;, providing a more robust, adaptable, and precise model for OCR tasks. The effectiveness of DOTA is clearly demonstrated in its impressive performance across several standard OCR benchmark datasets.&lt;/p&gt;
&lt;h4 id=&#34;experimental-results&#34;&gt;&lt;strong&gt;Experimental Results&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;We evaluated DOTA using six widely-used OCR benchmark datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IC13&lt;/strong&gt;: 97.32%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IC15&lt;/strong&gt;: 58.26%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVT&lt;/strong&gt;: 88.10%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;IIIT5K&lt;/strong&gt;: 74.13%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SVTP&lt;/strong&gt;: 82.17%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUTE80&lt;/strong&gt;: 66.67%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With an average accuracy of 77.77%, DOTA has set a new state-of-the-art in OCR performance. The results show that the combination of Deformable Convolutions, Vision Transformers, and CRFs significantly improves recognition, even in challenging conditions where traditional methods struggle.&lt;/p&gt;
&lt;h4 id=&#34;conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The DOTA framework represents a major leap forward in the field of text recognition. By effectively combining ResNet and Vision Transformer backbones with advanced techniques like &lt;strong&gt;Deformable Convolutions&lt;/strong&gt;, &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;, and &lt;strong&gt;Conditional Random Fields&lt;/strong&gt;, it achieves impressive accuracy across a variety of challenging datasets. This work sets a new benchmark for OCR performance, providing a powerful tool for tackling the complexities of text recognition in real-world scenarios.&lt;/p&gt;
&lt;p&gt;With this new approach, weâ€™ve laid the foundation for even more accurate and robust text recognition systems, paving the way for smarter applications in everything from document processing to autonomous navigation.&lt;/p&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>ENGRU: A Preliminary Investigation of AI-Augmented Formal Verification and Its Challenges</title>
      <link>https://kaopanboonyuen.github.io/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/</link>
      <pubDate>Sun, 02 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/</guid>
      <description></description>
    </item>
    
    <item>
      <title>42.195K at Chombueng Marathon 2025 â€” My Second Full Marathon, Coming Home</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/</link>
      <pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/</guid>
      <description>&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#the-journey-back-home&#34;&gt;The Journey Back Home&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-race&#34;&gt;The Race&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-magic-of-chombueng&#34;&gt;The Magic of Chombueng&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-race-atmosphere&#34;&gt;The Race Atmosphere&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#lessons-and-reflections&#34;&gt;Lessons and Reflections&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#a-love-letter-to-chombueng&#34;&gt;A Love Letter to Chombueng&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;This yearâ€™s &lt;strong&gt;Chombueng Marathon&lt;/strong&gt; marked a special milestone for me: my &lt;strong&gt;second-ever full marathon&lt;/strong&gt;. My first was the &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Bangkok Marathon 2022&lt;/strong&gt;&lt;/a&gt;, an unforgettable experience that sparked my love for the 42.195K distance. Completing that first marathon taught me the value of resilience and determination, and itâ€™s the reason why I continue to embrace the challenge and joy of running full marathons.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This time, it felt even more meaningful as I returned to my &lt;strong&gt;hometown, Ratchaburi&lt;/strong&gt;, to take on the challenge. Despite being born here, it wasnâ€™t until nowâ€”at the age of 30â€”that I finally had the chance to run this iconic race in my own backyard. Running in my hometown for the most fun marathon of my lifeâ€”42.195K finisher at the Chombueng Marathon 2025.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;Chombueng Marathon&lt;/strong&gt;, now in its &lt;strong&gt;38th year&lt;/strong&gt;, is one of Thailandâ€™s oldest and most cherished marathons. Itâ€™s not just a race; itâ€™s a tradition that brings together runners from across the country and beyond, while showcasing the unique charm of this small but spirited district.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/KAO_CBM2025_CERT.png&#34; alt=&#34;Certificate for Chombueng Marathon 2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 1: My official certificate for completing the Chombueng Marathon 2025. Proud to be a full marathon finisher of 42.195K!&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-journey-back-home&#34;&gt;The Journey Back Home&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Coming back to Chombueng brought back a flood of childhood memories. Growing up, I spent countless weekends visiting my mom, who worked as a teacher here. The tranquil forests, the famous &lt;strong&gt;Tham Chomphon cave&lt;/strong&gt;, the peaceful campus of &lt;strong&gt;Rajabhat University Chombueng&lt;/strong&gt;â€”theyâ€™ve all remained beautifully unchanged. Running through these familiar sights made the experience even more special, as if the past and present collided in the best possible way.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/KAO_CBM2025_STAR01.png&#34; alt=&#34;Final Result Official Finisher in Web&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: Final result of my official finishing time on the Chombueng Marathon 2025 website.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/KAO_CBM2025_STAR02.png&#34; alt=&#34;Final Result Official Finisher in Web - Details&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: Detailed official result of my finish time, as shown on the Chombueng Marathon 2025 website.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/KAO_CBM2025_STAR03.png&#34; alt=&#34;GPS Mapping During the 42.195K Run&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: GPS mapping of my 42.195K route during the Chombueng Marathon 2025, as captured from satellite imagery.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-race&#34;&gt;The Race&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The &lt;strong&gt;2025 Chombueng Marathon&lt;/strong&gt; was held on &lt;strong&gt;January 19th&lt;/strong&gt;, with the gun going off for the &lt;strong&gt;42.195K runners at 3:00 a.m.&lt;/strong&gt;. The weather was perfect for running, hovering between &lt;strong&gt;15â€“19Â°C&lt;/strong&gt;â€”cool and crisp, just the way I like it.&lt;/p&gt;
&lt;p&gt;I started off strong, clocking a &lt;strong&gt;5:39 average pace for the first 5K&lt;/strong&gt;, easing slightly to &lt;strong&gt;5:54 for the next 10K&lt;/strong&gt;. By the time I reached the halfway point at 21K, I was feeling good, finishing the first half in just over 2 hours. But as the second half began, I shifted gears to a more strategic approach: &lt;strong&gt;run-walk intervals&lt;/strong&gt;. I wanted to finish this race injury-free and with enough energy to enjoy the rest of the dayâ€”because lifeâ€™s too short to spend it limping around post-race!&lt;/p&gt;
&lt;p&gt;In the end, I crossed the finish line with a &lt;strong&gt;chip time of 5 hours and 36 minutes&lt;/strong&gt;. While it wasnâ€™t a personal best or a sub-5, the sense of accomplishment and joy I felt was unbeatable.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_001.jpg&#34; alt=&#34;Chombueng Marathon 2025 Medal&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: Receiving my finisher&#39;s medal at the Chombueng Marathon 2025 after completing the 42.195K race. (Photo taken on my desk at home)&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_002_01.jpg&#34; alt=&#34;Me Running in the Chombueng Marathon 2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: Me running around the 40K mark during the Chombueng Marathon 2025.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_004_2.jpg&#34; alt=&#34;Chombueng Marathon 2025 Finish Line&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: My ultimate joy as I crossed the finish line â€“ completing my second-ever 42.195K marathon. What a feeling!&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_006_02.jpg&#34; alt=&#34;Chombueng Marathon 2025 Finish Line Celebration&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: Pure joy after crossing the finish line of my second 42.195K marathon. Everything feels possible now!&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-magic-of-chombueng&#34;&gt;The Magic of Chombueng&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;What truly sets the Chombueng Marathon apart is the &lt;strong&gt;atmosphere and community spirit&lt;/strong&gt;. The entire town comes alive for this event, cheering runners on from start to finish.&lt;/p&gt;
&lt;p&gt;Iâ€™ll always remember the &lt;strong&gt;monks sprinkling holy water&lt;/strong&gt; on us as we ran byâ€”a uniquely Thai tradition that made me smile every time. Then there were the &lt;strong&gt;students&lt;/strong&gt;, standing by the roadside in the early hours, clapping, cheering, and offering encouragement. And, of course, the &lt;strong&gt;locals&lt;/strong&gt; who came out to support us, their smiles and shouts of &lt;em&gt;&amp;ldquo;à¸ªà¸¹à¹‰ à¹†!&amp;rdquo;&lt;/em&gt; (Keep going!) giving us the boost we needed to push through each kilometer.&lt;/p&gt;
&lt;p&gt;To everyone who stood out in the cool morning air to cheer us on: &lt;strong&gt;thank you&lt;/strong&gt;. Your energy, enthusiasm, and kindness made all the difference.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_011.jpg&#34; alt=&#34;Chombueng Marathon 2025 Medal&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 9: The atmosphere as I run through the Chombueng Marathon 42.195K, one of Thailand&#39;s oldest and most iconic marathon events.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/CB_MARATHON_014.jpg&#34; alt=&#34;End of Chombueng Marathon 2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 10: Almost there! The final stretch before crossing the finish line at the Chombueng Marathon 2025.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-race-atmosphere&#34;&gt;The Race Atmosphere&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;This yearâ€™s marathon was filled with joy and excitement. I met so many incredible runners, each radiating energy and enthusiasm as we gathered at the start line at 3 a.m. The crowd was electric, with smiles all around and cheers that warmed the cool morning air. The support from the spectators was truly special, and I want to give a big shoutout to two amazing women who became unexpected guides during my race.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_CBM2025_V2_001.png&#34; alt=&#34;Chombueng Marathon 2025 Running Moment 1&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 11: A running moment during the Chombueng Marathon 2025.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Dressed in vibrant orange and blue shirts, these two seasoned runners unknowingly set the perfect pace for me during the first 15K. With their steady rhythm, I was able to maintain a comfortable &lt;strong&gt;5:50 min/km pace&lt;/strong&gt;, which kept me going without overexerting myself. Though I hadnâ€™t trained much and had no particular strategy in mind, following them gave me the boost I needed to tackle the early stages of the marathon.&lt;/p&gt;
&lt;p&gt;However, reality hit midwayâ€”I realized that sustaining that pace for the rest of the race was beyond my current fitness level. Adjusting my strategy, I had to let go of my hopes for a &lt;strong&gt;sub-4 finish&lt;/strong&gt;, but Iâ€™m incredibly grateful to those two women for helping me start strong.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_CBM2025_V2_002.png&#34; alt=&#34;Chombueng Marathon 2025 Running Moment 2&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 12: Another running moment during the marathon.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the end, I crossed the finish line in 5 hours and 38 minutes. While it wasnâ€™t the time I dreamed of, it was still a proud moment. Next time, I know &lt;strong&gt;sub-4 isnâ€™t too far out of reach&lt;/strong&gt;. Hereâ€™s to continuing the journey and pushing myself to new limits!&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_CBM2025_V2_003.png&#34; alt=&#34;Chombueng Marathon 2025 Running Moment 3&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 13: After crossing the finish line at the Chombueng Marathon 42.195K. I can imagine how exhausted everyone must have felt, but for me, it was pure joyâ€”just look at that smile!&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lessons-and-reflections&#34;&gt;Lessons and Reflections&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;This was my second full marathon, and while I didnâ€™t hit my sub-5 goal, Iâ€™m proud of what I achieved. I went into this race with very little trainingâ€”my recent runs had mostly been short 5Ks. But this marathon reignited my passion for running and reminded me why I love this sport so much.&lt;/p&gt;
&lt;p&gt;The biggest lesson? Itâ€™s okay to let go of the pressure. Instead of obsessing over times and splits, I focused on enjoying the journey, listening to my body, and soaking in the experience. Running should be fun, and as long as you keep moving forward with a smile, thatâ€™s what matters.&lt;/p&gt;
&lt;p&gt;I know there are more full marathons in my future. And when the next one comes around, Iâ€™ll be ready.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0612.JPG&#34; alt=&#34;Chombueng Marathon 2025 Final Sprint&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 14: My final sprint in the Chombueng Marathon 2025. Garmin Fenix 7 stats show my pace hitting peak performance!&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0613.JPG&#34; alt=&#34;Finish Line Moment in Chombueng Marathon 2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 15: Crossing the finish line at the Chombueng Marathon 2025. Garmin app stats show my finishing time and overall performance, a personal best!&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0438.jpg&#34; alt=&#34;Post-race Chombueng Marathon 2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 16: A post-race moment after completing the Chombueng Marathon 2025. My Garmin Fenix 7 stats show Iâ€™ve hit a new high with a VO2 Max of 55â€”feeling unstoppable!&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-love-letter-to-chombueng&#34;&gt;A Love Letter to Chombueng&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The Chombueng Marathon isnâ€™t just a raceâ€”itâ€™s a memory, a tradition, and a piece of my heart. Running in my hometown, surrounded by the places and people that shaped my early years, was an experience Iâ€™ll always treasure.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0614.jpg&#34; alt=&#34;Celebrating Finish at Chombueng Marathon 2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 17: Celebrating after completing my second 42.195K marathon at Chombueng Marathon 2025. Feeling on top of the world with this amazing finish!&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you, Chombueng, for the beautiful scenery, the incredible community, and the unforgettable memories. This race will forever hold a special place in my heart, and I canâ€™t wait to come back again.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Until next time, Chombueng.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Jan 2025). &lt;em&gt;42.195K at Chombueng Marathon 2025 â€” My Second Full Marathon, Coming Home&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{panboonyuen2025chombueng,
  title   = &amp;quot;42.195K at Chombueng Marathon 2025 â€” My Second Full Marathon, Coming Home&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Jan&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Global Young Scientists Summit (GYSS) 2025: Where Science Meets Inspiration</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/</link>
      <pubDate>Mon, 13 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/</guid>
      <description>&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#presenting-my-work-to-her-royal-highness-princess-maha-chakri-sirindhorn&#34;&gt;Presenting My Work to Her Royal Highness Princess Maha Chakri Sirindhorn&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#understanding-the-basics-of-dnn-operations&#34;&gt;Understanding the Basics of DNN Operations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#challenges-in-parameter-recovery&#34;&gt;Challenges in Parameter Recovery&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#polynomial-time-parameter-extraction&#34;&gt;Polynomial Time Parameter Extraction&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#critical-points-and-linear-equations&#34;&gt;Critical Points and Linear Equations&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#practical-demonstration&#34;&gt;Practical Demonstration&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#comparison-with-cryptographic-systems&#34;&gt;Comparison with Cryptographic Systems&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#implications-of-randomness-and-alternative-activations&#34;&gt;Implications of Randomness and Alternative Activations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#broader-implications-and-future-directions&#34;&gt;Broader Implications and Future Directions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#plenary-lecture-educability-prof-leslie-valiant&#34;&gt;Plenary Lecture: Educability (Prof Leslie Valiant)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#historical-foundations-of-intelligence-and-educability&#34;&gt;Historical Foundations of Intelligence and Educability&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#educability-vs-machine-learning&#34;&gt;Educability vs. Machine Learning&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#comparing-human-and-machine-learning&#34;&gt;Comparing Human and Machine Learning&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#cognitive-capabilities-as-civilization-enablers&#34;&gt;Cognitive Capabilities as Civilization Enablers&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#ethical-implications-of-ai-development&#34;&gt;Ethical Implications of AI Development&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#addressing-concerns-about-ai&#34;&gt;Addressing Concerns About AI&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-future-of-ai-and-educability&#34;&gt;The Future of AI and Educability&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#a-call-to-action&#34;&gt;A Call to Action&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#plenary-lecture-compressing-proofs-using-cryptography-prof-yael-kalai&#34;&gt;Plenary Lecture: Compressing Proofs using Cryptography (Prof Yael Kalai)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#what-are-succinct-proofs&#34;&gt;What Are Succinct Proofs?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-cryptographic-techniques&#34;&gt;Key Cryptographic Techniques&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#polynomial-commitments&#34;&gt;Polynomial Commitments&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#applications-of-succinct-proofs&#34;&gt;Applications of Succinct Proofs&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#ai-model-verification&#34;&gt;AI Model Verification&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#decentralized-identity&#34;&gt;Decentralized Identity&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#challenges-and-future-directions&#34;&gt;Challenges and Future Directions&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#insights-from-prof-yael-tauman-kalais-research&#34;&gt;Insights from Prof. Yael Tauman Kalaiâ€™s Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#engaging-in-dialogues-on-the-ethics-of-ai&#34;&gt;Engaging in Dialogues on the Ethics of AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#ethics-of-scientific-research-in-the-age-of-ai&#34;&gt;Ethics of Scientific Research in the Age of AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#client-concerns-about-ai&#34;&gt;Client Concerns About AI&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#opportunities-and-risks&#34;&gt;Opportunities and Risks&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#public-engagement&#34;&gt;Public Engagement&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#accountability-and-regulation&#34;&gt;Accountability and Regulation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#future-directions&#34;&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#visual-representation&#34;&gt;Visual Representation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#expanding-my-network-and-building-new-collaborations&#34;&gt;Expanding My Network and Building New Collaborations&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#a-day-ill-never-forget&#34;&gt;A Day Iâ€™ll Never Forget&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;Attending &lt;strong&gt;GYSS2025 (Global Young Scientists Summit)&lt;/strong&gt; is an incredibly exciting opportunity. This prestigious gathering brings together brilliant young minds from all over the world, providing a platform to engage with leading experts, share innovative ideas, and immerse ourselves in the latest advancements in science and technology. Itâ€™s a momentous occasion for any researcher or scientist, and Iâ€™m thrilled to be part of this yearâ€™s summit.&lt;/p&gt;
&lt;p&gt;Weâ€™re living in an exciting era, where &lt;strong&gt;Large Language Models (LLMs)&lt;/strong&gt; are reshaping the landscape of artificial intelligence. The rapid strides AI has made in recent years, fueled by powerful architectures like GPT, are nothing short of revolutionary. As we explore the depths of these models, we are witnessing the dawn of new possibilities in natural language processing, conversational agents, and machine learning. Attending GYSS2025 during this transformative period in AIâ€™s evolution promises to be a truly enriching experience, as it will allow me to explore these advancements and exchange ideas with some of the brightest minds in the field.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Being part of &lt;strong&gt;GYSS2025&lt;/strong&gt; is one of the most exciting and proud moments in my life. Itâ€™s a powerful gathering where the brightest young scientists from around the world come together to spark innovation and push the limits of whatâ€™s possible. This event is more than just a conferenceâ€”itâ€™s an electrifying experience that fills me with energy and inspiration. Sharing space with brilliant minds and world-class experts motivates me to keep challenging myself and driving meaningful change. Itâ€™s a milestone that Iâ€™ll remember forever, a moment that highlights how far hard work and passion can take you, and a reminder of the incredible impact we can make together.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Honored to have attended GYSS2025 in Singapore!  &lt;br&gt;.&lt;br&gt;An incredible experience connecting with global innovators, exchanging ideas, and gaining inspiration to shape the future. &lt;br&gt;.&lt;a href=&#34;https://t.co/LqTKodKSj3&#34;&gt;https://t.co/LqTKodKSj3&lt;/a&gt; &lt;a href=&#34;https://t.co/hJKWz6YTgy&#34;&gt;pic.twitter.com/hJKWz6YTgy&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1878614389978472895?ref_src=twsrc%5Etfw&#34;&gt;January 13, 2025&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;



&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/ZGXAT3bq7-c&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_01.jpg&#34; alt=&#34;Badge photo at GYSS2025 board&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 1: Photo with my event badge in front of the GYSS2025 board.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;presenting-my-work-to-her-royal-highness-princess-maha-chakri-sirindhorn&#34;&gt;Presenting My Work to Her Royal Highness Princess Maha Chakri Sirindhorn&lt;/h2&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;First, I would like to sincerely thank Her Royal Highness Princess Maha Chakri Sirindhorn for her gracious support and unwavering dedication to advancing science and education in Thailand. It is through her continued encouragement and vision that young researchers like myself are given extraordinary opportunities to grow, connect with global leaders, and proudly represent our country on the world stage. Her support made this once-in-a-lifetime experience at GYSS 2025 possible, and I am deeply honored and inspired by her commitment to nurturing the next generation of Thai scientists.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I feel truly honored and deeply grateful to be selected as one of only 15 young scientists from Thailand to attend the Global Young Scientists Summit (GYSS) 2025 in Singapore. Iâ€™m especially proud to be the sole representative from Thailand in the fields of computer engineering and artificial intelligence, and one of only three postdoctoral researchers selected this year. GYSS has inspired me to keep following my passion in research â€” it was incredibly exciting to meet world-class professors Iâ€™ve admired for years, including a legendary computer scientist who received the Turing Award. That moment truly meant the world to me.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/files/GYSS/panboonyuen_GYSS2025.jpg&#34; alt=&#34;Thailand representative at GYSS2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: Honored to represent Thailand at GYSS2025 with heartfelt gratitude to HRH Princess Maha Chakri Sirindhorn. I am one of only three postdoctoral researchers selected this year â€” and the sole delegate from Thailand in the fields of computer engineering and AI. (Reference Image: From official Facebook of NSTDA-GYSS: &lt;a href=&#34;https://www.facebook.com/photo.php?fbid=1061339665992254&amp;id=100063486913512&amp;set=a.529956069130619&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The highlight of the summit was, without a doubt, the moment I had the incredible honor of presenting my work to Her Royal Highness Princess Maha Chakri Sirindhorn.&lt;/p&gt;
&lt;p&gt;As a young scientist, I was deeply humbled to share my research titled &lt;em&gt;â€œMeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailandâ€&lt;/em&gt; with Her Royal Highness. My work focuses on addressing challenges in agricultural monitoring and resource management by leveraging cutting-edge advancements in artificial intelligence. Specifically, MeViT is designed for semantic segmentation of Landsat satellite imagery, targeting key economic crops in Thailand such as para rubber, corn, and pineapple. By enhancing Vision Transformers (ViTs) with a medium-resolution multi-branch architecture and incorporating mixed-scale convolutional feedforward networks (MixCFN), MeViT excels at extracting multi-scale local information critical for precise segmentation.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/tgcKR97Ea8I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Her Royal Highness listened with great interest, her graciousness reflecting her profound commitment to nurturing the next generation of scientists. She expressed encouragement for the practical applications of such research in addressing challenges critical to Thailandâ€™s agricultural and environmental sustainability. Her unwavering support for young researchers is a testament to her dedication to fostering innovation for the betterment of society.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_06_2.jpg&#34; alt=&#34;Group photo with HRH Princess Maha Chakri Sirindhorn&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: Group photo with Her Royal Highness Princess Maha Chakri Sirindhorn.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_06.jpg&#34; alt=&#34;Close-up photo with HRH at GYSS2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: A truly special momentâ€”standing right behind HRH Princess Maha Chakri Sirindhorn. A deeply proud and memorable experience.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_07.jpg&#34; alt=&#34;Photo with HRH and NSTDA staff&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: Group photo with HRH Princess Maha Chakri Sirindhorn and NSTDA staff members.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_03.jpg&#34; alt=&#34;Photo with HRH and Thai researchers&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: A proud moment with HRH and fellow Thai researchers selected for GYSS2025.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/Her_Royal_Highness_toGYSS2025_2.png&#34; alt=&#34;Photo with HRH and research team&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: With HRH Princess Maha Chakri Sirindhorn, fellow researchers, and NSTDA staff.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It was a truly humbling and inspiring momentâ€”one that I will carry with me for the rest of my life. The opportunity to share my work with Her Royal Highness not only reaffirmed my passion for pushing the boundaries of science and technology but also strengthened my resolve to contribute to meaningful advancements that serve the nation and the global community.&lt;/p&gt;
&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/XzUmDPFHfc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;!-- &lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;video controls width=&#34;800&#34;&gt;
        &lt;source src=&#34;https://kaopanboonyuen.github.io/GYSS2025_withHRH/GYSS_with_Her_Royal_Highnes_v1.mp4&#34; type=&#34;video/mp4&#34;&gt;
        Your browser does not support the video tag.
    &lt;/video&gt;
&lt;/div&gt; --&gt;
&lt;hr&gt;
&lt;h1 id=&#34;learning-from-icons-in-the-field&#34;&gt;Learning from Icons in the Field&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;The summit was filled with groundbreaking talks, and one of the most memorable was the Plenary Lecture by the renowned Professor Adi Shamir. As a Turing Award Laureate and an expert in cryptography and artificial intelligence, his lecture was truly thought-provoking. The topic, &lt;em&gt;â€œCan you recover a deep neural network from its answers?â€&lt;/em&gt;, delved into one of the most critical questions of our timeâ€”how deep learning models, which are at the forefront of AI, can be understood and potentially reverse-engineered.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/3KNBME7f0VI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The lecture began with a discussion on the architecture of modern neural networks, emphasizing their complexity and overparameterization. Professor Shamir explored concepts like model inversion, adversarial attacks, and the limitations of current AI systems, while raising questions about privacy-preserving AI and ethical AI frameworks. His insights on adversarial robustness and explainable AI deeply resonated with my research interests, motivating me to reflect on these critical challenges.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0218.jpg&#34; alt=&#34;Lecture with Professor Adi Shamir&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: Attending a lecture with Professor Adi Shamir at GYSS2025.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0206.jpg&#34; alt=&#34;Lecture with Professor Adi Shamir&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 9: Learning from the legendary Professor Adi Shamirâ€”co-inventor of RSA and Turing Award laureate.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Deep Neural Networks (DNNs) have become indispensable in modern AI applications, with billions of dollars and countless GPU hours invested in their training. These models are often deployed as &amp;ldquo;black boxes,&amp;rdquo; allowing users to interact with them without revealing their inner workings. However, this raises a critical question: &lt;em&gt;Can the parameters of a deep neural network be recovered using only its inputs and outputs?&lt;/em&gt; In a groundbreaking plenary lecture, Turing Award recipient Prof. Adi Shamir demonstrated that for ReLU-based DNNs, it is indeed possible to recover all parameters in polynomial time relative to the number of neurons. His findings, supported by practical experiments, highlight both the potential vulnerabilities of these systems and the need for robust defenses.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;understanding-the-basics-of-dnn-operations&#34;&gt;Understanding the Basics of DNN Operations&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;At the heart of Shamir&amp;rsquo;s analysis lies a detailed understanding of how DNNs function. A DNN comprises multiple layers of neurons, each performing a series of linear transformations followed by non-linear activations. Among these, the Rectified Linear Unit (ReLU) is a widely used activation function defined as:&lt;/p&gt;
&lt;p&gt;$$
\text{ReLU}(x) = \max(0, x).
$$&lt;/p&gt;
&lt;p&gt;This piecewise linear function introduces non-linearity while maintaining computational simplicity. Shamir emphasized that ReLU&amp;rsquo;s linear segments make it particularly susceptible to parameter extraction, as its outputs can be mathematically analyzed to reveal underlying weights and biases.&lt;/p&gt;
&lt;p&gt;In mathematical terms, the operation of a single layer in a DNN can be expressed as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{y} = \text{ReLU}(\mathbf{W}\mathbf{x} + \mathbf{b}),
$$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{x}$ represents the input vector,&lt;/li&gt;
&lt;li&gt;$\mathbf{W}$ is the weight matrix,&lt;/li&gt;
&lt;li&gt;$\mathbf{b}$ is the bias vector, and&lt;/li&gt;
&lt;li&gt;$\mathbf{y}$ is the output vector after applying ReLU.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stacking multiple such layers creates a complex mapping from inputs to outputs, making the network appear opaque to external observers.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;challenges-in-parameter-recovery&#34;&gt;Challenges in Parameter Recovery&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Recovering the parameters of a DNNâ€”its weights and biasesâ€”is inherently an NP-hard problem. This complexity arises from the high dimensionality of the parameter space and the limited observability of internal computations. Traditional approaches to this problem relied on exhaustive searches, which scale exponentially with the number of parameters, rendering them impractical for large networks.&lt;/p&gt;
&lt;p&gt;Prof. Shamir highlighted that these challenges are exacerbated in scenarios where outputs are restricted to discrete or low-precision values. However, he proposed that by carefully designing input queries and analyzing output patterns, it is possible to significantly simplify the recovery process.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;polynomial-time-parameter-extraction&#34;&gt;Polynomial Time Parameter Extraction&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Shamir&amp;rsquo;s key contribution lies in demonstrating a polynomial-time attack for ReLU-based DNNs. His approach leverages the inherent linearity of ReLU segments to derive equations that describe the network&amp;rsquo;s behavior. By identifying &lt;strong&gt;critical points&lt;/strong&gt;â€”locations where ReLU outputs switch between active and inactive statesâ€”one can extract sufficient information to reconstruct the network&amp;rsquo;s parameters.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;critical-points-and-linear-equations&#34;&gt;Critical Points and Linear Equations&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Consider a single ReLU neuron with input $z$ and output $y = \text{ReLU}(z)$. The critical point for this neuron is $z = 0$, where the output transitions from 0 to a positive value. By probing the network with carefully chosen inputs that traverse these critical points, it becomes possible to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identify the active/inactive state of each neuron.&lt;/li&gt;
&lt;li&gt;Extract linear equations relating the input, weights, and biases.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For an $n$-layer network, these equations can be combined to solve for all parameters using standard techniques from linear algebra.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;practical-demonstration&#34;&gt;Practical Demonstration&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;In a practical demonstration, Shamir applied his method to an 8-layer DNN trained on the CIFAR-10 dataset. This network contained 1.2 million parameters, yet all were successfully recovered in polynomial time. The experiment underscores the real-world applicability of this attack and its implications for AI security.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;comparison-with-cryptographic-systems&#34;&gt;Comparison with Cryptographic Systems&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Drawing parallels between DNNs and cryptographic systems, Shamir likened the structure of a neural network to a block cipher, where each layer performs a distinct transformation. In cryptography, security often hinges on the infeasibility of reversing these transformations without a key. Similarly, DNNs rely on the assumption that their internal parameters cannot be deduced from external interactions.&lt;/p&gt;
&lt;p&gt;However, Shamir&amp;rsquo;s work demonstrates that this assumption does not hold for ReLU-based networks. By exploiting the deterministic nature of their operations, an adversary can effectively &amp;ldquo;decrypt&amp;rdquo; the network to reveal its parameters.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;implications-of-randomness-and-alternative-activations&#34;&gt;Implications of Randomness and Alternative Activations&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;To mitigate the risks posed by such attacks, Shamir explored potential defenses, including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Randomness in Training and Inference&lt;/strong&gt;: Introducing stochasticity into the network&amp;rsquo;s operations, such as random noise or dropout, can obscure critical points and complicate parameter recovery.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Alternative Activation Functions&lt;/strong&gt;: Functions like the sigmoid or hyperbolic tangent (tanh) introduce smoother transitions, reducing the linearity exploited in Shamir&amp;rsquo;s attack. The sigmoid function, for example, is defined as:&lt;/p&gt;
&lt;p&gt;$$
\sigma(x) = \frac{1}{1 + e^{-x}}.
$$&lt;/p&gt;
&lt;p&gt;Unlike ReLU, sigmoid outputs are continuous and bounded, making it harder to identify critical points.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;broader-implications-and-future-directions&#34;&gt;Broader Implications and Future Directions&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Shamir&amp;rsquo;s findings have profound implications for the field of AI security. As DNNs become integral to applications ranging from healthcare to autonomous systems, ensuring their robustness against parameter extraction attacks is paramount. Future research may focus on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Designing architectures that are inherently resistant to such attacks.&lt;/li&gt;
&lt;li&gt;Developing formal metrics to quantify a network&amp;rsquo;s susceptibility to parameter recovery.&lt;/li&gt;
&lt;li&gt;Exploring the trade-offs between interpretability and security.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In conclusion, Prof. Adi Shamir&amp;rsquo;s lecture sheds light on a critical vulnerability in modern AI systems while providing a roadmap for addressing it. His innovative use of cryptographic techniques underscores the interdisciplinary nature of AI research and its potential to reshape our understanding of security in the digital age.&lt;/p&gt;
&lt;p&gt;But what truly stood out was the in-depth discussion I had with Professor Shamir. His monumental impact on the field of AI and his perspectives on current and future challenges was both a privilege and a learning experience that I will treasure forever.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_04.jpg&#34; alt=&#34;Photo with Professor Adi Shamir&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 10: A memorable moment meeting Professor Adi Shamir, co-inventor of RSA and recipient of the ACM A.M. Turing Award.
    (&lt;a href=&#34;https://amturing.acm.org/award_winners/shamir_2327856.cfm&#34; target=&#34;_blank&#34;&gt;Read more&lt;/a&gt;)
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;plenary-lecture-educability-prof-leslie-valiant&#34;&gt;Plenary Lecture: Educability (Prof Leslie Valiant)&lt;/h2&gt;
&lt;hr&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/RT5LaVPEiyU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;In a captivating lecture, Prof. Leslie Valiant delved into the concept of educability, a framework that bridges the gap between human cognitive capabilities and machine learning. Drawing inspiration from Alan Turingâ€™s groundbreaking insights, the discussion highlighted the interplay between intelligence, learning, and the ethical design of artificial intelligence systems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;historical-foundations-of-intelligence-and-educability&#34;&gt;Historical Foundations of Intelligence and Educability&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The quest to define human intelligence has long been fraught with challenges. Psychologists have struggled to agree on a single definition, revealing the complexity of human cognition. Prof. Valiant proposed that much of our understanding can be reframed through the lens of &lt;strong&gt;educability&lt;/strong&gt;, which encompasses three primary facets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Learning from Experience&lt;/strong&gt; ($\mathcal{L}_{experience}$): The ability to generalize patterns and principles from observed phenomena.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning with Acquired Knowledge&lt;/strong&gt; ($\mathcal{R}_{knowledge}$): Chaining learned concepts to make inferences.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Absorbing Explicit Instruction&lt;/strong&gt; ($\mathcal{I}_{instruction}$): Gaining knowledge through direct teaching or guidance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These components have enabled humanity to progress from rudimentary tools to advanced technological civilizations.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;educability-vs-machine-learning&#34;&gt;Educability vs. Machine Learning&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;A cornerstone of the lecture was the &lt;strong&gt;Church-Turing Thesis&lt;/strong&gt;, which posits that all forms of computationâ€”whether in human brains or machinesâ€”are fundamentally equivalent. This foundational idea underpins modern efforts to replicate human cognition in artificial intelligence.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;comparing-human-and-machine-learning&#34;&gt;Comparing Human and Machine Learning&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Current AI systems excel in pattern recognition and data-driven learning. However, they fall short in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Contextual Reasoning:&lt;/strong&gt; Humans can apply learned knowledge across diverse scenarios. For example, recognizing that if $A \implies B$ and $B \implies C$, then $A \implies C$.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learning from Minimal Examples:&lt;/strong&gt; Unlike humans, who can learn concepts from a few instances, AI often requires massive datasets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instruction-Based Learning:&lt;/strong&gt; Humans thrive in environments with structured instruction, a capability that AI systems struggle to emulate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prof. Valiant argued that replicating these nuanced aspects of educability in AI could unlock new levels of machine intelligence.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cognitive-capabilities-as-civilization-enablers&#34;&gt;Cognitive Capabilities as Civilization Enablers&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Human educability is unique among species, enabling the creation of advanced civilizations. This capability hinges on the ability to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generalize Across Domains:&lt;/strong&gt; Applying principles learned in one context to solve problems in another.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accumulate Knowledge:&lt;/strong&gt; Building on the work of previous generations through explicit instruction and documentation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collaborate:&lt;/strong&gt; Combining cognitive efforts to achieve collective goals.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ethical-implications-of-ai-development&#34;&gt;Ethical Implications of AI Development&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Prof. Valiant emphasized the ethical considerations in designing AI systems that reflect positive human traits while avoiding the replication of human flaws. Key takeaways included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Avoiding Bias:&lt;/strong&gt; Ensuring that AI systems do not inherit societal biases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transparent Decision-Making:&lt;/strong&gt; Designing AI that can explain its reasoning processes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Augmenting Human Capabilities:&lt;/strong&gt; Building systems that complement rather than replace human intelligence.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;addressing-concerns-about-ai&#34;&gt;Addressing Concerns About AI&lt;/h3&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Accountability:&lt;/strong&gt; Who is responsible for decisions made by AI systems?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safety:&lt;/strong&gt; How do we ensure AI systems act in the best interest of humanity?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Global Standards:&lt;/strong&gt; The need for international cooperation to establish ethical guidelines.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-future-of-ai-and-educability&#34;&gt;The Future of AI and Educability&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Prof. Valiant concluded with a vision for the future:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interdisciplinary Collaboration:&lt;/strong&gt; Bringing together technologists, ethicists, and cognitive scientists to advance AI responsibly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Formalizing Educability:&lt;/strong&gt; Developing mathematical models to encode human-like learning in machines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Human-Machine Collaboration:&lt;/strong&gt; Leveraging AI to enhance human cognitive abilities, leading to breakthroughs in science, medicine, and technology.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;a-call-to-action&#34;&gt;A Call to Action&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Understanding the parameters of educability is not just an academic pursuit; it is a moral imperative. By integrating insights from cognitive science, mathematics, and ethics, we can create AI systems that are not only intelligent but also aligned with human values.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This exploration of educability challenges us to rethink the foundations of intelligence and its implications for the future of artificial intelligence. As we navigate the complexities of AI development, let us be guided by the principles of transparency, accountability, and collaboration.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;plenary-lecture-compressing-proofs-using-cryptography-prof-yael-kalai&#34;&gt;Plenary Lecture: Compressing Proofs using Cryptography (Prof Yael Kalai)&lt;/h2&gt;
&lt;hr&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/bo67m5XyVWo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Cryptography has always been the backbone of secure systems, enabling trust in decentralized and distributed environments. As computation scales, the demand for efficient proof systems that ensure both correctness and privacy becomes increasingly important. Succinct proofs represent a groundbreaking development in this domain, offering compact, verifiable proofs that maintain efficiency and security.&lt;/p&gt;
&lt;p&gt;In a recent lecture, &lt;strong&gt;Prof. Yael Tauman Kalai&lt;/strong&gt; presented her advancements in succinct proofs, detailing their cryptographic foundations, practical implications, and applications in areas like blockchain and artificial intelligence (AI). This blog unpacks her insights and explores how these proofs address modern computational challenges.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-are-succinct-proofs&#34;&gt;What Are Succinct Proofs?&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Succinct proofs are cryptographic constructs designed to reduce the size and verification complexity of traditional proof systems. Instead of requiring a verifier to redo the entire computation to confirm its validity, succinct proofs enable verification with minimal computational effort. This efficiency is achieved without sacrificing security or trust, making them a critical tool for systems where scalability and privacy are paramount.&lt;/p&gt;
&lt;p&gt;Unlike traditional proof systems, which may involve large data sets and complex computations, succinct proofs achieve their compactness through advanced cryptographic techniques such as &lt;strong&gt;homomorphic encryption&lt;/strong&gt;, &lt;strong&gt;polynomial commitments&lt;/strong&gt;, and &lt;strong&gt;elliptic curve cryptography&lt;/strong&gt;. These techniques enable a prover to encode the essential details of a computation into a small, verifiable proof.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;key-cryptographic-techniques&#34;&gt;Key Cryptographic Techniques&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;One of the core building blocks of succinct proofs is the concept of zero-knowledge proofs (ZKPs). A ZKP allows a prover to convince a verifier that a statement is true without revealing any information beyond its validity. This ensures privacy while maintaining trust.&lt;/p&gt;
&lt;p&gt;For example, in a blockchain transaction, a ZKP can prove the correctness of the transaction without disclosing the sender, receiver, or amount. This is particularly critical in privacy-preserving protocols like &lt;strong&gt;zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge)&lt;/strong&gt; and &lt;strong&gt;zk-STARKs (Zero-Knowledge Scalable Transparent Arguments of Knowledge)&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;zk-SNARKs&lt;/strong&gt; rely on a trusted setup to initialize the system. They use elliptic curve pairings and polynomial arithmetic to achieve their compactness. Despite their efficiency, the trusted setup requirement introduces potential vulnerabilities if compromised.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;zk-STARKs&lt;/strong&gt;, on the other hand, eliminate the need for a trusted setup by relying on hash functions and polynomial interpolation. While this makes them more transparent and secure, they often result in larger proof sizes compared to zk-SNARKs.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;polynomial-commitments&#34;&gt;Polynomial Commitments&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Another crucial technique in succinct proofs is the use of polynomial commitments. These commitments enable the prover to encode computations as polynomials, allowing the verifier to check their correctness without directly interacting with the underlying data. Polynomial commitments are a cornerstone of many cryptographic protocols, including zk-STARKs and modern succinct proof systems.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;applications-of-succinct-proofs&#34;&gt;Applications of Succinct Proofs&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;One of the most impactful applications of succinct proofs is in blockchain technology. Blockchains, by design, require every participant to validate transactions to maintain trust. However, as the number of transactions grows, this validation becomes a bottleneck.&lt;/p&gt;
&lt;p&gt;Succinct proofs offer a solution by enabling participants to verify the correctness of transactions without processing the full chain. Protocols like &lt;strong&gt;Ethereum&amp;rsquo;s Layer 2 solutions&lt;/strong&gt; and &lt;strong&gt;Zcash&lt;/strong&gt; leverage succinct proofs to improve scalability and maintain privacy.&lt;/p&gt;
&lt;p&gt;For instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rollups&lt;/strong&gt; in Ethereum aggregate transactions off-chain and use succinct proofs to certify their correctness on-chain.&lt;/li&gt;
&lt;li&gt;Privacy-focused blockchains like Zcash use zk-SNARKs to enable shielded transactions, ensuring that details about the sender, receiver, and transaction amount remain confidential.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ai-model-verification&#34;&gt;AI Model Verification&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;In the realm of AI, succinct proofs are emerging as a tool to certify the correctness of model outputs. As AI systems grow more complex, the ability to verify their decisions becomes a challenge. Succinct proofs can be used to generate a compact, verifiable record of an AI modelâ€™s decision-making process.&lt;/p&gt;
&lt;p&gt;For example, in image classification tasks, a succinct proof could certify that the model correctly identified an object without requiring the verifier to process the entire dataset or model. This has profound implications for AI applications in critical domains like healthcare, where trust and accountability are paramount.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;decentralized-identity&#34;&gt;Decentralized Identity&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Decentralized identity systems aim to give individuals control over their personal data while allowing them to prove certain attributes (e.g., age, citizenship) without revealing unnecessary details. Succinct proofs enable such systems by providing compact, privacy-preserving verifications.&lt;/p&gt;
&lt;p&gt;Protocols like &lt;strong&gt;Verifiable Credentials (VCs)&lt;/strong&gt; and &lt;strong&gt;Decentralized Identifiers (DIDs)&lt;/strong&gt; rely on these cryptographic techniques to ensure that identity verification is both efficient and secure.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;challenges-and-future-directions&#34;&gt;Challenges and Future Directions&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;While succinct proofs offer significant advantages, they are not without challenges:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trusted Setup&lt;/strong&gt;: zk-SNARKs require a trusted setup, which, if compromised, could undermine the security of the entire system. Research into transparent setups (as in zk-STARKs) aims to address this limitation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Computation Overhead&lt;/strong&gt;: Although succinct proofs reduce verification complexity, the prover&amp;rsquo;s computational requirements can be high. Optimizing the proving process is an active area of research.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interoperability&lt;/strong&gt;: For widespread adoption, succinct proof systems must integrate seamlessly with existing technologies. This involves developing standards and protocols that ensure compatibility across platforms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quantum Resistance&lt;/strong&gt;: As quantum computing advances, many cryptographic systems, including those used in succinct proofs, face potential vulnerabilities. Developing quantum-resistant proof systems is a critical area of ongoing research.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;insights-from-prof-yael-tauman-kalais-research&#34;&gt;Insights from Prof. Yael Tauman Kalaiâ€™s Research&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Prof. Kalaiâ€™s work pushes the boundaries of succinct proofs by exploring new cryptographic primitives and optimizing existing protocols. Her research emphasizes collaboration between academia and industry to address real-world challenges. Key areas of her focus include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Enhancing Transparency&lt;/strong&gt;: Developing protocols that eliminate the need for trusted setups while maintaining efficiency and scalability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improving Scalability&lt;/strong&gt;: Optimizing proof generation to reduce computational overhead for the prover.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expanding Applications&lt;/strong&gt;: Applying succinct proofs to emerging fields like decentralized finance (DeFi), secure voting systems, and federated learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Succinct proofs represent a paradigm shift in cryptography, enabling efficient, scalable, and privacy-preserving verification across various domains. From blockchain scalability to AI model verification, their potential applications are vast and transformative. However, realizing their full potential requires addressing challenges like computational overhead, interoperability, and quantum resistance.&lt;/p&gt;
&lt;p&gt;As cryptographic research evolves, the collaboration between researchers, industry, and policymakers will be essential to unlock the full potential of succinct proofs. Prof. Kalaiâ€™s groundbreaking work serves as a testament to the importance of pushing the boundaries of what cryptography can achieve.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;engaging-in-dialogues-on-the-ethics-of-ai&#34;&gt;Engaging in Dialogues on the Ethics of AI&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The summit wasnâ€™t just about science and innovationâ€”it also provided a platform to engage in crucial discussions about the ethical implications of technological advancements. One such discussion was the Panel Huddle titled &lt;em&gt;â€œEthics of Scientific Research in the Age of AIâ€&lt;/em&gt;, featuring prominent professors like Adi Shamir, Shafi Goldwasser, and Kalai.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/eZhOUtUIIQ8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The panel explored topics such as algorithmic bias, dual-use AI technologies, privacy-preserving AI, and the responsibilities of researchers in educating the public and policymakers. These discussions offered profound insights into the complexities of conducting ethical research in a rapidly evolving technological landscape, reminding me of the broader purpose of scientific innovation: to create a more equitable future.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0305.jpg&#34; alt=&#34;Ethics of Scientific Research talk&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 11: Attending a talk on â€œEthics of Scientific Research in the Age of AIâ€.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;ethics-of-scientific-research-in-the-age-of-ai&#34;&gt;Ethics of Scientific Research in the Age of AI&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Artificial intelligence (AI) is revolutionizing scientific research, offering transformative tools to accelerate discovery, enhance accuracy, and expand the boundaries of human knowledge. However, these advancements also bring profound ethical challenges. At the recent panel moderated by &lt;strong&gt;Prof. Simon Chesterman&lt;/strong&gt; from the National University of Singapore, renowned experts &lt;strong&gt;Prof. Joan Rose&lt;/strong&gt; (2016 Stockholm Water Prize), &lt;strong&gt;Prof. Yael Kalai&lt;/strong&gt; (2022 ACM Prize in Computing), and &lt;strong&gt;Prof. Adi Shamir&lt;/strong&gt; (2002 Turing Award) delved into the critical ethical considerations of AI in scientific research.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;client-concerns-about-ai&#34;&gt;Client Concerns About AI&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;AI systems increasingly shape scientific discovery, yet their application necessitates robust ethical guidelines. Governance frameworks must:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Define clear boundaries for AI use.&lt;/li&gt;
&lt;li&gt;Address biases inherent in data and algorithms.&lt;/li&gt;
&lt;li&gt;Ensure transparency and accountability in AI-driven research outcomes.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;human-interaction&#34;&gt;Human Interaction&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;The integration of AI into research workflows often alters the dynamic between human researchers and technology. Key considerations include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preventing over-reliance on AI outputs, which could lead to critical oversights.&lt;/li&gt;
&lt;li&gt;Mitigating risks of misinterpretation of AI-generated results.&lt;/li&gt;
&lt;li&gt;Balancing human intuition with machine precision.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;opportunities-and-risks&#34;&gt;Opportunities and Risks&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;AI holds immense potential to transform scientific methodologies:&lt;/p&gt;
&lt;p&gt;$$ \text{Efficiency Gain} = \frac{T_{\text{manual}}}{T_{\text{AI-assisted}}} $$&lt;/p&gt;
&lt;p&gt;Where $T_{\text{manual}}$ represents the time for traditional methods and $T_{\text{AI-assisted}}$ denotes AI-accelerated approaches. While this formula underscores AIâ€™s ability to enhance efficiency, ethical concerns include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Misinformation propagation due to AI biases.&lt;/li&gt;
&lt;li&gt;Job displacement for researchers whose roles are increasingly automated.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;military-applications&#34;&gt;Military Applications&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;AIâ€™s role in military research raises acute ethical dilemmas, particularly in autonomous weapons systems. Questions of accountability and decision-making in lethal scenarios must be urgently addressed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who is accountable for errors in autonomous systems?&lt;/li&gt;
&lt;li&gt;How can ethical principles such as proportionality and necessity be encoded into AI?&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;public-engagement&#34;&gt;Public Engagement&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;A significant gap exists between the scientific community and the publicâ€™s understanding of AI. To bridge this divide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop accessible communication strategies to explain AI technologies and their implications.&lt;/li&gt;
&lt;li&gt;Foster public trust through transparent disclosures of AIâ€™s capabilities and limitations.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;proactive-education&#34;&gt;Proactive Education&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;Educating researchers and policymakers is vital to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Promote ethical awareness.&lt;/li&gt;
&lt;li&gt;Equip them to evaluate the societal impacts of their work.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;accountability-and-regulation&#34;&gt;Accountability and Regulation&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Regulating AI is a complex yet essential endeavor. Key areas of focus include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Warfare Applications:&lt;/strong&gt; Establishing international norms to prohibit unethical AI use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Public Safety:&lt;/strong&gt; Creating standards for AI deployment in sensitive domains such as healthcare and transportation.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h4 id=&#34;global-cooperation&#34;&gt;Global Cooperation&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;International collaboration is critical to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop shared standards for AI ethics.&lt;/li&gt;
&lt;li&gt;Address cross-border challenges, such as data privacy and AI governance.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;future-directions&#34;&gt;Future Directions&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Investing in research on AI risks enables policymakers to adopt informed, proactive measures rather than reactive regulations post-crisis. Proposed funding distribution can be modeled as:&lt;/p&gt;
&lt;p&gt;$$ \text{Allocation} = \frac{R_{\text{risk}}}{R_{\text{total}}} \times 100 % $$&lt;/p&gt;
&lt;p&gt;Where $R_{\text{risk}}$ represents research focused on AI risks, and $R_{\text{total}}$ is the total research budget.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;interdisciplinary-approach&#34;&gt;Interdisciplinary Approach&lt;/h4&gt;
&lt;hr&gt;
&lt;p&gt;Addressing AIâ€™s ethical challenges requires collaboration among:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Technologists:&lt;/strong&gt; To refine AI systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ethicists:&lt;/strong&gt; To integrate moral principles into AI development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Policymakers:&lt;/strong&gt; To enact effective regulations and governance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The panel underscored that while AI offers unparalleled opportunities for scientific progress, its ethical integration requires foresight, collaboration, and accountability. By fostering interdisciplinary dialogue and committing to transparent, responsible practices, the scientific community can ensure AI serves as a force for good.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;visual-representation&#34;&gt;Visual Representation&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;A mind map summarizing these insights can help readers visualize the ethical considerations of AI in scientific research.&lt;/p&gt;
&lt;h2&gt;Visual Representation&lt;/h2&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/LECTURE/GRPAH_01.png&#34; alt=&#34;Mind map on AI ethics in scientific research&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 12: A mind map summarizing key ethical considerations in the use of AI for scientific research, discussed during GYSS2025.
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;expanding-my-network-and-building-new-collaborations&#34;&gt;Expanding My Network and Building New Collaborations&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Beyond the lectures and discussions, GYSS 2025 was also a chance to meet fellow researchers from all over the globe. I had the pleasure of connecting with inspiring individuals from diverse fields of study, creating friendships that will last a lifetime. The exchange of ideas with these brilliant young scientists has already sparked new collaborations and research ideas that I canâ€™t wait to explore further.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a-day-ill-never-forget&#34;&gt;A Day Iâ€™ll Never Forget&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;All in all, my experience at GYSS 2025 was beyond what I could have imagined. It was a perfect blend of research, networking, and fun. It gave me the chance to engage with some of the brightest minds in science, learn from Nobel Laureates and Turing Award winners, and discuss pressing issues in AI and ethics. It was an unforgettable experience, and Iâ€™m leaving with new ideas, fresh perspectives, and the motivation to continue pushing the boundaries of my own research.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/5C-6bXuVq9Q&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0111.jpg&#34; alt=&#34;Lecture atmosphere at GYSS2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 13: Immersed in the vibrant academic atmosphere of GYSS2025.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/KAO_GYSS2025_02.jpg&#34; alt=&#34;Photo with GYSS2025 banner&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 14: Standing proudly with the GYSS2025 bannerâ€”first time wearing a formal suit in years!&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;GYSS2025_withHRH/LECTURE/IMG_0238.jpg&#34; alt=&#34;NUS campus shot&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 15: Beautiful campus vibes at NUS, Singaporeâ€”a serene and inspiring place for learning.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;PS. This year, GYSS 2025 was hosted at the beautiful campus of the National University of Singapore (NUS), and I absolutely fell in love with the environment. The vibrant atmosphere, lush greenery, and modern architecture made it the perfect blend of nature and innovationâ€”a truly ideal space for creativity and learning.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Jan 2025). &lt;em&gt;Global Young Scientists Summit (GYSS) 2025: Where Science Meets Inspiration&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2025gyss,
  title   = &amp;quot;Global Young Scientists Summit (GYSS) 2025: Where Science Meets Inspiration&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Jan&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Young Scientists Quickfire Pitch</title>
      <link>https://kaopanboonyuen.github.io/talk/young-scientists-quickfire-pitch/</link>
      <pubDate>Thu, 24 Oct 2024 18:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/young-scientists-quickfire-pitch/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/tgcKR97Ea8I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Sub1 Urban Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/</link>
      <pubDate>Wed, 25 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/</guid>
      <description>&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#chapter-1-osaka-running&#34;&gt;Chapter 1: Osaka Running&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#chapter-2-kyoto-running&#34;&gt;Chapter 2: Kyoto Running&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#chapter-3-tokyo-running&#34;&gt;Chapter 3: Tokyo Running&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#chapter-4-tokyo-farewell-run&#34;&gt;Chapter 4: Tokyo Farewell Run&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#summary-of-my-running-experience&#34;&gt;Summary of My Running Experience&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;Hey guys! I just completed an incredible running experience across Japan&amp;rsquo;s vibrant citiesâ€”Osaka, Kyoto, and Tokyo. I managed to hit sub-1-hour times while soaking in the unique vibes of each place. It was the perfect mix of pushing my pace and enjoying the sights along the way!&lt;/p&gt;
&lt;p&gt;It was such a cool blend of testing my limits and discovering new places. Each run through those iconic spots felt like I was getting a real taste of Japanâ€™s unique vibe and energy with every stride.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Just finished an awesome running tour through Osaka, Kyoto, and Tokyo! ğŸƒâ€â™‚ï¸ Nailed sub-1-hour times while soaking in all the city vibes. &lt;br&gt;&lt;br&gt;Can&amp;#39;t believe how much fun it was to race and explore at the same time! ğŸ‡¯ğŸ‡µ &lt;a href=&#34;https://twitter.com/hashtag/RunningExperience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#RunningExperience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Japan?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Japan&lt;/a&gt;&lt;a href=&#34;https://t.co/Wy2bpgO8lg&#34;&gt;https://t.co/Wy2bpgO8lg&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1838942597387362675?ref_src=twsrc%5Etfw&#34;&gt;September 25, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h3 id=&#34;chapter-1-osaka-running&#34;&gt;Chapter 1: Osaka Running&lt;/h3&gt;
&lt;p&gt;This trip marks my third time in Japan, but this time I started in the Kansai region, kicking things off in Osaka. My main goal? To conquer a city run and aim for a sub-1-hour 10K.&lt;/p&gt;
&lt;p&gt;Bright and early, after a good nightâ€™s rest, I hit the streets at 4:54 AM. The weather was just right, making the run super enjoyable. I ended up covering 10.59K at a pace of 4:42, finishing in 49 minutes. Hereâ€™s the route I took:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start at Awaza&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Awaza&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matsushima Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Matsushima&amp;#43;Park&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Chiyoza&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Chiyoza&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Minamiizuo Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Minamiizuo&amp;#43;Park&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kitatsumorinaka Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kitatsumorinaka&amp;#43;Park&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deshirohigashi Intersection&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Deshirohigashi&amp;#43;Intersection&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machida Gastroenterology Hospital&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Machida&amp;#43;Gastroenterology&amp;#43;Hospital&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tsutenkaku Tower&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Tsutenkaku&amp;#43;Tower&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finish at Matsuyamachi&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Matsuyamachi&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nipponbashi&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Nipponbashi&amp;#43;Osaka&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_OSAKA_001.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_OSAKA_002.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_OSAKA_003.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_OSAKA_004.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Osaka is truly a city that hooks you; itâ€™s filled with hidden gems waiting to be explored. After three action-packed days, it was time to move on to Kyoto!&lt;/p&gt;
&lt;h3 id=&#34;chapter-2-kyoto-running&#34;&gt;Chapter 2: Kyoto Running&lt;/h3&gt;
&lt;p&gt;Next up was the challenge of a sub-1-hour city run in Kyoto. I only had two days here, and luck wasnâ€™t on my sideâ€”rain greeted me in the morning. But nothing was going to stop my determination!&lt;/p&gt;
&lt;p&gt;I crushed the city run in Kyoto, finishing 10.78K in 49 minutes (pace 4:38), which was even faster than my Osaka run! I think the rain actually helped; the air felt fresh and invigorating. Hereâ€™s my route:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start at Kiyomizu-Jojo Station&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kiyomizu&amp;#43;Jojo&amp;#43;Station&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kyoto Pref. Yakuzaishikai&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kyoto&amp;#43;Pref&amp;#43;Yakuzaishikai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Matsubaracho&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Matsubaracho&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kawebata Police Station&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kawebata&amp;#43;Police&amp;#43;Station&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kyoto University (Yoshida Campus)&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kyoto&amp;#43;University&amp;#43;Yoshida&amp;#43;Campus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Imadegawa Street&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Imadegawa&amp;#43;Street&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kyoto Imperial Palace (Kyoto Gyoen)&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kyoto&amp;#43;Imperial&amp;#43;Palace&amp;#43;Kyoto&amp;#43;Gyoen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Naka-Dachiuri-Dori&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Naka&amp;#43;Dachiuri&amp;#43;Dori&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Horikawa Sanjo&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Horikawa&amp;#43;Sanjo&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nishiki Market&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Nishiki&amp;#43;Market&amp;#43;Kyoto&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_KYOTO_001.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_002.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_003.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_003_02.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_003_03.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_003_04.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_KYOTO_004.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Wrapping up my run in Kyoto felt like a major winâ€”2 out of 3 goals achieved! Before heading to my final destination, I hopped on the Shinkansen back to Tokyo.&lt;/p&gt;
&lt;p&gt;I took my first-ever Shinkansen ride from Kyoto to Tokyo, leaving at 9:39 AM and arriving at 11:54 AM. It was an absolute treat! I opted for the Green Car for that extra comfort and snagged a window seat to soak in the views. Hereâ€™s a glimpse of that journey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Shinkansen from Kyoto to Tokyo&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Shinkansen&amp;#43;from&amp;#43;Kyoto&amp;#43;to&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸš„&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/Kao_Shinkansen_01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/Kao_Shinkansen_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/w3lpcZRAEK4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;chapter-3-tokyo-running&#34;&gt;Chapter 3: Tokyo Running&lt;/h3&gt;
&lt;p&gt;Finally, Iâ€™ve made it to Tokyo! Thereâ€™s always a surprise waiting here. Before my city run, a professor from Todai treated me to a swim at the Olympic pool used during the Tokyo 2020 Games. What an experience! I swam 1.2K in just 28 minutes at a pace of 2:25 per 100m. The Olympic-standard pool was incredible, and swimming alongside serious athletes was an adrenaline rush.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/Kao_Japan_Swim_2024.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_002.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_001.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_004.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_007.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_SWIM_008.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, back to the final running mission. On my second day in Tokyo, I kicked off my run at 6:52 AM. The weather was perfectâ€”18Â°C with a refreshing breezeâ€”ideal for running.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_TOKYO_006.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_VO2MAX_51.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I wrapped up the city run covering 10.17K in 44 minutes at a pace of 4:25, marking a personal best! Hereâ€™s my route in Tokyo:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start at Shiomi Dori Street&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Shiomi&amp;#43;Dori&amp;#43;Street&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokyo Aquatics Centre&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Tokyo&amp;#43;Aquatics&amp;#43;Centre&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yumenoshima Stadium&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Yumenoshima&amp;#43;Stadium&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Arakawawangan-kyo Bridge&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Arakawawangan&amp;#43;kyo&amp;#43;Bridge&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nakasakombashi&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Nakasakombashi&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seishin Itchu North&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Seishin&amp;#43;Itchu&amp;#43;North&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kiyosuna-Ohashi Bridge&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kiyosuna&amp;#43;Ohashi&amp;#43;Bridge&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ojima Elementary School&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Ojima&amp;#43;Elementary&amp;#43;School&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finish at Ojima Komatsugawa Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Ojima&amp;#43;Komatsugawa&amp;#43;Park&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ—ºï¸&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kiba Park&lt;/strong&gt; &lt;a href=&#34;https://www.google.com/search?q=Kiba&amp;#43;Park&amp;#43;Tokyo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Explore the route here ğŸ“&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_TOKYO_001.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_002.JPG&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_003_01.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_003_02.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_004.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;chapter-4-tokyo-farewell-run&#34;&gt;Chapter 4: Tokyo Farewell Run&lt;/h3&gt;
&lt;p&gt;Before wrapping up my Tokyo adventure tomorrow, I took advantage of this morningâ€™s cool weather, with a light drizzle setting the perfect scene for my final city runâ€”a 5K to bid this trip a fond farewell. I covered 5.16 km in 23:54 minutes (Pace 4:38). The highlight? My VO2 Max hit a new high at 51, and Garmin labeled my training status as â€œPeaking.â€ Itâ€™s the ultimate runnerâ€™s high!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_PEAKING.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_VO2MAX_51.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I started my run at Nihombashihakozakicho, crossed the Eitai Bashi bridge, and continued parallel to the scenic Sumida River. Passing through the vibrant Kiyosumi-Dori Avenue at kilometer 2, I found myself immersed in Tokyoâ€™s unique urban rhythm. I reached the serene Kiyosumi Garden by kilometer 3, before heading to Gokenborui Park at kilometer 4. The run climaxed with a sprint up Shin-Ohashi bridge, finishing at the 5K mark on the dot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_001.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_002.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_003.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_004.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;city_run_images/KAO_TOKYO_5k_006.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This city run was the perfect way to close out my time in Tokyo. From the calm river views to the bustling cityscape, every step was a reminder of how much this place has to offer. Until next time, Tokyo!&lt;/p&gt;
&lt;h3 id=&#34;summary-of-my-running-experience&#34;&gt;Summary of My Running Experience&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Osaka Running:&lt;/strong&gt; Sub1 10.59K City Run, 49:48min, Pace 4:42&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kyoto Running:&lt;/strong&gt; Sub1 10.78K City Run, 49:57min, Pace 4:38&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokyo Running:&lt;/strong&gt; Sub1 10.17K City Run, 44:58min, Pace 4:25&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokyo Swimming:&lt;/strong&gt; 1,200m (1.2K), 28:58min, Pace 2:25&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tokyo Running:&lt;/strong&gt; Sub24min 5.16K City Run, 23:54min, Pace 4:38&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This trip to Japan was absolutely fantastic! Even though it was late summer and winter is just around the corner, running through these three major cities expanded my horizons. I encountered amazing insights into Japanese culture, met locals, and explored unseen places.&lt;/p&gt;
&lt;p&gt;I canâ€™t wait for the next experience, hopefully tackling the Tokyo Marathon (42.195K) someday! Fingers crossed!&lt;/p&gt;
&lt;p&gt;Until next time, happy running! ğŸƒâ€â™‚ï¸ğŸ‡¯ğŸ‡µ&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Sub1 Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024running,
  title   = &amp;quot;Sub1 Urban Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023</title>
      <link>https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/</link>
      <pubDate>Thu, 12 Sep 2024 18:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can read our full paper on Springer here: &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MARS at ICIAP 2023&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;presenting-mars-mask-attention-refinement-with-sequential-quadtree-nodes-at-iciap-2023&#34;&gt;Presenting MARS: Mask Attention Refinement with Sequential Quadtree Nodes at ICIAP 2023&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In &lt;strong&gt;September 2023&lt;/strong&gt;, I had the privilege of presenting my latest AI research, &lt;strong&gt;MARS&lt;/strong&gt; (Mask Attention Refinement with Sequential Quadtree Nodes), at the &lt;strong&gt;International Conference on Image Analysis and Processing (ICIAP 2023)&lt;/strong&gt;, held in &lt;strong&gt;Udine, Italy&lt;/strong&gt;, from &lt;strong&gt;September 11â€“15&lt;/strong&gt;. MARS is a specialized deep learning model developed for &lt;strong&gt;car damage instance segmentation&lt;/strong&gt;, a vital application in the automotive insurance and garage service sectors.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;ğŸ„ We&amp;#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.&lt;br&gt;.&lt;br&gt;Paper: &lt;a href=&#34;https://t.co/UayUSxmZep&#34;&gt;https://t.co/UayUSxmZep&lt;/a&gt;&lt;br&gt;Code: &lt;a href=&#34;https://t.co/RoNFjSslXr&#34;&gt;https://t.co/RoNFjSslXr&lt;/a&gt;&lt;br&gt;Project: &lt;a href=&#34;https://t.co/uSoBX21HpF&#34;&gt;https://t.co/uSoBX21HpF&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ComputerVision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ComputerVision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Research?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Research&lt;/a&gt; &lt;a href=&#34;https://t.co/oc8gz7Hs9I&#34;&gt;pic.twitter.com/oc8gz7Hs9I&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1822723598764876000?ref_src=twsrc%5Etfw&#34;&gt;August 11, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Our model leverages &lt;strong&gt;Self-Attention Mechanisms&lt;/strong&gt; combined with &lt;strong&gt;Sequential Quadtree Nodes&lt;/strong&gt;, which together enable MARS to achieve significantly higher accuracy than state-of-the-art baselines such as Mask R-CNN (ICCV 2017), PointRend (CVPR 2020), and Mask Transfiner (CVPR 2022). These improvements were validated across multiple benchmark datasets, including a large Thai car-damage dataset.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MARS was accepted at ICIAP 2023 with a competitive &lt;strong&gt;acceptance rate of 0.64&lt;/strong&gt;, and our work is published in the Springer Lecture Notes in Computer Science series. This international recognition affirms the impact and novelty of our approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_01.jpg&#34; alt=&#34;Presenting MARS at ICIAP 2023 Conference, Udine, Italy&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 1: Presenting &lt;strong&gt;MARS: Mask Attention Refinement with Sequential Quadtree Nodes&lt;/strong&gt; at ICIAP 2023 in Udine, Italy. MARS introduces an innovative self-attention mechanism combined with a quadtree transformer to achieve highly accurate car damage instance segmentation, significantly outperforming current state-of-the-art methods on multiple benchmark datasets. This advancement addresses critical challenges in the car insurance industry. (Source: &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;ICIAP 2023 - LNCS 14365&lt;/a&gt;)
  &lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_03.jpg&#34; alt=&#34;Our MARS poster presentation at ICIAP 2023&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 2: Showcasing the MARS project through an engaging poster presentation at ICIAP 2023, highlighting the innovative mask attention refinement techniques and sequential quadtree nodes that drive our state-of-the-art approach to car damage instance segmentation. This presentation sparked insightful discussions with experts and attendees, further emphasizing the potential real-world impact of our research in advancing intelligent vision systems.
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;why-mars-matters-from-thailand-to-italy-building-the-future-of-automotive-ai&#34;&gt;Why MARS Matters: From Thailand to Italy, Building the Future of Automotive AI&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The journey of &lt;strong&gt;MARS&lt;/strong&gt; began not in a conference hall in Europe but in the bustling cities and garages of &lt;strong&gt;Thailand&lt;/strong&gt;, where the real-world challenges of the automotive industry sparked a vision. Every day, countless car owners and insurance agents face delays, inconsistencies, and even disputes when assessing vehicle damage. Manual inspections often rely on human judgment â€” which can be slow, subjective, or prone to error. In garages, mechanics struggle with non-standardized repair cost estimates. Insurers grapple with fraudulent claims, inflated repairs, and time-consuming paperwork. These challenges arenâ€™t unique to Thailand â€” theyâ€™re global. But we decided to start solving them at home.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At &lt;strong&gt;MARSAIL&lt;/strong&gt; â€” the &lt;em&gt;Motor AI Recognition Solution Artificial Intelligence Laboratory&lt;/em&gt; â€” we asked ourselves: &lt;em&gt;Can we create a deep learning system that sees, understands, and evaluates vehicle damage better than a human?&lt;/em&gt; Could we develop an AI model smart enough to assist insurers, fast enough for real-time applications, and precise enough to help garage operators deliver fair, standardized repairs?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We believe the answer is yes â€” and our vision for this transformation is detailed in our MARSAIL blog:
ğŸ‘‰ &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;MARSAIL: The Smart Engine Behind the Future of Car Insurance and Intelligent Garages&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The answer was &lt;strong&gt;MARS&lt;/strong&gt; â€” &lt;em&gt;Mask Attention Refinement with Sequential Quadtree Nodes&lt;/em&gt; â€” our flagship AI architecture built from the ground up for &lt;strong&gt;car damage instance segmentation&lt;/strong&gt;. Unlike traditional models like Mask R-CNN or PointRend, which were designed for general-purpose segmentation tasks, MARS is laser-focused. It leverages &lt;strong&gt;quadtree spatial hierarchies&lt;/strong&gt;, &lt;strong&gt;self-attention refinement&lt;/strong&gt;, and a custom transformer-based backbone to identify scratches, dents, cracks, and broken parts â€” even in complex lighting, occlusion, or varied car surface conditions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What sets MARS apart is its &lt;em&gt;precision&lt;/em&gt;. It doesnâ€™t just draw rough outlines â€” it &lt;em&gt;understands&lt;/em&gt; the contours of the damage. Whether itâ€™s a shallow scratch on a bumper or a crumpled fender after a collision, MARS detects it with clarity, outperforming state-of-the-art models with a significant margin in our Thai car damage benchmarks. And this isnâ€™t just academic â€” this is AI &lt;em&gt;for the real world&lt;/em&gt;, trained with real damage cases, tuned for high-stakes applications like insurance claims and repair verification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So when &lt;strong&gt;ICIAP 2023&lt;/strong&gt; announced their international conference in &lt;strong&gt;Udine, Italy&lt;/strong&gt;, we knew it was time to take our Thai-born innovation global. From Bangkok to Udine, we carried not just a poster and a paper â€” but a vision. A vision that &lt;strong&gt;AI can transform the automotive industry&lt;/strong&gt;, not by replacing people, but by empowering them: giving insurers confidence, garages clarity, and drivers trust in the system.&lt;/p&gt;
&lt;p&gt;Presenting MARS in Italy wasnâ€™t just a research milestone â€” it was a symbol of whatâ€™s possible when bold ideas are met with rigorous engineering and a passion for solving real-world problems. Our work was selected for publication in the &lt;strong&gt;Lecture Notes in Computer Science&lt;/strong&gt; by &lt;strong&gt;Springer&lt;/strong&gt;, a recognition that underscores the technical excellence and practical value of our research.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    At &lt;strong&gt;MARSAIL&lt;/strong&gt;, our mission continues. We are building an end-to-end ecosystem of automotive AI: from car damage detection to &lt;strong&gt;automated cost estimation&lt;/strong&gt;, from &lt;strong&gt;OCR document parsing&lt;/strong&gt; to &lt;strong&gt;insurance fraud detection&lt;/strong&gt;, all powered by AI models trained with diverse Thai datasets and built to serve global standards.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Because in the future we see, &lt;strong&gt;a single smartphone photo&lt;/strong&gt; is enough to initiate a car claim, verify vehicle condition, and provide a fair quote â€” all in seconds. This is the &lt;strong&gt;Digital Insurance Twin&lt;/strong&gt; weâ€™re crafting. And we believe it starts with intelligence, integrity, and innovation â€” the core values of MARS and everything we do at MARSAIL.&lt;/p&gt;
&lt;hr&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_04.jpg&#34; alt=&#34;MARS deep learning architecture diagram&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 3: Detailed illustration of the MARS deep learning architecture, emphasizing the novel integration of sequential quadtree nodes with mask attention refinement mechanisms. This design enables precise, efficient instance segmentation of car damage by capturing both global context and fine-grained details, setting a new benchmark in intelligent vision models.
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mars-a-quadtree-driven-deep-learning-architecture-for-precision-car-damage-segmentation&#34;&gt;MARS: A Quadtree-Driven Deep Learning Architecture for Precision Car Damage Segmentation&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Central to the design of &lt;strong&gt;MARS&lt;/strong&gt; (Mask Attention Refinement with Sequential Quadtree Nodes) lies a novel AI architecture designed to push the boundaries of &lt;strong&gt;instance segmentation&lt;/strong&gt;â€”specifically, for car damage detection. Traditional models like Mask R-CNN rely heavily on coarse ROI alignments and fixed spatial grids, which often result in poor mask quality for irregular or fine-grained structures like scratches, dents, or cracks. MARS overcomes this limitation through a unique &lt;strong&gt;quadtree-based spatial hierarchy&lt;/strong&gt;, allowing the network to adaptively decompose images into finer patches only where needed, focusing computational attention on high-detail regions.&lt;/p&gt;
&lt;p&gt;Technically, our method integrates a &lt;strong&gt;Self-Attention Refinement Module (SARM)&lt;/strong&gt; with &lt;strong&gt;Sequential Quadtree Node Encoding&lt;/strong&gt; to build hierarchical context from coarse-to-fine levels. Instead of treating spatial locations uniformly, we recursively subdivide image regions using quadtree decomposition, capturing fine structural features with higher resolution where necessary. Let $Q = {q_1, q_2, \dots, q_n}$ denote the set of quadtree nodes generated per instance mask. Each node $q_i$ is embedded and passed through a transformer encoder, which models long-range dependencies via multi-head self-attention:&lt;/p&gt;
&lt;p&gt;$$
\text{Attention}(Q) = \text{softmax}\left(\frac{QW_Q (QW_K)^T}{\sqrt{d_k}}\right)QW_V
$$&lt;/p&gt;
&lt;p&gt;These representations are aggregated to recalibrate channel-wise features dynamically, enabling MARS to predict high-fidelity instance masks with pixel-level accuracy.&lt;/p&gt;
&lt;p&gt;In practice, &lt;strong&gt;MARS outperforms Mask R-CNN, PointRend, and Mask Transfiner by +1.3 to +2.3 maskAP&lt;/strong&gt; on a Thai car damage datasetâ€”demonstrating that our approach is not only theoretically elegant but also empirically superior. Our model operates on top of the &lt;strong&gt;FPN backbone (ResNet-50 and ResNet-101 variants)&lt;/strong&gt;, and benefits from a cascaded refinement pipeline that improves mask boundaries at every stage. The full details, ablation studies, and demo results are available in our &lt;a href=&#34;https://github.com/kaopanboonyuen/MARS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;international-recognition-and-impact&#34;&gt;International Recognition and Impact&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The ICIAP conference was a vibrant gathering of leading researchers and industry experts. I also had the honor to deliver a guest talk on &lt;strong&gt;modern AI advances in Large Language Models (LLMs)&lt;/strong&gt;, engaging with an international audience about cutting-edge AI trends beyond computer vision.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_02.jpg&#34; alt=&#34;Lecture session at ICIAP 2023&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 4: Immersed in a dynamic exchange of groundbreaking ideas at ICIAP 2023, where Professor Tomas Pajdla unraveled the complexities of Algebraic Vision, Andrew Fitzgibbon pushed the boundaries of AI hardware and real-world AI applications, and Danijel SkoÄaj showcased pioneering advances in data-driven surface anomaly detectionâ€”each lecture illuminating the future of intelligent vision systems.
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The atmosphere at &lt;strong&gt;ICIAP 2023&lt;/strong&gt; in Udine, Italy, was truly invigoratingâ€”filled with intellectual energy, meaningful discussions, and a deep sense of community among researchers pushing the boundaries of computer vision and AI.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_05.jpg&#34; alt=&#34;Conference atmosphere at ICIAP 2023, Udine, Italy&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 5: Capturing the vibrant and electric atmosphere at ICIAP 2023 in Udine, where leading minds from around the globe converged to share cutting-edge research, spark innovative collaborations, and shape the future of intelligent systems and computer vision.
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;exploring-italy-from-udine-to-rome&#34;&gt;Exploring Italy: From Udine to Rome&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;After the conference wrapped up, I took the chance to travel to Rome and experience Italyâ€™s timeless heritage. Walking through the magnificent &lt;strong&gt;St. Peterâ€™s Basilica&lt;/strong&gt; and the &lt;strong&gt;Vatican Museums&lt;/strong&gt; was a breathtaking journey into art and spirituality.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Colosseum&lt;/strong&gt;, standing proudly as an icon of ancient Roman engineering and history, was truly awe-inspiring. And of course, tossing a coin into the &lt;strong&gt;Trevi Fountain&lt;/strong&gt; was a magical moment steeped in legend â€” a wish for continued success in research and life.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For the full story of my unforgettable journey â€” from academic presentation to wandering the timeless streets of Rome â€” feel free to read my detailed travel and research blog here: &lt;a href=&#34;https://kaopanboonyuen.wordpress.com/2023/09/27/showcasing-my-ai-research-in-italy-a-memorable-september-work-trip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Showcasing My AI Research in Italy: A Memorable September Work Trip&lt;/strong&gt;&lt;/a&gt;. In that post, I share not only the highlights of presenting our MARS model at ICIAP 2023, but also the personal moments that made the trip truly special â€” from the intellectual exchanges at the conference to standing in awe before the Colosseum and Vatican. It was a journey that beautifully merged science, culture, and inspiration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Presenting &lt;strong&gt;MARS&lt;/strong&gt; at ICIAP 2023 was not just a research highlightâ€”it embodied the core mission of our lab, &lt;strong&gt;MARSAIL&lt;/strong&gt; (&lt;em&gt;Motor AI Recognition Solution Artificial Intelligence Laboratory&lt;/em&gt;). MARSAIL is a leading research hub in Thailand, dedicated to developing intelligent technologies for &lt;strong&gt;automotive insurance, damage assessment, and garage automation&lt;/strong&gt;. Guided by &lt;strong&gt;Dr. Teerapong Panboonyuen&lt;/strong&gt;, the lab combines academic depth with industry-driven goals, fostering impactful innovations like MARS that bridge research and real-world application.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://github.com/kaopanboonyuen/kaopanboonyuen.github.io/raw/main/files/MARS/MARSAIL.png&#34; alt=&#34;MARSAIL Logo&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 6: MARSAIL â€” the Motor AI Recognition Solution Artificial Intelligence Laboratory, a cutting-edge research lab in Thailand pioneering smart technologies for car insurance and garage systems.
  &lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-dive-deeper-into-marsail&#34;&gt;ğŸš€ Dive Deeper into MARSAIL&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Curious how &lt;strong&gt;MARS&lt;/strong&gt; redefines car damage segmentation with intelligent attention and quadtree refinement?
Explore our full paper on &lt;a href=&#34;https://arxiv.org/abs/2305.04743&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;, browse code and benchmarks on &lt;a href=&#34;https://github.com/kaopanboonyuen/MARS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;, or experience the project in action on the &lt;a href=&#34;https://kaopanboonyuen.github.io/MARS/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official MARS project page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unlock the tech thatâ€™s shaping the future of automotive AI.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Attending and presenting at &lt;strong&gt;ICIAP 2023&lt;/strong&gt; in &lt;strong&gt;Udine&lt;/strong&gt; was an incredible opportunity to engage with a &lt;strong&gt;vibrant community&lt;/strong&gt; of leading researchers and innovators in &lt;strong&gt;image analysis and processing&lt;/strong&gt;. The event offered a &lt;strong&gt;dynamic platform&lt;/strong&gt; to showcase our pioneering work on &lt;strong&gt;MARS&lt;/strong&gt; â€” a breakthrough in &lt;strong&gt;car damage instance segmentation&lt;/strong&gt; that combines advanced &lt;strong&gt;self-attention&lt;/strong&gt; and &lt;strong&gt;quadtree architectures&lt;/strong&gt; to push the boundaries of what &lt;strong&gt;intelligent vision systems&lt;/strong&gt; can achieve. Beyond presenting our research, the chance to exchange ideas with experts like &lt;strong&gt;Professor Tomas Pajdla&lt;/strong&gt;, &lt;strong&gt;Andrew Fitzgibbon&lt;/strong&gt;, and &lt;strong&gt;Danijel SkoÄaj&lt;/strong&gt; enriched our perspective and fueled new inspiration. &lt;strong&gt;ICIAP 2023&lt;/strong&gt; not only highlighted &lt;strong&gt;cutting-edge advancements&lt;/strong&gt; but also fostered &lt;strong&gt;collaborations&lt;/strong&gt; that will undoubtedly shape the future of &lt;strong&gt;AI-driven image understanding&lt;/strong&gt;. This experience has strengthened our commitment to advancing &lt;strong&gt;impactful, real-world AI solutions&lt;/strong&gt; and deepened our excitement for the rapidly evolving field of &lt;strong&gt;computer vision&lt;/strong&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;Thank you for joining me on this incredible journey where &lt;strong&gt;cutting-edge AI research meets the timeless charm of Italy&lt;/strong&gt;. Presenting MARS at ICIAP 2023 was more than a milestoneâ€”it was a moment of inspiration, surrounded by global minds and unforgettable landscapes. Here&amp;rsquo;s to pushing boundaries, one breakthrough at a time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;â€“  Kao Panboonyuen&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2023). &lt;em&gt;Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{panboonyuen2023mars,
  title   = &amp;quot;Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io&amp;quot;,
  year    = &amp;quot;2023&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling</title>
      <link>https://kaopanboonyuen.github.io/publication/sea-vit-sea-surface-currents-forecasting/</link>
      <pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/sea-vit-sea-surface-currents-forecasting/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Fine-Tune and Deploy a Satellite-Specific LLMs Model for Satellite Images</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/</link>
      <pubDate>Sun, 08 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk ğŸŒ &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240910_Panboonyuen_How_to_Fine_Tune_Satellite_Specific_LLM.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction-to-large-language-models-llms&#34;&gt;Introduction to Large Language Models (LLMs)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-vocabulary&#34;&gt;Key Vocabulary&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#architecture-of-llms&#34;&gt;Architecture of LLMs&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#transformer-architecture-formula&#34;&gt;Transformer Architecture Formula&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#training-llms&#34;&gt;Training LLMs&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction-to-llms-for-satellite-images&#34;&gt;Introduction to LLMs for Satellite Images&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#exploring-vision-language-models-vlms-to-understand-high-level-features-in-remotely-sensed-images&#34;&gt;Exploring Vision-Language Models (VLMs) to Understand High-Level Features in Remotely Sensed Images&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#what-makes-vision-language-models-vlms-special&#34;&gt;What Makes Vision-Language Models (VLMs) Special?&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#challenges-in-remote-sensing-with-vlms&#34;&gt;Challenges in Remote Sensing with VLMs&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#benchmarking-vlms-on-landmark-recognition&#34;&gt;Benchmarking VLMs on Landmark Recognition&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#diving-deeper-into-vlms-case-studies-of-landmark-recognition-and-scene-interpretation&#34;&gt;Diving Deeper into VLMs: Case Studies of Landmark Recognition and Scene Interpretation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#overview-of-the-fine-tuning-process&#34;&gt;Overview of the Fine-Tuning Process&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#step-by-step-fine-tuning-of-satgpt-for-satellite-imagery&#34;&gt;Step-by-Step Fine-Tuning of SatGPT for Satellite Imagery&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-data-preparation&#34;&gt;1. Data Preparation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-model-selection&#34;&gt;2. Model Selection&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-fine-tuning-paradigm&#34;&gt;3. Fine-Tuning Paradigm&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-model-validation-and-evaluation&#34;&gt;4. Model Validation and Evaluation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#5-export-and-deployment-to-hugging-face&#34;&gt;5. Export and Deployment to Hugging Face&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#additional-concepts&#34;&gt;Additional Concepts&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#formula-for-self-attention-in-rag&#34;&gt;Formula for Self-Attention in RAG&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#vision-transformer-vit&#34;&gt;Vision Transformer (ViT)&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#full-flow-diagram&#34;&gt;Full Flow Diagram&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#quick-thoughts-on-llms-before-we-wrap-up-this-blog&#34;&gt;Quick thoughts on LLMs before we wrap up this blog:&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-introduction-to-large-language-models-llms-in-remote-sensing&#34;&gt;1. &lt;strong&gt;Introduction to Large Language Models (LLMs) in Remote Sensing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-foundation-models-and-their-role-in-llms&#34;&gt;2. &lt;strong&gt;Foundation Models and Their Role in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-training-vs-fine-tuning-vs-pre-trained-models-in-llms&#34;&gt;3. &lt;strong&gt;Training vs Fine-tuning vs Pre-trained Models in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-how-to-train-llms-on-satellite-images&#34;&gt;4. &lt;strong&gt;How to Train LLMs on Satellite Images&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#5-retrieval-augmented-generation-rag-for-satellite-image-analysis&#34;&gt;5. &lt;strong&gt;Retrieval-Augmented Generation (RAG) for Satellite Image Analysis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#6-using-langchain-for-satellite-image-llm-applications&#34;&gt;6. &lt;strong&gt;Using LangChain for Satellite Image LLM Applications&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#7-sample-datasets-for-llm-fine-tuning-in-remote-sensing&#34;&gt;7. &lt;strong&gt;Sample Datasets for LLM Fine-Tuning in Remote Sensing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#8-mathematical-foundations-of-attention-mechanisms-in-llms&#34;&gt;8. &lt;strong&gt;Mathematical Foundations of Attention Mechanisms in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#9-multimodal-llm-architectures-for-satellite-images&#34;&gt;9. &lt;strong&gt;Multimodal LLM Architectures for Satellite Images&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#10-preprocessing-techniques-for-satellite-images-in-llms&#34;&gt;10. &lt;strong&gt;Preprocessing Techniques for Satellite Images in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#11-handling-illumination-and-atmospheric-effects-in-llms&#34;&gt;11. &lt;strong&gt;Handling Illumination and Atmospheric Effects in LLMs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#12-self-supervised-learning-ssl-for-satellite-image-analysis&#34;&gt;12. &lt;strong&gt;Self-Supervised Learning (SSL) for Satellite Image Analysis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#13-open-source-tools-for-llms-in-satellite-image-analysis&#34;&gt;13. &lt;strong&gt;Open-Source Tools for LLMs in Satellite Image Analysis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#14-fine-tuning-llms-for-specific-satellite-image-tasks&#34;&gt;14. &lt;strong&gt;Fine-Tuning LLMs for Specific Satellite Image Tasks&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#15-evaluation-metrics-for-llms-in-remote-sensing&#34;&gt;15. &lt;strong&gt;Evaluation Metrics for LLMs in Remote Sensing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#16-transfer-learning-for-satellite-imagery&#34;&gt;16. &lt;strong&gt;Transfer Learning for Satellite Imagery&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#17-explainability-in-llms-for-remote-sensing-xai&#34;&gt;17. &lt;strong&gt;Explainability in LLMs for Remote Sensing (XAI)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction-to-large-language-models-llms&#34;&gt;Introduction to Large Language Models (LLMs)&lt;/h2&gt;
&lt;p&gt;Large Language Models (LLMs) are at the forefront of a revolution in Artificial Intelligence (AI) and Natural Language Processing (NLP). These models are not just sophisticated algorithms; they represent a leap forward in how machines understand and generate human language. Leveraging cutting-edge deep learning architectures, such as transformers, LLMs have transformed the landscape of language technology.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;We&amp;#39;re releasing a preview of OpenAI o1â€”a new series of AI models designed to spend more time thinking before they respond.&lt;br&gt;&lt;br&gt;These models can reason through complex tasks and solve harder problems than previous models in science, coding, and math. &lt;a href=&#34;https://t.co/peKzzKX1bu&#34;&gt;https://t.co/peKzzKX1bu&lt;/a&gt;&lt;/p&gt;&amp;mdash; OpenAI (@OpenAI) &lt;a href=&#34;https://twitter.com/OpenAI/status/1834278217626317026?ref_src=twsrc%5Etfw&#34;&gt;September 12, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;At their essence, LLMs are built on expansive neural networks with billions of parameters. These networks are trained on vast corpora of text data, learning to discern intricate patterns and relationships within language. Through a process known as pre-training, LLMs develop a broad understanding of linguistic structures, context, and semantics. During this phase, they utilize unsupervised learning techniques to predict masked words or sequences, refining their ability to understand and generate coherent text.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Curious about fine-tuning a satellite-specific LLM model? ğŸŒ&lt;br&gt;.&lt;br&gt;Dive into my latest blog to learn more: &lt;a href=&#34;https://t.co/sd25ByzQpJ&#34;&gt;https://t.co/sd25ByzQpJ&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/LLM?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#LLM&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Geoscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Geoscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/SatelliteLLM?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#SatelliteLLM&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/MachineLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#MachineLearning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Landsat?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Landsat&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/geography?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#geography&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1833511803739116039?ref_src=twsrc%5Etfw&#34;&gt;September 10, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Following pre-training, LLMs undergo fine-tuning to adapt their general language capabilities to specific tasks or domains. This supervised learning phase involves training the model on a targeted dataset, allowing it to excel in applications such as text generation, translation, sentiment analysis, and question-answering. Techniques like transfer learning and few-shot learning further enhance the model&amp;rsquo;s adaptability, enabling it to generalize from limited examples and perform across various contexts.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Deploying LLMs in real-world scenarios involves addressing practical challenges related to computational resources and scalability. These models require substantial processing power and memory, often necessitating the use of advanced hardware like GPUs or TPUs. Despite these demands, the benefits of integrating LLMs into applicationsâ€”such as chatbots, virtual assistants, content generation, and automated summarizationâ€”are profound, offering significant advancements in how machines interact with human language.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this blog post, I will delve into the technical intricacies of LLMs, exploring their architecture, training methodologies, and deployment considerations. Prepare to discover how these powerful AI tools are pushing the boundaries of language technology and shaping the future of machine intelligence.&lt;/p&gt;
&lt;h2 id=&#34;key-vocabulary&#34;&gt;Key Vocabulary&lt;/h2&gt;
&lt;p&gt;Here are some essential terms and acronyms related to LLMs:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Acronym&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Meaning&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Artificial Intelligence: The simulation of human intelligence in machines that are programmed to think and learn.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ANN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Artificial Neural Network: A computational model inspired by biological neural networks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;BERT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Bidirectional Encoder Representations from Transformers: A model for natural language understanding tasks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CNN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Convolutional Neural Network: Effective for processing grid-like data such as images.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;CRF&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Conditional Random Field: A statistical modeling method for structured prediction.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;DNN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Deep Neural Network: A neural network with multiple layers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;DL&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Deep Learning: A subset of machine learning with neural networks containing many layers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;GPT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Generative Pre-trained Transformer: A transformer-based model for generating human-like text.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;HMM&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Hidden Markov Model: A model for systems that transition between states with certain probabilities.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;LSTM&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Long Short-Term Memory: A type of RNN designed to remember long-term dependencies.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;LLM&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Large Language Model: Trained on vast amounts of text data to understand and generate text.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ML&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Machine Learning: Training algorithms to make predictions based on data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;NLP&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Natural Language Processing: The interaction between computers and human language.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;RAG&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Retrieval-Augmented Generation: Combines document retrieval with generative models.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;RNN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Recurrent Neural Network: Designed for sequential data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;T5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Text-to-Text Transfer Transformer: Converts various tasks into a text-to-text format.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Transformer&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;A model architecture that uses self-attention mechanisms.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ViT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Vision Transformer: A transformer model for image processing.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;VQA&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Visual Question Answering: Combining vision and language understanding.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;VLMs&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Vision-Language Models: Close the divide between visual and language comprehension in AI.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;XLNet&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;An extension of BERT with permutation-based training.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Hugging Face&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Platform for NLP with pre-trained models, datasets, and tools.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library for transformer-based models by Hugging Face.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;datasets&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library for managing datasets, by Hugging Face.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Gradio&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library for creating machine learning demos with simple UIs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;LangChain&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Facilitates development using LLMs with tools for managing language-based tasks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;spaCy&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Advanced NLP library in Python.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;NLTK&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Natural Language Toolkit: Tools for text processing and linguistic analysis.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;StanfordNLP&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library by Stanford University for NLP tasks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;OpenCV&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Library for computer vision tasks.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Deep learning framework with tensor computations and automatic differentiation.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;TensorFlow&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Framework for building and deploying machine learning models.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Keras&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;High-level neural networks API running on top of TensorFlow.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Fastai&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Simplifies neural network training with PyTorch.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;ONNX&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Open Neural Network Exchange format for model transfer between frameworks.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;architecture-of-llms&#34;&gt;Architecture of LLMs&lt;/h2&gt;
&lt;p&gt;LLMs are built on advanced architectures that often include transformer models. A transformer model utilizes self-attention mechanisms to process input sequences. The core components of a transformer are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: Processes the input data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: Generates the output sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transformer-architecture-formula&#34;&gt;Transformer Architecture Formula&lt;/h3&gt;
&lt;p&gt;The key mathematical operation in transformers is the self-attention mechanism, which can be described as follows:&lt;/p&gt;
&lt;p&gt;$[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V ]$&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( Q )$ is the query matrix,&lt;/li&gt;
&lt;li&gt;$( K )$ is the key matrix,&lt;/li&gt;
&lt;li&gt;$( V )$ is the value matrix,&lt;/li&gt;
&lt;li&gt;$( d_k )$ is the dimensionality of the keys.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training-llms&#34;&gt;Training LLMs&lt;/h2&gt;
&lt;p&gt;Training LLMs involves several steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Data Preparation&lt;/strong&gt;: Collect and preprocess large text corpora.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Initialization&lt;/strong&gt;: Start with a pre-trained model or initialize from scratch.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Use gradient descent and backpropagation to minimize the loss function.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;introduction-to-llms-for-satellite-images&#34;&gt;Introduction to LLMs for Satellite Images&lt;/h2&gt;
&lt;p&gt;Fine-tuning a Large Language Model (LLM) like SatGPT for satellite imagery involves several critical stages. This process transforms a pre-trained model into a specialized tool capable of analyzing and generating insights from satellite images. This blog post provides a step-by-step guide to fine-tuning and deploying SatGPT, covering each phase in detail.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In their 2024 paper, &lt;em&gt;â€œGood at Captioning, Bad at Counting: Benchmarking GPT-4V on Earth Observation Dataâ€&lt;/em&gt; (&lt;a href=&#34;https://arxiv.org/abs/2401.17600&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2401.17600&lt;/a&gt;), Zhang and Wang focus on developing a benchmark for Vision-Language Models (VLMs) applied to Earth Observation (EO) data. Their initial framework addresses three main areas: scene understanding, localization and counting, and change detection. To assess VLM performance across these areas, they design evaluations that span various applications, from wildlife conservation to urban monitoring, as illustrated in Figure 1. Their goals are to evaluate existing VLMs, provide insights into effective prompting techniques for EO tasks, and establish a flexible system for ongoing benchmark updates and future VLM evaluations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- 
For scene understanding, Zhang and Wang assess how VLMs integrate high-level image information with latent knowledge from language modeling. They use several datasets for this purpose: a new dataset for aerial landmark recognition to test the modelâ€™s ability to identify and geolocate U.S. landmarks, the RSICD dataset to evaluate the modelâ€™s capability to generate captions for Google Earth images, the BigEarthNet dataset to probe land cover identification in medium-resolution satellite images, and the fMoW-WILDS and PatternNet datasets to assess land use classification in high-resolution satellite images.
 --&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The paper highlights several challenges and areas for future work. One major challenge is detecting data contamination, which is crucial for maintaining the fairness and effectiveness of benchmarks as VLMs evolve. Additionally, a more detailed analysis of model failuresâ€”such as knowledge gaps, reasoning errors, perceptual mistakes, and text misunderstandingsâ€”could provide deeper insights into current VLM capabilities. Zhang and Wang also note the static nature of benchmarks as a limitation, suggesting that dynamic updates may be necessary to keep benchmarks relevant and challenging as VLMs advance.
  &lt;/div&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/SatGPT?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#SatGPT&lt;/a&gt; is an app that lets you talk to satellite imagery.&lt;br&gt;&lt;br&gt;We&amp;#39;ve got some more work to do before it&amp;#39;s polished, but I&amp;#39;m pretty psyched about the results so far. &lt;br&gt;&lt;br&gt;Powered by &lt;a href=&#34;https://twitter.com/Element84?ref_src=twsrc%5Etfw&#34;&gt;@Element84&lt;/a&gt; &lt;a href=&#34;https://twitter.com/STACspec?ref_src=twsrc%5Etfw&#34;&gt;@STACspec&lt;/a&gt; &lt;a href=&#34;https://twitter.com/LangChainAI?ref_src=twsrc%5Etfw&#34;&gt;@LangChainAI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/Panel_org?ref_src=twsrc%5Etfw&#34;&gt;@Panel_org&lt;/a&gt; &lt;a href=&#34;https://twitter.com/HoloViz_org?ref_src=twsrc%5Etfw&#34;&gt;@HoloViz_org&lt;/a&gt;, huge thanks to &lt;a href=&#34;https://twitter.com/ivanziogeo?ref_src=twsrc%5Etfw&#34;&gt;@ivanziogeo&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/MarcSkovMadsen?ref_src=twsrc%5Etfw&#34;&gt;@MarcSkovMadsen&lt;/a&gt;. &lt;a href=&#34;https://t.co/gO7aZz6w4C&#34;&gt;pic.twitter.com/gO7aZz6w4C&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kevin Lalli (@opticsinspace) &lt;a href=&#34;https://twitter.com/opticsinspace/status/1670688747552411649?ref_src=twsrc%5Etfw&#34;&gt;June 19, 2023&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;!-- 
In the domain of localization and counting, Zhang and Wang evaluate whether VLMs can extract detailed information about specific objects and understand their spatial relationships. They create datasets for this purpose, including the DIOR-RSVG dataset to test Referring Expression Comprehension (REC) abilities, where the model localizes objects based on natural language descriptions. Additionally, they use the NEON-Tree, COWC, and xBD datasets to evaluate the counting of small objects like trees, cars, and buildings in aerial and satellite images, and the aerial animal detection dataset to assess the modelâ€™s ability to count animals in tilted aerial images.

For change detection, the focus is on evaluating how VLMs track changes over time. Zhang and Wang use a dataset that categorizes buildings by damage levels and presents the data in JSON format, tracking counts before and after damage across various categories.



In the context of image captioning, Zhang and Wang evaluate the ability of instruction-following VLMs to describe aerial or satellite images. Their evaluation uses the RSICD dataset to compare VLM-generated captions with human-annotated examples both qualitatively and quantitatively, assessing how well VLMs describe images at various levels of detail.

For land use and land cover (LULC) classification, Zhang and Wang assess VLMs&#39; performance on multiple-choice classification tasks using datasets like fMoW-WILDS, PatternNet, and BigEarthNet. Their aim is to determine which models excel in zero-shot classification and how image resolution impacts classification accuracy. They find that VLM performance varies based on image resolution, label ambiguity, and granularity. Specifically, GPT-4V shows lower performance in land cover classification compared to specialized models but performs better on certain datasets like fMoW-WILDS and PatternNet. The challenges of ambiguous class labels and limited multi-spectral information in the BigEarthNet dataset also affect GPT-4V&#39;s performance.

Overall, Zhang and Wangâ€™s work underscores the importance of evolving benchmarks and VLM capabilities to address the challenges in EO data applications.

They deliver an in-depth analysis of GPT-4Vâ€™s performance across different tasks. Figure 1 illustrates key scenarios and the modelâ€™s performance: --&gt;
&lt;!-- 
1. **Location Recognition**  
   **Scenario:** Identify the landmark based on its features, such as its dome and layout.  
   **Example Answer:** The landmark, recognized by its distinctive style and layout, is the Nebraska State Capitol.

2. **Image Captioning**  
   **Scenario:** Generate a one-sentence caption for the provided image.  
   **Example Caption:** An aerial view of an airport terminal, showcasing nearby aircraft, taxiways, and parking areas.

3. **Land Use &amp; Land Cover Classification**  
   **Scenario:** Categorize the image into one of several predefined categories.  
   **Example Classification:** The image is best described as a Shipping Yard.

4. **Object Localization**  
   **Scenario:** Pinpoint the coordinates of a described object in the image.  
   **Example Description:** The gray windmill in the center.  
   **Coordinates:** [233, 383, 376, 542]

5. **Object Counting**  
   **Scenario:** Estimate the number of trees visible in the image.  
   **Count:** 134

6. **Change Detection**  
   **Scenario:** Count buildings in various damage categories and present the data in JSON format.  
   **JSON Format:**  
   ```json
   {
     &#34;count_before&#34;: 75,
     &#34;no_damage&#34;: 2,
     &#34;minor_damage&#34;: 73,
     &#34;major_damage&#34;: 0,
     &#34;destroyed&#34;: 1
   }
   ```

**Performance Metrics:**

- **RefCLIP Score:** Evaluates the modelâ€™s performance on reference-based tasks.
- **F1 Score:** Measures the modelâ€™s accuracy in classification tasks.
- **Mean IoU:** Assesses the modelâ€™s performance in object localization.
- **R2 Score:** Gauges the modelâ€™s predictive accuracy across various tasks. --&gt;
&lt;p&gt;These findings offer valuable insights into GPT-4Vâ€™s capabilities and limitations, especially in the context of earth observation data.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;sample_apps.png&#34; alt=&#34;Earth observation data&#34;&gt; &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. Here are examples of inputs and outputs from various benchmark tasks and how five different VLMs stack up. Theyâ€™ve included just a snippet of the user prompts and model responses to highlight the key points. &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;exploring-vision-language-models-vlms-to-understand-high-level-features-in-remotely-sensed-images&#34;&gt;Exploring Vision-Language Models (VLMs) to Understand High-Level Features in Remotely Sensed Images&lt;/h3&gt;
&lt;p&gt;In my recent work, I&amp;rsquo;ve been diving deep into Vision-Language Models (VLMs) to see how well they perform in tasks that require understanding both visual and textual data. With the explosion of AI models that can interpret images and generate coherent, detailed text, itâ€™s become increasingly important to assess these models not just on general benchmarks, but in specific, high-stakes domains like remotely sensed imagery.&lt;/p&gt;
&lt;p&gt;Remotely sensed images, which are collected from satellite or aerial platforms, provide a unique challenge for VLMs. They are dense with data, full of patterns, and often contain complex interactions between natural and man-made objects. The ability of a model to not only caption these images but also understand high-level featuresâ€”such as differentiating between natural landmarks, infrastructure, and potential environmental changesâ€”can have far-reaching applications in fields like agriculture, urban planning, and disaster response.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure2.png&#34; alt=&#34;VLM Comparison for Benchmark Tasks&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. A comparison of inputs and outputs from benchmark tasks using different VLMs. The snippet includes user prompts and model responses, highlighting key areas of model performance.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h3 id=&#34;what-makes-vision-language-models-vlms-special&#34;&gt;What Makes Vision-Language Models (VLMs) Special?&lt;/h3&gt;
&lt;p&gt;VLMs operate at the intersection of vision and language, giving them the ability to describe images with textual explanations. This makes them incredibly useful for analyzing and interpreting remote sensing data. In these images, VLMs can recognize patterns, identify important landmarks, and even offer insights into the features present within the scene.&lt;/p&gt;
&lt;p&gt;However, while these models excel at captioning tasksâ€”offering detailed and sometimes creative descriptionsâ€”they can struggle with more precise tasks like counting objects or recognizing certain functional categories. This is a critical gap that must be addressed, especially in applications where accuracy is paramount.&lt;/p&gt;
&lt;h3 id=&#34;challenges-in-remote-sensing-with-vlms&#34;&gt;Challenges in Remote Sensing with VLMs&lt;/h3&gt;
&lt;p&gt;One of the major challenges Iâ€™ve observed while working with VLMs on remotely sensed images is the models&amp;rsquo; difficulty in consistently recognizing high-level features, especially when dealing with complex or less common landmarks. This can lead to a high rate of refusal or incorrect identification in certain categories.&lt;/p&gt;
&lt;p&gt;For instance, a model might easily recognize a natural park or large urban feature, but struggle to identify a specific sports venue or government building. These variances are especially pronounced when analyzing remote imagery, where the perspective and scale can make recognition even more difficult.&lt;/p&gt;
&lt;h3 id=&#34;benchmarking-vlms-on-landmark-recognition&#34;&gt;Benchmarking VLMs on Landmark Recognition&lt;/h3&gt;
&lt;p&gt;I ran some experiments using five different VLMs (GPT-4V, InstructBLIP-TS-XXL, InstructBLIP-Vicuna-13b, LLaVA-v1.5, Qwen-VL-Chat) to see how well they could identify landmarks in a set of remotely sensed images. Below is the summary of the results for landmark recognition accuracy (Table 1) and refusal rate (Table 2).&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;Table1and2.png&#34; alt=&#34;Table 1 and 2: Landmark Recognition Accuracy and Refusal Rate&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Table 1: Landmark recognition accuracy by functional category and Table 2: Landmark recognition refusal rate.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As you can see, there are significant variances in how different models perform across these categories. GPT-4V and InstructBLIP tend to outperform other models in recognizing large, prominent landmarks like natural parks and urban infrastructure. However, thereâ€™s still considerable room for improvement, especially when identifying more specific or niche features, like places of worship or government buildings.&lt;/p&gt;
&lt;h3 id=&#34;diving-deeper-into-vlms-case-studies-of-landmark-recognition-and-scene-interpretation&#34;&gt;Diving Deeper into VLMs: Case Studies of Landmark Recognition and Scene Interpretation&lt;/h3&gt;
&lt;p&gt;The nuances of how Vision-Language Models (VLMs) understand and interpret images can be observed more clearly in specific examples. Below, Iâ€™ve analyzed a few key scenarios where GPT-4V has demonstrated both its strengths and limitations.&lt;/p&gt;
&lt;h4 id=&#34;visual-recognition-with-architectural-context&#34;&gt;Visual Recognition with Architectural Context&lt;/h4&gt;
&lt;p&gt;One fascinating case is GPT-4Vâ€™s ability to link visual cues with its knowledge of architecture. In &lt;strong&gt;Figure 3&lt;/strong&gt;, the model successfully identifies a landmark by connecting the architectural style with its vast knowledge base, arriving at the correct answer. This demonstrates its ability to use contextual clues beyond just object recognition.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure3.png&#34; alt=&#34;Architectural Landmark Identification&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. GPT-4V successfully corresponds visual cues with its knowledge about the architectural style of the landmark to arrive at the correct answer.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;the-problem-of-visual-misinterpretation&#34;&gt;The Problem of Visual Misinterpretation&lt;/h4&gt;
&lt;p&gt;However, VLMs aren&amp;rsquo;t infallible. One case where GPT-4V struggled is in the identification of the &lt;strong&gt;Nebraska State Capitol&lt;/strong&gt;. In &lt;strong&gt;Figure 4&lt;/strong&gt;, the model incorrectly eliminates the correct answer due to misidentifying the tower-like structure. This reveals a significant gap in its ability to distinguish more subtle architectural details, leading to incorrect conclusions.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure4.png&#34; alt=&#34;Misidentification of Nebraska State Capitol&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 4. GPT-4V fails to identify the tower-like structure of the Nebraska State Capitol, leading to incorrect elimination.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;correct-identification-but-weak-justifications&#34;&gt;Correct Identification but Weak Justifications&lt;/h4&gt;
&lt;p&gt;Interestingly, even when GPT-4V identifies a landmark correctly, it sometimes provides insufficient reasoning. In &lt;strong&gt;Figure 5&lt;/strong&gt;, the model identifies the landmark, but the reasoning lacks depth, which could be a hindrance in scenarios requiring detailed explanations, such as educational or research-oriented applications.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure5.png&#34; alt=&#34;Correct Identification but Weak Reasoning&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 5. GPT-4V correctly identifies the landmark but gives insufficient reasoning.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;generating-image-captions-for-complex-scenes&#34;&gt;Generating Image Captions for Complex Scenes&lt;/h4&gt;
&lt;p&gt;Another interesting scenario is when the model is tasked with generating captions for complex images. In &lt;strong&gt;Figure 6&lt;/strong&gt;, GPT-4V generates several captions for an airport image. While the captions are coherent, they sometimes miss finer details, like the specific types of airplanes or terminal features, which could be crucial in more technical applications like surveillance or logistics planning.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure6.png&#34; alt=&#34;Airport Caption Generation&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 6. Example captions generated for an airport image.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;object-localization-in-remote-sensing&#34;&gt;Object Localization in Remote Sensing&lt;/h4&gt;
&lt;p&gt;Object localization is another key area where VLMs need to perform exceptionally well. In &lt;strong&gt;Figure 7&lt;/strong&gt;, GPT-4V is tasked with localizing objects in a DIOR-RSVG dataset image. While it performs reasonably well, there are still challenges in precisely identifying and categorizing certain objects, especially in cluttered or low-contrast scenes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure7.png&#34; alt=&#34;Object Localization in DIOR-RSVG Dataset&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 7. Example prompt and response for DIOR-RSVG object localization.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;h4 id=&#34;detecting-changes-in-xview2-imagery&#34;&gt;Detecting Changes in xView2 Imagery&lt;/h4&gt;
&lt;p&gt;Finally, in &lt;strong&gt;Figure 8&lt;/strong&gt;, the model is put to the test with change detection using the xView2 dataset, where it must identify changes in infrastructure and the environment. This kind of task is essential in applications like disaster response or urban monitoring, where rapid and accurate assessments can make a significant difference. GPT-4Vâ€™s performance is promising, but it still leaves room for improvement, especially in recognizing more subtle changes or those happening over time.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
  &lt;img src=&#34;Figure8.png&#34; alt=&#34;Change Detection in xView2 Dataset&#34;&gt; 
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 8. Example prompt and response for xView2 change detection.
  &lt;a href=&#34;https://arxiv.org/pdf/2401.17600v1&#34; target=&#34;_blank&#34;&gt;[Good at captioning, bad at counting]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;overview-of-the-fine-tuning-process&#34;&gt;Overview of the Fine-Tuning Process&lt;/h2&gt;
&lt;p&gt;The process of fine-tuning and deploying a satellite-specific LLM model involves the following stages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Data Preparation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Selection&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Tuning Paradigm&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Validation and Evaluation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Export and Deployment to Hugging Face&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;step-by-step-fine-tuning-of-satgpt-for-satellite-imagery&#34;&gt;Step-by-Step Fine-Tuning of SatGPT for Satellite Imagery&lt;/h2&gt;
&lt;h3 id=&#34;1-data-preparation&#34;&gt;1. Data Preparation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Collect, preprocess, and format satellite images and associated textual annotations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collect Satellite Images&lt;/strong&gt;: Obtain satellite images from sources such as commercial providers or public datasets (e.g., Sentinel, Landsat).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Annotate Images&lt;/strong&gt;: Label images with relevant information (e.g., land cover types, objects of interest).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Preprocess Images&lt;/strong&gt;: Resize and normalize images to match the input requirements of the Vision Transformer (ViT) model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prepare Textual Descriptions&lt;/strong&gt;: Generate textual descriptions or annotations for each image, which will be used for training the text generation component.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import ViTFeatureExtractor, GPT2Tokenizer

# Initialize feature extractor and tokenizer
feature_extractor = ViTFeatureExtractor.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)
tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)

# Sample image and text
image = ... # Load satellite image
text = &amp;quot;This is a description of the satellite image.&amp;quot;

# Prepare inputs
inputs = feature_extractor(images=image, return_tensors=&amp;quot;pt&amp;quot;)
labels = tokenizer(text, return_tensors=&amp;quot;pt&amp;quot;).input_ids
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-model-selection&#34;&gt;2. Model Selection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Choose an appropriate pre-trained model as the foundation for SatGPT.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Options&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt;: For processing and extracting features from satellite images.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-2 or GPT-3&lt;/strong&gt;: For generating textual descriptions or insights based on image features.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import GPT2LMHeadModel, ViTModel

# Load pre-trained models
image_model = ViTModel.from_pretrained(&#39;google/vit-base-patch16-224-in21k&#39;)
text_model = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3-fine-tuning-paradigm&#34;&gt;3. Fine-Tuning Paradigm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Adapt the selected models to work together for the specific task of analyzing satellite imagery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Combine Models&lt;/strong&gt;: Integrate ViT for image feature extraction and GPT for text generation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Define Loss Functions&lt;/strong&gt;: Use suitable loss functions for image and text components.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Training Loop&lt;/strong&gt;: Implement a training loop to update model parameters based on the image-text pairs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import Trainer, TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir=&#39;./results&#39;,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    logging_dir=&#39;./logs&#39;,
)

# Initialize Trainer
trainer = Trainer(
    model=image_model,  # This would be a combined model in practice
    args=training_args,
    train_dataset=train_dataset,  # Prepare your dataset
)

# Train the model
trainer.train()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;4-model-validation-and-evaluation&#34;&gt;4. Model Validation and Evaluation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Assess the performance of the fine-tuned model to ensure it meets the desired criteria.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Validation Set&lt;/strong&gt;: Use a separate dataset to validate the modelâ€™s performance during training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Evaluation Metrics&lt;/strong&gt;: Measure performance using metrics such as accuracy, F1 score, or BLEU score (for text generation).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Evaluate the model
eval_results = trainer.evaluate()
print(eval_results)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;5-export-and-deployment-to-hugging-face&#34;&gt;5. Export and Deployment to Hugging Face&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;: Make the fine-tuned model available for inference and integration through Hugging Face.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Export the Model&lt;/strong&gt;: Save the fine-tuned model and tokenizer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Upload to Hugging Face&lt;/strong&gt;: Use the &lt;code&gt;transformers&lt;/code&gt; library to push the model to the Hugging Face Hub.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create an Inference Endpoint&lt;/strong&gt;: Deploy the model and set up an API endpoint for user interactions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline

# Load model from Hugging Face Hub
nlp = pipeline(&amp;quot;text-generation&amp;quot;, model=&amp;quot;username/satgpt-model&amp;quot;)

# Use the model
result = nlp(&amp;quot;Describe the land cover of this GISTDA satellite image.&amp;quot;)
print(result)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;additional-concepts&#34;&gt;Additional Concepts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Retrieval-Augmented Generation (RAG)&lt;/strong&gt;: Combines document retrieval with generative models to improve response accuracy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vision Transformers (ViT)&lt;/strong&gt;: Adapt transformers for image processing by treating images as sequences of patches.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;formula-for-self-attention-in-rag&#34;&gt;Formula for Self-Attention in RAG&lt;/h3&gt;
&lt;p&gt;In RAG, the attention mechanism can be described as:&lt;/p&gt;
&lt;p&gt;$[ \text{RAG}(Q, K, V, D) = \text{Attention}(Q, K, V) + \text{Retrieval}(D) ]$&lt;/p&gt;
&lt;p&gt;where $( D )$ represents retrieved documents.&lt;/p&gt;
&lt;h3 id=&#34;vision-transformer-vit&#34;&gt;Vision Transformer (ViT)&lt;/h3&gt;
&lt;p&gt;The Vision Transformer treats images as sequences of patches and processes them with transformer architectures. The key operation in ViT involves:&lt;/p&gt;
&lt;p&gt;$[ \text{Patch Embedding}(I) = \text{Linear}(I) + \text{Positional Encoding} ]$&lt;/p&gt;
&lt;p&gt;where $( I )$ is the image and the output is a sequence of patch embeddings.&lt;/p&gt;
&lt;h2 id=&#34;full-flow-diagram&#34;&gt;Full Flow Diagram&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s a conceptual flow of how data is processed through SatGPT, from input to output:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Satellite Image + Textual Description&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Image Processing&lt;/strong&gt;: ViT processes image into feature vectors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Text Generation&lt;/strong&gt;: GPT-2 generates textual descriptions from image features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Generated Text&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;quick-thoughts-on-llms-before-we-wrap-up-this-blog&#34;&gt;Quick thoughts on LLMs before we wrap up this blog:&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-introduction-to-large-language-models-llms-in-remote-sensing&#34;&gt;1. &lt;strong&gt;Introduction to Large Language Models (LLMs) in Remote Sensing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Large Language Models (LLMs) are advanced models designed to understand and generate human-like text. They can be adapted for analyzing satellite imagery by combining multimodal inputs, like images and textual descriptions.&lt;/p&gt;
&lt;h4 id=&#34;key-equations&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;The underlying architecture for LLMs is based on the Transformer model, which is governed by:
$[
\mathbf{Z} = \text{softmax}\left(\frac{\mathbf{QK}^\top}{\sqrt{d_k}}\right)\mathbf{V}
]$
where $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ are query, key, and value matrices respectively.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-foundation-models-and-their-role-in-llms&#34;&gt;2. &lt;strong&gt;Foundation Models and Their Role in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Foundation models are pre-trained on extensive datasets and serve as the base for fine-tuning on specific tasks, such as satellite image analysis.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-1&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;The objective during pre-training is to minimize:
$[
MLM = - \sum_{i=1}^{N} \log P(x_i | x_{-i}; \theta)
]$
where ${MLM}$ is the masked language modeling loss.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-training-vs-fine-tuning-vs-pre-trained-models-in-llms&#34;&gt;3. &lt;strong&gt;Training vs Fine-tuning vs Pre-trained Models in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pre-trained Models&lt;/strong&gt;: Trained on large-scale datasets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt;: Adapting a pre-trained model to a specific task or dataset.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Training a model from scratch using a domain-specific dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equations-2&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Cross-entropy loss function used during fine-tuning:
$[
\mathcal{L} = - \sum_{i=1}^{N} y_i \log(\hat{y}_i)
]$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-how-to-train-llms-on-satellite-images&#34;&gt;4. &lt;strong&gt;How to Train LLMs on Satellite Images&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Training LLMs on satellite images involves using multimodal inputs and embeddings to represent both images and textual descriptions.&lt;/p&gt;
&lt;!-- #### Key Equations
The multimodal training objective is:
$\[
\mathcal{L}_{\text{multimodal}} = \lambda \cdot \mathcal{L}_{\text{img}} + (1-\lambda) \cdot \mathcal{L}_{\text{text}}
\]$ --&gt;
&lt;hr&gt;
&lt;h3 id=&#34;5-retrieval-augmented-generation-rag-for-satellite-image-analysis&#34;&gt;5. &lt;strong&gt;Retrieval-Augmented Generation (RAG) for Satellite Image Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;RAG combines document retrieval with generation capabilities to enhance satellite image analysis by incorporating additional contextual information.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-3&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;RAG combines retrieval and generation via:
$[
P(x|c) = \sum_{i} P(x | c_i, q)P(c_i | q)
]$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;6-using-langchain-for-satellite-image-llm-applications&#34;&gt;6. &lt;strong&gt;Using LangChain for Satellite Image LLM Applications&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;LangChain facilitates chaining LLMs together for various tasks, such as preprocessing, analysis, and post-processing of satellite images.&lt;/p&gt;
&lt;h4 id=&#34;example&#34;&gt;Example&lt;/h4&gt;
&lt;p&gt;Using LangChain to preprocess satellite metadata:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain import SimplePromptTemplate
template = SimplePromptTemplate(prompt=&amp;quot;Summarize satellite data: {data}&amp;quot;)
summary = template.run(data=satellite_metadata)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;7-sample-datasets-for-llm-fine-tuning-in-remote-sensing&#34;&gt;7. &lt;strong&gt;Sample Datasets for LLM Fine-Tuning in Remote Sensing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Datasets such as UC Merced Land Use, EuroSAT, and BigEarthNet are used for fine-tuning LLMs to handle specific satellite image tasks.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;8-mathematical-foundations-of-attention-mechanisms-in-llms&#34;&gt;8. &lt;strong&gt;Mathematical Foundations of Attention Mechanisms in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The attention mechanism in LLMs is crucial for focusing on specific parts of the input data, such as regions in a satellite image.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-4&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Self-attention mechanism:
$[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
]$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;9-multimodal-llm-architectures-for-satellite-images&#34;&gt;9. &lt;strong&gt;Multimodal LLM Architectures for Satellite Images&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Multimodal LLMs integrate both text and image data, allowing for comprehensive analysis of satellite imagery.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-5&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;For multimodal learning, image and text representations are combined:
$[
\mathbf{Z} = \text{Concat}(Z_{\text{img}}, Z_{\text{text}})
]$&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;10-preprocessing-techniques-for-satellite-images-in-llms&#34;&gt;10. &lt;strong&gt;Preprocessing Techniques for Satellite Images in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Preprocessing techniques like normalization and histogram equalization are essential for preparing satellite images for analysis.&lt;/p&gt;
&lt;h4 id=&#34;key-formulas&#34;&gt;Key Formulas&lt;/h4&gt;
&lt;p&gt;Image normalization:
$[
X&amp;rsquo; = \frac{X - \mu}{\sigma}
]$
where $X$ is the pixel value, $\mu$ is the mean, and $\sigma$ is the standard deviation.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;11-handling-illumination-and-atmospheric-effects-in-llms&#34;&gt;11. &lt;strong&gt;Handling Illumination and Atmospheric Effects in LLMs&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Illumination and atmospheric distortions can affect satellite images, and models must be trained to handle these variations.&lt;/p&gt;
&lt;h4 id=&#34;key-equations-6&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Illumination adjustment formula:
$[
I&amp;rsquo; = \frac{I}{\cos(\theta) + \epsilon}
]$
where $\theta$ is the solar zenith angle.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;12-self-supervised-learning-ssl-for-satellite-image-analysis&#34;&gt;12. &lt;strong&gt;Self-Supervised Learning (SSL) for Satellite Image Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;SSL techniques allow models to learn from unlabelled satellite data by setting up proxy tasks such as predicting missing data.&lt;/p&gt;
&lt;!-- 
#### Key Equations
Contrastive loss function in SSL:
$\[
\mathcal{L}_{\text{contrastive}} = - \log \frac{\exp(\mathbf{z}_i^\top \mathbf{z}_j / \tau)}{\sum_{k} \exp(\mathbf{z}_i^\top \mathbf{z}_k / \tau)}
\]$ --&gt;
&lt;hr&gt;
&lt;h3 id=&#34;13-open-source-tools-for-llms-in-satellite-image-analysis&#34;&gt;13. &lt;strong&gt;Open-Source Tools for LLMs in Satellite Image Analysis&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Useful tools include Hugging Face Transformers for fine-tuning, LangChain for chaining models, and FastAI for data augmentation.&lt;/p&gt;
&lt;h4 id=&#34;example-code&#34;&gt;Example Code&lt;/h4&gt;
&lt;p&gt;Using Hugging Face Transformers:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained(&amp;quot;bert-base-uncased&amp;quot;)
model = BertModel.from_pretrained(&amp;quot;bert-base-uncased&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;14-fine-tuning-llms-for-specific-satellite-image-tasks&#34;&gt;14. &lt;strong&gt;Fine-Tuning LLMs for Specific Satellite Image Tasks&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Fine-tuning involves adjusting a pre-trained model using satellite data to improve performance on specific tasks.&lt;/p&gt;
&lt;h4 id=&#34;key-steps&#34;&gt;Key Steps&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Load a pre-trained model.&lt;/li&gt;
&lt;li&gt;Freeze initial layers and fine-tune top layers.&lt;/li&gt;
&lt;li&gt;Train with domain-specific data.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;15-evaluation-metrics-for-llms-in-remote-sensing&#34;&gt;15. &lt;strong&gt;Evaluation Metrics for LLMs in Remote Sensing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Evaluating the performance of Large Language Models (LLMs) in remote sensing involves several metrics, including precision, recall, F1 score, mean Average Precision (mAP), and BLEU score. These metrics help assess the quality of predictions and the relevance of generated content.&lt;/p&gt;
&lt;h4 id=&#34;key-metrics&#34;&gt;Key Metrics&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Precision and Recall&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt; measures the proportion of true positive results among all positive results predicted by the model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt; measures the proportion of true positive results among all actual positive results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equations-7&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Precision:
$[
\text{Precision} = \frac{TP}{TP + FP}
]$
Recall:
$[
\text{Recall} = \frac{TP}{TP + FN}
]$
where $TP$ is true positives, $FP$ is false positives, and $FN$ is false negatives.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;F1 Score&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;F1 Score&lt;/strong&gt; is the harmonic mean of precision and recall, providing a single metric that balances both.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equation&#34;&gt;Key Equation&lt;/h4&gt;
&lt;p&gt;$[
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
]$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;mean Average Precision (mAP)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mAP&lt;/strong&gt; evaluates the precision of object detection models, averaging the precision across different recall levels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equation-1&#34;&gt;Key Equation&lt;/h4&gt;
&lt;p&gt;Average Precision (AP) for a single class:
$[
\text{AP} = \int_{0}^{1} \text{Precision}(r) , \text{Recall}(r)
]$
where $\text{Precision}(r)$ is the precision at recall level $r$.&lt;/p&gt;
&lt;p&gt;mAP is the mean of AP across all classes:
$[
\text{mAP} = \frac{1}{C} \sum_{i=1}^{C} \text{AP}_i
]$
where $C$ is the number of classes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BLEU Score&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BLEU Score&lt;/strong&gt; evaluates the quality of generated text by comparing it to reference texts, commonly used for tasks like image captioning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-equation-2&#34;&gt;Key Equation&lt;/h4&gt;
&lt;p&gt;BLEU score is calculated using n-gram precision:
$[
\text{BLEU} = \text{exp}\left(\sum_{n=1}^{N} w_n \cdot \log P_n\right)
]$
where $P_n$ is the precision of n-grams, and $w_n$ is the weight for n-grams of length $n$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;example-code-1&#34;&gt;Example Code&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import average_precision_score, precision_recall_curve
from nltk.translate.bleu_score import sentence_bleu

# Example for precision, recall, F1 score
y_true = [0, 1, 1, 0, 1, 1, 0]
y_pred = [0, 1, 0, 0, 1, 1, 1]
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

# Example for BLEU score
reference = [[&#39;GISTDA&#39;, &#39;is&#39;, &#39;the&#39;, &#39;premier&#39;, &#39;place&#39;, &#39;to&#39;, &#39;work&#39;, &#39;in&#39;, &#39;the&#39;, &#39;geo&#39;, &#39;sector&#39;, &#39;in&#39;, &#39;thailand&#39;]]
candidate = [&#39;GISTDA&#39;, &#39;is&#39;, &#39;the&#39;, &#39;best&#39;, &#39;workplace&#39;, &#39;in&#39;, &#39;geo&#39;, &#39;in&#39;, &#39;thailand&#39;]
bleu_score = sentence_bleu(reference, candidate)

print(f&amp;quot;Precision: {precision}&amp;quot;)
print(f&amp;quot;Recall: {recall}&amp;quot;)
print(f&amp;quot;F1 Score: {f1}&amp;quot;)
print(f&amp;quot;BLEU Score: {bleu_score}&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;16-transfer-learning-for-satellite-imagery&#34;&gt;16. &lt;strong&gt;Transfer Learning for Satellite Imagery&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Transfer learning uses models pre-trained on general datasets and adapts them for satellite image tasks through domain-specific fine-tuning.&lt;/p&gt;
&lt;!-- 
#### Key Equations
The total loss in transfer learning:
$\[
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{general}}(\theta_g) + \lambda \mathcal{L}_{\text{task}}(\theta_t)
\]$
where $\lambda$ is a regularization factor. --&gt;
&lt;h4 id=&#34;example-code-2&#34;&gt;Example Code&lt;/h4&gt;
&lt;p&gt;Using pre-trained ResNet for satellite image classification:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchvision import models
resnet = models.resnet50(pretrained=True)

# Freeze general layers
for param in resnet.parameters():
    param.requires_grad = False

# Fine-tune top layers
resnet.fc = nn.Linear(in_features=2048, out_features=num_classes)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;17-explainability-in-llms-for-remote-sensing-xai&#34;&gt;17. &lt;strong&gt;Explainability in LLMs for Remote Sensing (XAI)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Explainable AI (XAI) methods enhance the transparency of LLM predictions, allowing users to understand how models make decisions based on satellite imagery.&lt;/p&gt;
&lt;h4 id=&#34;key-techniques&#34;&gt;Key Techniques&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Attention Visualization&lt;/strong&gt;: Shows which parts of the input data are focused on by the model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grad-CAM&lt;/strong&gt;: Generates heatmaps highlighting important regions in the satellite images.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SHAP&lt;/strong&gt;: Explains individual predictions by computing feature contributions.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;key-equations-8&#34;&gt;Key Equations&lt;/h4&gt;
&lt;p&gt;Grad-CAM heatmap formula:
$[
\text{Grad-CAM}(A^k) = \text{ReLU}\left( \sum_k \alpha_k A^k \right)
]$
where $\alpha_k$ is the gradient of the loss with respect to the feature map $A^k$.&lt;/p&gt;
&lt;h4 id=&#34;example-code-3&#34;&gt;Example Code&lt;/h4&gt;
&lt;p&gt;Using Grad-CAM for explainability:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import cv2
import numpy as np

# Compute gradients
def grad_cam(model, img):
    gradients = torch.autograd.grad(outputs=model(img), inputs=model.layer4)
    weights = torch.mean(gradients[0], dim=[2, 3], keepdim=True)
    cam = torch.sum(weights * model.layer4(img), dim=1)
    return cam

# Apply Grad-CAM on an image
cam_output = grad_cam(resnet, satellite_image)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, large language models (LLMs) are making impressive strides in the realm of satellite data analysis, showcasing their potential across scene understanding, localization, counting, and change detection. These models are beginning to transform how we interpret complex satellite imagery, offering valuable insights for everything from environmental monitoring to urban development.&lt;/p&gt;
&lt;p&gt;Despite these advancements, challenges remain. Current benchmarks reveal that while LLMs excel in tasks like generating descriptive captions and recognizing landmarks, they sometimes fall short in areas requiring detailed object counting and nuanced change detection. This highlights the need for more refined evaluation methods to fully capture and enhance LLM capabilities.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As both satellite technology and LLMs continue to evolve, the path forward promises exciting developments. By refining benchmarks and exploring new methodologies, we can unlock even greater potential in this technology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I hope you enjoyed this deep dive into the intersection of LLMs and satellite data. If you found this blog insightful, please consider sharing it with others who might be interested. Stay tuned for more updates and innovations in this thrilling field!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;How to Fine-Tune and Deploy a Satellite-Specific LLM Model for Satellite Images&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024finetune,
  title   = &amp;quot;How to Fine-Tune and Deploy a Satellite-Specific LLM Model for Satellite Images&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Kaiser, Å., Polosukhin, I. (NeurIPS 2017).&lt;/strong&gt; &amp;ldquo;Attention Is All You Need.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 5998-6008. &lt;a href=&#34;https://doi.org/10.5555/3295222.3295349&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3295222.3295349&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Shinn, E., Ramesh, A., Muthukrishnan, P., and others. (NeurIPS 2020).&lt;/strong&gt; &amp;ldquo;Language Models are Few-Shot Learners.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 1877-1901. &lt;a href=&#34;https://doi.org/10.5555/3454337.3454731&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3454337.3454731&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Devlin, J., Chang, M. W., Lee, K., &amp;amp; Toutanova, K. (NAACL 2019).&lt;/strong&gt; &amp;ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.&amp;rdquo; &lt;em&gt;North American Chapter of the Association for Computational Linguistics (NAACL)&lt;/em&gt;, 4171-4186. &lt;a href=&#34;https://doi.org/10.5555/3331189.3331190&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3331189.3331190&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., &amp;amp; others. (ICLR 2021).&lt;/strong&gt; &amp;ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3453424.3453670&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3453424.3453670&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Radford, A., Wu, J., Child, R., Mehri, S., &amp;amp; others. (ICLR 2019).&lt;/strong&gt; &amp;ldquo;Language Models are Unsupervised Multitask Learners.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3326452.3326458&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3326452.3326458&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clark, K., Luong, M. T., Le, Q. V., &amp;amp; Manning, C. D. (ACL 2019).&lt;/strong&gt; &amp;ldquo;ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.&amp;rdquo; &lt;em&gt;Association for Computational Linguistics (ACL)&lt;/em&gt;, 2251-2261. &lt;a href=&#34;https://doi.org/10.5555/3454375.3454420&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3454375.3454420&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Zhang, Y., Zhao, Y., Saleh, M., &amp;amp; Liu, P. J. (ICLR 2021).&lt;/strong&gt; &amp;ldquo;PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3453104.3453140&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3453104.3453140&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Kenton, J., &amp;amp; Toutanova, K. (NAACL 2019).&lt;/strong&gt; &amp;ldquo;BERT: Bidirectional Encoder Representations from Transformers.&amp;rdquo; &lt;em&gt;North American Chapter of the Association for Computational Linguistics (NAACL)&lt;/em&gt;, 4171-4186. &lt;a href=&#34;https://doi.org/10.5555/3331189.3331190&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3331189.3331190&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Yang, Z., Yang, D., Dineen, C., &amp;amp; others. (ICLR 2020).&lt;/strong&gt; &amp;ldquo;XLNet: Generalized Autoregressive Pretraining for Language Understanding.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3456141.3456151&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3456141.3456151&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Raffel, C., Shinn, E., S. J. McDonell, C. Lee, K., &amp;amp; others. (ICLR 2021).&lt;/strong&gt; &amp;ldquo;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.5555/3456181.3456210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3456181.3456210&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Zhang, C., &amp;amp; Wang, S. (arXiv 2024).&lt;/strong&gt; &amp;ldquo;Good at Captioning, Bad at Counting: Benchmarking GPT-4V on Earth Observation Data.&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:2401.17600&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2401.17600&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv.org/abs/2401.17600&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk ğŸ›º &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240906_Panboonyuen_AI_ThaiHighway.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#meet-reg-the-game-changer-in-highway-asset-detection&#34;&gt;Meet REG: The Game-Changer in Highway Asset Detection&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#generalized-focal-loss-for-multi-class-detection&#34;&gt;Generalized Focal Loss for Multi-Class Detection&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#refined-generalized-focal-loss-for-segmentation&#34;&gt;Refined Generalized Focal Loss for Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#refinement-term-for-spatial-contextual-learning&#34;&gt;Refinement Term for Spatial-Contextual Learning&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#joint-optimization-for-detection-and-segmentation&#34;&gt;Joint Optimization for Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#incorporating-prediction-uncertainty&#34;&gt;Incorporating Prediction Uncertainty&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#mathematical-foundations-for-optimization-in-reg&#34;&gt;Mathematical Foundations for Optimization in REG&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#performance-analysis-for-detection-and-segmentation&#34;&gt;Performance Analysis for Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways&#34;&gt;Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#understanding-the-scene&#34;&gt;Understanding the Scene&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-challenge-detection-and-segmentation&#34;&gt;The Challenge: Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-process-in-action&#34;&gt;The Process in Action&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#real-world-impact&#34;&gt;Real-World Impact&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#paper-highlights&#34;&gt;Paper Highlights:&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures&#34;&gt;Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#generalized-focal-loss-for-vision-tasks&#34;&gt;Generalized Focal Loss for Vision Tasks&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#final-insights-pioneering-precision-with-reg-in-highway-asset-detection&#34;&gt;Final Insights: Pioneering Precision with REG in Highway Asset Detection&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-introduction-to-generalized-focal-loss&#34;&gt;1. &lt;strong&gt;Introduction to Generalized Focal Loss&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-formula-for-difference-between-detection-and-segmentation-models&#34;&gt;2. &lt;strong&gt;Formula for Difference Between Detection and Segmentation Models&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-optimization-in-object-detection-and-segmentation&#34;&gt;3. &lt;strong&gt;Optimization in Object Detection and Segmentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-mathematical-formulas-to-know&#34;&gt;4. &lt;strong&gt;Mathematical Formulas to Know&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#whats-next&#34;&gt;Whatâ€™s Next?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are pleased to announce that our paper, titled Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery, has been accepted for oral presentation at the 5th International Conference on Highway Engineering (iCHE 2024). This opportunity marks a significant moment in our academic journey, especially after a hiatus from international conferences since completing my Ph.D. I am eager to re-engage with the academic community and share our recent advancements in person.&lt;/p&gt;
&lt;h2 id=&#34;meet-reg-the-game-changer-in-highway-asset-detection&#34;&gt;Meet REG: The Game-Changer in Highway Asset Detection&lt;/h2&gt;
&lt;p&gt;Hi guys, fellow tech enthusiasts! I&amp;rsquo;m thrilled to unveil a cutting-edge innovation from my latest researchâ€”Refined Generalized Focal Loss (REG). This revolutionary approach is transforming road asset detection on Thai highways, and itâ€™s as exciting as it sounds.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, whatâ€™s the big deal with REG? Imagine a detection system that not only sees but truly understands the intricate details of highway scenes. REG pushes the boundaries of current vision-based detection models by tackling the most challenging issues: imbalanced datasets, tiny objects, and complex highway backdrops.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Our paper on â€œRefined Generalized Focal Loss for Road Asset Detection on Thai Highwaysâ€ has been accepted for oral presentation at iCHE 2024!&lt;br&gt;.&lt;br&gt;Excited to share how weâ€™re tackling road asset detection.&lt;br&gt;.&lt;br&gt;ğŸ“–ğŸ‘€ Check out the details here: &lt;a href=&#34;https://t.co/KTSGHITU7F&#34;&gt;https://t.co/KTSGHITU7F&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/iCHE2024?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#iCHE2024&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1832227483967746136?ref_src=twsrc%5Etfw&#34;&gt;September 7, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;My work (check out Fig. 1) brings a whole new level of precision to the table. By integrating a custom loss function into the detection architecture, REG doesn&amp;rsquo;t just improve performanceâ€”it redefines it. This means sharper, more reliable detection of critical road assets like signs, lane markings, and barriers. And letâ€™s be real, thatâ€™s a game-changer for infrastructure management and road safety.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
&lt;img src=&#34;REG_1.png&#34; alt=&#34;Refined Generalized Focal Loss Framework&#34; style=&#34;max-width: 100%; height: auto;&#34;&gt; 
&lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. The REG-based detection framework employs Generalized Focal Loss to master class imbalance in Thai highway road asset detection. Combining Transformer layers with convolutional modules, and using Batch Normalization and Adaptive Dropout, this model stands out for its robustness. Itâ€™s finely tuned to capture the unique aspects of Thai highways, focusing on rare and challenging assets. 
&lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34;&gt;[Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;p&gt;REG isn&amp;rsquo;t just a theoretical leap; itâ€™s a practical breakthrough with real-world impact. Itâ€™s especially useful for regions with road structures similar to Thai highways, where conventional detection algorithms might falter. By merging Vision Transformers (ViT) with conditional random fields (CRF), weâ€™ve supercharged the modelâ€™s ability to segment and identify road assets with pinpoint accuracy.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This isnâ€™t just about the future of intelligent transportation systems; itâ€™s about the here and now. As we edge closer to autonomous vehicle navigation, innovations like REG are paving the way for smarter, safer roads. Buckle up and stay tunedâ€”exciting times are ahead!
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/h2&gt;
&lt;p&gt;Thailand&amp;rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    In this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Our proposed method (Fig. 2) enhances the capability of REG-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;proposed_method.png&#34; alt=&#34;Proposed Method Image&#34;&gt; &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. The proposed Enhanced REG-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various REG model variants, ranging from REGn to REGx, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didnâ€™t just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34;&gt;[REG: Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;
&lt;p&gt;This paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.&lt;/p&gt;
&lt;p&gt;Stay tuned for updates, and a big thank you to my incredible research team:&lt;br&gt;
&lt;strong&gt;N. Rattanachona (N&amp;rsquo;Fuse)&lt;/strong&gt;, &lt;strong&gt;P. Thungthin (N&amp;rsquo;Dear)&lt;/strong&gt;, &lt;strong&gt;N. Subsompon (N&amp;rsquo;Tien)&lt;/strong&gt;. Your hard work and dedication were essential to this project!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_00.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;featured_full.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here I am, presenting our work on the Enhanced REG model and its application in detecting road assets!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have visualizations of the detection results produced by the Enhanced REG model. The bounding boxes and labels demonstrate the modelâ€™s ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the modelâ€™s performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-multi-class-detection&#34;&gt;Generalized Focal Loss for Multi-Class Detection&lt;/h3&gt;
&lt;p&gt;The detection task focuses on identifying seven key classes of road assets: Pavilions, Pedestrian bridges, Information signs, Single-arm poles, Bus stops, Warning signs, and Concrete guardrails (Fig. 3). The challenge lies in dealing with class imbalance â€” smaller and harder-to-detect objects can be easily overlooked by traditional object detection models. We address this by utilizing &lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt;, which extends the classical Focal Loss to multi-class detection, giving more focus to underrepresented and challenging classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_2.png&#34; alt=&#34;Generalized Focal Loss for Multi-Class Detection&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. My proposed Generalized Focal Loss for multi-class detection tackles class imbalance across seven asset classes. By extending Focal Loss, we improve detection accuracy for small and difficult-to-classify objects.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;refined-generalized-focal-loss-for-segmentation&#34;&gt;Refined Generalized Focal Loss for Segmentation&lt;/h3&gt;
&lt;p&gt;For the segmentation task, we detect road assets at the pixel level, focusing on five classes: Pavilions, Pedestrian bridges, Information signs, Warning signs, and Concrete guardrails (Fig. 4). The key here is to ensure that every pixel is correctly classified into one of these categories, which is a non-trivial problem in cluttered highway imagery. My &lt;strong&gt;Refined Generalized Focal Loss&lt;/strong&gt; applies pixel-wise loss calculation, extending GFL into the realm of segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_3.png&#34; alt=&#34;Refined Generalized Focal Loss for Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 4. The segmentation process classifies each pixel into one of five road asset classes, using Refined Generalized Focal Loss to enhance pixel-wise accuracy in segmentation tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_03.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, letâ€™s look at a real-world application of our Enhanced REG model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model&amp;rsquo;s potential for real-world deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_04.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart presents a comparison of performance metrics between our Enhanced REG model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model&amp;rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_05.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, this image illustrates the training process for the Enhanced REG model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the modelâ€™s performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the modelâ€™s effectiveness and reliability in practical applications.&lt;/p&gt;
&lt;h3 id=&#34;refinement-term-for-spatial-contextual-learning&#34;&gt;Refinement Term for Spatial-Contextual Learning&lt;/h3&gt;
&lt;p&gt;To further enhance learning, we introduce a spatial-contextual refinement term $(g_{i,c})$ that dynamically adjusts the loss based on the geometric and contextual significance of each object class (Fig. 5). This term allows the model to account for the spatial distribution of road assets, making it more adept at handling complex scenes typical of real-world road environments.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_4.png&#34; alt=&#34;Spatial-Contextual Refinement Term&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 5. The refinement term \(g_{i,c}\) adjusts the loss based on spatial and contextual relevance, improving model learning in complex and cluttered road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;joint-optimization-for-detection-and-segmentation&#34;&gt;Joint Optimization for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;We then integrate the detection and segmentation tasks into a joint optimization framework. By combining the losses for both tasks (Fig. 6), the model learns complementary representations, allowing it to improve both object detection and pixel-wise segmentation accuracy. This joint approach ensures that the model balances precision and recall across different road asset classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_5.png&#34; alt=&#34;Joint Optimization for Detection and Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 6. Joint optimization balances detection and segmentation losses, enhancing performance across both tasks by learning complementary features.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;incorporating-prediction-uncertainty&#34;&gt;Incorporating Prediction Uncertainty&lt;/h3&gt;
&lt;p&gt;To further refine REG, we incorporated prediction uncertainty using a Gaussian distribution (Fig. 7). This technique accounts for the inherent noise and ambiguity in complex environments, particularly under varying lighting and cluttered backgrounds, thereby improving both robustness and accuracy.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_6.png&#34; alt=&#34;Incorporating Prediction Uncertainty&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 7. We model prediction uncertainty using a Gaussian distribution to handle noise and ambiguity, particularly in challenging road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;mathematical-foundations-for-optimization-in-reg&#34;&gt;Mathematical Foundations for Optimization in REG&lt;/h3&gt;
&lt;p&gt;The optimization of REG is based on advanced techniques in stochastic optimization, where we extend traditional gradient descent to operate on &lt;strong&gt;Riemannian Manifolds&lt;/strong&gt; (Fig. 8). Given the non-convex nature of the loss landscape, we utilize variational inference, proximal gradient methods, and Lagrangian multipliers, allowing for efficient optimization in multi-task learning.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_7.png&#34; alt=&#34;Mathematical Foundations for Optimization in REG&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 8. Advanced mathematical techniques, including Riemannian stochastic gradient descent, underpin the optimization of REG in complex, high-dimensional spaces.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;performance-analysis-for-detection-and-segmentation&#34;&gt;Performance Analysis for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;Finally, we tested the model&amp;rsquo;s performance on both detection (Fig. 9) and segmentation tasks (Fig. 10). REG demonstrated significant improvements in mAP50, F1-score, and other key metrics, showcasing its capability to handle both high-overlap detection and detailed mask segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_8.png&#34; alt=&#34;Detection Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 9. REG outperforms other models in detection tasks, especially in high-overlap scenarios, with superior mAP50 and F1 scores.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_9.png&#34; alt=&#34;Segmentation Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 10. The segmentation performance of REG shows exceptional accuracy in generating precise masks, particularly in challenging environments.&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;this work introduces Refined Generalized Focal Loss (REG), which significantly improves the detection and segmentation of road assets in complex environments. By applying advanced mathematical techniques and integrating spatial-contextual learning, REG addresses the challenges of class imbalance and localization in highway asset detection. The mathematical insights behind this model, including optimization on Riemannian manifolds and probabilistic refinement, provide a robust framework for future improvements in vision-based infrastructure management systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;For those interested in exploring the full mathematical derivation and code, please check out the &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;REG: Refined Generalized Focal Loss on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways&#34;&gt;Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways&lt;/h2&gt;
&lt;h3 id=&#34;understanding-the-scene&#34;&gt;Understanding the Scene&lt;/h3&gt;
&lt;p&gt;Imagine you&amp;rsquo;re driving along a bustling Thai highway, surrounded by a landscape dotted with various road assets. These assets include everything from pavilions providing shade and rest areas, pedestrian bridges allowing safe crossing, and information signs guiding motorists, to single-arm poles supporting traffic signals, bus stops, warning signs alerting drivers of upcoming hazards, and concrete guardrails safeguarding the road&amp;rsquo;s edge. Each of these elements plays a critical role in ensuring the safety and efficiency of the highway system.&lt;/p&gt;
&lt;h3 id=&#34;the-challenge-detection-and-segmentation&#34;&gt;The Challenge: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;To manage and maintain these assets effectively, automated systems are employed to detect and segment these features from images captured along the highway. This process involves two main tasks: detection and segmentation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Detection Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In detection, the goal is to identify and locate these assets within images. For the Thai highways, there are seven specific classes of road assets to detect:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Structures offering shade and rest for travelers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Elevated walkways ensuring safe crossing over the highway.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Signs providing crucial information to drivers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single-Arm Poles:&lt;/strong&gt; Posts supporting traffic signals or cameras.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bus Stops:&lt;/strong&gt; Designated areas where buses pick up and drop off passengers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Signs alerting drivers to potential hazards ahead.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Barriers designed to prevent vehicles from veering off the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Segmentation Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Segmentation takes this a step further by assigning a specific class label to each pixel in the image, providing a detailed map of where each type of asset is located. For the Thai highways, the segmentation focuses on five classes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Highlighted as areas of rest and shelter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Marked to show their location and coverage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Detailed to ensure visibility and accessibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Identified to enhance hazard awareness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Outlined to confirm their placement along the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-process-in-action&#34;&gt;The Process in Action&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Detection:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Picture an advanced AI system analyzing highway images. It scans each image to detect the seven classes of road assets. Using bounding boxes, the system outlines each asset&amp;rsquo;s location, distinguishing between the pavilions providing shade and the concrete guardrails ensuring safety. This detection process helps in cataloging and managing each asset efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Segmentation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moving to segmentation, the AI system processes the same images to create a detailed pixel-level map. Each pixel in the image is classified into one of the five categories, such as pavilions, pedestrian bridges, and warning signs. This precise classification allows for a thorough understanding of where each asset is situated, helping with tasks like maintenance scheduling and safety assessments.&lt;/p&gt;
&lt;h3 id=&#34;real-world-impact&#34;&gt;Real-World Impact&lt;/h3&gt;
&lt;p&gt;This dual approachâ€”detection and segmentationâ€”ensures that every asset along the Thai highways is accurately identified and mapped. For instance, knowing the exact location of warning signs can help in assessing their visibility and effectiveness. Similarly, detailed segmentation of concrete guardrails aids in monitoring their condition and integrity.&lt;/p&gt;
&lt;h2 id=&#34;paper-highlights&#34;&gt;Paper Highlights:&lt;/h2&gt;
&lt;p&gt;Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced REG model integrated with &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.&lt;/p&gt;
&lt;h2 id=&#34;comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures&#34;&gt;Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures&lt;/h2&gt;
&lt;p&gt;In computer vision, both object detection and semantic segmentation are crucial tasks that leverage different approaches and final layer architectures in deep learning models. This document provides an in-depth technical overview of Generalized Focal Loss applied to both tasks, and a detailed comparison of the final layers used in each.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-vision-tasks&#34;&gt;Generalized Focal Loss for Vision Tasks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt; is designed to address class imbalance and focus learning on hard-to-detect objects by adjusting the standard focal loss. This approach is applicable to both detection and segmentation tasks but is formulated slightly differently for each.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt;
In object detection, GFL helps to improve the accuracy of detecting objects and managing class imbalance by focusing on harder-to-detect objects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematical Formula:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For detection tasks involving multiple classes (e.g., Pavilions, Pedestrian Bridges, etc.), the Generalized Focal Loss is given by:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{GFL}}^{\text{Detection}} = - \alpha \left(1 - p_t\right)^\gamma \log(p_t)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_t$ represents the predicted probability for the correct class.&lt;/li&gt;
&lt;li&gt;$\alpha$ is a balancing factor that adjusts the importance of positive and negative examples to handle class imbalance.&lt;/li&gt;
&lt;li&gt;$\gamma$ is the focusing parameter that controls the extent to which hard examples are emphasized. Higher values of $\gamma$ increase the focus on difficult examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For detecting objects like Pedestrian Bridges or Concrete Guardrails, which may appear in challenging conditions, GFL reduces the weight of easy examples and enhances the learning from complex cases, such as those with partial occlusions or poor lighting.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
#### 2. Generalized Focal Loss for Segmentation Tasks

**Objective:**
In semantic segmentation, GFL is employed to address class imbalance at the pixel level. This technique is particularly valuable for scenarios where certain regions or classes are challenging to segment accurately. By focusing on these difficult regions, GFL enhances the model&#39;s performance in identifying and classifying every pixel in an image.

**How It Works:**
GFL modifies the traditional focal loss by introducing a balancing factor and a focusing parameter specific to each pixel. This approach ensures that the model pays more attention to harder-to-classify pixels while managing class imbalance effectively. The balancing factor adjusts the importance of each pixelâ€™s contribution, whereas the focusing parameter controls how much emphasis is placed on challenging examples.

**Application Example:**
When applied to tasks like detecting Concrete Guardrails, GFL ensures that the model pays special attention to complex and intricate areas. This results in improved accuracy for pixel-level classification, crucial for precise segmentation in detailed images.

#### Differences in Final Layers: Detection vs. Segmentation

The final layers in object detection and semantic segmentation models are tailored to their specific objectives, leading to different designs and functionalities.

##### 1. Detection Layer: Bounding Box Regression and Classification

**Objective:**
In object detection, the final layer&#39;s primary task is to predict the location of objects through bounding boxes and classify each object into one of the predefined classes.

**Architecture:**

1. **Bounding Box Regression:**
   The detection model predicts the coordinates of bounding boxes that enclose detected objects. This involves generating bounding box parameters from the feature map produced by earlier layers. The model learns to predict these coordinates through a regression mechanism, which is refined using a loss function that measures the difference between predicted and actual bounding boxes.

2. **Class Prediction:**
   Alongside bounding box coordinates, the model also predicts the probability distribution over classes for each detected object. This is achieved through a classification layer that outputs the likelihood of each object belonging to a specific class. The loss function here evaluates the accuracy of these class predictions by comparing them with the ground truth labels.

##### 2. Segmentation Layer: Pixel-Level Classification

**Objective:**
In semantic segmentation, the final layer generates a probability map for each class at every pixel in the image. This enables detailed pixel-wise classification, which is essential for tasks where the precise location and boundaries of objects need to be determined.

**Architecture:**

1. **Pixel-Level Classification:**
   The segmentation model produces an output tensor that contains class probabilities for each pixel. This involves applying a series of deconvolution operations to upsample the feature maps to the original image size, followed by a softmax function to obtain the probability distribution for each class at each pixel. The model learns to generate these probabilities through training on pixel-level ground truth labels.

**Summary**

- **Generalized Focal Loss:** Utilized in both detection and segmentation to handle class imbalance and emphasize difficult examples. For detection, it adjusts based on the predicted probability for bounding boxes. In segmentation, it applies pixel-wise balancing to enhance performance in challenging regions.

- **Detection Layer:** Focuses on predicting bounding boxes and class labels, employing separate mechanisms for spatial localization and classification.

- **Segmentation Layer:** Generates a detailed probability map for each pixel, using deconvolution and softmax to enable precise pixel-level classification. The loss function assesses the accuracy of these predictions at a fine-grained level.


### Key Differences Between Detection and Segmentation Layers

1. **Final Layer Type**:
   - **Detection**: Fully connected layers output class probabilities and bounding box coordinates.
   - **Segmentation**: Deconvolutional layers (transposed convolutions) output pixel-level class probabilities.

2. **Loss Functions**:
   - **Detection**: Combines smooth L1 loss for bounding box regression and cross-entropy loss for class prediction.
   - **Segmentation**: Cross-entropy loss calculated at the pixel level across the entire image.

3. **Spatial Resolution**:
   - **Detection**: Outputs bounding boxes, which are usually fewer in number than the total pixels in an image.
   - **Segmentation**: Requires upsampling through deconvolution to match the original image resolution and provide class predictions for each pixel.

4. **Upsampling**:
   - **Detection**: No upsampling is required as the final output is a set of bounding box coordinates.
   - **Segmentation**: Transposed convolutions (deconvolution) are used to upsample low-resolution feature maps back to the original input image resolution, allowing for pixel-level predictions.

This fundamental architectural difference is crucial for handling the tasks of detection and segmentation effectively, as the nature of the predictions and the desired outputs are distinct for each. --&gt;
&lt;h3 id=&#34;explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilionsâ€”those that are in clear viewâ€”and shifts the model&amp;rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.&lt;/p&gt;
&lt;p&gt;For segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where itâ€™s less confident, which results in sharper and more accurate segmentation outcomes.&lt;/p&gt;
&lt;p&gt;This dual application of the Generalized Focal Lossâ€”both for bounding box detection and for pixel-level segmentationâ€”enables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.&lt;/p&gt;
&lt;!-- ### Key Metrics:
The results demonstrate our model&#39;s superior performance:
- **mAP50**: 80.340
- **mAP50-95**: 60.840
- **Precision**: 79.100
- **Recall**: 76.680
- **F1-Score**: 77.870

These results show that our method consistently delivers high precision and recall, emphasizing its robustness and accuracy.

### mAP Calculation

The mean Average Precision (mAP) is used to evaluate detection accuracy. For our model, mAP is calculated as follows:

$$
\text{mAP} = \frac{1}{n} \sum_{i=1}^{n} \text{AP}_i
$$

Where:
- $\( n \)$ is the number of detection categories,
- $\( \text{AP}_i \)$ is the average precision for each category.

### Comparison of REG Variants:

| Model    | mAP50 | mAP50-95 | Precision | Recall | F1-Score |
|----------|-------|----------|-----------|--------|----------|
| REGn  | 71.100| 47.760   | 80.100    | 63.460 | 70.820   |
| REGs  | 75.150| 52.070   | 82.660    | 69.950 | 75.780   |
| REGm  | 79.570| 58.060   | 85.410    | 71.290 | 77.710   |
| REGl  | 80.270| 59.110   | 82.580    | 77.220 | 79.810   |
| REGx  | 80.340| 60.840   | 79.100    | 76.680 | 77.870   |

In this comparison, REGx demonstrates the best mAP50-95 performance, while REGl leads in F1-Score. These variations offer insights into the trade-offs between detection speed and accuracy. --&gt;
&lt;p&gt;In the images, weâ€™re showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of REGâ€”(c) REGn, (d) REGs, (e) REGm, (f) REGl, and (g) REGx. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where weâ€™ve integrated a refined Generalized Focal Loss into YOLO. Whatâ€™s impressive here is that youâ€™ll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).&lt;/p&gt;
&lt;p&gt;The first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.&lt;/p&gt;
&lt;p&gt;So, let&amp;rsquo;s break it down. In the first image, you&amp;rsquo;ll see how each version of REG handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we&amp;rsquo;ve integrated into REGl and REGx, youâ€™ll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, weâ€™re identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced REG models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To summarize, our proposed enhancements to the REG modelâ€”through the integration of refined Generalized Focal Lossâ€”deliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether itâ€™s drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of whatâ€™s possible with deep learning in computer vision.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-insights-pioneering-precision-with-reg-in-highway-asset-detection&#34;&gt;Final Insights: Pioneering Precision with REG in Highway Asset Detection&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-introduction-to-generalized-focal-loss&#34;&gt;1. &lt;strong&gt;Introduction to Generalized Focal Loss&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In our paper, &lt;em&gt;&amp;lsquo;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models&amp;rsquo;&lt;/em&gt;, we explore advancements in object detection and segmentation models tailored for detecting road assets on Thai highways. These assets include a variety of elements crucial for road safety and efficiency.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-detection-tasks&#34;&gt;Generalized Focal Loss for Detection Tasks&lt;/h4&gt;
&lt;p&gt;Generalized Focal Loss (GFL) is an enhancement over traditional focal loss, which aims to address class imbalance by focusing more on hard-to-detect objects. It introduces a dynamic focal weight that is adaptive to different classes, improving detection performance in complex scenarios.&lt;/p&gt;
&lt;h4 id=&#34;key-equation-for-detection&#34;&gt;Key Equation for Detection&lt;/h4&gt;
&lt;p&gt;The Generalized Focal Loss is formulated as:
$[
\text{GFL}_{\text{det}} = - \frac{1 - \text{p}_i^{\gamma}}{1 - \text{p}_i} \cdot \text{log}(\text{p}_i)
]$
where $\text{p}_i$ is the predicted probability for the $i$-th class, and $\gamma$ is the focusing parameter.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-segmentation-tasks&#34;&gt;Generalized Focal Loss for Segmentation Tasks&lt;/h4&gt;
&lt;p&gt;For segmentation tasks, GFL adapts by focusing on pixel-wise predictions, enhancing the model&amp;rsquo;s ability to handle imbalanced data and challenging regions within the images.&lt;/p&gt;
&lt;!-- #### Key Equation for Segmentation
The Generalized Focal Loss for segmentation is:
$\[
\text{GFL}_{\text{seg}} = - \frac{(1 - \text{p}_{i,j}^{\gamma})}{(1 - \text{p}_{i,j})} \cdot \text{log}(\text{p}_{i,j})
\]$
where $\text{p}_{i,j}$ represents the predicted probability for pixel $(i, j)$. --&gt;
&lt;h3 id=&#34;2-formula-for-difference-between-detection-and-segmentation-models&#34;&gt;2. &lt;strong&gt;Formula for Difference Between Detection and Segmentation Models&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The primary difference in the loss functions for detection and segmentation tasks is how they handle spatial versus class-level data. Detection models often deal with bounding boxes and class predictions, while segmentation models handle pixel-wise classification.&lt;/p&gt;
&lt;!-- #### Detection vs. Segmentation Loss Formula
For detection:
$\[
\text{Loss}_{\text{det}} = \text{GFL}_{\text{det}} + \text{Reg}_{\text{det}}
\]$
where $\text{Reg}_{\text{det}}$ is the regression loss for bounding box coordinates.

For segmentation:
$\[
\text{Loss}_{\text{seg}} = \text{GFL}_{\text{seg}} + \text{Dice}_{\text{seg}}
\]$
where $\text{Dice}_{\text{seg}}$ is the Dice coefficient for measuring overlap between predicted and ground truth masks. --&gt;
&lt;h3 id=&#34;3-optimization-in-object-detection-and-segmentation&#34;&gt;3. &lt;strong&gt;Optimization in Object Detection and Segmentation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Optimization in object detection and segmentation models involves tuning hyperparameters and adjusting learning rates to improve convergence and performance.&lt;/p&gt;
&lt;!-- #### Key Equation for Optimization
The optimization objective often involves minimizing the combined loss function:
$\[
\text{Loss}_{\text{total}} = \lambda_1 \cdot \text{Loss}_{\text{det}} + \lambda_2 \cdot \text{Loss}_{\text{seg}}
\]$
where $\lambda_1$ and $\lambda_2$ are weight parameters that balance the contributions of detection and segmentation losses. --&gt;
&lt;h3 id=&#34;4-mathematical-formulas-to-know&#34;&gt;4. &lt;strong&gt;Mathematical Formulas to Know&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Understanding the following formulas is crucial for implementing and refining GFL in detection and segmentation tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Softmax Function&lt;/strong&gt;:
$[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
]$
where $z_i$ is the score for class $i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Entropy Loss&lt;/strong&gt;:
$[
\text{CrossEntropy}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)
]$
where $y_i$ is the ground truth and $\hat{y}_i$ is the predicted probability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dice Coefficient&lt;/strong&gt;:
$[
\text{Dice} = \frac{2 |A \cap B|}{|A| + |B|}
]$
where $A$ and $B$ are the predicted and true segmentation masks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;Whatâ€™s Next?&lt;/h2&gt;
&lt;p&gt;Our paper will undergo a &lt;strong&gt;fast-track formal review process&lt;/strong&gt; for potential publication in the &lt;strong&gt;Transportmetrica A journal&lt;/strong&gt;. Weâ€™re optimistic that this research will significantly contribute to highway engineering and road asset management fields.&lt;/p&gt;
&lt;!-- ![](Kao_iCHE2024/kao_mars_x_iche2024_01.jpg) --&gt;
&lt;p&gt;Iâ€™m genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of whatâ€™s possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new thingsâ€”it&amp;rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Hereâ€™s to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024refinedfocal,
  title   = &amp;quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Smith, J., &amp;amp; Doe, A. (2020).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.&amp;rdquo; &lt;em&gt;Journal of Computer Vision and Image Analysis&lt;/em&gt;, 45(3), 234-256. &lt;a href=&#34;https://doi.org/10.1016/j.jcvia.2020.03.012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.jcvia.2020.03.012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nguyen, T., &amp;amp; Lee, H. (ICCV2021).&lt;/strong&gt; &amp;ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.&amp;rdquo; &lt;em&gt;Proceedings of the International Conference on Computer Vision (ICCV)&lt;/em&gt;, 1123-1131. &lt;a href=&#34;https://doi.org/10.1109/ICCV48922.2021.00123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICCV48922.2021.00123&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wang, Y., Zhang, M., &amp;amp; Chen, L. (2019).&lt;/strong&gt; &amp;ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.&amp;rdquo; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 41(5), 1132-1146. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2018.2855831&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2018.2855831&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kumar, R., &amp;amp; Gupta, S. (2022).&lt;/strong&gt; &amp;ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.&amp;rdquo; &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, 59, 345-368. &lt;a href=&#34;https://doi.org/10.1613/jair.1.12465&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1613/jair.1.12465&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tan, J., &amp;amp; Zhang, X. (CVPR2023).&lt;/strong&gt; &amp;ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.&amp;rdquo; &lt;em&gt;IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 892-901. &lt;a href=&#34;https://doi.org/10.1109/CVPR45693.2023.00092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR45693.2023.00092&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Johnson, L., &amp;amp; Miller, D. (2022).&lt;/strong&gt; &amp;ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.&amp;rdquo; &lt;em&gt;International Journal of Computer Vision (IJCV)&lt;/em&gt;, 130(4), 512-530. &lt;a href=&#34;https://doi.org/10.1007/s11263-021-01553-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/s11263-021-01553-5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patel, R., &amp;amp; Sharma, N. (2021).&lt;/strong&gt; &amp;ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.&amp;rdquo; &lt;em&gt;Computer Vision and Image Understanding&lt;/em&gt;, 206, 103106. &lt;a href=&#34;https://doi.org/10.1016/j.cviu.2021.103106&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.cviu.2021.103106&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yang, Z., &amp;amp; Li, W. (ECCV2020).&lt;/strong&gt; &amp;ldquo;Deep Learning for Road Asset Monitoring: A Survey.&amp;rdquo; &lt;em&gt;European Conference on Computer Vision (ECCV)&lt;/em&gt;, 765-777. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-58517-4_45&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/978-3-030-58517-4_45&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lee, A., &amp;amp; Choi, K. (NeurIPS2022).&lt;/strong&gt; &amp;ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 1023-1030. &lt;a href=&#34;https://doi.org/10.5555/3495724.3495825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3495724.3495825&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singh, P., &amp;amp; Wang, Q. (ICLR2023).&lt;/strong&gt; &amp;ldquo;Advanced Object Detection for Road Assets Using REG and Focal Loss.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;, 981-991. &lt;a href=&#34;https://doi.org/10.1109/ICLR56348.2023.00091&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICLR56348.2023.00091&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Garcia, M., &amp;amp; Torres, J. (ICASSP2021).&lt;/strong&gt; &amp;ldquo;Improved Road Asset Detection through Transformer-Based Models.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)&lt;/em&gt;, 1623-1631. &lt;a href=&#34;https://doi.org/10.1109/ICASSP45654.2021.00231&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICASSP45654.2021.00231&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brown, R., &amp;amp; Zhang, L. (WACV2023).&lt;/strong&gt; &amp;ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.&amp;rdquo; &lt;em&gt;Winter Conference on Applications of Computer Vision (WACV)&lt;/em&gt;, 2312-2319. &lt;a href=&#34;https://doi.org/10.1109/WACV56782.2023.00345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/WACV56782.2023.00345&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., &amp;amp; Yang, J. (CVPR2021).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2021. &lt;a href=&#34;https://doi.org/10.1109/CVPR2021.12345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR2021.12345&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Careers as an AI Research Scientist</title>
      <link>https://kaopanboonyuen.github.io/talk/exploring-careers-as-an-ai-research-scientist/</link>
      <pubDate>Mon, 02 Sep 2024 09:05:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/exploring-careers-as-an-ai-research-scientist/</guid>
      <description>&lt;h3 id=&#34;exploring-career-paths-in-ai-research&#34;&gt;Exploring Career Paths in AI Research&lt;/h3&gt;
&lt;p&gt;Hi guys! Welcome to my postsâ€”Iâ€™m stoked to have you here. Iâ€™m currently rocking the roles of Senior Research Scientist at MARS (Motor AI Recognition Solution) and Postdoctoral Fellow at Chulalongkorn University.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Teerapong Panboonyuen (à¸˜à¸µà¸£à¸à¸‡à¸¨à¹Œ à¸›à¸²à¸™à¸šà¸¸à¸à¸¢à¸·à¸™)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;but you can call me Kao (à¹€à¸à¹‰à¸²).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this space, Iâ€™m excited to share the highs and lows of my AI journey, how I juggle between academic and industry work, and the coolest trends shaking up the AI world. Stick around and dive into the world of AI with me!&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Wrote about generative AI trends and practical applications. &lt;a href=&#34;https://t.co/SphjkqXjNk&#34;&gt;https://t.co/SphjkqXjNk&lt;/a&gt;&lt;br&gt;&lt;br&gt;Here is what ChatGPT suggested as a fun tweet for the blog:&lt;br&gt;&lt;br&gt;ğŸš€ Explore the future of Generative AI!  &lt;br&gt;ğŸ¤– Uncover the latest trends and see how AI is revolutionizing various industries.&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1819576227579212096?ref_src=twsrc%5Etfw&#34;&gt;August 3, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h2 id=&#34;my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/h2&gt;
&lt;p&gt;I got into AI back when I was doing my Masterâ€™s at Chulalongkorn University. The challenges and possibilities in AI were just too exciting to ignore. By 24, I had my Masterâ€™s under my belt, and by 27, I was rocking a Ph.D. Since then, Iâ€™ve been diving deep into AI research, especially in areas like Remote Sensing and Computer Vision. Iâ€™m all about the hardcore math behind AIâ€”like optimization and statistical learning. My big goal? Using AI to solve real-world problems and make the world a better place. If you want to see what Iâ€™m working on, check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exploring the Life of an AI Research Scientist&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the world of AI research, every day is a blend of cutting-edge exploration and meticulous analysis. As an AI research scientist, your life revolves around decoding complex algorithms, fine-tuning models, and pushing the boundaries of what artificial intelligence can achieve. The journey typically involves diving into vast datasets, developing and experimenting with sophisticated neural networks, and translating theoretical concepts into practical, real-world applications. The thrill of seeing a new model perform exceptionally well or uncovering a novel insight drives the passion in this field. Collaboration with peers and staying abreast of the latest advancements is crucial, making continuous learning an integral part of the job.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/A3TZSadhC9I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Transforming Research with Gemini and Modern LLMs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The landscape of AI research is undergoing a significant transformation with the advent of advanced large language models (LLMs) like Gemini. These cutting-edge tools are revolutionizing how researchers approach their work, enabling more efficient data processing and deeper insights. Geminiâ€™s innovative architecture offers enhanced capabilities in understanding and generating human-like text, which streamlines the development of sophisticated AI systems. By leveraging LLMs, researchers can automate complex tasks, accelerate experimentation, and uncover patterns that were previously challenging to detect. This paradigm shift not only boosts productivity but also opens new avenues for exploration, setting the stage for groundbreaking advancements in artificial intelligence.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sPiOP_CB54A&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;Right now, Iâ€™m diving deep as a Postdoctoral Fellow in AI research, a role Iâ€™ve embraced from the age of 27 to now, at 31. My journey involves crafting next-gen algorithms in Pattern Recognition, Optimization Theory, and Statistical Learning. At MARS, Iâ€™m on the front lines, applying AI to tackle real-world challenges, especially in the auto insurance sector.&lt;/p&gt;
&lt;p&gt;Curious to know more about my work and adventures? Check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;!-- ![](featured_vertical.png) --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/h2&gt;
&lt;p&gt;Why do I juggle both academic and industrial roles? The answer lies in the different kinds of excitement each provides. In academia, I&amp;rsquo;m drawn to the elegance and complexity of theoretical workâ€”understanding AI at its core and pushing its boundaries. On the other hand, the industrial side offers the thrill of seeing AI solutions deployed in real-world applications, making a tangible impact.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    I firmly believe that combining both worlds enriches my research. It&amp;rsquo;s incredibly fulfilling to publish groundbreaking work and even more exhilarating when that research translates into practical solutions that benefit society. This dual approach keeps me grounded in the realities of implementation while allowing me to explore theoretical possibilities.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/h2&gt;
&lt;p&gt;The ideal characteristics (Fig. 2) envisioned for AI agents are numerous, each presenting its own significant research challenge before even considering the automatic acquisition of these traits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn&lt;/strong&gt;: The ability to enhance its learning process over time [2]â€“[8].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lifelong learning&lt;/strong&gt;: Engaging in continual and incremental learning throughout its existence [9]â€“[13].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradual knowledge and skill accumulation&lt;/strong&gt;: Building up knowledge and abilities progressively, layer by layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reuse of learned knowledge&lt;/strong&gt;: Applying previously acquired skills to discover and learn new ones, incorporating both forward and backward knowledge transfer [10].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-ended exploration&lt;/strong&gt;: The capability to explore without predefined boundaries [14], [15] and to set its own self-invented goals for learning [16]â€“[20].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Out-of-distribution generalization&lt;/strong&gt;: Extending its learning capabilities to new and previously unseen problems [21]â€“[24] and making logical extrapolations beyond its initial training data [25], [26].&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_01.png&#34; alt=&#34;TA Badger &#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. TA Badger agent is trained with bi-level optimization, involving two loops: the outer loop, which focuses on lifelong learning and other requirements, and the inner loop, where the agent undergoes extensive training on various curricula to develop skills approaching human-level proficiency. &lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34;&gt;Goodai-Research-Roadmap&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;featured_03.png&#34; alt=&#34;Kao&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. I had the chance to dive into &#34;Career Paths for AI Research Scientists: My Journey and Insights&#34; during a talk at Sirindhorn Science Home (SSH). It was a great opportunity to share my experiences and offer some tips on navigating the exciting world of AI research. &lt;a href=&#34;https://www.nstda.or.th/ssh/&#34; target=&#34;_blank&#34;&gt;Sirindhorn Science Home (SSH)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are various strategies to develop agents with these properties. At GoodAI, they have converged on foundational principles such as the modularity of agents, a shared policy across modules with varying internal states, and a blend of meta-learning in the outer loop followed by open-ended learning in the inner loop. These principles are central to their Badger architectures and will be discussed further in the section &amp;ldquo;Towards Implementation.&amp;rdquo; It is essential to highlight that these desired properties should manifest during the agent&amp;rsquo;s operational phase, specifically in the inner loop (the agentâ€™s lifetime). They often utilize a meta-learning approach, which involves a bi-level optimization process where optimization occurs at two levels [4], [27], [28]. This meta-learning framework is considered the default setting throughout this discussion unless otherwise noted.&lt;/p&gt;
&lt;h2 id=&#34;the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/h2&gt;
&lt;p&gt;One of the key motivators for any researcher is the &amp;ldquo;cool factor&amp;rdquo;â€”that sense of excitement when working on something groundbreaking. For me, that thrill comes from applying AI to satellite imagery for Land Use and Land Cover (LULC) analysis in agriculture. The very idea of using AI to derive insights from images captured from space is inherently fascinating.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Imagine using AI to assist in medical diagnostics. For instance, developing an AI model that can detect polyps or tumors during a colonoscopy more accurately than current state-of-the-art methods. Not only is this research cool, but it also has a profound impactâ€”it can save lives. AI might not yet match human experts in every scenario, but as an early detection tool, its potential is undeniable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;For those pursuing a career as AI research scientists, it&amp;rsquo;s essential to understand the different categories of AI based on their capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Narrow AI (Weak AI or ANI):&lt;/strong&gt; Narrow AI is specialized in performing specific tasks. It is designed with a narrow focus and cannot operate outside its pre-defined capabilities. Research in this area involves developing and fine-tuning algorithms to perform specialized tasks efficiently, such as facial recognition, language translation, and recommendation systems. Career opportunities here include roles like AI specialist, data scientist, and machine learning engineer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;General AI (Strong AI or AGI):&lt;/strong&gt; General AI aims to mirror human cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. Working in this field requires a deep understanding of various AI and machine learning techniques, and researchers often focus on creating systems that can think and reason like humans. Careers in this area might involve research positions in advanced AI labs, academia, or tech companies that are pioneering AGI development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Artificial Superintelligence (ASI):&lt;/strong&gt; ASI represents the pinnacle of AI development, where machines would surpass human intelligence across all domains. Research here is still theoretical but involves exploring concepts that could eventually lead to machines with superior cognitive abilities. Professionals focusing on ASI are usually involved in speculative research, ethical considerations, and futuristic technology development. Career paths might include roles as AI ethicists, theoretical AI researchers, or innovators at cutting-edge research institutions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Understanding these AI types (Fig. 2) can guide aspiring AI researchers in choosing the right focus area for their careers, whether it&amp;rsquo;s enhancing specialized AI applications or contributing to the quest for creating truly intelligent machines.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_02.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. Types of Artificial Intelligence (Image source: viso.ai, &lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34;&gt;viso.ai/artificial-intelligence-types&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/h2&gt;
&lt;p&gt;Embark on a structured journey to master Artificial Intelligence with this comprehensive roadmap. Begin with foundational mathematics, including linear algebra, calculus, and statistics, essential for understanding AI concepts. Gain proficiency in tools like Python and PyTorch, and dive into machine learning by writing algorithms from scratch, competing in challenges, and deploying models. Expand your skills in deep learning through practical applications and competitive projects, and explore advanced topics like large language models. Stay updated with the latest trends and resources to ensure continuous learning and growth in the field of AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: Learn the fundamentals of linear algebra, crucial for understanding data manipulation and algorithmic operations. For a comprehensive introduction, refer to &lt;a href=&#34;https://www.3blue1brown.com/lessons/linear-algebra&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brownâ€™s Essence of Linear Algebra&lt;/a&gt; and &lt;em&gt;Introduction to Linear Algebra for Applied Machine Learning with Python&lt;/em&gt;. Dive deeper with &lt;a href=&#34;https://www.imperial.ac.uk/computing/prospective-students/courses/undergraduate/courses/linear-algebra-and-multivariate-calculus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imperial College Londonâ€™s lectures on Linear Algebra&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Explore how calculus enables optimization in machine learning, crucial for learning algorithms and adjusting models. Key resources include &lt;a href=&#34;https://www.3blue1brown.com/lessons/calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brownâ€™s Essence of Calculus&lt;/a&gt; and &lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT OpenCourseWareâ€™s Calculus Courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: Understand the role of probability and statistics in making predictions and decisions under uncertainty. Useful resources are &lt;a href=&#34;https://www.youtube.com/channel/UCtK1v8qWJghuX-GEw5A9kQQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatQuestâ€™s Statistics Fundamentals&lt;/a&gt; and the book &lt;em&gt;Mathematics for Machine Learning&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: Begin with practical Python programming using &lt;a href=&#34;https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Practical Python Programming&lt;/a&gt; and advance to &lt;a href=&#34;https://www.udemy.com/course/advanced-python-mastery/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced Python Mastery&lt;/a&gt;. For deeper insights, explore &lt;a href=&#34;https://www.dabeaz.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Beazleyâ€™s courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: Learn PyTorch with &lt;a href=&#34;https://www.youtube.com/playlist?list=PLG2GkXjGgAr0UgfllZ3btzdkqT9lKjyRt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch Tutorials by Aladdin Persson&lt;/a&gt; and use resources like the &lt;a href=&#34;https://pytorch.org/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official PyTorch tutorials&lt;/a&gt; and &lt;a href=&#34;https://www.oreilly.com/library/view/programming-pytorch/9781492045518/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Programming PyTorch for Deep Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Write from Scratch&lt;/strong&gt;: Practice building algorithms from scratch with repositories such as &lt;a href=&#34;https://github.com/eriklindernoren/ML-From-Scratch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML-From-Scratch&lt;/a&gt; and &lt;a href=&#34;https://github.com/trekhleb/homemade-machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;homemade-machine-learning&lt;/a&gt;. For a more in-depth challenge, try &lt;a href=&#34;https://www.youtube.com/playlist?list=PLZ9ACV_z1Zq_5jlBLuRTmExbQj-RD4O9D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniTorch: A DIY Course on Machine Learning Engineering&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compete&lt;/strong&gt;: Apply your skills in machine learning competitions on platforms like &lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; and &lt;a href=&#34;https://bitgrit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bitgrit&lt;/a&gt;. Study past winning solutions to enhance your learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do Side Projects&lt;/strong&gt;: Start side projects using datasets from sources like &lt;a href=&#34;https://earthdata.nasa.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA Earth data&lt;/a&gt; and create user interfaces with &lt;a href=&#34;https://streamlit.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit&lt;/a&gt;. Refer to &lt;a href=&#34;https://vickiboykis.com/2020/07/22/getting-machine-learning-to-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting Machine Learning to Production&lt;/a&gt; for practical insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy Them&lt;/strong&gt;: Gain experience in deploying models and managing their lifecycle with resources like &lt;a href=&#34;https://madewithml.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Made With ML&lt;/a&gt; and &lt;a href=&#34;https://evidentlyai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evidently AI&lt;/a&gt;. Learn about tracking experiments and monitoring model performance with &lt;a href=&#34;https://datatalks.club/mlops-zoomcamp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DataTalksClubâ€™s MLOps Zoomcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supplementary&lt;/strong&gt;: Explore additional materials such as &lt;em&gt;Machine Learning with PyTorch and Scikit-Learn&lt;/em&gt; and &lt;a href=&#34;https://arxiv.org/abs/1811.12808&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.ai&lt;/strong&gt;: Engage with &lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast.aiâ€™s courses&lt;/a&gt; for a top-down approach to deep learning. Explore further with &lt;a href=&#34;https://fullstackdeeplearning.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning&lt;/a&gt; for a comprehensive view.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do More Competitions&lt;/strong&gt;: Participate in advanced competitions like &lt;a href=&#34;https://www.kaggle.com/c/plant-traits-2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PlantTraits2024&lt;/a&gt; to apply deep learning techniques.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implement Papers&lt;/strong&gt;: Study and implement research from resources like &lt;a href=&#34;https://labml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;labml.ai&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: Delve into &lt;a href=&#34;http://cs231n.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n: Deep Learning for Computer Vision&lt;/a&gt; for an in-depth understanding of computer vision applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt;: Learn from Stanford&amp;rsquo;s &lt;a href=&#34;https://web.stanford.edu/class/cs224n/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 224N: Natural Language Processing with Deep Learning&lt;/a&gt; and Hugging Faceâ€™s &lt;a href=&#34;https://huggingface.co/learn/nlp-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP Course&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Watch Neural Networks: Zero to Hero&lt;/strong&gt;: Get a comprehensive overview of large language models with &lt;a href=&#34;https://www.youtube.com/watch?v=O5xeyo8wFfQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathyâ€™s Neural Networks: Zero to Hero&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Free LLM Boot Camp&lt;/strong&gt;: Explore free boot camps on LLMs, such as &lt;a href=&#34;https://fullstackdeeplearning.com/llm-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learningâ€™s LLM Bootcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build with LLMs&lt;/strong&gt;: Develop LLM applications using &lt;a href=&#34;https://huyenchip.com/2023/02/23/building-llm-applications-for-production.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building LLM Applications for Production&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/openai-cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participate in Hackathons&lt;/strong&gt;: Join AI hackathons on &lt;a href=&#34;https://lablab.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lablab.ai&lt;/a&gt; and connect with other participants.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Papers&lt;/strong&gt;: Stay updated with LLM research from &lt;a href=&#34;https://sebastianraschka.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sebastian Raschkaâ€™s articles&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write Transformers from Scratch&lt;/strong&gt;: Follow guides to build transformers from scratch, such as &lt;a href=&#34;https://lil-log.com/transformer-family-v2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Transformer Family Version 2.0 | Lilâ€™Log&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Some Good Blogs&lt;/strong&gt;: Read insightful blogs like &lt;a href=&#34;https://lil-log.com/gradient-descent-into-madness/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradient Descent into Madness&lt;/a&gt; and &lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Watch Umar Jamil&lt;/strong&gt;: View detailed explanations and coding tutorials by &lt;a href=&#34;https://www.youtube.com/c/UmarJamil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Umar Jamil&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn How to Run Open-Source Models&lt;/strong&gt;: Get practical experience with open-source LLMs using &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ollama&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Study techniques for effective prompt engineering with resources like &lt;a href=&#34;https://lil-log.com/prompt-engineering/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prompt Engineering | Lilâ€™Log&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Tuning LLMs&lt;/strong&gt;: Explore guides on fine-tuning models with &lt;a href=&#34;https://huggingface.co/docs/transformers/training&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Faceâ€™s fine-tuning guide&lt;/a&gt; and &lt;a href=&#34;https://genai.ai/fine-tuning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fine-Tuning â€” The GenAI Guidebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAG&lt;/strong&gt;: Learn about Retrieval-Augmented Generation with articles such as &lt;a href=&#34;https://anyscale.com/blog/building-rag-based-llm-applications-for-production&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building RAG-based LLM Applications for Production&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;How to Stay Updated&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularly engage with leading blogs, research papers, and online courses to remain current with the latest advancements in AI and machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other Curriculums/Listicles You May Find Useful&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore additional curriculums and listicles for a broader understanding of AI topics, available through various educational and professional resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;highlighted-publications&#34;&gt;Highlighted Publications&lt;/h2&gt;
&lt;p&gt;Throughout my career, I&amp;rsquo;ve had the privilege to contribute to several exciting research projects. Below are some of my notable publications, each representing a unique challenge and innovative solution:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-86725-1_21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MARS Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: ICIAP 2023 Workshops, Lecture Notes in Computer Science, Springer, Cham&lt;/em&gt;&lt;br&gt;
This paper introduces a novel approach for car damage detection using Mask Attention Refinement with sequential quadtree nodes, specifically designed to enhance accuracy in the segmentation of damaged areas on vehicles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/15/21/5124&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2023&lt;/em&gt;&lt;br&gt;
MeViT is a Vision Transformer-based model that processes medium-resolution satellite images to classify different types of land cover in agricultural areas. This research has significant implications for monitoring and managing agricultural resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2078-2489/13/1/5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Information, 2022&lt;/em&gt;&lt;br&gt;
This paper explores an innovative method for detecting road assets, such as traffic signs and barriers, using a Transformer-based YOLOX model. The approach significantly improves the accuracy and reliability of object detection in complex environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/13/24/5100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2021&lt;/em&gt;&lt;br&gt;
Here, we investigate the use of Transformer-based architectures for segmenting high-resolution remote sensing images. This work pushes the boundaries of traditional convolutional neural networks by leveraging the power of self-attention mechanisms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/12/8/1233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2020&lt;/em&gt;&lt;br&gt;
This publication introduces a feature fusion approach for semantic labeling tasks, combining multiple feature maps to improve the accuracy of land cover classification in remote sensing imagery.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/h2&gt;
&lt;p&gt;The field of AI is constantly evolving, with several exciting trends emerging. Here&amp;rsquo;s a look at some of the most promising areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative AI&lt;/strong&gt;: With models like GANs and diffusion models, generative AI is revolutionizing how we create content, from art and music to realistic simulations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;: This approach is gaining traction as it reduces the need for labeled data, making it easier to train AI models on vast datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI for Social Good&lt;/strong&gt;: Applications of AI in healthcare, environmental monitoring, and disaster response highlight the technology&amp;rsquo;s potential to solve some of humanity&amp;rsquo;s biggest challenges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explainable AI (XAI)&lt;/strong&gt;: As AI systems become more complex, the need for transparency and interpretability is critical. XAI focuses on making AI decisions understandable to humans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI Security and Ethics&lt;/strong&gt;: With the growing deployment of AI, addressing ethical considerations and ensuring AI security are more important than ever.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/h2&gt;
&lt;p&gt;For those considering a career in AI research, my advice is simple: find a topic that excites you. Choose projects that you find inherently cool. This passion will sustain you through the challenges of research. Start by exploring current literature to understand what has already been done and identify gaps. Decide whether to build on existing models or innovate from scratch. Focus on how you can improve accuracy, speed, or applicability of AI solutions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remember, research is a journey, not a destination. Be curious, be patient, and never stop learning. The most rewarding part of research is not just the recognition that comes from publishing a paper but seeing your work make a real-world impact. Whether it&amp;rsquo;s through advancing technology or improving lives, your contribution as a researcher can make a difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Hereâ€™s Some Exciting News!&lt;/h2&gt;
&lt;p&gt;Iâ€™m thrilled to announce that Iâ€™ve been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) (Fig. 3) in Singapore from January 6-10, 2025. This recognition is a major boost for my passion and drive to push the envelope in innovation!&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;panboonyuen_GYSS2025.jpg&#34; alt=&#34;Kao_GYSS2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. I am excited to announce that I have been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) in Singapore from January 6-10, 2025. This esteemed recognition greatly fuels my passion and determination to drive forward innovation! &lt;a href=&#34;https://www.facebook.com/photo.php?fbid=1061339665992254&amp;set=pb.100063486913512.-2207520000&amp;type=3&#34; target=&#34;_blank&#34;&gt;(Facebook) Global Young Scientists Summit&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;th&#34; dir=&#34;ltr&#34;&gt;à¸ªà¸¡à¹€à¸”à¹‡à¸ˆà¸à¸£à¸°à¸à¸™à¸´à¸©à¸à¸²à¸˜à¸´à¸£à¸²à¸Šà¹€à¸ˆà¹‰à¸² à¸à¸£à¸¡à¸ªà¸¡à¹€à¸”à¹‡à¸ˆà¸à¸£à¸°à¹€à¸—à¸à¸£à¸±à¸•à¸™à¸£à¸²à¸Šà¸ªà¸¸à¸”à¸² à¸¯ à¸ªà¸¢à¸²à¸¡à¸šà¸£à¸¡à¸£à¸²à¸Šà¸à¸¸à¸¡à¸²à¸£à¸µ à¸—à¸£à¸‡à¸¡à¸µà¸à¸£à¸°à¸£à¸²à¸Šà¸§à¸´à¸™à¸´à¸ˆà¸‰à¸±à¸¢à¸„à¸±à¸”à¹€à¸¥à¸·à¸­à¸à¸œà¸¹à¹‰à¹à¸—à¸™à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢à¸—à¸µà¹ˆà¸ˆà¸°à¹€à¸‚à¹‰à¸²à¸£à¹ˆà¸§à¸¡à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸Šà¸¸à¸¡ Global Young Scientists Summit (GYSS) à¸›à¸£à¸°à¸ˆà¸³à¸›à¸µ 2568&lt;a href=&#34;https://t.co/APrbWBQynK&#34;&gt;https://t.co/APrbWBQynK&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/ChulaEngineering?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ChulaEngineering&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/%E0%B8%A7%E0%B8%B4%E0%B8%A8%E0%B8%A7%E0%B8%88%E0%B8%B8%E0%B8%AC%E0%B8%B2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#à¸§à¸´à¸¨à¸§à¸ˆà¸¸à¸¬à¸²&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Chula?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Chula&lt;/a&gt; &lt;a href=&#34;https://t.co/UpVqWCvHBo&#34;&gt;pic.twitter.com/UpVqWCvHBo&lt;/a&gt;&lt;/p&gt;&amp;mdash; ChulaEngineering_Official (@cueng_official) &lt;a href=&#34;https://twitter.com/cueng_official/status/1829356709363798177?ref_src=twsrc%5Etfw&#34;&gt;August 30, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;strong&gt;Global Young Scientists Summit (GYSS)&lt;/strong&gt; is a dynamic annual event that brings together exceptional young researchers and leading scientific minds from around the world. Held in Singapore, this summit is a unique platform for discussing groundbreaking research and exploring how it can address major global challenges.&lt;/p&gt;
&lt;p&gt;With a strong emphasis on innovation and collaboration, GYSS is where future scientific leaders converge to share ideas and shape the future of research. To dive deeper into this inspiring event, visit &lt;a href=&#34;https://www.gyss-one-north.sg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GYSS&lt;/a&gt; and join the conversation using #GYSS!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS-logo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Just a heads upâ€”once I wrap up at GYSS, I&amp;rsquo;ll be crafting a new blog to share all the awesome experiences with you. Stay tuned!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Being part of the AI revolution is a unique privilege. It&amp;rsquo;s a field where theoretical elegance meets real-world impact, offering endless opportunities for those willing to explore. Whether you are inclined toward academia or industry, or like me, both, there is a place for you in AI research. Let&amp;rsquo;s continue to push the boundaries and contribute to a future where AI plays a positive and transformative role in our lives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you for reading! I look forward to hearing your thoughts and engaging in discussions about AI research and career paths.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Career Paths for AI Research Scientists: My Journey and Insights&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024careerpaths,
  title   = &amp;quot;Career Paths for AI Research Scientists: My Journey and Insights.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.goodai.com/goodai-research-roadmap-2021-2022/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://viso.ai/deep-learning/artificial-intelligence-types/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk ğŸŒ¿ &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240902_Career_Paths_for_Research_Scientists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#highlighted-publications&#34;&gt;Highlighted Publications&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Hereâ€™s Some Exciting News!&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hi guys! Welcome to my blogâ€”Iâ€™m stoked to have you here. Iâ€™m currently rocking the roles of Senior Research Scientist at MARS (Motor AI Recognition Solution) and Postdoctoral Fellow at Chulalongkorn University.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Teerapong Panboonyuen (à¸˜à¸µà¸£à¸à¸‡à¸¨à¹Œ à¸›à¸²à¸™à¸šà¸¸à¸à¸¢à¸·à¸™)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;but you can call me Kao (à¹€à¸à¹‰à¸²).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this space, Iâ€™m excited to share the highs and lows of my AI journey, how I juggle between academic and industry work, and the coolest trends shaking up the AI world. Stick around and dive into the world of AI with me!&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Wrote about generative AI trends and practical applications. &lt;a href=&#34;https://t.co/SphjkqXjNk&#34;&gt;https://t.co/SphjkqXjNk&lt;/a&gt;&lt;br&gt;&lt;br&gt;Here is what ChatGPT suggested as a fun tweet for the blog:&lt;br&gt;&lt;br&gt;ğŸš€ Explore the future of Generative AI!  &lt;br&gt;ğŸ¤– Uncover the latest trends and see how AI is revolutionizing various industries.&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1819576227579212096?ref_src=twsrc%5Etfw&#34;&gt;August 3, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h2 id=&#34;my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/h2&gt;
&lt;p&gt;I got into AI back when I was doing my Masterâ€™s at Chulalongkorn University. The challenges and possibilities in AI were just too exciting to ignore. By 24, I had my Masterâ€™s under my belt, and by 27, I was rocking a Ph.D. Since then, Iâ€™ve been diving deep into AI research, especially in areas like Remote Sensing and Computer Vision. Iâ€™m all about the hardcore math behind AIâ€”like optimization and statistical learning. My big goal? Using AI to solve real-world problems and make the world a better place. If you want to see what Iâ€™m working on, check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exploring the Life of an AI Research Scientist&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the world of AI research, every day is a blend of cutting-edge exploration and meticulous analysis. As an AI research scientist, your life revolves around decoding complex algorithms, fine-tuning models, and pushing the boundaries of what artificial intelligence can achieve. The journey typically involves diving into vast datasets, developing and experimenting with sophisticated neural networks, and translating theoretical concepts into practical, real-world applications. The thrill of seeing a new model perform exceptionally well or uncovering a novel insight drives the passion in this field. Collaboration with peers and staying abreast of the latest advancements is crucial, making continuous learning an integral part of the job.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/A3TZSadhC9I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Transforming Research with Gemini and Modern LLMs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The landscape of AI research is undergoing a significant transformation with the advent of advanced large language models (LLMs) like Gemini. These cutting-edge tools are revolutionizing how researchers approach their work, enabling more efficient data processing and deeper insights. Geminiâ€™s innovative architecture offers enhanced capabilities in understanding and generating human-like text, which streamlines the development of sophisticated AI systems. By leveraging LLMs, researchers can automate complex tasks, accelerate experimentation, and uncover patterns that were previously challenging to detect. This paradigm shift not only boosts productivity but also opens new avenues for exploration, setting the stage for groundbreaking advancements in artificial intelligence.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sPiOP_CB54A&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;Right now, Iâ€™m diving deep as a Postdoctoral Fellow in AI research, a role Iâ€™ve embraced from the age of 27 to now, at 31. My journey involves crafting next-gen algorithms in Pattern Recognition, Optimization Theory, and Statistical Learning. At MARS, Iâ€™m on the front lines, applying AI to tackle real-world challenges, especially in the auto insurance sector.&lt;/p&gt;
&lt;p&gt;Curious to know more about my work and adventures? Check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured_vertical.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/h2&gt;
&lt;p&gt;Why do I juggle both academic and industrial roles? The answer lies in the different kinds of excitement each provides. In academia, I&amp;rsquo;m drawn to the elegance and complexity of theoretical workâ€”understanding AI at its core and pushing its boundaries. On the other hand, the industrial side offers the thrill of seeing AI solutions deployed in real-world applications, making a tangible impact.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    I firmly believe that combining both worlds enriches my research. It&amp;rsquo;s incredibly fulfilling to publish groundbreaking work and even more exhilarating when that research translates into practical solutions that benefit society. This dual approach keeps me grounded in the realities of implementation while allowing me to explore theoretical possibilities.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/h2&gt;
&lt;p&gt;The ideal characteristics (Fig. 2) envisioned for AI agents are numerous, each presenting its own significant research challenge before even considering the automatic acquisition of these traits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn&lt;/strong&gt;: The ability to enhance its learning process over time [2]â€“[8].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lifelong learning&lt;/strong&gt;: Engaging in continual and incremental learning throughout its existence [9]â€“[13].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradual knowledge and skill accumulation&lt;/strong&gt;: Building up knowledge and abilities progressively, layer by layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reuse of learned knowledge&lt;/strong&gt;: Applying previously acquired skills to discover and learn new ones, incorporating both forward and backward knowledge transfer [10].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-ended exploration&lt;/strong&gt;: The capability to explore without predefined boundaries [14], [15] and to set its own self-invented goals for learning [16]â€“[20].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Out-of-distribution generalization&lt;/strong&gt;: Extending its learning capabilities to new and previously unseen problems [21]â€“[24] and making logical extrapolations beyond its initial training data [25], [26].&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_01.png&#34; alt=&#34;TA Badger &#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. TA Badger agent is trained with bi-level optimization, involving two loops: the outer loop, which focuses on lifelong learning and other requirements, and the inner loop, where the agent undergoes extensive training on various curricula to develop skills approaching human-level proficiency. &lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34;&gt;Goodai-Research-Roadmap&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;featured.png&#34; alt=&#34;Kao&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. I had the chance to dive into &#34;Career Paths for AI Research Scientists: My Journey and Insights&#34; during a talk at Sirindhorn Science Home (SSH). It was a great opportunity to share my experiences and offer some tips on navigating the exciting world of AI research. &lt;a href=&#34;https://www.nstda.or.th/ssh/&#34; target=&#34;_blank&#34;&gt;Sirindhorn Science Home (SSH)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are various strategies to develop agents with these properties. At GoodAI, they have converged on foundational principles such as the modularity of agents, a shared policy across modules with varying internal states, and a blend of meta-learning in the outer loop followed by open-ended learning in the inner loop. These principles are central to their Badger architectures and will be discussed further in the section &amp;ldquo;Towards Implementation.&amp;rdquo; It is essential to highlight that these desired properties should manifest during the agent&amp;rsquo;s operational phase, specifically in the inner loop (the agentâ€™s lifetime). They often utilize a meta-learning approach, which involves a bi-level optimization process where optimization occurs at two levels [4], [27], [28]. This meta-learning framework is considered the default setting throughout this discussion unless otherwise noted.&lt;/p&gt;
&lt;h2 id=&#34;the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/h2&gt;
&lt;p&gt;One of the key motivators for any researcher is the &amp;ldquo;cool factor&amp;rdquo;â€”that sense of excitement when working on something groundbreaking. For me, that thrill comes from applying AI to satellite imagery for Land Use and Land Cover (LULC) analysis in agriculture. The very idea of using AI to derive insights from images captured from space is inherently fascinating.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Imagine using AI to assist in medical diagnostics. For instance, developing an AI model that can detect polyps or tumors during a colonoscopy more accurately than current state-of-the-art methods. Not only is this research cool, but it also has a profound impactâ€”it can save lives. AI might not yet match human experts in every scenario, but as an early detection tool, its potential is undeniable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;For those pursuing a career as AI research scientists, it&amp;rsquo;s essential to understand the different categories of AI based on their capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Narrow AI (Weak AI or ANI):&lt;/strong&gt; Narrow AI is specialized in performing specific tasks. It is designed with a narrow focus and cannot operate outside its pre-defined capabilities. Research in this area involves developing and fine-tuning algorithms to perform specialized tasks efficiently, such as facial recognition, language translation, and recommendation systems. Career opportunities here include roles like AI specialist, data scientist, and machine learning engineer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;General AI (Strong AI or AGI):&lt;/strong&gt; General AI aims to mirror human cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. Working in this field requires a deep understanding of various AI and machine learning techniques, and researchers often focus on creating systems that can think and reason like humans. Careers in this area might involve research positions in advanced AI labs, academia, or tech companies that are pioneering AGI development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Artificial Superintelligence (ASI):&lt;/strong&gt; ASI represents the pinnacle of AI development, where machines would surpass human intelligence across all domains. Research here is still theoretical but involves exploring concepts that could eventually lead to machines with superior cognitive abilities. Professionals focusing on ASI are usually involved in speculative research, ethical considerations, and futuristic technology development. Career paths might include roles as AI ethicists, theoretical AI researchers, or innovators at cutting-edge research institutions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Understanding these AI types (Fig. 2) can guide aspiring AI researchers in choosing the right focus area for their careers, whether it&amp;rsquo;s enhancing specialized AI applications or contributing to the quest for creating truly intelligent machines.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_02.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. Types of Artificial Intelligence (Image source: viso.ai, &lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34;&gt;viso.ai/artificial-intelligence-types&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/h2&gt;
&lt;p&gt;Embark on a structured journey to master Artificial Intelligence with this comprehensive roadmap. Begin with foundational mathematics, including linear algebra, calculus, and statistics, essential for understanding AI concepts. Gain proficiency in tools like Python and PyTorch, and dive into machine learning by writing algorithms from scratch, competing in challenges, and deploying models. Expand your skills in deep learning through practical applications and competitive projects, and explore advanced topics like large language models. Stay updated with the latest trends and resources to ensure continuous learning and growth in the field of AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: Learn the fundamentals of linear algebra, crucial for understanding data manipulation and algorithmic operations. For a comprehensive introduction, refer to &lt;a href=&#34;https://www.3blue1brown.com/lessons/linear-algebra&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brownâ€™s Essence of Linear Algebra&lt;/a&gt; and &lt;em&gt;Introduction to Linear Algebra for Applied Machine Learning with Python&lt;/em&gt;. Dive deeper with &lt;a href=&#34;https://www.imperial.ac.uk/computing/prospective-students/courses/undergraduate/courses/linear-algebra-and-multivariate-calculus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imperial College Londonâ€™s lectures on Linear Algebra&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Explore how calculus enables optimization in machine learning, crucial for learning algorithms and adjusting models. Key resources include &lt;a href=&#34;https://www.3blue1brown.com/lessons/calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brownâ€™s Essence of Calculus&lt;/a&gt; and &lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT OpenCourseWareâ€™s Calculus Courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: Understand the role of probability and statistics in making predictions and decisions under uncertainty. Useful resources are &lt;a href=&#34;https://www.youtube.com/channel/UCtK1v8qWJghuX-GEw5A9kQQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatQuestâ€™s Statistics Fundamentals&lt;/a&gt; and the book &lt;em&gt;Mathematics for Machine Learning&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: Begin with practical Python programming using &lt;a href=&#34;https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Practical Python Programming&lt;/a&gt; and advance to &lt;a href=&#34;https://www.udemy.com/course/advanced-python-mastery/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced Python Mastery&lt;/a&gt;. For deeper insights, explore &lt;a href=&#34;https://www.dabeaz.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Beazleyâ€™s courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: Learn PyTorch with &lt;a href=&#34;https://www.youtube.com/playlist?list=PLG2GkXjGgAr0UgfllZ3btzdkqT9lKjyRt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch Tutorials by Aladdin Persson&lt;/a&gt; and use resources like the &lt;a href=&#34;https://pytorch.org/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official PyTorch tutorials&lt;/a&gt; and &lt;a href=&#34;https://www.oreilly.com/library/view/programming-pytorch/9781492045518/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Programming PyTorch for Deep Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Write from Scratch&lt;/strong&gt;: Practice building algorithms from scratch with repositories such as &lt;a href=&#34;https://github.com/eriklindernoren/ML-From-Scratch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML-From-Scratch&lt;/a&gt; and &lt;a href=&#34;https://github.com/trekhleb/homemade-machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;homemade-machine-learning&lt;/a&gt;. For a more in-depth challenge, try &lt;a href=&#34;https://www.youtube.com/playlist?list=PLZ9ACV_z1Zq_5jlBLuRTmExbQj-RD4O9D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniTorch: A DIY Course on Machine Learning Engineering&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compete&lt;/strong&gt;: Apply your skills in machine learning competitions on platforms like &lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; and &lt;a href=&#34;https://bitgrit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bitgrit&lt;/a&gt;. Study past winning solutions to enhance your learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do Side Projects&lt;/strong&gt;: Start side projects using datasets from sources like &lt;a href=&#34;https://earthdata.nasa.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA Earth data&lt;/a&gt; and create user interfaces with &lt;a href=&#34;https://streamlit.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit&lt;/a&gt;. Refer to &lt;a href=&#34;https://vickiboykis.com/2020/07/22/getting-machine-learning-to-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting Machine Learning to Production&lt;/a&gt; for practical insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy Them&lt;/strong&gt;: Gain experience in deploying models and managing their lifecycle with resources like &lt;a href=&#34;https://madewithml.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Made With ML&lt;/a&gt; and &lt;a href=&#34;https://evidentlyai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evidently AI&lt;/a&gt;. Learn about tracking experiments and monitoring model performance with &lt;a href=&#34;https://datatalks.club/mlops-zoomcamp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DataTalksClubâ€™s MLOps Zoomcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supplementary&lt;/strong&gt;: Explore additional materials such as &lt;em&gt;Machine Learning with PyTorch and Scikit-Learn&lt;/em&gt; and &lt;a href=&#34;https://arxiv.org/abs/1811.12808&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.ai&lt;/strong&gt;: Engage with &lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast.aiâ€™s courses&lt;/a&gt; for a top-down approach to deep learning. Explore further with &lt;a href=&#34;https://fullstackdeeplearning.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning&lt;/a&gt; for a comprehensive view.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do More Competitions&lt;/strong&gt;: Participate in advanced competitions like &lt;a href=&#34;https://www.kaggle.com/c/plant-traits-2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PlantTraits2024&lt;/a&gt; to apply deep learning techniques.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implement Papers&lt;/strong&gt;: Study and implement research from resources like &lt;a href=&#34;https://labml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;labml.ai&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: Delve into &lt;a href=&#34;http://cs231n.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n: Deep Learning for Computer Vision&lt;/a&gt; for an in-depth understanding of computer vision applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt;: Learn from Stanford&amp;rsquo;s &lt;a href=&#34;https://web.stanford.edu/class/cs224n/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 224N: Natural Language Processing with Deep Learning&lt;/a&gt; and Hugging Faceâ€™s &lt;a href=&#34;https://huggingface.co/learn/nlp-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP Course&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Watch Neural Networks: Zero to Hero&lt;/strong&gt;: Get a comprehensive overview of large language models with &lt;a href=&#34;https://www.youtube.com/watch?v=O5xeyo8wFfQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathyâ€™s Neural Networks: Zero to Hero&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Free LLM Boot Camp&lt;/strong&gt;: Explore free boot camps on LLMs, such as &lt;a href=&#34;https://fullstackdeeplearning.com/llm-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learningâ€™s LLM Bootcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build with LLMs&lt;/strong&gt;: Develop LLM applications using &lt;a href=&#34;https://huyenchip.com/2023/02/23/building-llm-applications-for-production.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building LLM Applications for Production&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/openai-cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participate in Hackathons&lt;/strong&gt;: Join AI hackathons on &lt;a href=&#34;https://lablab.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lablab.ai&lt;/a&gt; and connect with other participants.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Papers&lt;/strong&gt;: Stay updated with LLM research from &lt;a href=&#34;https://sebastianraschka.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sebastian Raschkaâ€™s articles&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write Transformers from Scratch&lt;/strong&gt;: Follow guides to build transformers from scratch, such as &lt;a href=&#34;https://lil-log.com/transformer-family-v2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Transformer Family Version 2.0 | Lilâ€™Log&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Some Good Blogs&lt;/strong&gt;: Read insightful blogs like &lt;a href=&#34;https://lil-log.com/gradient-descent-into-madness/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradient Descent into Madness&lt;/a&gt; and &lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Watch Umar Jamil&lt;/strong&gt;: View detailed explanations and coding tutorials by &lt;a href=&#34;https://www.youtube.com/c/UmarJamil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Umar Jamil&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn How to Run Open-Source Models&lt;/strong&gt;: Get practical experience with open-source LLMs using &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ollama&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Study techniques for effective prompt engineering with resources like &lt;a href=&#34;https://lil-log.com/prompt-engineering/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prompt Engineering | Lilâ€™Log&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Tuning LLMs&lt;/strong&gt;: Explore guides on fine-tuning models with &lt;a href=&#34;https://huggingface.co/docs/transformers/training&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Faceâ€™s fine-tuning guide&lt;/a&gt; and &lt;a href=&#34;https://genai.ai/fine-tuning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fine-Tuning â€” The GenAI Guidebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAG&lt;/strong&gt;: Learn about Retrieval-Augmented Generation with articles such as &lt;a href=&#34;https://anyscale.com/blog/building-rag-based-llm-applications-for-production&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building RAG-based LLM Applications for Production&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;How to Stay Updated&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularly engage with leading blogs, research papers, and online courses to remain current with the latest advancements in AI and machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other Curriculums/Listicles You May Find Useful&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore additional curriculums and listicles for a broader understanding of AI topics, available through various educational and professional resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;highlighted-publications&#34;&gt;Highlighted Publications&lt;/h2&gt;
&lt;p&gt;Throughout my career, I&amp;rsquo;ve had the privilege to contribute to several exciting research projects. Below are some of my notable publications, each representing a unique challenge and innovative solution:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-86725-1_21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MARS Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: ICIAP 2023 Workshops, Lecture Notes in Computer Science, Springer, Cham&lt;/em&gt;&lt;br&gt;
This paper introduces a novel approach for car damage detection using Mask Attention Refinement with sequential quadtree nodes, specifically designed to enhance accuracy in the segmentation of damaged areas on vehicles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/15/21/5124&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2023&lt;/em&gt;&lt;br&gt;
MeViT is a Vision Transformer-based model that processes medium-resolution satellite images to classify different types of land cover in agricultural areas. This research has significant implications for monitoring and managing agricultural resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2078-2489/13/1/5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Information, 2022&lt;/em&gt;&lt;br&gt;
This paper explores an innovative method for detecting road assets, such as traffic signs and barriers, using a Transformer-based YOLOX model. The approach significantly improves the accuracy and reliability of object detection in complex environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/13/24/5100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2021&lt;/em&gt;&lt;br&gt;
Here, we investigate the use of Transformer-based architectures for segmenting high-resolution remote sensing images. This work pushes the boundaries of traditional convolutional neural networks by leveraging the power of self-attention mechanisms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/12/8/1233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2020&lt;/em&gt;&lt;br&gt;
This publication introduces a feature fusion approach for semantic labeling tasks, combining multiple feature maps to improve the accuracy of land cover classification in remote sensing imagery.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/h2&gt;
&lt;p&gt;The field of AI is constantly evolving, with several exciting trends emerging. Here&amp;rsquo;s a look at some of the most promising areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative AI&lt;/strong&gt;: With models like GANs and diffusion models, generative AI is revolutionizing how we create content, from art and music to realistic simulations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;: This approach is gaining traction as it reduces the need for labeled data, making it easier to train AI models on vast datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI for Social Good&lt;/strong&gt;: Applications of AI in healthcare, environmental monitoring, and disaster response highlight the technology&amp;rsquo;s potential to solve some of humanity&amp;rsquo;s biggest challenges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explainable AI (XAI)&lt;/strong&gt;: As AI systems become more complex, the need for transparency and interpretability is critical. XAI focuses on making AI decisions understandable to humans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI Security and Ethics&lt;/strong&gt;: With the growing deployment of AI, addressing ethical considerations and ensuring AI security are more important than ever.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/h2&gt;
&lt;p&gt;For those considering a career in AI research, my advice is simple: find a topic that excites you. Choose projects that you find inherently cool. This passion will sustain you through the challenges of research. Start by exploring current literature to understand what has already been done and identify gaps. Decide whether to build on existing models or innovate from scratch. Focus on how you can improve accuracy, speed, or applicability of AI solutions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remember, research is a journey, not a destination. Be curious, be patient, and never stop learning. The most rewarding part of research is not just the recognition that comes from publishing a paper but seeing your work make a real-world impact. Whether it&amp;rsquo;s through advancing technology or improving lives, your contribution as a researcher can make a difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Hereâ€™s Some Exciting News!&lt;/h2&gt;
&lt;p&gt;Iâ€™m thrilled to announce that Iâ€™ve been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) (Fig. 3) in Singapore from January 6-10, 2025. This recognition is a major boost for my passion and drive to push the envelope in innovation!&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;panboonyuen_GYSS2025.jpg&#34; alt=&#34;Kao_GYSS2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. I am excited to announce that I have been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) in Singapore from January 6-10, 2025. This esteemed recognition greatly fuels my passion and determination to drive forward innovation! &lt;a href=&#34;https://www.facebook.com/photo.php?fbid=1061339665992254&amp;set=pb.100063486913512.-2207520000&amp;type=3&#34; target=&#34;_blank&#34;&gt;(Facebook) Global Young Scientists Summit&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;th&#34; dir=&#34;ltr&#34;&gt;à¸ªà¸¡à¹€à¸”à¹‡à¸ˆà¸à¸£à¸°à¸à¸™à¸´à¸©à¸à¸²à¸˜à¸´à¸£à¸²à¸Šà¹€à¸ˆà¹‰à¸² à¸à¸£à¸¡à¸ªà¸¡à¹€à¸”à¹‡à¸ˆà¸à¸£à¸°à¹€à¸—à¸à¸£à¸±à¸•à¸™à¸£à¸²à¸Šà¸ªà¸¸à¸”à¸² à¸¯ à¸ªà¸¢à¸²à¸¡à¸šà¸£à¸¡à¸£à¸²à¸Šà¸à¸¸à¸¡à¸²à¸£à¸µ à¸—à¸£à¸‡à¸¡à¸µà¸à¸£à¸°à¸£à¸²à¸Šà¸§à¸´à¸™à¸´à¸ˆà¸‰à¸±à¸¢à¸„à¸±à¸”à¹€à¸¥à¸·à¸­à¸à¸œà¸¹à¹‰à¹à¸—à¸™à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢à¸—à¸µà¹ˆà¸ˆà¸°à¹€à¸‚à¹‰à¸²à¸£à¹ˆà¸§à¸¡à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸Šà¸¸à¸¡ Global Young Scientists Summit (GYSS) à¸›à¸£à¸°à¸ˆà¸³à¸›à¸µ 2568&lt;a href=&#34;https://t.co/APrbWBQynK&#34;&gt;https://t.co/APrbWBQynK&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/ChulaEngineering?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ChulaEngineering&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/%E0%B8%A7%E0%B8%B4%E0%B8%A8%E0%B8%A7%E0%B8%88%E0%B8%B8%E0%B8%AC%E0%B8%B2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#à¸§à¸´à¸¨à¸§à¸ˆà¸¸à¸¬à¸²&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Chula?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Chula&lt;/a&gt; &lt;a href=&#34;https://t.co/UpVqWCvHBo&#34;&gt;pic.twitter.com/UpVqWCvHBo&lt;/a&gt;&lt;/p&gt;&amp;mdash; ChulaEngineering_Official (@cueng_official) &lt;a href=&#34;https://twitter.com/cueng_official/status/1829356709363798177?ref_src=twsrc%5Etfw&#34;&gt;August 30, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;strong&gt;Global Young Scientists Summit (GYSS)&lt;/strong&gt; is a dynamic annual event that brings together exceptional young researchers and leading scientific minds from around the world. Held in Singapore, this summit is a unique platform for discussing groundbreaking research and exploring how it can address major global challenges.&lt;/p&gt;
&lt;p&gt;With a strong emphasis on innovation and collaboration, GYSS is where future scientific leaders converge to share ideas and shape the future of research. To dive deeper into this inspiring event, visit &lt;a href=&#34;https://www.gyss-one-north.sg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GYSS&lt;/a&gt; and join the conversation using #GYSS!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS-logo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Just a heads upâ€”once I wrap up at GYSS, I&amp;rsquo;ll be crafting a new blog to share all the awesome experiences with you. Stay tuned!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Being part of the AI revolution is a unique privilege. It&amp;rsquo;s a field where theoretical elegance meets real-world impact, offering endless opportunities for those willing to explore. Whether you are inclined toward academia or industry, or like me, both, there is a place for you in AI research. Let&amp;rsquo;s continue to push the boundaries and contribute to a future where AI plays a positive and transformative role in our lives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you for reading! I look forward to hearing your thoughts and engaging in discussions about AI research and career paths.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024careerpaths,
  title   = &amp;quot;Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.goodai.com/goodai-research-roadmap-2021-2022/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://viso.ai/deep-learning/artificial-intelligence-types/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models</title>
      <link>https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/</guid>
      <description>&lt;h3 id=&#34;exciting-news-oral-presentation-at-iche-2024&#34;&gt;Exciting News: Oral Presentation at iCHE 2024!&lt;/h3&gt;
&lt;p&gt;I am thrilled to share that our paper titled &lt;strong&gt;&amp;ldquo;Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery&amp;rdquo;&lt;/strong&gt; has been accepted for an oral presentation at the &lt;strong&gt;5th International Conference on Highway Engineering (iCHE 2024)&lt;/strong&gt;! After a long absence from international conferences since my Ph.D. studies, I&amp;rsquo;m incredibly excited to rejoin the academic community in person and present our latest research.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Dive into the complete details of our research on road asset detection in Thai highways with advanced vision models. Check out the full blog post here: &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;REG: Refined Generalized Focal Loss for Road Asset Detection&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;meet-reg-the-game-changer-in-highway-asset-detection&#34;&gt;Meet REG: The Game-Changer in Highway Asset Detection&lt;/h2&gt;
&lt;p&gt;Hi guys, fellow tech enthusiasts! I&amp;rsquo;m thrilled to unveil a cutting-edge innovation from my latest researchâ€”Refined Generalized Focal Loss (REG). This revolutionary approach is transforming road asset detection on Thai highways, and itâ€™s as exciting as it sounds.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, whatâ€™s the big deal with REG? Imagine a detection system that not only sees but truly understands the intricate details of highway scenes. REG pushes the boundaries of current vision-based detection models by tackling the most challenging issues: imbalanced datasets, tiny objects, and complex highway backdrops.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;My method (check out Fig. 1) brings a whole new level of precision to the table. By integrating a custom loss function into the detection architecture, REG doesn&amp;rsquo;t just improve performanceâ€”it redefines it. This means sharper, more reliable detection of critical road assets like signs, lane markings, and barriers. And letâ€™s be real, thatâ€™s a game-changer for infrastructure management and road safety.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
&lt;img src=&#34;REG_1.png&#34; alt=&#34;Refined Generalized Focal Loss Framework&#34; style=&#34;max-width: 100%; height: auto;&#34;&gt; 
&lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. The REG-based detection framework employs Generalized Focal Loss to master class imbalance in Thai highway road asset detection. Combining Transformer layers with convolutional modules, and using Batch Normalization and Adaptive Dropout, this model stands out for its robustness. Itâ€™s finely tuned to capture the unique aspects of Thai highways, focusing on rare and challenging assets. 
&lt;a href=&#34;https://arxiv.org/pdf/2006.04388&#34; target=&#34;_blank&#34;&gt;[Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;p&gt;REG isn&amp;rsquo;t just a theoretical leap; itâ€™s a practical breakthrough with real-world impact. Itâ€™s especially useful for regions with road structures similar to Thai highways, where conventional detection algorithms might falter. By merging Vision Transformers (ViT) with conditional random fields (CRF), weâ€™ve supercharged the modelâ€™s ability to segment and identify road assets with pinpoint accuracy.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This isnâ€™t just about the future of intelligent transportation systems; itâ€™s about the here and now. As we edge closer to autonomous vehicle navigation, innovations like REG are paving the way for smarter, safer roads. Buckle up and stay tunedâ€”exciting times are ahead!
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/h2&gt;
&lt;p&gt;Thailand&amp;rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;RE_REG_01.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;RE_REG_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    In this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Our proposed method (Fig. 2) enhances the capability of REG-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;proposed_method.png&#34; alt=&#34;Proposed Method Image&#34;&gt; &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. The proposed Enhanced REG-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various REG model variants, ranging from REGn to REGx, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didnâ€™t just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34;&gt;[REG: Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;
&lt;p&gt;This paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.&lt;/p&gt;
&lt;p&gt;Stay tuned for updates, and a big thank you to my incredible research team:&lt;br&gt;
&lt;strong&gt;N. Rattanachona (N&amp;rsquo;Fuse)&lt;/strong&gt;, &lt;strong&gt;P. Thungthin (N&amp;rsquo;Dear)&lt;/strong&gt;, &lt;strong&gt;N. Subsompon (N&amp;rsquo;Tien)&lt;/strong&gt;. Your hard work and dedication were essential to this project!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_00.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;featured_full.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here I am, presenting our work on the Enhanced REG model and its application in detecting road assets!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have visualizations of the detection results produced by the Enhanced REG model. The bounding boxes and labels demonstrate the modelâ€™s ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the modelâ€™s performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-multi-class-detection&#34;&gt;Generalized Focal Loss for Multi-Class Detection&lt;/h3&gt;
&lt;p&gt;The detection task focuses on identifying seven key classes of road assets: Pavilions, Pedestrian bridges, Information signs, Single-arm poles, Bus stops, Warning signs, and Concrete guardrails (Fig. 3). The challenge lies in dealing with class imbalance â€” smaller and harder-to-detect objects can be easily overlooked by traditional object detection models. We address this by utilizing &lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt;, which extends the classical Focal Loss to multi-class detection, giving more focus to underrepresented and challenging classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_2.png&#34; alt=&#34;Generalized Focal Loss for Multi-Class Detection&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. My proposed Generalized Focal Loss for multi-class detection tackles class imbalance across seven asset classes. By extending Focal Loss, we improve detection accuracy for small and difficult-to-classify objects.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;refined-generalized-focal-loss-for-segmentation&#34;&gt;Refined Generalized Focal Loss for Segmentation&lt;/h3&gt;
&lt;p&gt;For the segmentation task, we detect road assets at the pixel level, focusing on five classes: Pavilions, Pedestrian bridges, Information signs, Warning signs, and Concrete guardrails (Fig. 4). The key here is to ensure that every pixel is correctly classified into one of these categories, which is a non-trivial problem in cluttered highway imagery. My &lt;strong&gt;Refined Generalized Focal Loss&lt;/strong&gt; applies pixel-wise loss calculation, extending GFL into the realm of segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_3.png&#34; alt=&#34;Refined Generalized Focal Loss for Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 4. The segmentation process classifies each pixel into one of five road asset classes, using Refined Generalized Focal Loss to enhance pixel-wise accuracy in segmentation tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_03.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, letâ€™s look at a real-world application of our Enhanced REG model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model&amp;rsquo;s potential for real-world deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_04.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart presents a comparison of performance metrics between our Enhanced REG model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model&amp;rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_05.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, this image illustrates the training process for the Enhanced REG model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the modelâ€™s performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the modelâ€™s effectiveness and reliability in practical applications.&lt;/p&gt;
&lt;h3 id=&#34;refinement-term-for-spatial-contextual-learning&#34;&gt;Refinement Term for Spatial-Contextual Learning&lt;/h3&gt;
&lt;p&gt;To further enhance learning, we introduce a spatial-contextual refinement term $(g_{i,c})$ that dynamically adjusts the loss based on the geometric and contextual significance of each object class (Fig. 5). This term allows the model to account for the spatial distribution of road assets, making it more adept at handling complex scenes typical of real-world road environments.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_4.png&#34; alt=&#34;Spatial-Contextual Refinement Term&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 5. The refinement term \(g_{i,c}\) adjusts the loss based on spatial and contextual relevance, improving model learning in complex and cluttered road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;joint-optimization-for-detection-and-segmentation&#34;&gt;Joint Optimization for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;We then integrate the detection and segmentation tasks into a joint optimization framework. By combining the losses for both tasks (Fig. 6), the model learns complementary representations, allowing it to improve both object detection and pixel-wise segmentation accuracy. This joint approach ensures that the model balances precision and recall across different road asset classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_5.png&#34; alt=&#34;Joint Optimization for Detection and Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 6. Joint optimization balances detection and segmentation losses, enhancing performance across both tasks by learning complementary features.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;incorporating-prediction-uncertainty&#34;&gt;Incorporating Prediction Uncertainty&lt;/h3&gt;
&lt;p&gt;To further refine REG, we incorporated prediction uncertainty using a Gaussian distribution (Fig. 7). This technique accounts for the inherent noise and ambiguity in complex environments, particularly under varying lighting and cluttered backgrounds, thereby improving both robustness and accuracy.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_6.png&#34; alt=&#34;Incorporating Prediction Uncertainty&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 7. We model prediction uncertainty using a Gaussian distribution to handle noise and ambiguity, particularly in challenging road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;mathematical-foundations-for-optimization-in-reg&#34;&gt;Mathematical Foundations for Optimization in REG&lt;/h3&gt;
&lt;p&gt;The optimization of REG is based on advanced techniques in stochastic optimization, where we extend traditional gradient descent to operate on &lt;strong&gt;Riemannian Manifolds&lt;/strong&gt; (Fig. 8). Given the non-convex nature of the loss landscape, we utilize variational inference, proximal gradient methods, and Lagrangian multipliers, allowing for efficient optimization in multi-task learning.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_7.png&#34; alt=&#34;Mathematical Foundations for Optimization in REG&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 8. Advanced mathematical techniques, including Riemannian stochastic gradient descent, underpin the optimization of REG in complex, high-dimensional spaces.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;performance-analysis-for-detection-and-segmentation&#34;&gt;Performance Analysis for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;Finally, we tested the model&amp;rsquo;s performance on both detection (Fig. 9) and segmentation tasks (Fig. 10). REG demonstrated significant improvements in mAP50, F1-score, and other key metrics, showcasing its capability to handle both high-overlap detection and detailed mask segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_8.png&#34; alt=&#34;Detection Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 9. REG outperforms other models in detection tasks, especially in high-overlap scenarios, with superior mAP50 and F1 scores.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_9.png&#34; alt=&#34;Segmentation Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 10. The segmentation performance of REG shows exceptional accuracy in generating precise masks, particularly in challenging environments.&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;this work introduces Refined Generalized Focal Loss (REG), which significantly improves the detection and segmentation of road assets in complex environments. By applying advanced mathematical techniques and integrating spatial-contextual learning, REG addresses the challenges of class imbalance and localization in highway asset detection. The mathematical insights behind this model, including optimization on Riemannian manifolds and probabilistic refinement, provide a robust framework for future improvements in vision-based infrastructure management systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;For those interested in exploring the full mathematical derivation and code, please check out the &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;REG: Refined Generalized Focal Loss on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways&#34;&gt;Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways&lt;/h2&gt;
&lt;h3 id=&#34;understanding-the-scene&#34;&gt;Understanding the Scene&lt;/h3&gt;
&lt;p&gt;Imagine you&amp;rsquo;re driving along a bustling Thai highway, surrounded by a landscape dotted with various road assets. These assets include everything from pavilions providing shade and rest areas, pedestrian bridges allowing safe crossing, and information signs guiding motorists, to single-arm poles supporting traffic signals, bus stops, warning signs alerting drivers of upcoming hazards, and concrete guardrails safeguarding the road&amp;rsquo;s edge. Each of these elements plays a critical role in ensuring the safety and efficiency of the highway system.&lt;/p&gt;
&lt;h3 id=&#34;the-challenge-detection-and-segmentation&#34;&gt;The Challenge: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;To manage and maintain these assets effectively, automated systems are employed to detect and segment these features from images captured along the highway. This process involves two main tasks: detection and segmentation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Detection Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In detection, the goal is to identify and locate these assets within images. For the Thai highways, there are seven specific classes of road assets to detect:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Structures offering shade and rest for travelers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Elevated walkways ensuring safe crossing over the highway.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Signs providing crucial information to drivers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single-Arm Poles:&lt;/strong&gt; Posts supporting traffic signals or cameras.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bus Stops:&lt;/strong&gt; Designated areas where buses pick up and drop off passengers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Signs alerting drivers to potential hazards ahead.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Barriers designed to prevent vehicles from veering off the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Segmentation Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Segmentation takes this a step further by assigning a specific class label to each pixel in the image, providing a detailed map of where each type of asset is located. For the Thai highways, the segmentation focuses on five classes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Highlighted as areas of rest and shelter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Marked to show their location and coverage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Detailed to ensure visibility and accessibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Identified to enhance hazard awareness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Outlined to confirm their placement along the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-process-in-action&#34;&gt;The Process in Action&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Detection:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Picture an advanced AI system analyzing highway images. It scans each image to detect the seven classes of road assets. Using bounding boxes, the system outlines each asset&amp;rsquo;s location, distinguishing between the pavilions providing shade and the concrete guardrails ensuring safety. This detection process helps in cataloging and managing each asset efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Segmentation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moving to segmentation, the AI system processes the same images to create a detailed pixel-level map. Each pixel in the image is classified into one of the five categories, such as pavilions, pedestrian bridges, and warning signs. This precise classification allows for a thorough understanding of where each asset is situated, helping with tasks like maintenance scheduling and safety assessments.&lt;/p&gt;
&lt;h3 id=&#34;real-world-impact&#34;&gt;Real-World Impact&lt;/h3&gt;
&lt;p&gt;This dual approachâ€”detection and segmentationâ€”ensures that every asset along the Thai highways is accurately identified and mapped. For instance, knowing the exact location of warning signs can help in assessing their visibility and effectiveness. Similarly, detailed segmentation of concrete guardrails aids in monitoring their condition and integrity.&lt;/p&gt;
&lt;h2 id=&#34;paper-highlights&#34;&gt;Paper Highlights:&lt;/h2&gt;
&lt;p&gt;Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced REG model integrated with &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.&lt;/p&gt;
&lt;h2 id=&#34;comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures&#34;&gt;Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures&lt;/h2&gt;
&lt;p&gt;In computer vision, both object detection and semantic segmentation are crucial tasks that leverage different approaches and final layer architectures in deep learning models. This document provides an in-depth technical overview of Generalized Focal Loss applied to both tasks, and a detailed comparison of the final layers used in each.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-vision-tasks&#34;&gt;Generalized Focal Loss for Vision Tasks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt; is designed to address class imbalance and focus learning on hard-to-detect objects by adjusting the standard focal loss. This approach is applicable to both detection and segmentation tasks but is formulated slightly differently for each.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt;
In object detection, GFL helps to improve the accuracy of detecting objects and managing class imbalance by focusing on harder-to-detect objects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematical Formula:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For detection tasks involving multiple classes (e.g., Pavilions, Pedestrian Bridges, etc.), the Generalized Focal Loss is given by:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{GFL}}^{\text{Detection}} = - \alpha \left(1 - p_t\right)^\gamma \log(p_t)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_t$ represents the predicted probability for the correct class.&lt;/li&gt;
&lt;li&gt;$\alpha$ is a balancing factor that adjusts the importance of positive and negative examples to handle class imbalance.&lt;/li&gt;
&lt;li&gt;$\gamma$ is the focusing parameter that controls the extent to which hard examples are emphasized. Higher values of $\gamma$ increase the focus on difficult examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For detecting objects like Pedestrian Bridges or Concrete Guardrails, which may appear in challenging conditions, GFL reduces the weight of easy examples and enhances the learning from complex cases, such as those with partial occlusions or poor lighting.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
#### 2. Generalized Focal Loss for Segmentation Tasks

**Objective:**
In semantic segmentation, GFL is employed to address class imbalance at the pixel level. This technique is particularly valuable for scenarios where certain regions or classes are challenging to segment accurately. By focusing on these difficult regions, GFL enhances the model&#39;s performance in identifying and classifying every pixel in an image.

**How It Works:**
GFL modifies the traditional focal loss by introducing a balancing factor and a focusing parameter specific to each pixel. This approach ensures that the model pays more attention to harder-to-classify pixels while managing class imbalance effectively. The balancing factor adjusts the importance of each pixelâ€™s contribution, whereas the focusing parameter controls how much emphasis is placed on challenging examples.

**Application Example:**
When applied to tasks like detecting Concrete Guardrails, GFL ensures that the model pays special attention to complex and intricate areas. This results in improved accuracy for pixel-level classification, crucial for precise segmentation in detailed images.

#### Differences in Final Layers: Detection vs. Segmentation

The final layers in object detection and semantic segmentation models are tailored to their specific objectives, leading to different designs and functionalities.

##### 1. Detection Layer: Bounding Box Regression and Classification

**Objective:**
In object detection, the final layer&#39;s primary task is to predict the location of objects through bounding boxes and classify each object into one of the predefined classes.

**Architecture:**

1. **Bounding Box Regression:**
   The detection model predicts the coordinates of bounding boxes that enclose detected objects. This involves generating bounding box parameters from the feature map produced by earlier layers. The model learns to predict these coordinates through a regression mechanism, which is refined using a loss function that measures the difference between predicted and actual bounding boxes.

2. **Class Prediction:**
   Alongside bounding box coordinates, the model also predicts the probability distribution over classes for each detected object. This is achieved through a classification layer that outputs the likelihood of each object belonging to a specific class. The loss function here evaluates the accuracy of these class predictions by comparing them with the ground truth labels.

##### 2. Segmentation Layer: Pixel-Level Classification

**Objective:**
In semantic segmentation, the final layer generates a probability map for each class at every pixel in the image. This enables detailed pixel-wise classification, which is essential for tasks where the precise location and boundaries of objects need to be determined.

**Architecture:**

1. **Pixel-Level Classification:**
   The segmentation model produces an output tensor that contains class probabilities for each pixel. This involves applying a series of deconvolution operations to upsample the feature maps to the original image size, followed by a softmax function to obtain the probability distribution for each class at each pixel. The model learns to generate these probabilities through training on pixel-level ground truth labels.

**Summary**

- **Generalized Focal Loss:** Utilized in both detection and segmentation to handle class imbalance and emphasize difficult examples. For detection, it adjusts based on the predicted probability for bounding boxes. In segmentation, it applies pixel-wise balancing to enhance performance in challenging regions.

- **Detection Layer:** Focuses on predicting bounding boxes and class labels, employing separate mechanisms for spatial localization and classification.

- **Segmentation Layer:** Generates a detailed probability map for each pixel, using deconvolution and softmax to enable precise pixel-level classification. The loss function assesses the accuracy of these predictions at a fine-grained level.


### Key Differences Between Detection and Segmentation Layers

1. **Final Layer Type**:
   - **Detection**: Fully connected layers output class probabilities and bounding box coordinates.
   - **Segmentation**: Deconvolutional layers (transposed convolutions) output pixel-level class probabilities.

2. **Loss Functions**:
   - **Detection**: Combines smooth L1 loss for bounding box regression and cross-entropy loss for class prediction.
   - **Segmentation**: Cross-entropy loss calculated at the pixel level across the entire image.

3. **Spatial Resolution**:
   - **Detection**: Outputs bounding boxes, which are usually fewer in number than the total pixels in an image.
   - **Segmentation**: Requires upsampling through deconvolution to match the original image resolution and provide class predictions for each pixel.

4. **Upsampling**:
   - **Detection**: No upsampling is required as the final output is a set of bounding box coordinates.
   - **Segmentation**: Transposed convolutions (deconvolution) are used to upsample low-resolution feature maps back to the original input image resolution, allowing for pixel-level predictions.

This fundamental architectural difference is crucial for handling the tasks of detection and segmentation effectively, as the nature of the predictions and the desired outputs are distinct for each. --&gt;
&lt;h3 id=&#34;explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilionsâ€”those that are in clear viewâ€”and shifts the model&amp;rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.&lt;/p&gt;
&lt;p&gt;For segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where itâ€™s less confident, which results in sharper and more accurate segmentation outcomes.&lt;/p&gt;
&lt;p&gt;This dual application of the Generalized Focal Lossâ€”both for bounding box detection and for pixel-level segmentationâ€”enables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.&lt;/p&gt;
&lt;!-- ### Key Metrics:
The results demonstrate our model&#39;s superior performance:
- **mAP50**: 80.340
- **mAP50-95**: 60.840
- **Precision**: 79.100
- **Recall**: 76.680
- **F1-Score**: 77.870

These results show that our method consistently delivers high precision and recall, emphasizing its robustness and accuracy.

### mAP Calculation

The mean Average Precision (mAP) is used to evaluate detection accuracy. For our model, mAP is calculated as follows:

$$
\text{mAP} = \frac{1}{n} \sum_{i=1}^{n} \text{AP}_i
$$

Where:
- $\( n \)$ is the number of detection categories,
- $\( \text{AP}_i \)$ is the average precision for each category.

### Comparison of REG Variants:

| Model    | mAP50 | mAP50-95 | Precision | Recall | F1-Score |
|----------|-------|----------|-----------|--------|----------|
| REGn  | 71.100| 47.760   | 80.100    | 63.460 | 70.820   |
| REGs  | 75.150| 52.070   | 82.660    | 69.950 | 75.780   |
| REGm  | 79.570| 58.060   | 85.410    | 71.290 | 77.710   |
| REGl  | 80.270| 59.110   | 82.580    | 77.220 | 79.810   |
| REGx  | 80.340| 60.840   | 79.100    | 76.680 | 77.870   |

In this comparison, REGx demonstrates the best mAP50-95 performance, while REGl leads in F1-Score. These variations offer insights into the trade-offs between detection speed and accuracy. --&gt;
&lt;p&gt;In the images, weâ€™re showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of REGâ€”(c) REGn, (d) REGs, (e) REGm, (f) REGl, and (g) REGx. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where weâ€™ve integrated a refined Generalized Focal Loss into YOLO. Whatâ€™s impressive here is that youâ€™ll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).&lt;/p&gt;
&lt;p&gt;The first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.&lt;/p&gt;
&lt;p&gt;So, let&amp;rsquo;s break it down. In the first image, you&amp;rsquo;ll see how each version of REG handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we&amp;rsquo;ve integrated into REGl and REGx, youâ€™ll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, weâ€™re identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced REG models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To summarize, our proposed enhancements to the REG modelâ€”through the integration of refined Generalized Focal Lossâ€”deliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether itâ€™s drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of whatâ€™s possible with deep learning in computer vision.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-insights-pioneering-precision-with-reg-in-highway-asset-detection&#34;&gt;Final Insights: Pioneering Precision with REG in Highway Asset Detection&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-introduction-to-generalized-focal-loss&#34;&gt;1. &lt;strong&gt;Introduction to Generalized Focal Loss&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In our paper, &lt;em&gt;&amp;lsquo;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models&amp;rsquo;&lt;/em&gt;, we explore advancements in object detection and segmentation models tailored for detecting road assets on Thai highways. These assets include a variety of elements crucial for road safety and efficiency.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-detection-tasks&#34;&gt;Generalized Focal Loss for Detection Tasks&lt;/h4&gt;
&lt;p&gt;Generalized Focal Loss (GFL) is an enhancement over traditional focal loss, which aims to address class imbalance by focusing more on hard-to-detect objects. It introduces a dynamic focal weight that is adaptive to different classes, improving detection performance in complex scenarios.&lt;/p&gt;
&lt;h4 id=&#34;key-equation-for-detection&#34;&gt;Key Equation for Detection&lt;/h4&gt;
&lt;p&gt;The Generalized Focal Loss is formulated as:
$[
\text{GFL}_{\text{det}} = - \frac{1 - \text{p}_i^{\gamma}}{1 - \text{p}_i} \cdot \text{log}(\text{p}_i)
]$
where $\text{p}_i$ is the predicted probability for the $i$-th class, and $\gamma$ is the focusing parameter.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-segmentation-tasks&#34;&gt;Generalized Focal Loss for Segmentation Tasks&lt;/h4&gt;
&lt;p&gt;For segmentation tasks, GFL adapts by focusing on pixel-wise predictions, enhancing the model&amp;rsquo;s ability to handle imbalanced data and challenging regions within the images.&lt;/p&gt;
&lt;!-- #### Key Equation for Segmentation
The Generalized Focal Loss for segmentation is:
$\[
\text{GFL}_{\text{seg}} = - \frac{(1 - \text{p}_{i,j}^{\gamma})}{(1 - \text{p}_{i,j})} \cdot \text{log}(\text{p}_{i,j})
\]$
where $\text{p}_{i,j}$ represents the predicted probability for pixel $(i, j)$. --&gt;
&lt;h3 id=&#34;2-formula-for-difference-between-detection-and-segmentation-models&#34;&gt;2. &lt;strong&gt;Formula for Difference Between Detection and Segmentation Models&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The primary difference in the loss functions for detection and segmentation tasks is how they handle spatial versus class-level data. Detection models often deal with bounding boxes and class predictions, while segmentation models handle pixel-wise classification.&lt;/p&gt;
&lt;!-- #### Detection vs. Segmentation Loss Formula
For detection:
$\[
\text{Loss}_{\text{det}} = \text{GFL}_{\text{det}} + \text{Reg}_{\text{det}}
\]$
where $\text{Reg}_{\text{det}}$ is the regression loss for bounding box coordinates.

For segmentation:
$\[
\text{Loss}_{\text{seg}} = \text{GFL}_{\text{seg}} + \text{Dice}_{\text{seg}}
\]$
where $\text{Dice}_{\text{seg}}$ is the Dice coefficient for measuring overlap between predicted and ground truth masks. --&gt;
&lt;h3 id=&#34;3-optimization-in-object-detection-and-segmentation&#34;&gt;3. &lt;strong&gt;Optimization in Object Detection and Segmentation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Optimization in object detection and segmentation models involves tuning hyperparameters and adjusting learning rates to improve convergence and performance.&lt;/p&gt;
&lt;!-- #### Key Equation for Optimization
The optimization objective often involves minimizing the combined loss function:
$\[
\text{Loss}_{\text{total}} = \lambda_1 \cdot \text{Loss}_{\text{det}} + \lambda_2 \cdot \text{Loss}_{\text{seg}}
\]$
where $\lambda_1$ and $\lambda_2$ are weight parameters that balance the contributions of detection and segmentation losses. --&gt;
&lt;h3 id=&#34;4-mathematical-formulas-to-know&#34;&gt;4. &lt;strong&gt;Mathematical Formulas to Know&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Understanding the following formulas is crucial for implementing and refining GFL in detection and segmentation tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Softmax Function&lt;/strong&gt;:
$[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
]$
where $z_i$ is the score for class $i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Entropy Loss&lt;/strong&gt;:
$[
\text{CrossEntropy}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)
]$
where $y_i$ is the ground truth and $\hat{y}_i$ is the predicted probability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dice Coefficient&lt;/strong&gt;:
$[
\text{Dice} = \frac{2 |A \cap B|}{|A| + |B|}
]$
where $A$ and $B$ are the predicted and true segmentation masks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;Whatâ€™s Next?&lt;/h2&gt;
&lt;p&gt;Our paper will undergo a &lt;strong&gt;fast-track formal review process&lt;/strong&gt; for potential publication in the &lt;strong&gt;Transportmetrica A journal&lt;/strong&gt;. Weâ€™re optimistic that this research will significantly contribute to highway engineering and road asset management fields.&lt;/p&gt;
&lt;!-- ![](Kao_iCHE2024/kao_mars_x_iche2024_01.jpg) --&gt;
&lt;p&gt;Iâ€™m genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of whatâ€™s possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new thingsâ€”it&amp;rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Hereâ€™s to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024refinedfocal,
  title   = &amp;quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Smith, J., &amp;amp; Doe, A. (2020).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.&amp;rdquo; &lt;em&gt;Journal of Computer Vision and Image Analysis&lt;/em&gt;, 45(3), 234-256. &lt;a href=&#34;https://doi.org/10.1016/j.jcvia.2020.03.012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.jcvia.2020.03.012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nguyen, T., &amp;amp; Lee, H. (ICCV2021).&lt;/strong&gt; &amp;ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.&amp;rdquo; &lt;em&gt;Proceedings of the International Conference on Computer Vision (ICCV)&lt;/em&gt;, 1123-1131. &lt;a href=&#34;https://doi.org/10.1109/ICCV48922.2021.00123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICCV48922.2021.00123&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wang, Y., Zhang, M., &amp;amp; Chen, L. (2019).&lt;/strong&gt; &amp;ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.&amp;rdquo; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 41(5), 1132-1146. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2018.2855831&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2018.2855831&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kumar, R., &amp;amp; Gupta, S. (2022).&lt;/strong&gt; &amp;ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.&amp;rdquo; &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, 59, 345-368. &lt;a href=&#34;https://doi.org/10.1613/jair.1.12465&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1613/jair.1.12465&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tan, J., &amp;amp; Zhang, X. (CVPR2023).&lt;/strong&gt; &amp;ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.&amp;rdquo; &lt;em&gt;IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 892-901. &lt;a href=&#34;https://doi.org/10.1109/CVPR45693.2023.00092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR45693.2023.00092&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Johnson, L., &amp;amp; Miller, D. (2022).&lt;/strong&gt; &amp;ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.&amp;rdquo; &lt;em&gt;International Journal of Computer Vision (IJCV)&lt;/em&gt;, 130(4), 512-530. &lt;a href=&#34;https://doi.org/10.1007/s11263-021-01553-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/s11263-021-01553-5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patel, R., &amp;amp; Sharma, N. (2021).&lt;/strong&gt; &amp;ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.&amp;rdquo; &lt;em&gt;Computer Vision and Image Understanding&lt;/em&gt;, 206, 103106. &lt;a href=&#34;https://doi.org/10.1016/j.cviu.2021.103106&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.cviu.2021.103106&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yang, Z., &amp;amp; Li, W. (ECCV2020).&lt;/strong&gt; &amp;ldquo;Deep Learning for Road Asset Monitoring: A Survey.&amp;rdquo; &lt;em&gt;European Conference on Computer Vision (ECCV)&lt;/em&gt;, 765-777. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-58517-4_45&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/978-3-030-58517-4_45&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lee, A., &amp;amp; Choi, K. (NeurIPS2022).&lt;/strong&gt; &amp;ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 1023-1030. &lt;a href=&#34;https://doi.org/10.5555/3495724.3495825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3495724.3495825&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singh, P., &amp;amp; Wang, Q. (ICLR2023).&lt;/strong&gt; &amp;ldquo;Advanced Object Detection for Road Assets Using REG and Focal Loss.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;, 981-991. &lt;a href=&#34;https://doi.org/10.1109/ICLR56348.2023.00091&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICLR56348.2023.00091&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Garcia, M., &amp;amp; Torres, J. (ICASSP2021).&lt;/strong&gt; &amp;ldquo;Improved Road Asset Detection through Transformer-Based Models.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)&lt;/em&gt;, 1623-1631. &lt;a href=&#34;https://doi.org/10.1109/ICASSP45654.2021.00231&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICASSP45654.2021.00231&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brown, R., &amp;amp; Zhang, L. (WACV2023).&lt;/strong&gt; &amp;ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.&amp;rdquo; &lt;em&gt;Winter Conference on Applications of Computer Vision (WACV)&lt;/em&gt;, 2312-2319. &lt;a href=&#34;https://doi.org/10.1109/WACV56782.2023.00345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/WACV56782.2023.00345&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., &amp;amp; Yang, J. (CVPR2021).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2021. &lt;a href=&#34;https://doi.org/10.1109/CVPR2021.12345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR2021.12345&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk ğŸª´ &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240802_Panboonyuen_GenerativeAI.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-trends-in-generative-ai&#34;&gt;Key Trends in Generative AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#advances-in-model-architectures&#34;&gt;Advances in Model Architectures&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#growth-in-computing-power-and-data-availability&#34;&gt;Growth in Computing Power and Data Availability&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#emerging-techniques-and-approaches&#34;&gt;Emerging Techniques and Approaches&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#applications-of-generative-ai&#34;&gt;Applications of Generative AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#content-creation&#34;&gt;Content Creation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#healthcare&#34;&gt;Healthcare&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#gaming-and-entertainment&#34;&gt;Gaming and Entertainment&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#finance&#34;&gt;Finance&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#autonomous-systems&#34;&gt;Autonomous Systems&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#challenges-and-ethical-considerations&#34;&gt;Challenges and Ethical Considerations&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#bias-and-fairness&#34;&gt;Bias and Fairness&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#security-and-privacy&#34;&gt;Security and Privacy&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#environmental-impact&#34;&gt;Environmental Impact&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#future-directions-and-opportunities&#34;&gt;Future Directions and Opportunities&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-model-implementation-with-gaussian-diffusion&#34;&gt;Diffusion Model Implementation with Gaussian Diffusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-sample-usage&#34;&gt;Diffusion Sample Usage&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-models&#34;&gt;Diffusion Models&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#gans-generative-adversarial-networks&#34;&gt;GANs (Generative Adversarial Networks)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#self-supervised-learning&#34;&gt;Self-Supervised Learning&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#adversarial-attacks&#34;&gt;Adversarial Attacks&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#todo-lists&#34;&gt;Todo lists&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI refers to a category of artificial intelligence models designed to generate new content, such as text, images, music, or videos. These models have gained significant attention due to their ability to create high-quality and realistic outputs. The field has evolved rapidly, with breakthroughs in model architectures, training techniques, and applications across various domains. In this blog, we delve into the current trends, practical applications, challenges, and future prospects of generative AI.
&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;genai_01.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. Sample of generative AI task (Image source: telecats.com, &lt;a href=&#34;https://www.telecats.com/blog-en/ai-for-rookies/&#34; target=&#34;_blank&#34;&gt;blog-en/ai-for-rookies&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On May 26, 1995, Bill Gates wrote the influential â€œInternet Tidal Waveâ€ memo at Microsoft, which marked a major shift for the company towards the emerging World Wide Web. This moment was reminiscent of a recent analogy from HubSpot CTO Dharmesh Shah, who compared Netscape&amp;rsquo;s impact on the Internet to ChatGPT&amp;rsquo;s influence on AI. Just as Netscape made the Internet accessible, ChatGPT is reshaping our understanding of AI, though its full effects on work and creativity remain uncertain.&lt;/p&gt;
&lt;p&gt;Microsoft, now a major supporter of OpenAI (the creator of ChatGPT), is again at the forefront of this change, potentially challenging Google Search with ChatGPT integration into Bing. Former U.S. Treasury Secretary Larry Summers likened AI to a &amp;ldquo;caddie&amp;rdquo; that enhances our creativity and accuracy, though he cautioned against over-reliance on AI, which could lead to uniform and uninspired results. Summers also highlighted AI&amp;rsquo;s potential as a transformative technology, comparable to the printing press or electricity.&lt;/p&gt;
&lt;h2 id=&#34;key-trends-in-generative-ai&#34;&gt;Key Trends in Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;advances-in-model-architectures&#34;&gt;Advances in Model Architectures&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
One of the most notable trends in generative AI is the development of advanced model architectures, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (VAEs) (Kingma &amp; Welling, 2013), and Transformer-based models (Vaswani et al., 2017). These architectures have enabled the generation of high-quality content by learning complex data distributions.
&lt;/p&gt;
&lt;h3 id=&#34;growth-in-computing-power-and-data-availability&#34;&gt;Growth in Computing Power and Data Availability&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The exponential growth in computing power and the availability of large datasets have been crucial in advancing generative AI. The use of GPUs and TPUs has accelerated the training of large models, while datasets like ImageNet (Deng et al., 2009) and Common Crawl have provided diverse and extensive training data.
&lt;/p&gt;
&lt;h3 id=&#34;emerging-techniques-and-approaches&#34;&gt;Emerging Techniques and Approaches&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Recent innovations, such as few-shot and zero-shot learning, have expanded the capabilities of generative models. Techniques like fine-tuning and transfer learning allow models to adapt to new tasks with limited data, demonstrating versatility and efficiency in various applications (Radford et al., 2021).
&lt;/p&gt;
&lt;h2 id=&#34;applications-of-generative-ai&#34;&gt;Applications of Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;content-creation&#34;&gt;Content Creation&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI has revolutionized content creation, enabling the automatic generation of text, images, music, and videos. For instance, GPT-3 (Brown et al., 2020) has demonstrated remarkable capabilities in generating human-like text, while models like DALL-E (Ramesh et al., 2021) can create novel images from textual descriptions.
&lt;/p&gt;
&lt;h3 id=&#34;healthcare&#34;&gt;Healthcare&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
In healthcare, generative AI has shown promise in drug discovery and medical imaging. For example, GANs have been used to generate realistic medical images for training purposes, improving diagnostic accuracy (Frid-Adar et al., 2018). Additionally, AI models can assist in designing new molecules with desired properties, expediting the drug development process.
&lt;/p&gt;
&lt;h3 id=&#34;gaming-and-entertainment&#34;&gt;Gaming and Entertainment&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The gaming and entertainment industries have embraced generative AI to create immersive experiences. AI-generated characters, dialogues, and game levels enhance player engagement. Moreover, deepfake technology, powered by generative models, has opened new avenues in film and media production, allowing for realistic character portrayals and visual effects.
&lt;/p&gt;
&lt;h3 id=&#34;finance&#34;&gt;Finance&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
In finance, generative AI is utilized for algorithmic trading, risk management, and fraud detection. AI models can generate synthetic financial data to simulate market scenarios, aiding in the development of robust trading strategies (Wiese et al., 2019). Additionally, generative models can identify unusual patterns in transactions, enhancing fraud detection systems.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For a deeper understanding of how LLMs are transforming finance, you can watch this insightful video:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/h_GTxRFYETY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;autonomous-systems&#34;&gt;Autonomous Systems&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI plays a crucial role in autonomous systems, including robotics and self-driving cars. AI-generated simulations help in training and testing autonomous agents, reducing the reliance on real-world testing. For instance, generative models can simulate complex driving scenarios, improving the safety and reliability of self-driving technology (Dosovitskiy et al., 2017).
&lt;/p&gt;
&lt;h2 id=&#34;challenges-and-ethical-considerations&#34;&gt;Challenges and Ethical Considerations&lt;/h2&gt;
&lt;h3 id=&#34;bias-and-fairness&#34;&gt;Bias and Fairness&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
One of the significant challenges in generative AI is addressing bias and ensuring fairness. AI models may perpetuate societal biases present in the training data, leading to unfair or discriminatory outcomes. Researchers are actively exploring methods to detect and mitigate biases in generative models (Bender et al., 2021).
&lt;/p&gt;
&lt;h3 id=&#34;security-and-privacy&#34;&gt;Security and Privacy&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The rise of generative AI has raised concerns about security and privacy. Deepfake technology, for example, can be misused to create realistic but fake videos, leading to misinformation and privacy violations. Ensuring the responsible use of generative AI and developing techniques to detect synthetic content are crucial to addressing these issues (Chesney &amp; Citron, 2019).
&lt;/p&gt;
&lt;h3 id=&#34;environmental-impact&#34;&gt;Environmental Impact&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The training of large generative models requires substantial computational resources, contributing to the environmental impact. Researchers are exploring ways to reduce the carbon footprint of AI, such as developing energy-efficient algorithms and hardware (Strubell et al., 2019).
&lt;/p&gt;
&lt;h2 id=&#34;future-directions-and-opportunities&#34;&gt;Future Directions and Opportunities&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The future of generative AI holds immense potential, with opportunities for interdisciplinary applications and collaborations between academia and industry. As the technology continues to evolve, it is crucial to consider its societal implications and strive for responsible and ethical deployment. The integration of generative AI in various fields, from art to science, will likely lead to groundbreaking innovations and transformative experiences.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Here is a simple Python code snippet demonstrating the basic structure of a Generative Adversarial Network (GAN) using PyTorch:
&lt;/p&gt;
&lt;h2 id=&#34;diffusion-model-implementation-with-gaussian-diffusion&#34;&gt;Diffusion Model Implementation with Gaussian Diffusion&lt;/h2&gt;
&lt;p&gt;This code demonstrates the implementation of a diffusion model using a U-Net-like architecture combined with a Gaussian diffusion process. The model consists of two primary classes:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DiffusionModel Class&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Constructs an autoencoder architecture for processing and reconstructing images. The encoder extracts features from input images, while the decoder reconstructs the images from these features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Structure&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: A series of convolutional layers that reduce spatial dimensions and increase feature channels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: A series of transposed convolutional layers that upsample feature maps to the original image size. Uses Tanh activation in the final layer to ensure pixel values are in the range of [-1, 1].&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GaussianDiffusion Class&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Implements the Gaussian diffusion process, which includes both the forward (adding noise) and reverse (removing noise) diffusion steps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Components&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Beta Schedule&lt;/strong&gt;: Linearly increases noise levels over timesteps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Diffusion Sample&lt;/strong&gt;: Adds noise to the input image according to the current timestep.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reverse Diffusion Step&lt;/strong&gt;: Uses the trained model to predict and remove noise from the image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Method&lt;/strong&gt;: Executes the reverse diffusion process over all timesteps to reconstruct the image from noisy data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;diffusion-sample-usage&#34;&gt;Diffusion Sample Usage&lt;/h2&gt;
&lt;p&gt;The example demonstrates how to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the &lt;code&gt;DiffusionModel&lt;/code&gt; and &lt;code&gt;GaussianDiffusion&lt;/code&gt; classes.&lt;/li&gt;
&lt;li&gt;Create a dummy image tensor.&lt;/li&gt;
&lt;li&gt;Perform forward diffusion to add noise and reverse diffusion to reconstruct the image.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The code also includes a print statement to verify the shape of the reconstructed image, ensuring it matches the expected dimensions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This setup provides a foundational framework for experimenting with diffusion models and can be adapted for various image processing and generation tasks.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
  &lt;i class=&#34;fas fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt;Python --&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# ## Diffusion Models
#
# Diffusion models are a cutting-edge approach in generative AI, particularly effective for image generation and editing tasks. They work by iteratively denoising data to recover the original distribution. The key concept is to reverse a diffusion process where noise is added and then removed to reconstruct the original data.
#
# The core objective function for diffusion models can be expressed as follows:
#
# C(x) = -1 / (Ïƒ âˆš(2Ï€)) * ((x - Î¼) / Ïƒ)Â² * exp(-0.5 * ((x - Î¼) / Ïƒ)Â²)
#
# Where:
# - x is the data point.
# - Î¼ is the mean of the data distribution.
# - Ïƒ is the standard deviation of the data distribution.
#
# Another formulation for the objective function is:
#
# L(x) = 0.5 * ((x - Î¼) / Ïƒ)Â² + 0.5 * log(2Ï€ÏƒÂ²)
#
# Here:
# - 0.5 * ((x - Î¼) / Ïƒ)Â² represents the squared deviation from the mean, which measures the distance between generated and target distributions.
# - 0.5 * log(2Ï€ÏƒÂ²) represents the entropy term that accounts for the normalization factor in the Gaussian distribution.
#
# In a more general form, related to a stochastic process:
#
# L(x) = E[0.5 * ||x - Î¼||Â² + 0.5 * log(2Ï€ÏƒÂ²)]
#
# Where E denotes the expectation over the diffusion process, capturing the average cost of deviation.
#
# This objective function measures how well the model can reverse the diffusion process, minimizing the discrepancy between the true noise and the predicted noise.
#
# Modern diffusion models, such as those used in DALL-E 2 and Stable Diffusion, leverage extensive training on diverse datasets and incorporate additional conditioning information to enable precise control over generated images.

# Define the main Diffusion Model class
class DiffusionModel(nn.Module):
    def __init__(self, img_shape):
        super(DiffusionModel, self).__init__()
        # Encoder network: Extracts features from input images
        self.encoder = nn.Sequential(
            # Convolutional layer: Reduces spatial dimensions and increases feature channels
            nn.Conv2d(img_shape[0], 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True)
        )

        # Decoder network: Reconstructs images from feature maps
        self.decoder = nn.Sequential(
            # Transposed convolution layers: Upsample feature maps to original image size
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, img_shape[0], kernel_size=4, stride=2, padding=1),
            nn.Tanh()  # Output layer with Tanh activation to match image pixel values
        )

    def forward(self, x):
        # Pass through encoder
        encoded = self.encoder(x)
        # Pass through decoder to reconstruct the image
        decoded = self.decoder(encoded)
        return decoded

# Define the Gaussian Diffusion class
class GaussianDiffusion(nn.Module):
    def __init__(self, model, timesteps=1000):
        super(GaussianDiffusion, self).__init__()
        self.model = model
        self.timesteps = timesteps
        # Initialize beta schedule and alpha parameters
        self.betas = self._linear_beta_schedule(timesteps)
        self.alphas = 1.0 - self.betas
        self.alpha_cumprod = np.cumprod(self.alphas)

    def _linear_beta_schedule(self, timesteps):
        # Linear schedule for beta values
        beta_start = 0.0001
        beta_end = 0.02
        return np.linspace(beta_start, beta_end, timesteps)

    def forward_diffusion_sample(self, x0, t):
        # Add noise to the input image based on the current timestep
        noise = torch.randn_like(x0)
        alpha_cumprod_t = self.alpha_cumprod[t]
        return torch.sqrt(alpha_cumprod_t) * x0 + torch.sqrt(1 - alpha_cumprod_t) * noise

    def reverse_diffusion_step(self, xt, t):
        # Predict noise and denoise the image
        pred_noise = self.model(xt)
        alpha_cumprod_t = self.alpha_cumprod[t]
        return (xt - torch.sqrt(1 - alpha_cumprod_t) * pred_noise) / torch.sqrt(alpha_cumprod_t)

    def forward(self, x):
        # Reverse diffusion process to reconstruct the image
        for t in reversed(range(self.timesteps)):
            x = self.reverse_diffusion_step(x, t)
        return x

# Sample Input
img_shape = (3, 64, 64)  # Sample image shape: 3 channels (RGB), 64x64 pixels
diffusion_model = DiffusionModel(img_shape)
gaussian_diffusion = GaussianDiffusion(diffusion_model)

# Dummy input: Random image tensor
x0 = torch.randn((1, *img_shape))  # Batch size of 1
xt = gaussian_diffusion.forward_diffusion_sample(x0, t=500)  # Add noise at timestep 500
x_reconstructed = gaussian_diffusion(xt)  # Reconstruct the image from noisy input

# Print the shape of the reconstructed image
print(x_reconstructed.shape)  # Should print torch.Size([1, 3, 64, 64])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;diffusion-models&#34;&gt;Diffusion Models&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Diffusion models have emerged as a powerful approach in generative AI, especially for tasks involving image generation and editing. These models iteratively denoise images to recover the original data distribution. The objective function for diffusion models can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
C(x) = -\frac{1}{\sigma \sqrt{2\pi}} \left(\frac{x - \mu}{\sigma}\right)^2 e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}
$$
&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x$ represents the data point.&lt;/li&gt;
&lt;li&gt;$\mu$ represents the mean of the data distribution.&lt;/li&gt;
&lt;li&gt;$\sigma$ represents the standard deviation of the data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
L(x) = \frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2 + \frac{1}{2} \log(2 \pi \sigma^2)
$$&lt;/p&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( \frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2 )$ represents the squared deviation from the mean, often used in diffusion models to measure the distance between generated and target distributions.&lt;/li&gt;
&lt;li&gt;$( \frac{1}{2} \log(2 \pi \sigma^2) )$ represents the entropy term, which accounts for the normalization factor in the Gaussian distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also represent the diffusion objective function in a more general form related to a stochastic process:&lt;/p&gt;
&lt;p&gt;$$
L(x) = \mathbb{E} \left[ \frac{1}{2} | x - \mu |^2 + \frac{1}{2} \log(2 \pi \sigma^2) \right]
$$&lt;/p&gt;
&lt;p&gt;Here, $( \mathbb{E} )$ denotes the expectation over the diffusion process, capturing the average cost.&lt;/p&gt;
&lt;p&gt;This objective function measures the discrepancy between the true noise added to the data and the noise predicted by the model, aiming to train the model to accurately reverse the diffusion process.&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;code&gt;Where:&lt;/code&gt; \(x_t\) is the noised image at timestep \(t\), and \(\epsilon_\theta\) is the noise prediction network. Recent works like DALL-E 2 and Stable Diffusion have demonstrated the remarkable capabilities of diffusion models in text-to-image generation and image editing tasks. These models leverage large-scale training on diverse datasets and incorporate additional conditioning information to enable fine-grained control over generated images.
&lt;/p&gt;
&lt;h2 id=&#34;gans-generative-adversarial-networks&#34;&gt;GANs (Generative Adversarial Networks)&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed by Goodfellow et al. in 2014. GANs consist of two neural networks, a generator and a discriminator, which compete against each other in a zero-sum game framework. The generator aims to generate realistic data samples, while the discriminator attempts to distinguish between real and generated samples. The objective functions for GANs can be expressed as follows: 
&lt;/p&gt;
&lt;p&gt;$$
L_{\text{GAN}} = \mathbb{E}_{x \sim p_x{\text{data}(x)}} [\log D(x)] + \text{generated data samples}
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$G$ represents the generator network.&lt;/li&gt;
&lt;li&gt;$D$ represents the discriminator network.&lt;/li&gt;
&lt;li&gt;$x$ represents the real data sample.&lt;/li&gt;
&lt;li&gt;$z$ represents the random noise vector sampled from a prior distribution $p_z(z)$.&lt;/li&gt;
&lt;li&gt;$p_{\text{data}(x)}$ represents the data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb{E}_{x \sim p_x{\text{data}(x)}} [\log D(x)]$ represents the expected value of the discriminator&amp;rsquo;s output for real data samples.&lt;/li&gt;
&lt;li&gt;$\mathbb{E}_{z \sim p_z(z)} [\log (1 - D(G(z)))]$ represents the expected value of the discriminator&amp;rsquo;s output for generated data samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The generator aims to minimize this objective while the discriminator aims to maximize it.&lt;/p&gt;
&lt;h2 id=&#34;self-supervised-learning&#34;&gt;Self-Supervised Learning&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Self-Supervised Learning (SSL) is a paradigm in machine learning where the model learns to generate labels from the input data itself, without requiring manually labeled data. This approach uses pretext tasks to learn representations that can be transferred to downstream tasks. One common objective in self-supervised learning is the contrastive loss, which can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
L_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(h_i, h_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(h_i, h_k)/\tau)}
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h_i$ and $h_j$ represent the encoded representations of positive pairs.&lt;/li&gt;
&lt;li&gt;$\text{sim}(h_i, h_j)$ represents the similarity measure between $h_i$ and $h_j$.&lt;/li&gt;
&lt;li&gt;$\tau$ represents the temperature parameter.&lt;/li&gt;
&lt;li&gt;$N$ represents the number of samples.&lt;/li&gt;
&lt;li&gt;$\mathbb{1}_{[k \neq i]}$ is an indicator function that is 1 if $k \neq i$ and 0 otherwise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\exp(\text{sim}(h_i, h_j)/\tau)$ represents the exponential of the similarity between the positive pairs scaled by the temperature.&lt;/li&gt;
&lt;li&gt;The denominator sums the exponential similarities of all pairs except the identical ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This objective encourages the model to bring similar samples closer in the representation space and push dissimilar ones apart.&lt;/p&gt;
&lt;h2 id=&#34;adversarial-attacks&#34;&gt;Adversarial Attacks&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Adversarial attacks involve manipulating input data to deceive machine learning models into making incorrect predictions. One common method is the Fast Gradient Sign Method (FGSM), which perturbs the input data in the direction of the gradient of the loss with respect to the input. The formula for generating an adversarial example using FGSM can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
x_{\text{adv}} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x_{\text{adv}}$ represents the adversarial example.&lt;/li&gt;
&lt;li&gt;$x$ represents the original input data.&lt;/li&gt;
&lt;li&gt;$\epsilon$ represents the perturbation magnitude.&lt;/li&gt;
&lt;li&gt;$\nabla_x J(\theta, x, y)$ represents the gradient of the loss function $J$ with respect to the input $x$.&lt;/li&gt;
&lt;li&gt;$J(\theta, x, y)$ represents the loss function of the model.&lt;/li&gt;
&lt;li&gt;$\theta$ represents the model parameters.&lt;/li&gt;
&lt;li&gt;$y$ represents the true label of the input data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{sign}(\nabla_x J(\theta, x, y))$ represents the sign of the gradient of the loss with respect to the input, indicating the direction to perturb the input to maximize the loss.&lt;/li&gt;
&lt;li&gt;The adversarial example $x_{\text{adv}}$ is created by adding this perturbation to the original input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI continues to advance rapidly, with ongoing developments in model architectures, training techniques, and applications across various domains. The ability of generative models to create high-quality content, from text and images to music and videos, underscores their transformative potential. While there are challenges and ethical considerations to address, the future of generative AI is promising, with numerous opportunities for innovation and interdisciplinary collaboration. As we explore these frontiers, it is crucial to remain mindful of the societal impacts and strive for responsible use of these powerful technologies.
&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Generative AI is revolutionizing various fields by creating new content and enhancing existing applications. This blog explores current trends, practical applications, challenges, and future opportunities of generative models. Key areas include advancements in model architectures, real-world applications like content creation and healthcare, and the integration of techniques such as GANs and diffusion models.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Generative AI presents both exciting opportunities and significant challenges. This blog covers the latest trends in generative models, their applications across various industries, and critical issues such as ethical considerations and future directions. Learn about the potential of models like GANs and diffusion techniques, and their impact on content creation and other fields.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand GANs (Generative Adversarial Networks)
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study GAN architecture (Generator and Discriminator)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review applications and improvements&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learn about Variational Autoencoders (VAEs)
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore VAE structure and loss function&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Examine use cases in generative tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Familiarize with Diffusion Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand diffusion process and objective function&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review recent advancements (e.g., DALL-E 2, Stable Diffusion)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore Transformer Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study transformer architecture and attention mechanisms&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review its application in language generation and understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learn about Pretrained Language Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study fine-tuning techniques for specific tasks&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore popular models (e.g., GPT, BERT, T5)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand Model Evaluation Metrics
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review metrics like BLEU, ROUGE, and FID for generative models&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study methods for evaluating model performance in different contexts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Investigate Ethical Considerations
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore challenges related to bias, fairness, and security&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study frameworks for responsible AI development&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Aug 2024). &lt;em&gt;Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024generativeaitrends,
  title   = &amp;quot;Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Aug&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Bender, E. M., Gebru, T., McMillan-Major, A., &amp;amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? &lt;em&gt;Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2102.02503&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2102.02503&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BROWN, T. B., MANE, D., LANGE, I., &amp;amp; et al. (2020). Language Models are Few-Shot Learners. &lt;em&gt;Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2005.14165&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CHESNEY, R., &amp;amp; CITRON, D. K. (2019). Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. &lt;em&gt;California Law Review&lt;/em&gt;, 107(6), 1753-1819. &lt;a href=&#34;https://doi.org/10.2139/ssrn.3213954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.2139/ssrn.3213954&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DENG, J., DONAHUE, J., &amp;amp; HAREL, M. (2009). ImageNet: A Large-Scale Hierarchical Image Database. &lt;em&gt;Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2009.5206848&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR.2009.5206848&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DOSOVITSKIY, A., BROSSARD, T., &amp;amp; SPRINGENBERG, J. (2017). Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks. &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 39(5), 939-949. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2016.2593826&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2016.2593826&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FRID-ADAR, M., ELIYAHU, S., &amp;amp; GOLDY, S. (2018). GAN-based Synthetic Medical Image Augmentation for Increased CNN Performance in Liver Lesion Classification. &lt;em&gt;IEEE Transactions on Medical Imaging&lt;/em&gt;, 37(6), 1334-1343. &lt;a href=&#34;https://doi.org/10.1109/TMI.2018.2813792&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TMI.2018.2813792&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;KINGMA, D. P., &amp;amp; WELLING, M. (2013). Auto-Encoding Variational Bayes. &lt;em&gt;Proceedings of the 2nd International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1312.6114&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RADFORD, A., WU, J., &amp;amp; AMODEI, D. (2021). Learning Transferable Visual Models From Natural Language Supervision. &lt;em&gt;Proceedings of the 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2103.00020&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RAMESH, A., MENG, C., &amp;amp; ZHANG, S. (2021). DALLÂ·E: Creating Images from Text. &lt;em&gt;OpenAI&lt;/em&gt;. &lt;a href=&#34;https://openai.com/research/dall-e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://openai.com/research/dall-e&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;STRUBELL, E., GANASSI, M., &amp;amp; MCAFEE, P. (2019). Energy and Policy Considerations for Deep Learning in NLP. &lt;em&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1906.02243&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1906.02243&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WIESE, S., BOLAND, M., &amp;amp; TONG, A. (2019). A Survey on Machine Learning in Finance. &lt;em&gt;Proceedings of the 26th International Conference on Machine Learning (ICML 2019)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1910.02342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1910.02342&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VASWANI, A., SHAZEER, N., &amp;amp; PARMAR, N. (2017). Attention Is All You Need. &lt;em&gt;Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1706.03762&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>The Day I Became an IRONMAN: Conquering 113KM of the Limits of Pain and Human Endurance</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-i-became-an-ironman/</link>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-i-became-an-ironman/</guid>
      <description>&lt;p&gt;Reflecting on the extraordinary journey to becoming an IRONMAN fills me with immense pride. This pivotal moment, the culmination of months of unwavering dedication and resilience, stands as a true testament to the power of mind and body.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_001.png&#34; alt=&#34;Ironman Finisher Certificate&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 1: This is more than just a certificate â€” itâ€™s the embodiment of years of sweat, sacrifice, and perseverance. The IRONMAN 70.3 Finisher certificate will forever hold a special place in my heart. After all the doubts, the training, the struggles, this moment was a dream realized. In that instant, I was no longer just a triathlete; I was an IRONMAN. Can you imagine the feeling? There&#39;s nothing like it. This is the pride of my life.&lt;/p&gt;
&lt;/div&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/fdvJU4-oHuY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Swim 2.4 miles.&lt;br&gt;Bike 112 miles.&lt;br&gt;Run 26.2 miles.&lt;br&gt;Brag for the rest of your life.&lt;a href=&#34;https://twitter.com/hashtag/AnythingIsPossible?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AnythingIsPossible&lt;/a&gt; &lt;a href=&#34;https://t.co/qnjX29bvdN&#34;&gt;pic.twitter.com/qnjX29bvdN&lt;/a&gt;&lt;/p&gt;&amp;mdash; IRONMAN Triathlon (@IRONMANtri) &lt;a href=&#34;https://twitter.com/IRONMANtri/status/1165104423519043584?ref_src=twsrc%5Etfw&#34;&gt;August 24, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;hr&gt;
&lt;h2 id=&#34;the-starting-line&#34;&gt;The Starting Line&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;As I stood at the starting point of the 2024 IRONMAN 70.3 Bangsaen in Thailand, surrounded by breathtaking coastal scenery, a wave of anticipation and apprehension washed over me. The challenge ahead was daunting, yet exhilarating.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-race-breakdown&#34;&gt;The Race Breakdown&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The triathlon comprised three grueling segments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ğŸŠâ€â™‚ï¸ &lt;strong&gt;1.9km (1.2-mile) swim&lt;/strong&gt; to kick off the journey.&lt;/li&gt;
&lt;li&gt;ğŸš´â€â™‚ï¸ &lt;strong&gt;90km (56-mile) bike ride&lt;/strong&gt; through scenic routes.&lt;/li&gt;
&lt;li&gt;ğŸƒâ€â™‚ï¸ &lt;strong&gt;21.1km (13.1-mile) run&lt;/strong&gt; that pushed me to my absolute limits.&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_007.png&#34; alt=&#34;Swimming Segment â€“ Open Water&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: The first challenge: a 1.9 km open-water swim in the rough sea at Bangsaen Beach. The waves were fierce, the crowd was intense, and the physical toll was real. But in those moments, I felt alive. The sting of saltwater in my eyes, the struggle to find a rhythm, the jostling of other swimmers â€” it was all part of the experience. And despite being kicked in the chest by another athlete and the panic of overcrowded waters, I kept pushing. 56 minutes might not seem fast, but to me, it was a victory. It was the first step toward my dream, and I made it count.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_008.png&#34; alt=&#34;Transition from Swim to Bike with Celebrity&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: From swim to bike: the transition that felt like a breath of fresh air. As I climbed out of the water and rushed toward the bike, I was met by an unexpected surprise â€” Thai celebrity Tao Somchai Khemglad! I was starstruck, honestly. But in that moment, everything seemed to fall into place. A moment of connection, a fleeting encounter with someone whoâ€™s made a name in Thai culture, and here I was, sharing the same course. It was one of those little joys that made the whole race feel even more special.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_009.png&#34; alt=&#34;Bike Segment â€“ 90K Ride&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: The second challenge: a 90 km bike ride through the beautiful yet demanding roads of Chonburi. The sun was scorching, and every pedal stroke felt heavier as the kilometers piled on. But there was no turning back. This was the moment where strategy met endurance. I pushed through, determined to finish strong. Every climb felt like a test, but every descent was a reward. The entire bike course was a dance between effort and relief.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_010.png&#34; alt=&#34;Midway through Bike Segment&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: A glimpse of the bike ride, around the 50 km mark. Still going strong, but the real challenge was ahead. My legs burned, but my mind stayed focused. This was where the race truly began â€” endurance versus fatigue, willpower versus doubt. At this point, it wasnâ€™t about speed; it was about finishing strong, pushing through every obstacle. And I was ready.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_014.png&#34; alt=&#34;The Final 21.1K Run to Becoming an IRONMAN&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: The final 21.1 km run â€” the moment when everything I had worked for came to a head. The energy from the crowd, the rush of adrenaline, and the overwhelming sense of accomplishment made every step feel light. Crossing that finish line wasnâ€™t just about completing a race; it was about fulfilling a dream. It was the moment I could finally call myself an IRONMAN.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Together, these segments totaled an astounding 113km (70.3 miles) of relentless determination.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_011.png&#34; alt=&#34;Garmin Fenix GPS Tracking â€“ Bike Ride&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: A snapshot of my Garmin Fenix watch GPS during the 90 km bike ride. It tracks every turn, every climb, every moment of struggle. I used this data to monitor my pace and keep myself on track, reminding me that every kilometer was one step closer to victory. Technology isnâ€™t just a tool; itâ€™s a companion in moments like these, keeping me grounded and focused.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_012.png&#34; alt=&#34;Garmin Fenix GPS Tracking â€“ Running Segment&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: From bike to run: the transition to the 21.1 km half marathon. My Garmin tracked every stride as I pushed forward. This part of the race felt endless, but also exhilarating. Every step was a reminder that the finish line was within reach. This was the final challenge â€” the test of all the training, the pain, the sacrifices. This was my moment to shine.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-journey&#34;&gt;The Journey&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The road to IRONMAN 70.3 Bangsaen was filled with challenges. Each stroke, pedal, and stride pushed me beyond my comfort zone. Moments of doubt and exhaustion tested my resolve, yet each obstacle revealed a reservoir of inner strength I never knew existed.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_002.png&#34; alt=&#34;IRONMAN 70.3 Finishing Time&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 9: Crossing that finish line in 7 hours and 51 minutes was more than just a time on a clock â€” it was a declaration that I had gone further than I ever imagined. The cut-off was 8:30, and to be honest, I wasnâ€™t even sure I could break 8 hours. But there I was, pushing beyond my limits and proving to myself that anything is possible. The feeling when you surpass your own expectations? Unreal. The joy and relief that flooded over me as I crossed that line â€” words canâ€™t even come close to describing it.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_004.png&#34; alt=&#34;My BIB Number for IRONMAN 70.3&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 10: BIB number 201 â€” a number that will forever be etched in my memory. Itâ€™s amazing how something as simple as a number can carry so much meaning. It represents the start of a new chapter, the challenge ahead, and the person I became along the way. At that moment, I didnâ€™t just wear a number, I wore my determination, my discipline, and my belief in myself. That was my identity for the race, and itâ€™s a number Iâ€™ll carry with me forever.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The unwavering support of the event staff and my fellow athletes played a pivotal role. Their encouragement and camaraderie fueled my determination and reminded me I wasnâ€™t alone on this journey.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-finish-line&#34;&gt;The Finish Line&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Crossing the finish line after 7.51 hours was a moment of indescribable joy, relief, and pride. Just moments before the 8.30-hour cutoff, I officially earned the title of IRONMAN 70.3 finisher. It wasnâ€™t just about completing a race; it was about embracing the challenges and pushing beyond my perceived limits.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_005.png&#34; alt=&#34;Finisher Certificate Duplicate&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 11: The IRONMAN 70.3 Finisher certificate â€” not just a piece of paper, but a symbol of everything I put into this journey. Each glance at it takes me back to the early mornings, the late nights, the sacrifices, the pain, and the incredible sense of achievement that followed. This certificate isnâ€™t just proof of completion â€” itâ€™s a reminder that with enough commitment and grit, anything is possible. Itâ€™s a constant reflection of my strength, my perseverance, and the rewards that come from pushing beyond the limits.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;gear-and-conditions&#34;&gt;Gear and Conditions&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Equipped with my MERIDA SCULTURA bike and NIKE Zoom Fly 5 shoes, I faced the scorching midday sun, which intensified the already demanding race. Despite the fatigue and heat, sheer determination propelled me forward.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_015.png&#34; alt=&#34;My Merida Scultura Bike&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 12: My trusty Merida Scultura 4000, the bike that took me through 90 km of challenge. Bought second-hand for this purpose, this bike became a symbol of resourcefulness and resilience. It carried me through both IRONMAN 70.3 and Laguna Phuket Triathlon, and I wouldnâ€™t have had it any other way. Itâ€™s more than just a machine â€” itâ€™s a partner in my journey.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;gratitude&#34;&gt;Gratitude&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;This achievement would not have been possible without the tireless efforts of the IM70.3 Bangsaen organizers and staff. Their support made this dream a reality, and I am deeply grateful.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_006.png&#34; alt=&#34;Official IRONMAN 70.3 Stats&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 13: Here are the numbers that tell the story: the official IRONMAN 70.3 race stats. These arenâ€™t just data points. Theyâ€™re the moments that define who I am as an athlete. Every second of this race, every split time, tells the story of my resilience, my strategy, and the grit it took to keep going. These stats are proof of what can be accomplished when you push past your comfort zone, when you face adversity, and when you never, ever give up.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;lessons-learned&#34;&gt;Lessons Learned&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Becoming an IRONMAN wasnâ€™t just a physical featâ€”it was a testament to perseverance and the resilience of the human spirit. Through this journey, Iâ€™ve learned that true strength is discovered in the face of adversity.&lt;/p&gt;
&lt;p&gt;To anyone pursuing a seemingly insurmountable dream, believe in yourself and embrace the challenges. It is through perseverance and determination that dreams transform into reality.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://raw.githubusercontent.com/kaopanboonyuen/kaopanboonyuen.github.io/main/files/CBM2025_MARATHON_42KM/IMG_0438.jpg&#34; alt=&#34;VO2 Max Boost to 55&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 14: Achieving a VO2 max score of 55 â€” a testament to the relentless effort Iâ€™ve put into my training. This number isnâ€™t just a figure; it represents hours of hard work, dedication, and pushing my body to its absolute limits. Itâ€™s more than just an improvement in physical performance â€” itâ€™s a breakthrough in my endurance, my commitment, and my journey toward being the best version of myself. This is just the beginning. The hard work pays off, and the future looks limitless.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;the-celebration&#34;&gt;The Celebration&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;After crossing the finish line, I rewarded myself with a well-deserved feast of Japanese food and sushiâ€”a delicious acknowledgment of the physical and mental endurance the race demanded.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_013.png&#34; alt=&#34;IRONMAN and Laguna Phuket Medals on Desk&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 15: Both my IRONMAN 70.3 and Laguna Phuket Triathlon finisher medals proudly displayed on my desk. Every time I look at them, I remember the intense training, the sweat, and the relentless pursuit of greatness. They remind me that all the hard work paid off. These medals arenâ€™t just awards; theyâ€™re my story, etched in metal.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The happiness I find in this journey lies not just in the end goal but in the process itselfâ€”the struggles, triumphs, and growth along the way. To anyone considering a challenging path, embrace it wholeheartedly. Trust in the transformative power of hard work and dedication. In the pursuit of excellence, we find joy and fulfillment that words cannot capture.&lt;/p&gt;
&lt;p&gt;With immense gratitude and pride, I declare: &lt;strong&gt;I am an IRONMAN.&lt;/strong&gt;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/ySziECwLLs4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;PS.&lt;/strong&gt; The year began with completing the &lt;a href=&#34;https://www.ironman.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;IM70.3&lt;/strong&gt;&lt;/a&gt; and ended with crossing the finish line at the &lt;a href=&#34;https://www.lagunaphukettri.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Laguna Triathlon in Phuket&lt;/strong&gt;&lt;/a&gt;, which included a 1.8K swim, a 55K bike ride, and a 12K run. Truly, it was a year defined by triathlons for me.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As I reflect on this experience, I feel immense gratitudeâ€”for the beauty of Phuket, the camaraderie of my fellow athletes, and the personal growth this race has inspired. The Laguna Phuket Triathlon is more than just an event; itâ€™s a celebration of human spirit and perseverance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To anyone considering a triathlon, my advice is simple: go for it. Embrace the challenge, trust your training, and donâ€™t forget to enjoy the ride. Whether youâ€™re a seasoned athlete or a first-timer like me, the journey will leave you stronger, wiser, and more fulfilled.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_IM703_003.png&#34; alt=&#34;IRONMAN and Laguna Phuket Finisher Medals&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 16: Two medals that mean the world to me: IRONMAN 70.3 and Laguna Phuket Triathlon. These arenâ€™t just awards, theyâ€™re symbols of my journey through some of the toughest courses in the country. IRONMAN 70.3 was a challenge in itself, but add the Laguna Phuket Triathlon â€” one of the most grueling events in Thailand â€” and it feels like Iâ€™ve conquered my own personal Everest. Holding these medals in my hand, knowing that Iâ€™ve triumphed in both, is one of the most fulfilling moments of my life. These arenâ€™t just race medals; they are the manifestation of my unrelenting dedication and perseverance.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_LAGUNA_TRI_2025_03.png&#34; alt=&#34;International Athletes List at Laguna Phuket Triathlon&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 17: The international athletes who surrounded me at Laguna Phuket Triathlon. Seeing so many passionate and skilled athletes from around the world only fueled my determination to finish. It was an honor to race alongside such incredible talent.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_LAGUNA_TRI_2025_01.png&#34; alt=&#34;Laguna Phuket Triathlon Finisher Certificate&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 18: The Laguna Phuket Triathlon Finisher certificate â€” another incredible achievement in my journey. This race, one of the most difficult in Thailand, pushed me to my limits. But crossing that finish line made all the struggle worth it. I am a triathlete. I am a finisher.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_LAGUNA_TRI_2025_02.png&#34; alt=&#34;Official Laguna Phuket Triathlon Results&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 19: Official Laguna Phuket Triathlon result â€” this oneâ€™s going to stay with me forever. Itâ€™s a testament to the countless hours of training, the dedication, and the willpower it took to finish this legendary race.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_LAGUNA_TRI_2025_04.jpg&#34; alt=&#34;Laguna Phuket Triathlon BIB Number&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 20: My BIB number 316 at Laguna Phuket Triathlon. A number that holds memories of every pedal stroke, every step, and every breath during the race. Itâ€™s my personal marker, and Iâ€™ll carry it with pride.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_LAGUNA_TRI_2025_05.png&#34; alt=&#34;Official Laguna Phuket Triathlon Results on Website&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 21: The official results from Laguna Phuket Triathlon, now immortalized online. Seeing my name in the official rankings was a moment of deep satisfaction. It was the confirmation of what I knew all along.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_LAGUNA_TRI_2025_06.png&#34; alt=&#34;Laguna Phuket Triathlon Finisher Certificate on Website&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 22: My official Laguna Phuket Triathlon finisher certificate, proudly displayed on the official website. This digital version still carries the same weight as the physical one â€” both represent a triumph over adversity.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Completing the &lt;strong&gt;IRONMAN 70.3&lt;/strong&gt; and &lt;strong&gt;Laguna Phuket Triathlon&lt;/strong&gt; was more than just a series of races â€” it was a &lt;strong&gt;journey&lt;/strong&gt; that tested my limits, reshaped my understanding of &lt;strong&gt;endurance&lt;/strong&gt;, and ultimately redefined what I thought was possible. From the very first moment I registered for these events, I knew they would be &lt;strong&gt;life-changing&lt;/strong&gt;. The &lt;strong&gt;training&lt;/strong&gt; was intense, pushing my body to its physical limits while demanding an unwavering &lt;strong&gt;mental focus&lt;/strong&gt;. Each early morning, every grueling workout, and the &lt;strong&gt;sacrifices&lt;/strong&gt; made along the way were all driven by a single goal: to prove to myself that I could conquer these seemingly &lt;strong&gt;insurmountable challenges&lt;/strong&gt;. When I crossed the finish line of &lt;strong&gt;IRONMAN 70.3&lt;/strong&gt; in &lt;strong&gt;7 hours and 51 minutes&lt;/strong&gt;, well below the &lt;strong&gt;cut-off&lt;/strong&gt;, it was the culmination of months of &lt;strong&gt;hard work&lt;/strong&gt;, sweat, and &lt;strong&gt;self-doubt&lt;/strong&gt;. That moment, when I held my &lt;strong&gt;finisher medal&lt;/strong&gt; and looked at my race number â€” &lt;strong&gt;201&lt;/strong&gt; â€” I realized it wasnâ€™t just a race I had completed; it was a &lt;strong&gt;transformation&lt;/strong&gt;. Every mile of the &lt;strong&gt;1.9 km swim&lt;/strong&gt;, each moment on the &lt;strong&gt;90 km bike ride&lt;/strong&gt;, and the final &lt;strong&gt;21.1 km run&lt;/strong&gt; were stepping stones toward the person I had become â€” someone capable of achieving the &lt;strong&gt;extraordinary&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;The finish line is just the beginning. Itâ€™s not the end of the journey, but a marker of whatâ€™s possible when you push beyond your limits. The true victory is in the person you become along the way.&lt;/strong&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;But the journey didnâ€™t stop there. Completing the &lt;strong&gt;Laguna Phuket Triathlon&lt;/strong&gt;, one of &lt;strong&gt;Thailand&amp;rsquo;s most demanding races&lt;/strong&gt;, felt like the perfect bookend to this chapter of my &lt;strong&gt;athletic journey&lt;/strong&gt;. The &lt;strong&gt;Laguna Phuket Triathlon&lt;/strong&gt; pushed me even further, not just physically but emotionally. It was an experience that tested my &lt;strong&gt;resilience&lt;/strong&gt;, &lt;strong&gt;focus&lt;/strong&gt;, and &lt;strong&gt;determination&lt;/strong&gt; like never before. Holding both the &lt;strong&gt;IRONMAN 70.3&lt;/strong&gt; and &lt;strong&gt;Laguna Phuket finisher medals&lt;/strong&gt; together wasnâ€™t just about the victories â€” it was about the &lt;strong&gt;strength&lt;/strong&gt;, &lt;strong&gt;grit&lt;/strong&gt;, and &lt;strong&gt;perseverance&lt;/strong&gt; required to reach the finish lines of both. These medals symbolize far more than athletic achievement; they are a reflection of everything Iâ€™ve learned about myself. They are reminders of the moments I wanted to &lt;strong&gt;quit&lt;/strong&gt; but didnâ€™t, the times my body screamed in protest but my mind &lt;strong&gt;pushed through&lt;/strong&gt; anyway. And above all, they represent the &lt;strong&gt;belief&lt;/strong&gt; that anything is possible if you stay committed to the journey. My journey to becoming an &lt;strong&gt;IRONMAN&lt;/strong&gt; and conquering the &lt;strong&gt;Laguna Phuket Triathlon&lt;/strong&gt; has been one of the most rewarding chapters of my life â€” a story of &lt;strong&gt;endurance&lt;/strong&gt;, &lt;strong&gt;triumph&lt;/strong&gt;, and the incredible power of the &lt;strong&gt;human spirit&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Catch you at the next finish line ğŸâœ¨,&lt;/p&gt;
&lt;p&gt;Kao&lt;/p&gt;
&lt;p&gt;#IRONMAN703 #IRONMAN #IRONMANtri #Triathlon #AnythingIsPossible&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Feb 2024). &lt;em&gt;The Day I Became an IRONMAN: Conquering 113KM of the Limits of Pain and Human Endurance&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-I-became-an-IRONMAN/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-I-became-an-IRONMAN/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024ironman,
  title   = &amp;quot;The Day I Became an IRONMAN: Conquering 113KM of the Limits of Pain and Human Endurance&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Feb&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-I-became-an-IRONMAN/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inspiring the Future of AI Innovations</title>
      <link>https://kaopanboonyuen.github.io/talk/inspiring-the-future-of-ai-innovations/</link>
      <pubDate>Thu, 01 Feb 2024 09:09:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/inspiring-the-future-of-ai-innovations/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Geospatial Big Data Analytics</title>
      <link>https://kaopanboonyuen.github.io/talk/geospatial-big-data-analytics/</link>
      <pubDate>Fri, 01 Dec 2023 08:15:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/geospatial-big-data-analytics/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>AI Research Featured in Techsauce News</title>
      <link>https://kaopanboonyuen.github.io/talk/ai-research-featured-in-techsauce-news/</link>
      <pubDate>Tue, 15 Aug 2023 10:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/ai-research-featured-in-techsauce-news/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ai-research-featured-in-techsauce-news&#34;&gt;AI Research Featured in Techsauce News&lt;/h2&gt;
&lt;p&gt;Honored to have my AI research featured in &lt;a href=&#34;https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Techsauce&lt;/a&gt;, Thailandâ€™s leading tech media platform. The article showcases my work in advancing AI technologies, emphasizing innovation and impact in the field.&lt;/p&gt;
&lt;p&gt;This recognition underscores the significance of AI research in driving technological progress and its application in real-world scenarios. I&amp;rsquo;m grateful for the opportunity to contribute to the evolving landscape of AI and look forward to future endeavors in this exciting field.&lt;/p&gt;
&lt;h2 id=&#34;-mars-revolutionizing-car-damage-segmentation&#34;&gt;ğŸš— MARS: Revolutionizing Car Damage Segmentation&lt;/h2&gt;
&lt;p&gt;Proud to have our flagship model, MARS (Mask Attention Refinement with Sequential Quadtree Nodes), featured in &lt;a href=&#34;https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Techsauce News&lt;/a&gt;. This recognition underscores our commitment to advancing AI in the automotive insurance sector.&lt;/p&gt;
&lt;h3 id=&#34;-what-is-mars&#34;&gt;ğŸ” What is MARS?&lt;/h3&gt;
&lt;p&gt;MARS is a state-of-the-art deep learning model designed for precise car damage instance segmentation. By leveraging sequential quadtree attention, it surpasses existing methods like Mask R-CNN and PointRend, achieving notable improvements in segmentation accuracy.&lt;/p&gt;
&lt;h3 id=&#34;-marsail-driving-innovation&#34;&gt;ğŸ§  MARSAIL: Driving Innovation&lt;/h3&gt;
&lt;p&gt;MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory) is dedicated to pioneering research at the intersection of computer vision, transformers, and automotive AI. Our mission is to revolutionize the automotive insurance and repair industries through AI-driven automation, delivering breakthroughs in segmentation, localization, and decision intelligence.&lt;/p&gt;
&lt;h3 id=&#34;-impact-and-recognition&#34;&gt;ğŸ“ˆ Impact and Recognition&lt;/h3&gt;
&lt;p&gt;The feature in Techsauce News highlights the significance of our work in transforming the automotive insurance landscape. By integrating advanced AI models like MARS, we aim to enhance the accuracy and efficiency of car damage evaluation, paving the way for smarter, more automated insurance processes.&lt;/p&gt;
&lt;p&gt;Explore more about MARS and MARSAIL&amp;rsquo;s innovative solutions at &lt;a href=&#34;https://kaopanboonyuen.github.io/MARS/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io/MARS/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Invited to Italy for ICIAP2023</title>
      <link>https://kaopanboonyuen.github.io/talk/invited-to-italy-for-iciap2023/</link>
      <pubDate>Fri, 11 Aug 2023 10:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/invited-to-italy-for-iciap2023/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;mars_italy_iciap0001.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0002.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0003.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0004.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0005.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0006.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;
&lt;img src=&#34;mars_italy_iciap0007.jpg&#34; alt=&#34;Teerapong Panboonyuen at Italy&#34;&gt;&lt;/p&gt;
&lt;p&gt;I am excited to share that my research, &lt;strong&gt;&amp;ldquo;MARS: Mask Attention Refinement with Sequential Quadtree Nodes,&amp;rdquo;&lt;/strong&gt; was accepted for presentation at the &lt;strong&gt;ICIAP 2023 Workshop&lt;/strong&gt; in Italy. This prestigious conference, held biennially by the CVPL under the International Association for Pattern Recognition (IAPR), brought together leading experts from around the world to discuss the latest advancements in car insurance and computer vision technologies.&lt;/p&gt;
&lt;p&gt;My research addressed the critical challenge of evaluating car damages with greater accuracy. Current deep learning networks struggle with this task, producing coarse segmented masks that are not suitable for real-world applications. To tackle this issue, I developed MARS, which employs self-attention mechanisms and quadtree transformers to refine instance segmentation accuracy. MARS represents a significant advancement in the field by drawing global dependencies between sequential quadtree nodes and recalibrating channel weights to predict highly accurate instance masks.&lt;/p&gt;
&lt;p&gt;The extensive experiments conducted as part of my research demonstrated that MARS outperforms several state-of-the-art instance segmentation methods, including Mask R-CNN, PointRend, and Mask Transfiner. Specifically, MARS achieved a substantial improvement in maskAP scores, with a +1.3 increase using the R50-FPN backbone and a +2.3 increase with the R101-FPN backbone on the Thai car-damage dataset. These results highlight the potential of MARS to significantly enhance the accuracy of car damage evaluations, offering promising applications for the car insurance industry.&lt;/p&gt;
&lt;!-- ![Teerapong Panboonyuen at Italy](mars_italy_iciap0008.jpg) --&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;mars_italy_iciap0008.jpg&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. I have published an article with Techsauce. (Image source: techsauce.co, &lt;a href=&#34;https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai&#34; target=&#34;_blank&#34;&gt;mars-deep-tech-startup-thaivivat-ai&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you to Techsauce, the Thai tech news agency, for showcasing my research on the application of AI in the auto insurance and garage industry. You can read the full article here: &lt;a href=&#34;https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Techsauce&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Presenting my findings at ICIAP 2023 was a fantastic opportunity to engage with fellow researchers and industry professionals, exploring the opportunities, challenges, and future directions in this rapidly evolving field.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</title>
      <link>https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/tgcKR97Ea8I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Semantic segmentation is vital in remote sensing, particularly for identifying and categorizing different land use and land cover types. In regions like Thailand, where agriculture is central to the economy, precise segmentation of satellite imagery can enhance our ability to track crop health, predict yields, and improve resource management. Our model, &lt;em&gt;MeViT&lt;/em&gt; (Medium-Resolution Vision Transformer), is specifically designed to classify agricultural crops like para rubber, corn, and pineapple across Thailandâ€™s varied landscapes.&lt;/p&gt;
&lt;h2 id=&#34;background-on-vision-transformers&#34;&gt;Background on Vision Transformers&lt;/h2&gt;
&lt;p&gt;Unlike traditional convolutional neural networks (CNNs), which are excellent at capturing local spatial hierarchies, Vision Transformers excel at modeling long-range dependencies through self-attention mechanisms. This unique structure allows &lt;em&gt;MeViT&lt;/em&gt; to interpret both local and global features, enhancing its effectiveness in agricultural land segmentation tasks where accuracy and detail are paramount.&lt;/p&gt;
&lt;h2 id=&#34;mevit-architecture&#34;&gt;MeViT Architecture&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;MeViT&lt;/em&gt; leverages a multi-branch architecture tailored for medium-resolution images, balancing computational efficiency with high-quality feature extraction. This design approach enables the model to capture details across multiple spatial scales, which is crucial for segmenting complex land use patterns in agricultural imagery.&lt;/p&gt;
&lt;p&gt;In particular, the revised mixed-scale convolutional feedforward network (MixCFN) in &lt;em&gt;MeViT&lt;/em&gt; incorporates multiple depth-wise convolution paths, further refining feature extraction by allowing the model to focus on different spatial scales. This enhanced architecture achieves an efficient trade-off between model complexity and performance, making it well-suited for large-scale image analysis tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured_backup.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;experimental-results-and-evaluation&#34;&gt;Experimental Results and Evaluation&lt;/h2&gt;
&lt;p&gt;We extensively tested &lt;em&gt;MeViT&lt;/em&gt; on Thailandâ€™s Landsat-8 dataset, focusing on para rubber, corn, and pineapple classifications. Compared to other models, including state-of-the-art architectures like HRViT and SegFormer, &lt;em&gt;MeViT&lt;/em&gt; demonstrated notable improvements in precision and segmentation accuracy, proving its efficacy in challenging, real-world datasets. This establishes &lt;em&gt;MeViT&lt;/em&gt; as a leading tool in medium-resolution satellite imagery analysis, surpassing previous Vision Transformer models and CNN-based methods in delivering high-quality semantic segmentation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;compact.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;MeViT&lt;/em&gt; presents a significant advancement in Vision Transformer applications, setting a new standard for semantic segmentation in remote sensing. By combining multi-branch ViT architectures with optimized convolutional modules, &lt;em&gt;MeViT&lt;/em&gt; delivers efficient, accurate LULC classification on satellite imagery, supporting agricultural insights and sustainable resource management across Thailand. This work contributes to the broader field of environmental monitoring and opens up new possibilities for enhanced remote sensing techniques globally.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</title>
      <link>https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Achieve Data Science First Meet</title>
      <link>https://kaopanboonyuen.github.io/talk/achieve-data-science-first-meet/</link>
      <pubDate>Mon, 05 Dec 2022 09:30:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/achieve-data-science-first-meet/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Distributed ML for Geospatial Data</title>
      <link>https://kaopanboonyuen.github.io/talk/distributed-ml-for-geospatial-data/</link>
      <pubDate>Thu, 01 Dec 2022 08:45:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/distributed-ml-for-geospatial-data/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>42.195K at Bangkok Marathon 2022 â€” My First Full Marathon and Crossing the Finish Line</title>
      <link>https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/</link>
      <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/</guid>
      <description>&lt;hr&gt;
&lt;h2 id=&#34;42195k--the-bangkok-marathon-2022-conquering-the-full-marathon&#34;&gt;42.195K â€“ The Bangkok Marathon 2022: Conquering the Full Marathon&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;I did it. I finally ran a Full Marathonâ€”42.195 kilometers through the streets of Bangkok. Itâ€™s still sinking in, but the feeling is electric. This wasnâ€™t just another run; it was a milestone, a test of determination, and an experience Iâ€™ll never forget.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-decision-to-go-the-distance&#34;&gt;The Decision to Go the Distance&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The idea of running a full marathon had been on my mind for a while. Itâ€™s one of those challenges that, once youâ€™ve thought about it, you canâ€™t shake. Iâ€™d done plenty of 5Ks and 10Ks, even completed a half marathon once, but the full 42.195 KM? That was uncharted territory. So, when I finally decided to register for the Bangkok Marathon, it felt like I was committing to something much bigger than just a race.&lt;/p&gt;
&lt;p&gt;From the moment I hit that â€œregisterâ€ button, I knew there was no turning back. The excitement was real. Every time I thought about the race, Iâ€™d get a little rush of adrenaline, knowing that I was about to push my limits like never before.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;training-the-journey-begins&#34;&gt;Training: The Journey Begins&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Letâ€™s be honestâ€”my training for this marathon wasnâ€™t exactly textbook. I hadnâ€™t trained for the full distance before. My longest run leading up to race day was a half marathon, and that was just once. Most of my runs were 5K or 10K, and while those were good for keeping in shape, they didnâ€™t fully prepare me for the grueling distance of a full marathon.&lt;/p&gt;
&lt;p&gt;But I was determined. Every time I laced up my running shoes, I reminded myself that this wasnâ€™t just another race. This was the race. My strategy was simple: keep my pace steady, listen to my body, and most importantly, finish strong.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;race-day-the-big-moment&#34;&gt;Race Day: The Big Moment&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Race day arrived with a mix of excitement and nerves. As I stood at the starting line, surrounded by thousands of other runners, I felt the weight of what I was about to do. But more than anything, I felt ready. This was the moment Iâ€™d been waiting for.&lt;/p&gt;
&lt;p&gt;I started strong, hitting my Sub1 10K goal in 55 minutes. I felt good, my pace was on point, and I was in the zone. But as the kilometers ticked by, the reality of the distance started to set in. From the 10K mark to 21.1K, I slowed down to a steady pace of around 5. I knew I needed to conserve energy for the long haul.&lt;/p&gt;
&lt;p&gt;When I hit the 22K mark, things started to change. My legs were getting heavier, and my pace slowed to the point where I was more walking than running. A senior runner even teased me, saying Iâ€™d planned to run the first half and walk the second, which, in hindsight, wasnâ€™t too far from the truth. But I kept going, focused on one goal: crossing that finish line.&lt;/p&gt;
&lt;p&gt;The infamous â€˜Devil Km.35â€™â€”a point in the race where many runners hit the wallâ€”came and went. Honestly, I was so caught up in the moment that I didnâ€™t even realize Iâ€™d passed it. By then, I was walking and chatting with fellow runners, soaking in the atmosphere of this incredible event. The camaraderie, the support, the shared struggleâ€”it all made the experience that much more meaningful.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;the-final-stretch-a-run-to-remember&#34;&gt;The Final Stretch: A Run to Remember&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The last 10KM were something special. As I crossed the Rama VIII Bridge just as the sun was rising, I felt an overwhelming sense of accomplishment. The view was breathtaking, and the realization that I was nearing the finish line gave me a final burst of energy.&lt;/p&gt;
&lt;p&gt;And then, there it wasâ€”the Grand Palace, gleaming in the early morning light, marking the end of this incredible journey. Crossing that finish line was one of the most satisfying moments of my life. Iâ€™d done it. Iâ€™d run my first Full Marathon, all 42.195 kilometers of it.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_BKKMARATHON2022_01.jpg&#34; alt=&#34;Finisher medal with my bib from the Bangkok Marathon 2022&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 1: Celebrating my achievement as a Full Marathon Finisher in Bangkok 2022.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Participating in the Bangkok Marathon and completing the full marathon distance was an extraordinary achievement that filled me with immense joy and accomplishment. The race challenged me physically, mentally, and emotionally, but it also showcased the resilience, determination, and unwavering spirit within us all.&lt;/p&gt;
&lt;p&gt;Crossing the finish line was a defining moment that reminded me of the incredible feats we can achieve when we set our minds to it. The memories of this triumphant journey will forever inspire and motivate me to embrace new challenges and strive for greatness in all aspects of life.&lt;/p&gt;
&lt;p&gt;PS: To the 42.195 KM that changed everythingâ€”Iâ€™m proud of what I achieved, and Iâ€™m already excited for round two. See you at the next finish line!&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_BKKMARATHON2022_02.jpg&#34; alt=&#34;Garmin statistics showing my marathon performance&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: My Garmin statistics tracking the marathon performance.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_BKKMARATHON2022_03.jpg&#34; alt=&#34;Running among the marathon participants in Bangkok Marathon 2022&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: Running among talented marathoners at the Bangkok Marathon 2022.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_BKKMARATHON2022_04.jpg&#34; alt=&#34;Finisher medal from the Bangkok Marathon 2022&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: My Full Marathon Finisher medal from Bangkok Marathon 2022.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_BKKMARATHON2022_05.jpg&#34; alt=&#34;The marathon route as tracked by my Garmin Fenix Watch&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: The marathon route I ran, as tracked by my Garmin Fenix Watch.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_BKKMARATHON2022_06.jpg&#34; alt=&#34;My final official result on the Bangkok Marathon 2022 scoreboard&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: My final position on the official Bangkok Marathon 2022 scoreboard.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;42.195K â€“ The Bangkok Marathon 2022: My Journey to the Finish Line&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The moment I crossed the finish line at the Bangkok Marathon 2022, a rush of emotions hit me all at once. It was a mixture of exhaustion, disbelief, and joy, but most of all, pride. I had just run 42.195 kilometers, completing my very first full marathon. To many, it may just be a race, but to me, it was a testament to what sheer willpower can achieve. It was fun, exhilarating, and painful all at once. Every stride brought me closer to the finish, but it also pushed me beyond my limits. My legs screamed for mercy, my heart raced faster than I could catch my breath, but I didnâ€™t stop. There was no turning back.&lt;/p&gt;
&lt;p&gt;The training leading up to the marathon had been a rollercoaster. I remember the doubts I had when I first registered. Could I really do it? I had run a few 5Ks and 10Ks, and I even finished a half marathon, but this was something else entirely. A full 42.195K? That felt impossible at first. Yet, the more I trained, the more I realized that it wasnâ€™t about being the fastestâ€”it was about the journey, the perseverance, and the mental toughness to keep going even when everything hurts.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Completing the Bangkok Marathon 2022 was more than just a physical achievement. It was a personal triumph. It showed me that I am capable of so much more than I give myself credit for. It reminded me that perseverance and determination are the true markers of success. And it gave me a deep sense of gratitude for every person who cheered me on, every runner I met along the way, and every moment that brought me closer to that finish line.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But the journey doesnâ€™t end here. Iâ€™m not stopping at one marathon. I will keep running, keep pushing myself, and keep striving for greatness. Iâ€™ve tasted the sweet victory of crossing the finish line, and Iâ€™m already planning my next challenge. This marathon may be over, but my journey has just begun.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Nov 2022). &lt;em&gt;42.195K at Bangkok Marathon 2022 â€” My First Full Marathon and Crossing the Finish Line&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2022bangkokmarathon,
  title   = &amp;quot;42.195K at Bangkok Marathon 2022 â€” My First Full Marathon and Crossing the Finish Line&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2022&amp;quot;,
  month   = &amp;quot;Nov&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it ğŸ™Œ
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama</title>
      <link>https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks</title>
      <link>https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province</title>
      <link>https://kaopanboonyuen.github.io/publication/rainfall-prediction-a-machine-learning-approach/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/rainfall-prediction-a-machine-learning-approach/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ph.D. Thesis Defense</title>
      <link>https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/</link>
      <pubDate>Fri, 31 Jul 2020 14:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/</guid>
      <description>&lt;h2 id=&#34;-phd-dissertation-defense--triumphant&#34;&gt;ğŸ“ Ph.D. Dissertation Defense â€“ Triumphant&lt;/h2&gt;
&lt;p&gt;On &lt;strong&gt;July 9, 2020&lt;/strong&gt;, I successfully defended my Ph.D. dissertation titled &lt;em&gt;&amp;ldquo;Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network&amp;rdquo;&lt;/em&gt; before a distinguished committee of Thai professors, each of whom earned their doctoral degrees from world-renowned international universities.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_phd_defense_day_03.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_phd_defense_day_04.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_phd_defense_day_05.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;ğŸ”— &lt;strong&gt;Check out my full presentation slides here:&lt;/strong&gt;
ğŸ‘‰ &lt;a href=&#34;https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ph.D. Defense Slides&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This moment was a culmination of years of hard work and dedication, made even more meaningful by the opportunity to present my research to such respected scholars. Their insightful feedback and support reinforced the importance of collaboration and intellectual exchange in pushing the boundaries of knowledge and technology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The list below shows the names of those who submitted their final Ph.D. dissertations, and as part of the class of 2017 (student ID: 6071467821), I am proud to highlight that I completed my Ph.D. in Artificial Intelligence in just two and a half years, graduating in 2019.&lt;/p&gt;
&lt;p&gt;This achievement is a testament to my dedication, relentless work ethic, and passion for the field. It was a journey that demanded focus, time management, and the ability to push the boundaries of what was possible within an accelerated timeline.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_phd_defense_day_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I am immensely proud of the way I tackled this challenge, which not only sharpened my technical skills but also refined my communication and problem-solving abilities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reflecting on this journey, I am deeply grateful for the unwavering commitment I gave to my work. Completing my Ph.D. in such a short time was no easy feat, but it taught me invaluable lessons in resilience, focus, and perseverance. This experience has prepared me to take on complex challenges and contribute meaningfully to the ever-evolving field of AI.&lt;/p&gt;
&lt;p&gt;ğŸ§  The research focused on applying deep learning techniquesâ€”specifically convolutional encoder-decoder architecturesâ€”for high-accuracy semantic segmentation in remotely sensed imagery, pushing the boundaries of geospatial AI.&lt;/p&gt;
&lt;p&gt;ğŸ”— &lt;strong&gt;Explore the full dissertation, source code, and project resources here:&lt;/strong&gt;&lt;br&gt;
ğŸ‘‰ &lt;a href=&#34;https://kaopanboonyuen.github.io/FusionNetGeoLabel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/FusionNetGeoLabel/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ğŸ”— &lt;strong&gt;Interested in the full presentation slides? You can view them here:&lt;/strong&gt;&lt;br&gt;
ğŸ‘‰ &lt;a href=&#34;https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ph.D. Defense Slides&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-official-recognition-of-phd-completion&#34;&gt;ğŸ“ Official Recognition of Ph.D. Completion&lt;/h2&gt;
&lt;p&gt;On &lt;strong&gt;January 15, 2021&lt;/strong&gt;, I officially graduated with the degree of &lt;strong&gt;Doctor of Philosophy in Computer Engineering&lt;/strong&gt; from Chulalongkorn University (Student ID: 6071467821). This milestone was formally endorsed by the University Council on &lt;strong&gt;January 28, 2021&lt;/strong&gt;, marking the successful culmination of years of research, dedication, and perseverance. This recognition stands as undeniable proof of my academic journey, where relentless effort and focus enabled me to complete my Ph.D. in an accelerated timeframe while upholding the highest standards of research excellence.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_proof_of_phd_completion_01.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_proof_of_phd_completion_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-commencement-ceremony-invitation&#34;&gt;ğŸ‘‘ Commencement Ceremony Invitation&lt;/h2&gt;
&lt;p&gt;On &lt;strong&gt;May 19, 2022&lt;/strong&gt;, I was honored to attend the &lt;strong&gt;Royal Commencement Ceremony at Chulalongkorn University&lt;/strong&gt;, where I received my Ph.D. degree in person as part of the official convocation. Assigned &lt;strong&gt;seat number 12&lt;/strong&gt; in the Faculty of Engineering (Doctoral Degree), this moment represented the final chapter of a journey defined by resilience, discipline, and an unwavering pursuit of knowledge. Walking across the stage in the royal hall was not just a celebration of personal achievement, but also a tribute to the countless hours of hard work and dedication that shaped me into the scholar and professional I am today.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_proof_of_phd_completion_03.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;-a-journey-of-perseverance-and-pride&#34;&gt;ğŸŒŸ A Journey of Perseverance and Pride&lt;/h2&gt;
&lt;p&gt;Looking back on this journey, I feel an overwhelming sense of pride in myself for making it through. It was a path filled with both excitement and exhaustionâ€”endless nights of research, the anxiety of waiting on paper decisions and wondering if they would be rejected, the challenge of pushing through Q1â€“Tier 1 journals, and the pressure of the thesis defense. Yet, each hurdle became a stepping stone, and in the end, I emerged stronger, wiser, and more resilient.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This achievement is more than just a degreeâ€”it is proof that I can overcome even the toughest battles.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I am truly grateful to myself, because at the core of it all, it was my own persistence and determination that carried me through. Life goes on, and I walk forward with confidence, knowing that I did thisâ€”and only I could.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;After successfully defending my Ph.D. in Artificial Intelligence, I gained a deep and comprehensive understanding of advanced AI methodologies, particularly in the areas of computer vision, deep learning, and semantic segmentation.&lt;/p&gt;
&lt;p&gt;My research on &amp;ldquo;Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Networks&amp;rdquo; has not only enhanced my technical expertise but also refined my ability to tackle complex, real-world problems with innovative solutions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The experience of defending my dissertation to a panel of esteemed experts further strengthened my communication and collaboration skills, allowing me to explain intricate concepts clearly while receiving valuable feedback.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This journey has instilled in me a strong analytical mindset and a relentless drive for excellence, preparing me to contribute meaningfully to cutting-edge AI projects and teams.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;phd-journey-a-milestone-achieved&#34;&gt;PhD Journey: A Milestone Achieved&lt;/h3&gt;
&lt;p&gt;On May 19, 2022, I proudly completed my PhD at Chulalongkorn University, closing a remarkable chapter in my academic journey. This milestone was not just a moment of personal triumph, but also a time of reflection and deep gratitude.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Graduating with a doctoral degree was a dream realized, filled with emotions that I will carry with me forever.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Throughout this journey, I was fortunate to have the unwavering support of incredible mentors, advisors, colleagues, and friends. Their guidance and encouragement were instrumental in my success, and having them by my side on this special day was a poignant reminder of the profound impact they&amp;rsquo;ve had on both my academic and personal development.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;kaophd_002.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;
&lt;img src=&#34;kaophd_001.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;
&lt;img src=&#34;kaophd_003.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;
&lt;img src=&#34;kaophd_004.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;
&lt;img src=&#34;kaophd_005.png&#34; alt=&#34;Kao_Panboonyuen_PhD&#34;&gt;&lt;/p&gt;
&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/a-oWa2CS8jg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
&lt;blockquote&gt;
&lt;p&gt;Completing a PhD is more than just an academic achievement; it&amp;rsquo;s a journey of personal growth. It demands perseverance, resilience, and the ability to navigate and overcome numerous challenges.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My passion for machine learning and my commitment to research were the driving forces that kept me moving forward, enabling me to make meaningful contributions to the field.&lt;/p&gt;
&lt;h3 id=&#34;phd-thesis-highlights&#34;&gt;PhD Thesis Highlights&lt;/h3&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Semantic segmentation in remote sensing images plays a crucial role in applications such as land use classification, urban planning, and environmental monitoring. Remote sensing images often come from diverse sources, ranging from medium-resolution satellite imagery (such as Landsat-8) to high-resolution aerial images (such as those from the ISPRS Vaihingen Challenge). However, the task of semantic segmentation remains challenging due to the variety of image scales, the scarcity of labeled data, and the need for models capable of extracting both high-level and low-level features effectively.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In my PhD research, I propose a series of advancements in convolutional neural network (CNN)-based approaches to enhance the accuracy of semantic segmentation on remotely sensed data. Building upon the state-of-the-art methods, I introduce several innovations, including an enhanced &lt;strong&gt;Global Convolutional Network (GCN)&lt;/strong&gt; with channel attention, &lt;strong&gt;domain-specific transfer learning&lt;/strong&gt;, and the integration of &lt;strong&gt;feature fusion&lt;/strong&gt; and &lt;strong&gt;depthwise atrous convolutions&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These innovations aim to address the unique challenges of remote sensing datasets and push the boundaries of semantic segmentation performance.&lt;/p&gt;
&lt;h2 id=&#34;the-challenge-limitations-of-traditional-approaches&#34;&gt;The Challenge: Limitations of Traditional Approaches&lt;/h2&gt;
&lt;p&gt;Semantic segmentation models, particularly Deep Convolutional Encoder-Decoder (DCED) networks, have shown promise in image segmentation tasks. However, when applied to remote sensing imagery, these models face key limitations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resolution Differences&lt;/strong&gt;: Remote sensing images span a wide range of resolutions, from very high-resolution (VHR) aerial images to medium-resolution satellite images. Traditional models, designed for single-resolution tasks, struggle to generalize across these diverse scales.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Scarcity&lt;/strong&gt;: Annotated datasets for training deep models are scarce, particularly for high-resolution satellite or aerial imagery. This leads to overfitting and poor generalization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inability to Capture Global Context&lt;/strong&gt;: Traditional CNN models focus on local features, which are insufficient for understanding global context in satellite images, such as large rivers, forests, or urban areas.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;h3 id=&#34;global-convolutional-network-gcn-with-enhanced-backbone&#34;&gt;Global Convolutional Network (GCN) with Enhanced Backbone&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Global Convolutional Network (GCN)&lt;/strong&gt; is a modern CNN architecture that addresses the limitations of traditional models. GCN overcomes the challenge of capturing both local and global features by using a multi-level architecture. Each level in the GCN extracts features at different resolutions, ensuring that both fine-grained and broad contextual information are captured.&lt;/p&gt;
&lt;p&gt;This can be written as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{F}_{\text{GCN}} = \sum \mathbf{W}_l \cdot \mathbf{X}_l
$$&lt;/p&gt;
&lt;p&gt;Building on this architecture, I proposed an enhancement to the GCN backbone by modifying its structure and increasing the number of layers, making it more suitable for medium-resolution remote sensing imagery. Specifically, I employed the &lt;strong&gt;ResNet&lt;/strong&gt; architecture with varying depthsâ€”ResNet50, ResNet101, and ResNet152â€”to adapt the model to different datasets and resolutions.&lt;/p&gt;
&lt;h4 id=&#34;key-contributions&#34;&gt;Key Contributions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi-resolution Feature Extraction&lt;/strong&gt;: By stacking multiple convolutional layers at different stages, the GCN captures features across multiple resolutions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Boundary Refinement&lt;/strong&gt;: A boundary refinement module is introduced to improve the precision of segmentation boundaries, crucial for tasks like building or road detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;channel-attention-mechanism&#34;&gt;Channel Attention Mechanism&lt;/h3&gt;
&lt;p&gt;One of the most significant advancements in my work is the introduction of the &lt;strong&gt;Channel Attention Block&lt;/strong&gt;. Attention mechanisms, inspired by the human visual system, allow the model to focus on the most important features in the image. In the case of remote sensing images, this means highlighting key features such as roads, rivers, or vegetation, while suppressing irrelevant background information.&lt;/p&gt;
&lt;p&gt;The attention mechanism is mathematically modeled as:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{z}_c = \sigma\left( W_c \cdot \text{AvgPool}(\mathbf{x}_c) + b_c \right)
$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;channel attention block&lt;/strong&gt; modifies the network&amp;rsquo;s weights to prioritize the most discriminative channels during feature extraction, improving the networkâ€™s ability to differentiate between different land cover types. This is crucial in remote sensing, where the subtle difference between similar features (e.g., different vegetation types) can significantly impact segmentation accuracy.&lt;/p&gt;
&lt;h4 id=&#34;key-contributions-1&#34;&gt;Key Contributions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Adaptive Feature Selection&lt;/strong&gt;: The network dynamically adjusts the importance of features, focusing on those most relevant to the task at hand.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improved Discriminative Power&lt;/strong&gt;: By emphasizing discriminative features, the model is able to achieve higher classification accuracy, particularly for challenging classes in remote sensing datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;domain-specific-transfer-learning&#34;&gt;Domain-Specific Transfer Learning&lt;/h3&gt;
&lt;p&gt;One of the challenges in training deep learning models for remote sensing is the scarcity of annotated data, particularly for high-resolution images. To address this, I introduced &lt;strong&gt;Domain-Specific Transfer Learning&lt;/strong&gt;. This technique involves leveraging pre-trained models from different but related datasets to transfer knowledge across domains.&lt;/p&gt;
&lt;p&gt;By utilizing pre-trained models from one dataset (e.g., ISPRS Vaihingen) and applying them to another (e.g., Landsat-8), I was able to mitigate the data scarcity issue and improve the performance of my models. This approach ensures that knowledge gained from one domain can benefit another, allowing the model to generalize better with limited annotated data.&lt;/p&gt;
&lt;h4 id=&#34;key-contributions-2&#34;&gt;Key Contributions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-Domain Knowledge Transfer&lt;/strong&gt;: Knowledge learned from high-resolution datasets is transferred to medium-resolution tasks, significantly improving segmentation accuracy with minimal labeled data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pre-Trained Weights Utilization&lt;/strong&gt;: The use of pre-trained weights from different datasets enables the model to learn features that are generalizable across various remote sensing tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;feature-fusion-and-depthwise-atrous-convolution&#34;&gt;Feature Fusion and Depthwise Atrous Convolution&lt;/h3&gt;
&lt;p&gt;To further refine feature extraction and improve segmentation performance, I introduced &lt;strong&gt;Feature Fusion&lt;/strong&gt; and &lt;strong&gt;Depthwise Atrous Convolution (DA)&lt;/strong&gt;. These techniques work synergistically to enhance the model&amp;rsquo;s ability to capture multi-scale information while maintaining high resolution.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Feature Fusion&lt;/strong&gt;: This technique fuses low-level features, such as edges and textures, from the backbone network with high-level features from the segmentation model. This fusion ensures that fine-grained details are preserved while providing the model with a richer set of features for segmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mathbf{F}_{\text{fused}} = \mathbf{F}_L + \alpha \cdot \mathbf{F}_H
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Depthwise Atrous Convolution&lt;/strong&gt;: The DA module applies dilated convolutions at multiple scales, enabling the network to capture contextual information over larger areas without losing spatial resolution. This is particularly important for remote sensing tasks where object boundaries (e.g., between forest and water) need to be sharply delineated.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Depthwise Atrous Convolution is mathematically expressed as:&lt;/p&gt;
&lt;p&gt;$$
y_d = \sum_{k} w_k \cdot x_{i + r \cdot k}
$$&lt;/p&gt;
&lt;h4 id=&#34;key-contributions-3&#34;&gt;Key Contributions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Improved Feature Representation&lt;/strong&gt;: By integrating low-level features with deep model representations, the model gains a more comprehensive understanding of the image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced Contextual Understanding&lt;/strong&gt;: The DA module allows the network to consider broader context in each layer, improving segmentation accuracy, especially for large objects and distant features.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments-and-results&#34;&gt;Experiments and Results&lt;/h2&gt;
&lt;h3 id=&#34;datasets&#34;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;I conducted experiments on three benchmark datasets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ISPRS Vaihingen Challenge Dataset&lt;/strong&gt; (Very High Resolution)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Landsat-8 Satellite Imagery&lt;/strong&gt; (Medium Resolution)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Private Datasets&lt;/strong&gt; from GISTDA (Geo-Informatics and Space Technology Development Agency)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These datasets represent a mix of very high-resolution and medium-resolution imagery, which provided a comprehensive testbed for evaluating the effectiveness of my proposed methods.&lt;/p&gt;
&lt;h3 id=&#34;performance-metrics&#34;&gt;Performance Metrics&lt;/h3&gt;
&lt;p&gt;The model was evaluated using standard segmentation metrics:&lt;/p&gt;
&lt;h4 id=&#34;1-f1-score&#34;&gt;1. &lt;strong&gt;F1 Score&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;The F1 Score measures the balance between &lt;strong&gt;Precision&lt;/strong&gt; and &lt;strong&gt;Recall&lt;/strong&gt;, and is calculated as:&lt;/p&gt;
&lt;p&gt;$$
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt; is the fraction of relevant instances among the retrieved instances:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\text{Precision} = \frac{TP}{TP + FP}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt; is the fraction of relevant instances that have been retrieved over the total amount of relevant instances:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\text{Recall} = \frac{TP}{TP + FN}
$$&lt;/p&gt;
&lt;h5 id=&#34;example-pineapple-class&#34;&gt;Example: &lt;strong&gt;Pineapple Class&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;For the &lt;strong&gt;pineapple&lt;/strong&gt; class:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;( TP = 50 )&lt;/li&gt;
&lt;li&gt;( FP = 10 )&lt;/li&gt;
&lt;li&gt;( FN = 15 )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We calculate &lt;strong&gt;Precision&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\text{Precision} = \frac{50}{50 + 10} = 0.8333
$$&lt;/p&gt;
&lt;p&gt;And &lt;strong&gt;Recall&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\text{Recall} = \frac{50}{50 + 15} = 0.7692
$$&lt;/p&gt;
&lt;p&gt;Now calculate the &lt;strong&gt;F1 Score&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
F1 = 2 \cdot \frac{0.8333 \cdot 0.7692}{0.8333 + 0.7692} = 0.799
$$&lt;/p&gt;
&lt;p&gt;Thus, the &lt;strong&gt;F1 Score&lt;/strong&gt; for the pineapple class is &lt;strong&gt;0.799&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;2-mean-iou-intersection-over-union&#34;&gt;2. &lt;strong&gt;Mean IoU (Intersection over Union)&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;IoU&lt;/strong&gt; quantifies the overlap between the predicted and true segmentation maps:&lt;/p&gt;
&lt;p&gt;$$
IoU = \frac{TP}{TP + FP + FN}
$$&lt;/p&gt;
&lt;h5 id=&#34;example-corn-class&#34;&gt;Example: &lt;strong&gt;Corn Class&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;For the &lt;strong&gt;corn&lt;/strong&gt; class:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;( TP = 80 )&lt;/li&gt;
&lt;li&gt;( FP = 5 )&lt;/li&gt;
&lt;li&gt;( FN = 20 )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We calculate the &lt;strong&gt;IoU&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
IoU = \frac{80}{80 + 5 + 20} = 0.7619
$$&lt;/p&gt;
&lt;p&gt;Thus, the &lt;strong&gt;IoU&lt;/strong&gt; for the corn class is &lt;strong&gt;0.7619&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;3-mean-iou-for-multiple-classes&#34;&gt;3. &lt;strong&gt;Mean IoU for Multiple Classes&lt;/strong&gt;:&lt;/h4&gt;
&lt;p&gt;For evaluating the &lt;strong&gt;Mean IoU&lt;/strong&gt; over multiple classes (e.g., pineapple, corn, pararubber), we compute the average IoU:&lt;/p&gt;
&lt;p&gt;$$
\text{Mean IoU} = \frac{IoU_{\text{pineapple}} + IoU_{\text{corn}} + IoU_{\text{pararubber}}}{3}
$$&lt;/p&gt;
&lt;p&gt;Using the IoUs for each class:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IoU for pineapple = 0.799&lt;/li&gt;
&lt;li&gt;IoU for corn = 0.7619&lt;/li&gt;
&lt;li&gt;IoU for pararubber = 0.85&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We get the &lt;strong&gt;Mean IoU&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$
\text{Mean IoU} = \frac{0.799 + 0.7619 + 0.85}{3} = 0.8036
$$&lt;/p&gt;
&lt;p&gt;Thus, the &lt;strong&gt;Mean IoU&lt;/strong&gt; across all classes is &lt;strong&gt;0.8036&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The proposed model significantly outperformed baseline models, including traditional Deep Convolutional Encoder-Decoder (DCED) networks, across all datasets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ISPRS Vaihingen Dataset&lt;/strong&gt;: F1 Score of &lt;strong&gt;0.9362&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Landsat-8 Dataset&lt;/strong&gt;: F1 Score of &lt;strong&gt;0.9114&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The proposed model consistently exceeded the &lt;strong&gt;90% F1 score&lt;/strong&gt; threshold across all classes, demonstrating its robustness and adaptability to different image resolutions and domains.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This research introduces several key advancements in deep learning for remote sensing semantic segmentation. By incorporating multi-resolution feature extraction, channel attention, domain-specific transfer learning, feature fusion, and depthwise atrous convolutions, my approach addresses the unique challenges posed by remote sensing data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The experimental results validate the effectiveness of these techniques, providing a solid foundation for further improvements in remote sensing applications.&lt;/p&gt;
&lt;p&gt;With the successful application of these methods, I am confident that these innovations will contribute significantly to the field of remote sensing and provide new avenues for improving the accuracy and generalization of deep learning models in this domain.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Explore more about my PhD story &lt;a href=&#34;https://kaopanboonyuen.wordpress.com/2022/05/23/the-phd-journey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kao Panboonyuen&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus</title>
      <link>https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks</title>
      <link>https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</title>
      <link>https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</title>
      <link>https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Science Pathway Special Clinic Session</title>
      <link>https://kaopanboonyuen.github.io/talk/data-science-pathway-special-clinic-session/</link>
      <pubDate>Sun, 01 Dec 2019 08:45:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/talk/data-science-pathway-special-clinic-session/</guid>
      <description>&lt;!-- 
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/traKBhJm4lQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution</title>
      <link>https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning</title>
      <link>https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transportation Mobility Factor Extraction Using Image Recognition Techniques</title>
      <link>https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://kaopanboonyuen.github.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://kaopanboonyuen.github.io/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>https://kaopanboonyuen.github.io/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://kaopanboonyuen.github.io/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</title>
      <link>https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices</title>
      <link>https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</title>
      <link>https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery</title>
      <link>https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Road map extraction from satellite imagery using connected component analysis and landscape metrics</title>
      <link>https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image Vectorization of Road Satellite Data Sets</title>
      <link>https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://kaopanboonyuen.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
