<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>computer-vision | Teerapong Panboonyuen</title>
    <link>https://kaopanboonyuen.github.io/tag/computer-vision/</link>
      <atom:link href="https://kaopanboonyuen.github.io/tag/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>computer-vision</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>¬©2025 Kao Panboonyuen</copyright><lastBuildDate>Wed, 02 Jul 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png</url>
      <title>computer-vision</title>
      <link>https://kaopanboonyuen.github.io/tag/computer-vision/</link>
    </image>
    
    <item>
      <title>ALBERT vs SLICK: MARSAIL‚Äôs New AI Fashion for Real-Time Car Insurance and Garages</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/</link>
      <pubDate>Wed, 02 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can explore our GitHub project page üì¶ &lt;a href=&#34;https://kaopanboonyuen.github.io/MARS/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#albert-vs-slick-the-new-ai-fashion-at-marsail&#34;&gt;ALBERT vs SLICK: The New AI Fashion at MARSAIL&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#slick-revolutionizing-car-damage-segmentation-with-knowledge-enhanced-ai-at-marsail&#34;&gt;SLICK: Revolutionizing Car Damage Segmentation with Knowledge-Enhanced AI at MARSAIL&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#inspiration-from-andrej-karpathy-embracing-the-new-era-of-ai-innovation-at-marsail&#34;&gt;Inspiration from Andrej Karpathy: Embracing the New Era of AI Innovation at MARSAIL&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#albert-the-teacher-model--precision-at-a-cost&#34;&gt;ALBERT: The Teacher Model ‚Äî Precision at a Cost&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#slick-the-student-model--lightning-speed-meets-smart-knowledge&#34;&gt;SLICK: The Student Model ‚Äî Lightning Speed Meets Smart Knowledge&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-teaching-machines-to-see-smarter-and-faster-the-marsail-teacher-student-model&#34;&gt;üöò Teaching machines to see smarter and faster: the MARSAIL teacher-student model&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#what-is-the-teacher-student-model&#34;&gt;What is the teacher-student model?&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#measuring-size-and-efficiency-teacher-vs-student&#34;&gt;Measuring size and efficiency: teacher vs. student&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#how-does-the-student-learn-from-the-teacher&#34;&gt;How does the student learn from the teacher?&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#results-that-speak-volumes&#34;&gt;Results that speak volumes&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#final-layer-feature-summaries&#34;&gt;Final layer feature summaries&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#why-does-this-matter-for-car-insurance&#34;&gt;Why does this matter for car insurance?&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#mathematical-insight-simplified&#34;&gt;Mathematical Insight (Simplified)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#published-research&#34;&gt;Published Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#looking-ahead&#34;&gt;Looking Ahead&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;albert-vs-slick-the-new-ai-fashion-at-marsail&#34;&gt;ALBERT vs SLICK: The New AI Fashion at MARSAIL&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In the fast-moving world of automotive insurance, where accuracy and turnaround time can make or break the customer experience, &lt;strong&gt;MARSAIL&lt;/strong&gt; (Motor AI Recognition Solution Artificial Intelligence Laboratory) stands at the forefront of transformation. Based in Thailand and led by the visionary &lt;strong&gt;Dr. Teerapong Panboonyuen&lt;/strong&gt; ‚Äî affectionately known as &lt;strong&gt;Dr. Kao&lt;/strong&gt; ‚Äî MARSAIL is redefining how artificial intelligence is used in car insurance and garage ecosystems. The lab&amp;rsquo;s mission is clear: to blend deep research with real-world impact. Earlier this year, Dr. Kao shared on &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1822723598764876000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt; the debut of &lt;strong&gt;MARS&lt;/strong&gt;, an innovative architecture built on Attention Refinement with Sequential Quadtree Nodes. With its combination of scientific rigor and practical relevance, MARS isn&amp;rsquo;t just another academic model ‚Äî it&amp;rsquo;s a bold step forward in computer vision and deep learning, designed to solve tangible problems in automotive analysis.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üçÑ We&amp;#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.&lt;br&gt;.&lt;br&gt;Paper: &lt;a href=&#34;https://t.co/UayUSxmZep&#34;&gt;https://t.co/UayUSxmZep&lt;/a&gt;&lt;br&gt;Code: &lt;a href=&#34;https://t.co/RoNFjSslXr&#34;&gt;https://t.co/RoNFjSslXr&lt;/a&gt;&lt;br&gt;Project: &lt;a href=&#34;https://t.co/uSoBX21HpF&#34;&gt;https://t.co/uSoBX21HpF&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ComputerVision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ComputerVision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Research?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Research&lt;/a&gt; &lt;a href=&#34;https://t.co/oc8gz7Hs9I&#34;&gt;pic.twitter.com/oc8gz7Hs9I&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1822723598764876000?ref_src=twsrc%5Etfw&#34;&gt;August 11, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Riding on this momentum, MARSAIL has unveiled two game-changing models in 2025: &lt;strong&gt;ALBERT&lt;/strong&gt; and &lt;strong&gt;SLICK&lt;/strong&gt;. These systems are not just incremental updates ‚Äî they represent a complete rethink of how damage detection and claim assessment can be automated with AI. ALBERT is optimized for real-time car damage classification with high precision, while SLICK focuses on smart localization and segmentation of damage areas, tailored specifically for insurance workflows. Together, they offer insurers and garages tools that are faster, smarter, and more reliable than ever before. Backed by advanced machine learning techniques and a commitment to open research, MARSAIL is helping Thailand ‚Äî and the region ‚Äî become a serious global player in automotive AI innovation. Whether you&amp;rsquo;re in the lab or on the road, MARSAIL is making sure AI drives the future.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://github.com/kaopanboonyuen/kaopanboonyuen.github.io/raw/main/files/MARS/MARSAIL.png&#34; alt=&#34;MARSAIL Logo&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 1: MARSAIL ‚Äî a leading research lab in Thailand focused on applying AI to car insurance and automotive service innovations.
  &lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;These models represent more than just technological progress; they embody a new philosophy in AI-powered insurance: a seamless synergy between &lt;strong&gt;uncompromising accuracy&lt;/strong&gt; and &lt;strong&gt;lightning-fast efficiency&lt;/strong&gt;, inspired by the teacher-student paradigm.&lt;/p&gt;
&lt;p&gt;At the core of this paradigm lies &lt;strong&gt;ALBERT&lt;/strong&gt;, the ‚Äúteacher‚Äù ‚Äî a powerhouse model meticulously engineered for razor-sharp precision. ALBERT dives deep into images, discerning the finest scratches, dents, and cracks with near-human expertise. It‚Äôs a master of detail, leaving no nuance unseen, perfect for complex offline investigations and comprehensive damage evaluations where absolute accuracy is essential.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In today‚Äôs fast-paced insurance ecosystem, speed is just as crucial as accuracy, particularly when it comes to frontline claim processing and on-the-spot damage assessments. This is where SLICK, the &amp;ldquo;student&amp;rdquo; model, truly shines. Guided by the advanced expertise of ALBERT, SLICK is engineered for agility and lightning-fast performance, delivering precise damage detection results in real-time. Whether running on edge devices or mobile phones, its optimized architecture allows insurance agents and repair shops to streamline their operations, making decisions faster without ever sacrificing quality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;slick-revolutionizing-car-damage-segmentation-with-knowledge-enhanced-ai-at-marsail&#34;&gt;SLICK: Revolutionizing Car Damage Segmentation with Knowledge-Enhanced AI at MARSAIL&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The recent Google AI video on Knowledge Distillation: A Good Teacher is Patient and Consistent offers valuable insights into how AI models can be trained efficiently by leveraging a &amp;ldquo;teacher-student&amp;rdquo; framework. The key takeaway from this approach is that a well-trained teacher model can pass down its knowledge to a student model, significantly improving performance and generalization. This technique has sparked new ideas for MARSAIL and our work on SLICK (Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation).&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/gZPUGje1PCI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Today, we introduced Gemini Robotics On-Device ü§ñ&lt;br&gt;&lt;br&gt;üß™ Designed for rapid experimentation with dexterous manipulation.&lt;br&gt;ü¶æ Adaptable to new tasks through fine-tuning to improve performance.&lt;br&gt;üëü Optimized to run locally with low-latency inference.&lt;br&gt;&lt;br&gt;Learn more‚Ä¶ &lt;a href=&#34;https://t.co/h2d1TZ49qm&#34;&gt;https://t.co/h2d1TZ49qm&lt;/a&gt;&lt;/p&gt;&amp;mdash; Google AI (@GoogleAI) &lt;a href=&#34;https://twitter.com/GoogleAI/status/1937554536966619399?ref_src=twsrc%5Etfw&#34;&gt;June 24, 2025&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/p&gt;
&lt;p&gt;In the same spirit of knowledge transfer, SLICK takes inspiration from the concept of &amp;ldquo;distilling&amp;rdquo; knowledge from large, complex models to create an AI system capable of precise, real-world car damage segmentation. Through components like Selective Part Segmentation, Localization-Aware Attention, and Knowledge Fusion, SLICK enhances the ability of AI models to focus on and accurately segment vehicle parts‚Äîeven under challenging conditions like occlusions and deformations. Much like the patient and consistent teacher-student relationship in knowledge distillation, SLICK learns from vast datasets (including synthetic crash data and real-world insurance records) to ensure robustness and adaptability across a variety of damage scenarios.&lt;/p&gt;
&lt;p&gt;At MARSAIL, inspired by Google AI&amp;rsquo;s knowledge distillation, we‚Äôre applying these principles to create an AI system that not only improves segmentation accuracy but also optimizes the entire automotive insurance and repair workflow. With SLICK, we are ready to bring this advanced AI to Thailand, enhancing efficiency, reducing fraud, and setting new standards for the industry.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;inspiration-from-andrej-karpathy-embracing-the-new-era-of-ai-innovation-at-marsail&#34;&gt;Inspiration from Andrej Karpathy: Embracing the New Era of AI Innovation at MARSAIL&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;In the video &amp;ldquo;Andrej Karpathy: Software Is Changing (Again)&amp;rdquo;, Karpathy discusses how artificial intelligence and deep learning are driving a new wave of transformation across industries. At MARSAIL, we deeply resonate with his perspective that AI is not just evolving‚Äîit‚Äôs fundamentally reshaping how we approach problem-solving and automation. Inspired by Karpathy‚Äôs vision, we‚Äôre applying the latest in AI research to redefine the way car damage estimation, insurance claims, and repair workflows are handled. Just as Karpathy highlights the importance of AI in software development, MARSAIL is leveraging cutting-edge AI models like SLICK to bring accuracy, speed, and efficiency to the automotive sector, helping to transform the Thai automotive insurance ecosystem into a more intelligent and scalable system.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/LCEmiRjPEtQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üî• New (1h56m) video lecture: &amp;quot;Let&amp;#39;s build GPT: from scratch, in code, spelled out.&amp;quot;&lt;a href=&#34;https://t.co/2pKsvgi3dE&#34;&gt;https://t.co/2pKsvgi3dE&lt;/a&gt; &lt;br&gt;We build and train a Transformer following the &amp;quot;Attention Is All You Need&amp;quot; paper in the language modeling setting and end up with the core of nanoGPT. &lt;a href=&#34;https://t.co/6dzimsYPB9&#34;&gt;pic.twitter.com/6dzimsYPB9&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andrej Karpathy (@karpathy) &lt;a href=&#34;https://twitter.com/karpathy/status/1615398117683388417?ref_src=twsrc%5Etfw&#34;&gt;January 17, 2023&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;By aligning our research with the principles Karpathy discusses, MARSAIL is at the forefront of AI-driven innovation in the automotive space, bringing faster, more reliable, and trustworthy solutions to insurers, garages, and customers alike.&lt;/p&gt;
&lt;p&gt;Together, &lt;strong&gt;ALBERT&lt;/strong&gt; and &lt;strong&gt;SLICK&lt;/strong&gt; form a powerful duo that bridges the traditional divide between accuracy and efficiency ‚Äî offering the best of both worlds to revolutionize car insurance workflows across Thailand and beyond.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;albert-the-teacher-model--precision-at-a-cost&#34;&gt;ALBERT: The Teacher Model ‚Äî Precision at a Cost&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;ALBERT stands for &lt;em&gt;Advanced Localization and Bidirectional Encoder Representations from Transformers&lt;/em&gt;. This model is designed to be highly accurate and detailed in detecting subtle damages such as small scratches, dents, and cracks on vehicles. It leverages a vision transformer architecture enhanced with localized deformable tokens and parameter sharing to precisely focus on critical damage regions.&lt;/p&gt;
&lt;p&gt;However, this precision comes with a computational cost. ALBERT requires powerful CUDA-enabled GPUs and is relatively slow, making it ideal for offline batch processing or scenarios where accuracy takes precedence over speed.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/ALBERT_01.png&#34; alt=&#34;MARSAIL ALBERT Model Result&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 1: MARSAIL-ALBERT model showcasing detailed and precise damage segmentation results.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;slick-the-student-model--lightning-speed-meets-smart-knowledge&#34;&gt;SLICK: The Student Model ‚Äî Lightning Speed Meets Smart Knowledge&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;To address real-time insurance needs, MARSAIL developed &lt;strong&gt;SLICK&lt;/strong&gt; ‚Äî &lt;em&gt;Selective Localization and Instance Calibration with Knowledge&lt;/em&gt;. This model distills knowledge from ALBERT and integrates domain-specific insurance metadata like bumper zones and vehicle model weak points.&lt;/p&gt;
&lt;p&gt;SLICK boosts processing speed by over &lt;strong&gt;700%&lt;/strong&gt; compared to ALBERT, enabling instant damage assessments on edge devices or mobile apps without sacrificing much accuracy. Its adaptive attention mechanism dynamically calibrates segmentation proposals using contextual knowledge graphs, making it robust under varying light, weather, and occlusion conditions.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/SLICK_01.png&#34; alt=&#34;MARSAIL SLICK Model Result&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: MARSAIL-SLICK model delivering rapid, knowledge-enhanced damage segmentation optimized for real-time insurance workflows.&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-teaching-machines-to-see-smarter-and-faster-the-marsail-teacher-student-model&#34;&gt;üöò Teaching machines to see smarter and faster: the MARSAIL teacher-student model&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;In the race to deliver the best car damage detection for insurance claims, MARSAIL takes a cutting-edge approach inspired by how humans learn: through mentorship. Our &lt;strong&gt;teacher-student model&lt;/strong&gt; architecture pairs a high-capacity ‚Äúteacher‚Äù network with a lean, speedy ‚Äústudent‚Äù model, capturing the best of both worlds ‚Äî precision and efficiency.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_01.png&#34; alt=&#34;Teacher-Student Model Concept Architecture&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: Conceptual architecture of the MARSAIL teacher-student model (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;what-is-the-teacher-student-model&#34;&gt;What is the teacher-student model?&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Think of the &lt;strong&gt;teacher&lt;/strong&gt; as a seasoned expert with a deep understanding of vehicle damage nuances ‚Äî it‚Äôs large, powerful, and painstakingly precise. The &lt;strong&gt;student&lt;/strong&gt;, meanwhile, is like an apprentice: smaller, faster, and designed to perform well in real-world settings with limited resources.&lt;/p&gt;
&lt;p&gt;The magic happens when the student &lt;strong&gt;learns to mimic&lt;/strong&gt; the teacher&amp;rsquo;s insights without needing to replicate its full complexity. This process is known as &lt;strong&gt;knowledge distillation&lt;/strong&gt; ‚Äî where the teacher‚Äôs ‚Äúsoft‚Äù predictions guide the student‚Äôs training, helping it grasp subtle visual patterns that would be hard to learn from raw data alone.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_02.png&#34; alt=&#34;Simple Teacher-Student Model Concept&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: Simplified overview of the teacher-student learning framework (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;measuring-size-and-efficiency-teacher-vs-student&#34;&gt;Measuring size and efficiency: teacher vs. student&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;To illustrate the trade-off, here‚Äôs a glimpse of the teacher and student model sizes trained on the CIFAR-10 dataset. The teacher is notably larger but more precise, while the student‚Äôs compact size enables rapid inference ‚Äî crucial for insurance agents working on the go.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_03.png&#34; alt=&#34;Teacher Size (left) and Student Size (right) on CIFAR-10 dataset&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: Visual comparison of teacher (left) and student (right) model sizes (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;how-does-the-student-learn-from-the-teacher&#34;&gt;How does the student learn from the teacher?&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The training process involves the student observing both the teacher‚Äôs output and the ground truth, gradually adjusting itself to replicate the teacher‚Äôs nuanced judgments. This dual supervision accelerates the student‚Äôs learning curve, enabling it to deliver near-teacher accuracy with significantly fewer parameters.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_04.png&#34; alt=&#34;How to Learn from Teacher-Student&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: Diagram showing how the student model learns from the teacher model (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;results-that-speak-volumes&#34;&gt;Results that speak volumes&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;On multiple datasets and architectures (including CNN and ResNet), MARSAIL‚Äôs teacher-student training methods consistently improved student model accuracy across the board ‚Äî sometimes by over 3% compared to training without guidance.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;No KD (%)&lt;/th&gt;
&lt;th&gt;BLKD (%)&lt;/th&gt;
&lt;th&gt;TAKD (%)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;CIFAR-10&lt;/td&gt;
&lt;td&gt;70.16&lt;/td&gt;
&lt;td&gt;72.57&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;73.51&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;CIFAR-100&lt;/td&gt;
&lt;td&gt;41.09&lt;/td&gt;
&lt;td&gt;44.57&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;44.92&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet&lt;/td&gt;
&lt;td&gt;CIFAR-10&lt;/td&gt;
&lt;td&gt;88.52&lt;/td&gt;
&lt;td&gt;88.65&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;88.98&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet&lt;/td&gt;
&lt;td&gt;CIFAR-100&lt;/td&gt;
&lt;td&gt;61.37&lt;/td&gt;
&lt;td&gt;61.41&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;61.82&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ResNet&lt;/td&gt;
&lt;td&gt;ImageNet&lt;/td&gt;
&lt;td&gt;65.20&lt;/td&gt;
&lt;td&gt;66.60&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;67.36&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_05.png&#34; alt=&#34;Result Comparison of Models on CIFAR datasets&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: Model accuracy comparison showing improvement using knowledge distillation techniques (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;final-layer-feature-summaries&#34;&gt;Final layer feature summaries&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;This image visualizes how different layers in each model contribute to the final representation, highlighting the efficiency gains from knowledge distillation that help the student model stay compact yet powerful.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_TS_MODEL_06.png&#34; alt=&#34;Summary of Final Layers in Each Model&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: Summary of final layer features in teacher and student models (Image source: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34;&gt;Daily Dose of Data Science&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-does-this-matter-for-car-insurance&#34;&gt;Why does this matter for car insurance?&lt;/h3&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speed without compromise:&lt;/strong&gt; Insurance agents and garages need fast, reliable damage detection on smartphones or edge devices. The student model delivers rapid results, trained under the teacher‚Äôs expert supervision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resource efficiency:&lt;/strong&gt; Smaller models reduce computational costs and power consumption, enabling scalable deployment across Thailand‚Äôs wide insurance ecosystem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Robust accuracy:&lt;/strong&gt; Guided by the teacher, the student avoids common pitfalls of lightweight models, maintaining high performance even in challenging real-world conditions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accuracy vs. Speed:&lt;/strong&gt; ALBERT excels in detailed offline analysis, perfect for complex claim investigations. SLICK offers instant, reliable damage detection to accelerate frontline claim approvals and garage estimates.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hardware Flexibility:&lt;/strong&gt; ALBERT demands high-end GPUs; SLICK can run efficiently on more modest, real-world devices ‚Äî a game changer for field agents and repair shops.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Knowledge Integration:&lt;/strong&gt; SLICK‚Äôs use of insurance-specific metadata bridges the gap between raw image analysis and domain expertise, improving real-world applicability.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mathematical-insight-simplified&#34;&gt;Mathematical Insight (Simplified)&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;At the heart of our system lies a sophisticated process that refines how the model understands and represents visual data at every step. Imagine ALBERT as a multi-layered brain that carefully adjusts its internal view of an image piece by piece. At each layer, it uses two powerful tools: one that lets it look broadly across different parts of the image to understand overall patterns (multi-scale self-attention), and another that processes these insights through focused, step-by-step reasoning (a multilayer perceptron).&lt;/p&gt;
&lt;p&gt;$$
z^{(l+1)} = z^{(l)} + \text{MSA}(\text{LN}(z^{(l)})) + \text{MLP}(\text{LN}(z^{(l)}))
$$&lt;/p&gt;
&lt;p&gt;This dynamic combination helps ALBERT balance the big picture with fine details, ensuring that the model not only recognizes individual features but also how they relate to each other in context. To keep this learning smooth and stable, it applies a normalization step‚Äîsimilar to tuning an instrument‚Äîto make sure each layer‚Äôs output remains consistent and meaningful.&lt;/p&gt;
&lt;p&gt;Parallel to this, SLICK operates like an intelligent curator, enhancing the model‚Äôs confidence in its predictions. It does this by merging two streams of knowledge: the direct visual cues from the image itself and additional information pulled from a structured knowledge graph‚Äîthink of this as a database of domain-specific facts and relationships.&lt;/p&gt;
&lt;p&gt;$$
s_{mask} = \sigma(W_q [f_{img} | f_{kg}]) + b
$$&lt;/p&gt;
&lt;p&gt;To blend these inputs effectively, SLICK employs a gating mechanism that acts like a smart filter or valve. This gate carefully weighs how much influence the visual data and the knowledge graph should each have in shaping the final mask quality scores. By doing so, the model doesn‚Äôt just rely on what it sees but also on what it knows about the world, leading to sharper, more reliable segmentation.&lt;/p&gt;
&lt;p&gt;In essence, this combination of refined visual understanding and context-aware knowledge integration lets our system adapt its focus dynamically‚Äîprioritizing regions and details that matter most for accurate damage assessment and claim processing.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;published-research&#34;&gt;Published Research&lt;/h2&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;ALBERT: &lt;a href=&#34;https://arxiv.org/abs/2506.10524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2506.10524&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SLICK: &lt;a href=&#34;https://arxiv.org/abs/2506.10528&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2506.10528&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;looking-ahead&#34;&gt;Looking Ahead&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;MARSAIL continues to innovate by balancing AI model accuracy and deployment efficiency. ALBERT and SLICK represent the cutting edge of automotive AI, ready to transform insurance claim processes in Thailand and beyond ‚Äî enabling smarter, faster, and fairer car insurance.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Jul 2025). &lt;em&gt;ALBERT vs SLICK: MARSAIL‚Äôs New AI Fashion for Real-Time Car Insurance and Garages&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{panboonyuen2025albert_slick,
  title={ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation},
  author={Panboonyuen, Teerapong},
  journal={arXiv preprint arXiv:2506.10524},
  year={2025},
  url={https://arxiv.org/abs/2506.10524}
}

@article{panboonyuen2025slick,
  title={SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance},
  author={Panboonyuen, Teerapong},
  journal={arXiv preprint arXiv:2506.10528},
  year={2025},
  url={https://arxiv.org/abs/2506.10528}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Found this blog insightful? Consider sharing it with friends or researchers in the automotive or insurance tech industry. üöó
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Panboonyuen, Teerapong.  &amp;ldquo;ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation.&amp;rdquo; arXiv preprint arXiv:2506.10524 (2025). &lt;a href=&#34;https://arxiv.org/abs/2506.10524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2506.10524&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Panboonyuen, Teerapong.  &amp;ldquo;SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance.&amp;rdquo; arXiv preprint arXiv:2506.10528 (2025). &lt;a href=&#34;https://arxiv.org/abs/2506.10528&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2506.10528&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Knowledge Distillation with Teacher Assistant for Model Compression: &lt;a href=&#34;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>MARSAIL: The Smart Engine Powering the Future of Car Insurance and Intelligent Garages</title>
      <link>https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can explore our GitHub project page üì¶ &lt;a href=&#34;https://kaopanboonyuen.github.io/MARS/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#ai-driven-car-damage-estimating-global-innovations-and-marsails-vision-for-thailand&#34;&gt;AI-driven Car Damage Estimating: Global Innovations and MARSAIL&amp;rsquo;s Vision for Thailand&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#mars-our-flagship-ai&#34;&gt;MARS: Our Flagship AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#marsail-2025-research-highlights&#34;&gt;MARSAIL 2025: Research Highlights&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-albert-efficient-transformer-for-automotive-localization&#34;&gt;1. ALBERT: Efficient Transformer for Automotive Localization&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-slick-knowledge-enhanced-instance-segmentation&#34;&gt;2. SLICK: Knowledge-Enhanced Instance Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-dota-deformable-optimized-transformer-for-ocr&#34;&gt;3. DOTA: Deformable Optimized Transformer for OCR&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-mars-mask-attention-refinement-with-sequential-quadtree-nodes-for-car-damage-instance-segmentation&#34;&gt;4. MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-what-sets-our-approach-apart&#34;&gt;üîç what sets our approach apart&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-how-it-works-in-practice&#34;&gt;üß† how it works in practice&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-why-it-matters-for-insurers--workshops&#34;&gt;üèéÔ∏è why it matters for insurers &amp;amp; workshops&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#-conclusions--next-steps&#34;&gt;‚úÖ conclusions &amp;amp; next steps&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#fighting-fraud-with-intelligent-automation&#34;&gt;Fighting Fraud with Intelligent Automation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#visual-damage-interpretation--smarter-and-faster&#34;&gt;Visual Damage Interpretation ‚Äî Smarter and Faster&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#translating-damage-into-precise-repair-estimates&#34;&gt;Translating Damage Into Precise Repair Estimates&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#ai-powered-fraud-detection--catching-the-unseen&#34;&gt;AI-Powered Fraud Detection ‚Äî Catching the Unseen&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#comprehensive-automated-reporting-for-faster-decisions&#34;&gt;Comprehensive, Automated Reporting for Faster Decisions&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#why-marsail-leads-the-charge&#34;&gt;Why MARSAIL Leads the Charge&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#join-the-future-of-auto-insurance&#34;&gt;Join the Future of Auto Insurance&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#future-work-beyond-damage--toward-full-lifecycle-intelligence&#34;&gt;Future Work: Beyond Damage ‚Äì Toward Full-Lifecycle Intelligence&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;p&gt;MARSAIL, or the Motor AI Recognition Solution Artificial Intelligence Laboratory, is Thailand‚Äôs pioneering AI research hub dedicated to automotive insurance and repair. Under the direction of Dr. Teerapong Panboonyuen (Dr. Kao), the lab develops deep learning models to analyze car damage, estimate repair costs, and automate claim handling ‚Äî empowering insurers and garages with intelligence, precision, and speed.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://github.com/kaopanboonyuen/kaopanboonyuen.github.io/raw/main/files/MARS/MARSAIL.png&#34; alt=&#34;MARSAIL Logo&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;
    Figure 1: MARSAIL ‚Äî a standout AI research lab in Thailand, leading innovation in car insurance and garage solutions under the guidance of Dr. Teerapong Panboonyuen (known to us as Dr. Kao).
  &lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;The journey of MARSAIL began with a bold, urgent question: ‚ÄúCan we make car insurance in Thailand smarter, faster, and fraud-free using AI?‚Äù For decades, first-class auto insurance underwriting in Thailand has been heavily reliant on manual inspections and human judgments ‚Äî prone to delays, inconsistencies, and at times, manipulation. Garage estimates for labor and parts are also far from standardized, often resulting in disputes, inefficiencies, and customer dissatisfaction. Seeing these gaps not as limitations but as opportunities, MARSAIL was born with a national mission in mind: to revolutionize Thailand‚Äôs automotive insurance ecosystem through scalable, verifiable, and explainable artificial intelligence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We envisioned a future where a single image of a damaged vehicle could trigger an accurate AI-driven diagnosis ‚Äî identifying every dent, crack, and broken part, then instantly calculating the repair cost and parts required. This would not only expedite claims processing but also create a tamper-proof digital trail, significantly reducing fraud in the industry. At the same time, garage operators would benefit from AI-driven estimates that standardize costs and accelerate service turnaround time ‚Äî allowing businesses to grow with transparency and trust.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üçÑ We&amp;#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.&lt;br&gt;.&lt;br&gt;Paper: &lt;a href=&#34;https://t.co/UayUSxmZep&#34;&gt;https://t.co/UayUSxmZep&lt;/a&gt;&lt;br&gt;Code: &lt;a href=&#34;https://t.co/RoNFjSslXr&#34;&gt;https://t.co/RoNFjSslXr&lt;/a&gt;&lt;br&gt;Project: &lt;a href=&#34;https://t.co/uSoBX21HpF&#34;&gt;https://t.co/uSoBX21HpF&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ComputerVision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ComputerVision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Research?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Research&lt;/a&gt; &lt;a href=&#34;https://t.co/oc8gz7Hs9I&#34;&gt;pic.twitter.com/oc8gz7Hs9I&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1822723598764876000?ref_src=twsrc%5Etfw&#34;&gt;August 11, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;But we didn‚Äôt stop at computer vision. We realized that to fully automate the claims process, we also needed to read and understand the documents involved ‚Äî from the front of a Thai driver‚Äôs license to the Vehicle Identification Number (VIN), mileage, license plate, or even the Thai National ID card of the claimant. That‚Äôs why MARSAIL also expanded into NLP and OCR research, building powerful models that combine vision and language to intelligently extract, verify, and reason over structured and unstructured vehicle-related data.&lt;/p&gt;
&lt;p&gt;This work is now powering what we call the ‚ÄúDigital Insurance Twin‚Äù ‚Äî a complete AI ecosystem that mirrors and manages every car‚Äôs insurance lifecycle. For example, an insurer can use our models to approve or reject first-class coverage applications by automatically analyzing the condition of a vehicle and matching it against risk profiles. Garages can plug into our engine to generate real-time quotes validated by AI, reducing negotiation overhead and instilling transparency. For every photo and every document, there‚Äôs an AI model working behind the scenes to ensure authenticity, consistency, and fairness.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At the heart of MARSAIL‚Äôs vision is not just efficiency but integrity. By minimizing human subjectivity and manual paperwork, our AI platform safeguards against inflated repairs, ghost accidents, and fraudulent identity claims. This builds long-term trust across the ecosystem: between insurers and policyholders, garages and customers, and regulators and service providers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Today, we train not only car damage models but also multimodal models that fuse vision and language, including transformer-based LLMs optimized for the insurance domain. We‚Äôre teaching machines to reason with documents, anticipate inconsistencies, and provide contextual understanding ‚Äî allowing for seamless automation of tasks that previously required expert-level human input. Whether it‚Äôs reading a worn-out VIN from an old pickup truck or parsing a scanned Thai ID under poor lighting, MARSAIL&amp;rsquo;s AI agents are learning, adapting, and improving with every new sample.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ai-driven-car-damage-estimating-global-innovations-and-marsails-vision-for-thailand&#34;&gt;AI-driven Car Damage Estimating: Global Innovations and MARSAIL&amp;rsquo;s Vision for Thailand&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;The global automotive AI landscape is evolving rapidly, with companies like Tractable and Mitchell leading the way in using artificial intelligence to assess car damage, estimate repair costs, and streamline the claims process. For instance, Tractable‚Äôs AI photo-estimating system has already gained significant attention for its ability to analyze images of vehicle damage and provide accurate repair estimates. You can explore one of their impressive demonstrations in the video below:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/dvbF2BmkM3Q&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;What does Martin Ellingsworth, Executive Managing Director, P&amp;amp;C Insurance Intelligence at J.D. Power have to say about the impact AI is having on the P&amp;amp;C customer experience?&lt;br&gt;&lt;br&gt;Dive into our full report here: &lt;a href=&#34;https://t.co/XP4sL6ntsC&#34;&gt;https://t.co/XP4sL6ntsC&lt;/a&gt; &lt;a href=&#34;https://t.co/1YCB7E7Sse&#34;&gt;pic.twitter.com/1YCB7E7Sse&lt;/a&gt;&lt;/p&gt;&amp;mdash; Tractable (@tractable_ai) &lt;a href=&#34;https://twitter.com/tractable_ai/status/1656316477580902404?ref_src=twsrc%5Etfw&#34;&gt;May 10, 2023&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Another example comes from Mitchell‚Äôs partnership with AI technology to enhance auto insurance workflows. This collaboration showcases the future of intelligent claims processing and damage estimation, as seen in the video below:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hfas2mTIZyA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;These two AI-driven innovations are setting the stage for a new era in automotive insurance, and at MARSAIL, we aim to bring similar advancements to Thailand. Our cutting-edge AI research is designed to deliver an intelligent, scalable solution for vehicle damage estimation and claims processing. By harnessing deep learning and multimodal AI models, MARSAIL will revolutionize how car damage is assessed and how repairs are priced, ensuring transparency, reducing fraud, and optimizing the entire insurance lifecycle. Just like these global players, MARSAIL will offer a seamless, efficient, and trust-enhancing solution for the Thai automotive insurance market.&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/L1J3yXUE7yU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Aba5afny294&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Let us drive this transformation ‚Äî not just with code and data, but with vision, integrity, and purpose.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mars-our-flagship-ai&#34;&gt;MARS: Our Flagship AI&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;At MARSAIL‚Äôs mission lies our flagship model ‚Äî MARS (Mask Attention Refinement with Sequential Quadtree Nodes). This architecture was designed with a singular goal: to perform fine-grained instance segmentation of car damage in a real-world automotive insurance setting, under various lighting, angle, and occlusion conditions. Unlike conventional approaches like Mask R-CNN or PointRend, MARS integrates hierarchical spatial reasoning using a quadtree decomposition fused with multi-level self-attention to enable both coarse-to-fine detection and spatial localization of subtle visual cues.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/MARS/img/MARS_003.png&#34; alt=&#34;MARS Example Output&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: Visual example of MARS model detecting vehicle damage areas with high precision across complex body surfaces.
&lt;/div&gt;
&lt;p&gt;The core concept behind MARS draws inspiration from traditional quadtree spatial partitioning methods. Unlike standard convolutional encoders, which often struggle to preserve important structural details in large feature maps due to fixed processing limits, MARS takes a more adaptive approach. It uses a trainable system that breaks an image into regions based on how visually complex each area is. Simpler areas are processed more lightly, while more detailed or chaotic regions get extra attention.&lt;/p&gt;
&lt;p&gt;$$
M = \sum_{i=1}^{N} g(q_i) \cdot A(q_i) \cdot F(q_i)
$$&lt;/p&gt;
&lt;p&gt;This leads to what the team calls Sequential Quadtree Nodes (SQN) ‚Äî a smart structure that expands only where deeper analysis is needed. Each of these nodes contributes to the overall segmentation by combining spatial focus, a confidence check, and fine-tuned local adjustments. The system selectively decides which parts of the image matter most, refining the final output by integrating all these localized insights into one coherent segmentation result.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;https://kaopanboonyuen.github.io/MARS/img/featured.png&#34; alt=&#34;MARS Model Architecture&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: MARS architecture integrates attention refinement, quadtree partitioning, and sequential mask propagation modules to enable instance-level car damage segmentation.
&lt;/div&gt;
&lt;p&gt;In our experiments on Thai car damage datasets, MARS with a ResNet101-FPN backbone outperforms all existing baselines, yielding a +2.3 maskAP improvement over PointRend and +1.8 over Mask Transfiner (Panboonyuen et al. 2023). More impressively, MARS demonstrates resilience in heavily occluded scenes, where traditional architectures often over-segment or misclassify minor damages such as scratches, dents, and broken trims.&lt;/p&gt;
&lt;p&gt;Beyond segmentation accuracy, MARS was designed with inference cost and deployment scalability in mind. The attention and SQN modules are modular and parallelizable, allowing fast inference on GPU-based edge devices commonly deployed in insurance claim centers. In practice, MARS processes a high-resolution damage image in under 180ms on an NVIDIA T4 GPU, enabling real-time integration into mobile claim apps and garage estimation tools.&lt;/p&gt;
&lt;p&gt;For a comprehensive explanation of the method, experimental protocol, and benchmark comparisons, readers are encouraged to refer to our full publication &lt;a href=&#34;https://arxiv.org/abs/2305.04743&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; (Panboonyuen et al. 2023).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;marsail-2025-research-highlights&#34;&gt;MARSAIL 2025: Research Highlights&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;As we move into 2025, MARSAIL continues to push the frontier of AI for automotive insurance and repair through multimodal AI systems, low-parameter transformer architectures, and high-resolution OCR solutions tailored for real-world document processing. These efforts aim to expand our automation ecosystem beyond just car damage detection ‚Äî now encompassing risk prediction, document understanding, vehicle condition analytics, and identity verification pipelines. Below we highlight three key models from our most recent research drop.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-albert-efficient-transformer-for-automotive-localization&#34;&gt;1. ALBERT: Efficient Transformer for Automotive Localization&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;ALBERT (Advanced Localization and Bidirectional Encoder Representations from Transformers) is our compact vision transformer (ViT) tailored for car damage assessment. Unlike standard ViT and DETR-based approaches, ALBERT introduces structural inductive biases using localized deformable tokens and parameter sharing to reduce model size and memory usage while preserving high-resolution localization.&lt;/p&gt;
&lt;p&gt;Mathematically, the ALBERT encoder layer refines input embeddings using the following formulation:&lt;/p&gt;
&lt;p&gt;$$
z^{(l+1)} = z^{(l)} + \text{MSA}(\text{LN}(z^{(l)})) + \text{MLP}(\text{LN}(z^{(l)}))
$$&lt;/p&gt;
&lt;p&gt;Where MSA is a multi-scale self-attention with learnable spatial offsets adapted to car damage priors. Through selective hard sampling and token grouping, ALBERT reduces GPU memory by 40% during training while achieving higher IoU for small scratches and localized bumper cracks. This model is ideal for mobile deployments in insurance apps and in-vehicle camera assessments (&lt;a href=&#34;http://arxiv.org/abs/2506.10524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2506.10524&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;2-slick-knowledge-enhanced-instance-segmentation&#34;&gt;2. SLICK: Knowledge-Enhanced Instance Segmentation&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;SLICK (Selective Localization and Instance Calibration with Knowledge) augments vision transformer-based instance segmentation using both spatial priors and domain knowledge graphs. By integrating policy-driven insurance metadata ‚Äî e.g., bumper policy zones, model-specific weak points ‚Äî SLICK dynamically adjusts attention weights and segmentation proposals. This results in better prediction under diverse lighting, weather, and occlusion scenarios, especially for aging or modified vehicles.&lt;/p&gt;
&lt;p&gt;We define a mask quality score enhanced by contextual gating:&lt;/p&gt;
&lt;p&gt;$$
s_{mask} = \sigma(W_q [f_{img} | f_{kg}]) + b
$$&lt;/p&gt;
&lt;p&gt;SLICK delivers +2.5 maskAP improvement over MARS in high-noise scenes, setting a new benchmark in our internal Thai Vehicle Damage dataset (&lt;a href=&#34;http://arxiv.org/abs/2506.10528&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2506.10528&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;3-dota-deformable-optimized-transformer-for-ocr&#34;&gt;3. DOTA: Deformable Optimized Transformer for OCR&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;DOTA (Deformable Optimized Transformer Architecture) is our OCR engine designed for reading Thai ID cards, driver licenses, vehicle registration books, VIN plates, and inspection forms in real-world garage conditions. Using a retrieval-augmented architecture and deformable attention blocks, DOTA can accurately recognize text even under motion blur, glare, or partial occlusion.&lt;/p&gt;
&lt;p&gt;Unlike traditional CRNN or standard transformer OCRs, DOTA uses a hybrid vision-language training objective:&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;4-mars-mask-attention-refinement-with-sequential-quadtree-nodes-for-car-damage-instance-segmentation&#34;&gt;4. MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;At &lt;strong&gt;MARSAIL&lt;/strong&gt;, under the guidance of Dr. Teerapong Panboonyuen (‡∏î‡∏£. ‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô) or Dr. Kao (‡∏î‡∏£. ‡πÄ‡∏Å‡πâ‡∏≤), our team is on a mission: to fuse cutting-edge &lt;strong&gt;vision systems&lt;/strong&gt; with real-world auto insurance workflows. Our latest work‚Äîpublished in Springer‚Äîintroduces a dynamic way to analyze vehicle damage and process claims faster, smarter, and more precisely than ever before.&lt;/p&gt;
&lt;h3 id=&#34;-what-sets-our-approach-apart&#34;&gt;üîç what sets our approach apart&lt;/h3&gt;
&lt;p&gt;Standard image-analysis tools often struggle to keep structural details intact when dealing with complex scenes or large images. In our paper (Springer, Chapter 3), we introduce a &lt;strong&gt;hierarchical, attention-driven method&lt;/strong&gt; that adapts dynamically to the visual complexity of each region:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Smart partitioning&lt;/strong&gt; ‚Äî the system learns to divide an image into smaller, more granular regions where details matter most.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Targeted refinement&lt;/strong&gt; ‚Äî complex zones receive deeper analysis, while simpler areas are processed more lightly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sequential quadtree nodes&lt;/strong&gt; ‚Äî these elements expand only in areas needing sharper focus, concentrating computing power where it counts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-how-it-works-in-practice&#34;&gt;üß† how it works in practice&lt;/h3&gt;
&lt;p&gt;Each &lt;strong&gt;quadtree node&lt;/strong&gt; contributes to the final damage map by combining:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Spatial focus&lt;/strong&gt; ‚Äî where exactly to look,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Confidence-based gating&lt;/strong&gt; ‚Äî deciding &lt;em&gt;how much&lt;/em&gt; to trust that region,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local feature tweaks&lt;/strong&gt; ‚Äî refining the outcome with sharp, situational insight.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Imagine a damaged car: intricate scratches around the headlight trigger more detailed analysis, while broader, simpler zones are processed more efficiently. The combined result is a crisp and accurate segmentation mask covering the entire vehicle.&lt;/p&gt;
&lt;h3 id=&#34;-why-it-matters-for-insurers--workshops&#34;&gt;üèéÔ∏è why it matters for insurers &amp;amp; workshops&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster, smarter estimates&lt;/strong&gt; ‚Äî automation tackles repetitive image tasks instantly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sharper insights&lt;/strong&gt; ‚Äî context-aware focus pinpoints damage with precision.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficiency-driven&lt;/strong&gt; ‚Äî resources are optimized for the parts of the image that matter, reducing processing load.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalable architecture&lt;/strong&gt; ‚Äî adaptable from single images to massive fleets seamlessly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;-conclusions--next-steps&#34;&gt;‚úÖ conclusions &amp;amp; next steps&lt;/h3&gt;
&lt;p&gt;Published under the banner of &amp;ldquo;MARSAIL&amp;rdquo; in the latest Springer volume, this work signals a bold step in auto-insurance intelligence. With a pioneering lab approach and visionary leadership from Dr. Kao, MARSAIL is crafting systems that are fast, precise, and endlessly adaptable.&lt;/p&gt;
&lt;p&gt;Up next: integrating our quadtree vision engine into live insurer workflows, and testing it in real-world garages across Thailand.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Reference: &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Springer Nature Link&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;Auto insurance fraud is a massive challenge that costs billions worldwide every year, slowing down claim processing and increasing premiums for honest customers. At &lt;strong&gt;MARSAIL&lt;/strong&gt; (Motor AI Recognition Solution Artificial Intelligence Laboratory), we are on the frontlines of this battle, deploying cutting-edge AI to automate and streamline the entire insurance claim process ‚Äî from damage detection to fraud prevention ‚Äî without relying on manual human intervention.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;fighting-fraud-with-intelligent-automation&#34;&gt;Fighting Fraud with Intelligent Automation&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Fraudsters exploit loopholes and manual processes in traditional insurance workflows, making it difficult for companies to detect suspicious claims quickly and accurately. MARSAIL leverages advanced computer vision and domain-specific knowledge to transform this game. By analyzing images and metadata from vehicles in real-time, our AI models not only identify damages precisely but also flag potential fraud patterns, speeding up claim approval and reducing false payouts.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_01.jpg&#34; alt=&#34;Using AI to Automate Car Insurance Claims&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: Demonstration of AI-powered automation in car insurance claims (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;visual-damage-interpretation--smarter-and-faster&#34;&gt;Visual Damage Interpretation ‚Äî Smarter and Faster&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;Using state-of-the-art AI, MARSAIL interprets vehicle damage from photos submitted through mobile apps or web platforms. This automated damage assessment removes subjectivity and human error, ensuring fair and consistent evaluation across all claims.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_02.jpeg&#34; alt=&#34;AI Interpreting Car Damage in Application&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: AI-driven damage interpretation on insurance applications and web portals (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;translating-damage-into-precise-repair-estimates&#34;&gt;Translating Damage Into Precise Repair Estimates&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;MARSAIL‚Äôs AI doesn‚Äôt stop at identifying damage. It accurately translates the visual information into detailed labor and parts costs, providing transparent and consistent repair estimates that speed up approvals and payments.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_03.jpeg&#34; alt=&#34;AI Converting Damage to Labor and Parts Cost&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 6: AI converting damage images into labor and parts cost estimates (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;ai-powered-fraud-detection--catching-the-unseen&#34;&gt;AI-Powered Fraud Detection ‚Äî Catching the Unseen&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;One of MARSAIL‚Äôs critical strengths is its ability to detect anomalies and fraudulent claims. By cross-referencing damage patterns, metadata, and historical data, the AI flags suspicious cases automatically, drastically reducing fraud risk and protecting insurers and honest customers alike.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_04.jpeg&#34; alt=&#34;AI Detecting Fraud in Auto Insurance Claims&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 7: AI-powered fraud detection in insurance claims (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;comprehensive-automated-reporting-for-faster-decisions&#34;&gt;Comprehensive, Automated Reporting for Faster Decisions&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;MARSAIL‚Äôs automated reporting delivers a clear, itemized breakdown of all repair costs and identified damages, allowing claims adjusters and repair shops to make faster, more informed decisions ‚Äî with complete transparency and no human bias.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/MARSAIL_FROM_OTHER_SAMPLE_05.jpeg&#34; alt=&#34;Final AI Report of Car Damage and Repair Cost&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 8: Final AI-generated report summarizing car damage and repair costs (Image source: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34;&gt;Addenda Tech&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-marsail-leads-the-charge&#34;&gt;Why MARSAIL Leads the Charge&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;By automating the entire claims process ‚Äî from damage detection and cost estimation to fraud detection and reporting ‚Äî MARSAIL empowers insurers with unmatched speed, accuracy, and trustworthiness. This end-to-end AI solution minimizes human error and bias, reduces processing time dramatically, and safeguards the insurance ecosystem from fraudulent activities.&lt;/p&gt;
&lt;p&gt;The result? Faster claims, fairer settlements, and a more secure insurance future ‚Äî powered entirely by intelligent automation.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;join-the-future-of-auto-insurance&#34;&gt;Join the Future of Auto Insurance&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;MARSAIL is not just innovating AI for automotive damage detection; we are transforming the entire insurance experience ‚Äî making it smarter, safer, and more efficient. Together, we‚Äôre winning the battle against auto insurance fraud and paving the way for a better, fairer tomorrow.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;future-work-beyond-damage--toward-full-lifecycle-intelligence&#34;&gt;Future Work: Beyond Damage ‚Äì Toward Full-Lifecycle Intelligence&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Looking forward, our roadmap involves the development of a unified AI engine that combines damage reasoning, document verification, driving behavior analysis, and vehicle lifecycle forecasting. This would support real-time insurance pricing, repair prioritization, fraud detection, and second-hand vehicle valuation. MARSAIL is also training domain-specific large language models (LLMs) to interact with policyholders in Thai, automatically generate claim reports, and retrieve legal clauses for garage‚Äìinsurance negotiations.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re collaborating with garages and insurers across Thailand to create a national automotive data lake for federated learning ‚Äî enabling privacy-respecting model training across decentralized garages without centralizing sensitive vehicle data. Our ultimate goal is to support the Thai government‚Äôs vision for a Smart Nation and to make MARSAIL the Southeast Asian leader in Automotive Intelligence Infrastructure.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Jul 2025). &lt;em&gt;MARSAIL: The Smart Engine Powering the Future of Car Insurance and Intelligent Garages&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{panboonyuen2023mars,
  title={MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation},
  author={Panboonyuen, Teerapong, et al.},
  booktitle={International Conference on Image Analysis and Processing},
  year={2023},
  organization={Springer}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Found this blog insightful? Consider sharing it with friends or researchers in the automotive or insurance tech industry. üöó
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Panboonyuen, Teerapong, et al. &amp;ldquo;Mars: Mask attention refinement with sequential quadtree nodes for car damage instance segmentation.&amp;rdquo; International Conference on Image Analysis and Processing. Cham: Springer Nature Switzerland, 2023.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wang, X., X. Li, and Z. Wu. (2023). &lt;em&gt;Cardd: A new dataset for vision-based car damage detection&lt;/em&gt;. IEEE Transactions on Intelligent Transportation Systems, 24(7), 7202-7214.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Elbhrawy, A. S., M. A. Belal, and M. S. Hassanein. (2024). &lt;em&gt;CES: Cost Estimation System for Enhancing the Processing of Car Insurance Claims&lt;/em&gt;. Journal of Computing and Communication, 3(1), 55-69.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Amirfakhrian, M., and M. Parhizkar. (2021). &lt;em&gt;Integration of image segmentation and fuzzy theory to improve the accuracy of damage detection areas in traffic accidents&lt;/em&gt;. Journal of Big Data, 8(1), 1‚Äì17.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Arnab, A., and P. H. S. Torr. (2017). &lt;em&gt;Pixelwise instance segmentation with a dynamically instantiated network&lt;/em&gt;. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 441-450.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bolya, D., C. Zhou, F. Xiao, and Y. J. Lee. (2019). &lt;em&gt;Yolact: Real-time instance segmentation&lt;/em&gt;. Proceedings of the IEEE/CVF International Conference on Computer Vision, 9157‚Äì9166.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chen, K., J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, et al. (2019). &lt;em&gt;Hybrid task cascade for instance segmentation&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4974‚Äì4983.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Chen, H., K. Sun, Z. Tian, C. Shen, Y. Huang, and Y. Yan. (2020). &lt;em&gt;Blendmask: Top-down meets bottom-up for instance segmentation&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8573‚Äì8581.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Girshick, R. (2015). &lt;em&gt;Fast R-CNN&lt;/em&gt;. Proceedings of the IEEE International Conference on Computer Vision, 1440‚Äì1448.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;He, K., G. Gkioxari, P. Doll√°r, and R. Girshick. (2017). &lt;em&gt;Mask R-CNN&lt;/em&gt;. Proceedings of the IEEE International Conference on Computer Vision, 2961‚Äì2969.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;J√µeveer, K., and K. Kepp. (2023). &lt;em&gt;What drives drivers? Switching, learning, and the impact of claims in car insurance&lt;/em&gt;. Journal of Behavioral and Experimental Economics, 103, 101993.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ke, L., M. Danelljan, X. Li, Y.-W. Tai, C.-K. Tang, and F. Yu. (2022). &lt;em&gt;Mask Transfiner for high-quality instance segmentation&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4412‚Äì4421.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kirillov, A., Y. Wu, K. He, and R. Girshick. (2020). &lt;em&gt;PointRend: Image segmentation as rendering&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9799‚Äì9808.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Macedo, A. M., C. V. Cardoso, J. S. M. Neto, et al. (2021). &lt;em&gt;Car insurance fraud: The role of vehicle repair workshops&lt;/em&gt;. International Journal of Law, Crime and Justice, 65, 100456.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parhizkar, M., and M. Amirfakhrian. (2022). &lt;em&gt;Car detection and damage segmentation in the real scene using a deep learning approach&lt;/em&gt;. International Journal of Intelligent Robotics and Applications, 6(2), 231‚Äì245.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Panboonyuen, T. (2019). &lt;em&gt;Semantic segmentation on remotely sensed images using deep convolutional encoder-decoder neural network&lt;/em&gt;. Ph.D. Thesis, Chulalongkorn University.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pasupa, K., P. Kittiworapanya, N. Hongngern, and K. Woraratpanya. (2022). &lt;em&gt;Evaluation of deep learning algorithms for semantic segmentation of car parts&lt;/em&gt;. Complex &amp;amp; Intelligent Systems, 8(5), 3613‚Äì3625.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weisburd, S. (2015). &lt;em&gt;Identifying moral hazard in car insurance contracts&lt;/em&gt;. Review of Economics and Statistics, 97(2), 301‚Äì313.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wang, X., R. Zhang, T. Kong, L. Li, and C. Shen. (2020). &lt;em&gt;Solov2: Dynamic and fast instance segmentation&lt;/em&gt;. Advances in Neural Information Processing Systems, 33, 17721‚Äì17732.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Xie, E., P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, and P. Luo. (2020). &lt;em&gt;Polarmask: Single shot instance segmentation with polar representation&lt;/em&gt;. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12193‚Äì12202.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zhang, Q., X. Chang, and S. Bian. (2020). &lt;em&gt;Vehicle-damage-detection segmentation algorithm based on improved Mask R-CNN&lt;/em&gt;. IEEE Access, 8, 6997‚Äì7004.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The battle against auto insurance fraud ‚Äì and how AI can help win it: &lt;a href=&#34;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023</title>
      <link>https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/</link>
      <pubDate>Thu, 12 Sep 2024 18:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can read our full paper on Springer here: &lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MARS at ICIAP 2023&lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;presenting-mars-mask-attention-refinement-with-sequential-quadtree-nodes-at-iciap-2023&#34;&gt;Presenting MARS: Mask Attention Refinement with Sequential Quadtree Nodes at ICIAP 2023&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;In &lt;strong&gt;September 2023&lt;/strong&gt;, I had the privilege of presenting my latest AI research, &lt;strong&gt;MARS&lt;/strong&gt; (Mask Attention Refinement with Sequential Quadtree Nodes), at the &lt;strong&gt;International Conference on Image Analysis and Processing (ICIAP 2023)&lt;/strong&gt;, held in &lt;strong&gt;Udine, Italy&lt;/strong&gt;, from &lt;strong&gt;September 11‚Äì15&lt;/strong&gt;. MARS is a specialized deep learning model developed for &lt;strong&gt;car damage instance segmentation&lt;/strong&gt;, a vital application in the automotive insurance and garage service sectors.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;üçÑ We&amp;#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.&lt;br&gt;.&lt;br&gt;Paper: &lt;a href=&#34;https://t.co/UayUSxmZep&#34;&gt;https://t.co/UayUSxmZep&lt;/a&gt;&lt;br&gt;Code: &lt;a href=&#34;https://t.co/RoNFjSslXr&#34;&gt;https://t.co/RoNFjSslXr&lt;/a&gt;&lt;br&gt;Project: &lt;a href=&#34;https://t.co/uSoBX21HpF&#34;&gt;https://t.co/uSoBX21HpF&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ComputerVision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ComputerVision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/DeepLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DeepLearning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Research?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Research&lt;/a&gt; &lt;a href=&#34;https://t.co/oc8gz7Hs9I&#34;&gt;pic.twitter.com/oc8gz7Hs9I&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1822723598764876000?ref_src=twsrc%5Etfw&#34;&gt;August 11, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;Our model leverages &lt;strong&gt;Self-Attention Mechanisms&lt;/strong&gt; combined with &lt;strong&gt;Sequential Quadtree Nodes&lt;/strong&gt;, which together enable MARS to achieve significantly higher accuracy than state-of-the-art baselines such as Mask R-CNN (ICCV 2017), PointRend (CVPR 2020), and Mask Transfiner (CVPR 2022). These improvements were validated across multiple benchmark datasets, including a large Thai car-damage dataset.&lt;/p&gt;
&lt;p&gt;MARS was accepted at ICIAP 2023 with a competitive acceptance rate of 0.64, and our work is published in the Springer Lecture Notes in Computer Science series. This international recognition affirms the impact and novelty of our approach.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_01.jpg&#34; alt=&#34;Me at the ICIAP 2023 Conference in Udine, Italy&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 1: Presenting MARS at the ICIAP 2023 conference, Udine, Italy.
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_03.jpg&#34; alt=&#34;Our MARS poster presentation at ICIAP 2023&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 2: Poster presentation showcasing MARS at ICIAP 2023.
&lt;/div&gt;
&lt;p&gt;Of course! Here&amp;rsquo;s a &lt;strong&gt;compelling, long-form motivation section&lt;/strong&gt; for your blog, crafted to impress readers and highlight the significance of &lt;strong&gt;MARS&lt;/strong&gt;, your mission from &lt;strong&gt;Thailand to Italy&lt;/strong&gt;, and the vision of &lt;strong&gt;MARSAIL&lt;/strong&gt; in transforming car insurance and garage automation through AI:&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;why-mars-matters-from-thailand-to-italy-building-the-future-of-automotive-ai&#34;&gt;Why MARS Matters: From Thailand to Italy, Building the Future of Automotive AI&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The journey of &lt;strong&gt;MARS&lt;/strong&gt; began not in a conference hall in Europe but in the bustling cities and garages of &lt;strong&gt;Thailand&lt;/strong&gt;, where the real-world challenges of the automotive industry sparked a vision. Every day, countless car owners and insurance agents face delays, inconsistencies, and even disputes when assessing vehicle damage. Manual inspections often rely on human judgment ‚Äî which can be slow, subjective, or prone to error. In garages, mechanics struggle with non-standardized repair cost estimates. Insurers grapple with fraudulent claims, inflated repairs, and time-consuming paperwork. These challenges aren‚Äôt unique to Thailand ‚Äî they‚Äôre global. But we decided to start solving them at home.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;At &lt;strong&gt;MARSAIL&lt;/strong&gt; ‚Äî the &lt;em&gt;Motor AI Recognition Solution Artificial Intelligence Laboratory&lt;/em&gt; ‚Äî we asked ourselves: &lt;em&gt;Can we create a deep learning system that sees, understands, and evaluates vehicle damage better than a human?&lt;/em&gt; Could we develop an AI model smart enough to assist insurers, fast enough for real-time applications, and precise enough to help garage operators deliver fair, standardized repairs?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We believe the answer is yes ‚Äî and our vision for this transformation is detailed in our MARSAIL blog:
üëâ &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;MARSAIL: The Smart Engine Behind the Future of Car Insurance and Intelligent Garages&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The answer was &lt;strong&gt;MARS&lt;/strong&gt; ‚Äî &lt;em&gt;Mask Attention Refinement with Sequential Quadtree Nodes&lt;/em&gt; ‚Äî our flagship AI architecture built from the ground up for &lt;strong&gt;car damage instance segmentation&lt;/strong&gt;. Unlike traditional models like Mask R-CNN or PointRend, which were designed for general-purpose segmentation tasks, MARS is laser-focused. It leverages &lt;strong&gt;quadtree spatial hierarchies&lt;/strong&gt;, &lt;strong&gt;self-attention refinement&lt;/strong&gt;, and a custom transformer-based backbone to identify scratches, dents, cracks, and broken parts ‚Äî even in complex lighting, occlusion, or varied car surface conditions.&lt;/p&gt;
&lt;p&gt;What sets MARS apart is its &lt;em&gt;precision&lt;/em&gt;. It doesn‚Äôt just draw rough outlines ‚Äî it &lt;em&gt;understands&lt;/em&gt; the contours of the damage. Whether it‚Äôs a shallow scratch on a bumper or a crumpled fender after a collision, MARS detects it with clarity, outperforming state-of-the-art models with a significant margin in our Thai car damage benchmarks. And this isn‚Äôt just academic ‚Äî this is AI &lt;em&gt;for the real world&lt;/em&gt;, trained with real damage cases, tuned for high-stakes applications like insurance claims and repair verification.&lt;/p&gt;
&lt;p&gt;So when &lt;strong&gt;ICIAP 2023&lt;/strong&gt; announced their international conference in &lt;strong&gt;Udine, Italy&lt;/strong&gt;, we knew it was time to take our Thai-born innovation global. From Bangkok to Udine, we carried not just a poster and a paper ‚Äî but a vision. A vision that &lt;strong&gt;AI can transform the automotive industry&lt;/strong&gt;, not by replacing people, but by empowering them: giving insurers confidence, garages clarity, and drivers trust in the system.&lt;/p&gt;
&lt;p&gt;Presenting MARS in Italy wasn‚Äôt just a research milestone ‚Äî it was a symbol of what‚Äôs possible when bold ideas are met with rigorous engineering and a passion for solving real-world problems. Our work was selected for publication in the &lt;strong&gt;Lecture Notes in Computer Science&lt;/strong&gt; by &lt;strong&gt;Springer&lt;/strong&gt;, a recognition that underscores the technical excellence and practical value of our research.&lt;/p&gt;
&lt;p&gt;At &lt;strong&gt;MARSAIL&lt;/strong&gt;, our mission continues. We are building an end-to-end ecosystem of automotive AI: from car damage detection to &lt;strong&gt;automated cost estimation&lt;/strong&gt;, from &lt;strong&gt;OCR document parsing&lt;/strong&gt; to &lt;strong&gt;insurance fraud detection&lt;/strong&gt;, all powered by AI models trained with diverse Thai datasets and built to serve global standards. MARS is just the beginning.&lt;/p&gt;
&lt;p&gt;Because in the future we see, &lt;strong&gt;a single smartphone photo&lt;/strong&gt; is enough to initiate a car claim, verify vehicle condition, and provide a fair quote ‚Äî all in seconds. This is the &lt;strong&gt;Digital Insurance Twin&lt;/strong&gt; we‚Äôre crafting. And we believe it starts with intelligence, integrity, and innovation ‚Äî the core values of MARS and everything we do at MARSAIL.&lt;/p&gt;
&lt;p&gt;Let‚Äôs drive the future ‚Äî together. From Thailand to Italy, from inspiration to impact.&lt;/p&gt;
&lt;hr&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_04.jpg&#34; alt=&#34;MARS deep learning architecture diagram&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 3: Illustration of the MARS architecture highlighting sequential quadtree nodes and mask attention refinement.
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mars-a-quadtree-driven-deep-learning-architecture-for-precision-car-damage-segmentation&#34;&gt;MARS: A Quadtree-Driven Deep Learning Architecture for Precision Car Damage Segmentation&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;Central to the design of &lt;strong&gt;MARS&lt;/strong&gt; (Mask Attention Refinement with Sequential Quadtree Nodes) lies a novel AI architecture designed to push the boundaries of &lt;strong&gt;instance segmentation&lt;/strong&gt;‚Äîspecifically, for car damage detection. Traditional models like Mask R-CNN rely heavily on coarse ROI alignments and fixed spatial grids, which often result in poor mask quality for irregular or fine-grained structures like scratches, dents, or cracks. MARS overcomes this limitation through a unique &lt;strong&gt;quadtree-based spatial hierarchy&lt;/strong&gt;, allowing the network to adaptively decompose images into finer patches only where needed, focusing computational attention on high-detail regions.&lt;/p&gt;
&lt;p&gt;Technically, our method integrates a &lt;strong&gt;Self-Attention Refinement Module (SARM)&lt;/strong&gt; with &lt;strong&gt;Sequential Quadtree Node Encoding&lt;/strong&gt; to build hierarchical context from coarse-to-fine levels. Instead of treating spatial locations uniformly, we recursively subdivide image regions using quadtree decomposition, capturing fine structural features with higher resolution where necessary. Let $Q = {q_1, q_2, \dots, q_n}$ denote the set of quadtree nodes generated per instance mask. Each node $q_i$ is embedded and passed through a transformer encoder, which models long-range dependencies via multi-head self-attention:&lt;/p&gt;
&lt;p&gt;$$
\text{Attention}(Q) = \text{softmax}\left(\frac{QW_Q (QW_K)^T}{\sqrt{d_k}}\right)QW_V
$$&lt;/p&gt;
&lt;p&gt;These representations are aggregated to recalibrate channel-wise features dynamically, enabling MARS to predict high-fidelity instance masks with pixel-level accuracy.&lt;/p&gt;
&lt;p&gt;In practice, &lt;strong&gt;MARS outperforms Mask R-CNN, PointRend, and Mask Transfiner by +1.3 to +2.3 maskAP&lt;/strong&gt; on a Thai car damage dataset‚Äîdemonstrating that our approach is not only theoretically elegant but also empirically superior. Our model operates on top of the &lt;strong&gt;FPN backbone (ResNet-50 and ResNet-101 variants)&lt;/strong&gt;, and benefits from a cascaded refinement pipeline that improves mask boundaries at every stage. The full details, ablation studies, and demo results are available in our &lt;a href=&#34;https://github.com/kaopanboonyuen/MARS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;international-recognition-and-impact&#34;&gt;International Recognition and Impact&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;The ICIAP conference was a vibrant gathering of leading researchers and industry experts. I also had the honor to deliver a guest talk on &lt;strong&gt;modern AI advances in Large Language Models (LLMs)&lt;/strong&gt;, engaging with an international audience about cutting-edge AI trends beyond computer vision.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_02.jpg&#34; alt=&#34;Guest Speaker talk at ICIAP 2023 on modern AI in LLMs&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 4: Speaking as a guest at ICIAP 2023 on modern AI in Large Language Models.
&lt;/div&gt;
&lt;p&gt;The conference atmosphere was inspiring and energizing.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;img/KAO_ICIAP_05.jpg&#34; alt=&#34;Conference atmosphere at ICIAP 2023, Udine, Italy&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Figure 5: The vibrant atmosphere at ICIAP 2023 in Udine.
&lt;/div&gt;
&lt;hr&gt;
&lt;h2 id=&#34;exploring-italy-from-udine-to-rome&#34;&gt;Exploring Italy: From Udine to Rome&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;After the conference wrapped up, I took the chance to travel to Rome and experience Italy‚Äôs timeless heritage. Walking through the magnificent &lt;strong&gt;St. Peter‚Äôs Basilica&lt;/strong&gt; and the &lt;strong&gt;Vatican Museums&lt;/strong&gt; was a breathtaking journey into art and spirituality.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Colosseum&lt;/strong&gt;, standing proudly as an icon of ancient Roman engineering and history, was truly awe-inspiring. And of course, tossing a coin into the &lt;strong&gt;Trevi Fountain&lt;/strong&gt; was a magical moment steeped in legend ‚Äî a wish for continued success in research and life.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For the full story of my unforgettable journey ‚Äî from academic presentation to wandering the timeless streets of Rome ‚Äî feel free to read my detailed travel and research blog here: &lt;a href=&#34;https://kaopanboonyuen.wordpress.com/2023/09/27/showcasing-my-ai-research-in-italy-a-memorable-september-work-trip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Showcasing My AI Research in Italy: A Memorable September Work Trip&lt;/strong&gt;&lt;/a&gt;. In that post, I share not only the highlights of presenting our MARS model at ICIAP 2023, but also the personal moments that made the trip truly special ‚Äî from the intellectual exchanges at the conference to standing in awe before the Colosseum and Vatican. It was a journey that beautifully merged science, culture, and inspiration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;learn-more&#34;&gt;Learn More&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;For a deep dive into MARS, check out our paper and demos available on &lt;a href=&#34;https://arxiv.org/abs/2305.04743&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt; and our GitHub repository:&lt;br&gt;
&lt;a href=&#34;https://github.com/kaopanboonyuen/MARS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/kaopanboonyuen/MARS&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thank you for following my journey combining research and travel. Italy‚Äôs beauty and the excitement of sharing new AI breakthroughs have been unforgettable.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2023). &lt;em&gt;Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{panboonyuen2023mars,
  title   = &amp;quot;Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io&amp;quot;,
  year    = &amp;quot;2023&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it üôå
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk üõ∫ &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240906_Panboonyuen_AI_ThaiHighway.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#meet-reg-the-game-changer-in-highway-asset-detection&#34;&gt;Meet REG: The Game-Changer in Highway Asset Detection&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#generalized-focal-loss-for-multi-class-detection&#34;&gt;Generalized Focal Loss for Multi-Class Detection&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#refined-generalized-focal-loss-for-segmentation&#34;&gt;Refined Generalized Focal Loss for Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#refinement-term-for-spatial-contextual-learning&#34;&gt;Refinement Term for Spatial-Contextual Learning&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#joint-optimization-for-detection-and-segmentation&#34;&gt;Joint Optimization for Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#incorporating-prediction-uncertainty&#34;&gt;Incorporating Prediction Uncertainty&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#mathematical-foundations-for-optimization-in-reg&#34;&gt;Mathematical Foundations for Optimization in REG&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#performance-analysis-for-detection-and-segmentation&#34;&gt;Performance Analysis for Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways&#34;&gt;Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#understanding-the-scene&#34;&gt;Understanding the Scene&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-challenge-detection-and-segmentation&#34;&gt;The Challenge: Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#the-process-in-action&#34;&gt;The Process in Action&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#real-world-impact&#34;&gt;Real-World Impact&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#paper-highlights&#34;&gt;Paper Highlights:&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures&#34;&gt;Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#generalized-focal-loss-for-vision-tasks&#34;&gt;Generalized Focal Loss for Vision Tasks&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#final-insights-pioneering-precision-with-reg-in-highway-asset-detection&#34;&gt;Final Insights: Pioneering Precision with REG in Highway Asset Detection&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#1-introduction-to-generalized-focal-loss&#34;&gt;1. &lt;strong&gt;Introduction to Generalized Focal Loss&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#2-formula-for-difference-between-detection-and-segmentation-models&#34;&gt;2. &lt;strong&gt;Formula for Difference Between Detection and Segmentation Models&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#3-optimization-in-object-detection-and-segmentation&#34;&gt;3. &lt;strong&gt;Optimization in Object Detection and Segmentation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#4-mathematical-formulas-to-know&#34;&gt;4. &lt;strong&gt;Mathematical Formulas to Know&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#whats-next&#34;&gt;What‚Äôs Next?&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are pleased to announce that our paper, titled Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery, has been accepted for oral presentation at the 5th International Conference on Highway Engineering (iCHE 2024). This opportunity marks a significant moment in our academic journey, especially after a hiatus from international conferences since completing my Ph.D. I am eager to re-engage with the academic community and share our recent advancements in person.&lt;/p&gt;
&lt;h2 id=&#34;meet-reg-the-game-changer-in-highway-asset-detection&#34;&gt;Meet REG: The Game-Changer in Highway Asset Detection&lt;/h2&gt;
&lt;p&gt;Hi guys, fellow tech enthusiasts! I&amp;rsquo;m thrilled to unveil a cutting-edge innovation from my latest research‚ÄîRefined Generalized Focal Loss (REG). This revolutionary approach is transforming road asset detection on Thai highways, and it‚Äôs as exciting as it sounds.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So, what‚Äôs the big deal with REG? Imagine a detection system that not only sees but truly understands the intricate details of highway scenes. REG pushes the boundaries of current vision-based detection models by tackling the most challenging issues: imbalanced datasets, tiny objects, and complex highway backdrops.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Our paper on ‚ÄúRefined Generalized Focal Loss for Road Asset Detection on Thai Highways‚Äù has been accepted for oral presentation at iCHE 2024!&lt;br&gt;.&lt;br&gt;Excited to share how we‚Äôre tackling road asset detection.&lt;br&gt;.&lt;br&gt;üìñüëÄ Check out the details here: &lt;a href=&#34;https://t.co/KTSGHITU7F&#34;&gt;https://t.co/KTSGHITU7F&lt;/a&gt;&lt;br&gt;.&lt;a href=&#34;https://twitter.com/hashtag/iCHE2024?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#iCHE2024&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#AI&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1832227483967746136?ref_src=twsrc%5Etfw&#34;&gt;September 7, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;My work (check out Fig. 1) brings a whole new level of precision to the table. By integrating a custom loss function into the detection architecture, REG doesn&amp;rsquo;t just improve performance‚Äîit redefines it. This means sharper, more reliable detection of critical road assets like signs, lane markings, and barriers. And let‚Äôs be real, that‚Äôs a game-changer for infrastructure management and road safety.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; 
&lt;img src=&#34;REG_1.png&#34; alt=&#34;Refined Generalized Focal Loss Framework&#34; style=&#34;max-width: 100%; height: auto;&#34;&gt; 
&lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. The REG-based detection framework employs Generalized Focal Loss to master class imbalance in Thai highway road asset detection. Combining Transformer layers with convolutional modules, and using Batch Normalization and Adaptive Dropout, this model stands out for its robustness. It‚Äôs finely tuned to capture the unique aspects of Thai highways, focusing on rare and challenging assets. 
&lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34;&gt;[Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;
&lt;p&gt;REG isn&amp;rsquo;t just a theoretical leap; it‚Äôs a practical breakthrough with real-world impact. It‚Äôs especially useful for regions with road structures similar to Thai highways, where conventional detection algorithms might falter. By merging Vision Transformers (ViT) with conditional random fields (CRF), we‚Äôve supercharged the model‚Äôs ability to segment and identify road assets with pinpoint accuracy.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This isn‚Äôt just about the future of intelligent transportation systems; it‚Äôs about the here and now. As we edge closer to autonomous vehicle navigation, innovations like REG are paving the way for smarter, safer roads. Buckle up and stay tuned‚Äîexciting times are ahead!
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/h2&gt;
&lt;p&gt;Thailand&amp;rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    In this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Our proposed method (Fig. 2) enhances the capability of REG-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;proposed_method.png&#34; alt=&#34;Proposed Method Image&#34;&gt; &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. The proposed Enhanced REG-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various REG model variants, ranging from REGn to REGx, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didn‚Äôt just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34;&gt;[REG: Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;
&lt;p&gt;This paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.&lt;/p&gt;
&lt;p&gt;Stay tuned for updates, and a big thank you to my incredible research team:&lt;br&gt;
&lt;strong&gt;N. Rattanachona (N&amp;rsquo;Fuse)&lt;/strong&gt;, &lt;strong&gt;P. Thungthin (N&amp;rsquo;Dear)&lt;/strong&gt;, &lt;strong&gt;N. Subsompon (N&amp;rsquo;Tien)&lt;/strong&gt;. Your hard work and dedication were essential to this project!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_00.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;featured_full.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here I am, presenting our work on the Enhanced REG model and its application in detecting road assets!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have visualizations of the detection results produced by the Enhanced REG model. The bounding boxes and labels demonstrate the model‚Äôs ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the model‚Äôs performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-multi-class-detection&#34;&gt;Generalized Focal Loss for Multi-Class Detection&lt;/h3&gt;
&lt;p&gt;The detection task focuses on identifying seven key classes of road assets: Pavilions, Pedestrian bridges, Information signs, Single-arm poles, Bus stops, Warning signs, and Concrete guardrails (Fig. 3). The challenge lies in dealing with class imbalance ‚Äî smaller and harder-to-detect objects can be easily overlooked by traditional object detection models. We address this by utilizing &lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt;, which extends the classical Focal Loss to multi-class detection, giving more focus to underrepresented and challenging classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_2.png&#34; alt=&#34;Generalized Focal Loss for Multi-Class Detection&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. My proposed Generalized Focal Loss for multi-class detection tackles class imbalance across seven asset classes. By extending Focal Loss, we improve detection accuracy for small and difficult-to-classify objects.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;refined-generalized-focal-loss-for-segmentation&#34;&gt;Refined Generalized Focal Loss for Segmentation&lt;/h3&gt;
&lt;p&gt;For the segmentation task, we detect road assets at the pixel level, focusing on five classes: Pavilions, Pedestrian bridges, Information signs, Warning signs, and Concrete guardrails (Fig. 4). The key here is to ensure that every pixel is correctly classified into one of these categories, which is a non-trivial problem in cluttered highway imagery. My &lt;strong&gt;Refined Generalized Focal Loss&lt;/strong&gt; applies pixel-wise loss calculation, extending GFL into the realm of segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_3.png&#34; alt=&#34;Refined Generalized Focal Loss for Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 4. The segmentation process classifies each pixel into one of five road asset classes, using Refined Generalized Focal Loss to enhance pixel-wise accuracy in segmentation tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_03.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, let‚Äôs look at a real-world application of our Enhanced REG model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model&amp;rsquo;s potential for real-world deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_04.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart presents a comparison of performance metrics between our Enhanced REG model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model&amp;rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_05.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, this image illustrates the training process for the Enhanced REG model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the model‚Äôs performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the model‚Äôs effectiveness and reliability in practical applications.&lt;/p&gt;
&lt;h3 id=&#34;refinement-term-for-spatial-contextual-learning&#34;&gt;Refinement Term for Spatial-Contextual Learning&lt;/h3&gt;
&lt;p&gt;To further enhance learning, we introduce a spatial-contextual refinement term $(g_{i,c})$ that dynamically adjusts the loss based on the geometric and contextual significance of each object class (Fig. 5). This term allows the model to account for the spatial distribution of road assets, making it more adept at handling complex scenes typical of real-world road environments.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_4.png&#34; alt=&#34;Spatial-Contextual Refinement Term&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 5. The refinement term \(g_{i,c}\) adjusts the loss based on spatial and contextual relevance, improving model learning in complex and cluttered road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;joint-optimization-for-detection-and-segmentation&#34;&gt;Joint Optimization for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;We then integrate the detection and segmentation tasks into a joint optimization framework. By combining the losses for both tasks (Fig. 6), the model learns complementary representations, allowing it to improve both object detection and pixel-wise segmentation accuracy. This joint approach ensures that the model balances precision and recall across different road asset classes.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_5.png&#34; alt=&#34;Joint Optimization for Detection and Segmentation&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 6. Joint optimization balances detection and segmentation losses, enhancing performance across both tasks by learning complementary features.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;incorporating-prediction-uncertainty&#34;&gt;Incorporating Prediction Uncertainty&lt;/h3&gt;
&lt;p&gt;To further refine REG, we incorporated prediction uncertainty using a Gaussian distribution (Fig. 7). This technique accounts for the inherent noise and ambiguity in complex environments, particularly under varying lighting and cluttered backgrounds, thereby improving both robustness and accuracy.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_6.png&#34; alt=&#34;Incorporating Prediction Uncertainty&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 7. We model prediction uncertainty using a Gaussian distribution to handle noise and ambiguity, particularly in challenging road scenes.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;mathematical-foundations-for-optimization-in-reg&#34;&gt;Mathematical Foundations for Optimization in REG&lt;/h3&gt;
&lt;p&gt;The optimization of REG is based on advanced techniques in stochastic optimization, where we extend traditional gradient descent to operate on &lt;strong&gt;Riemannian Manifolds&lt;/strong&gt; (Fig. 8). Given the non-convex nature of the loss landscape, we utilize variational inference, proximal gradient methods, and Lagrangian multipliers, allowing for efficient optimization in multi-task learning.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_7.png&#34; alt=&#34;Mathematical Foundations for Optimization in REG&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 8. Advanced mathematical techniques, including Riemannian stochastic gradient descent, underpin the optimization of REG in complex, high-dimensional spaces.&lt;/p&gt;
&lt;/div&gt;
&lt;h3 id=&#34;performance-analysis-for-detection-and-segmentation&#34;&gt;Performance Analysis for Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;Finally, we tested the model&amp;rsquo;s performance on both detection (Fig. 9) and segmentation tasks (Fig. 10). REG demonstrated significant improvements in mAP50, F1-score, and other key metrics, showcasing its capability to handle both high-overlap detection and detailed mask segmentation.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_8.png&#34; alt=&#34;Detection Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 9. REG outperforms other models in detection tasks, especially in high-overlap scenarios, with superior mAP50 and F1 scores.&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
    &lt;img src=&#34;REG_9.png&#34; alt=&#34;Segmentation Performance&#34;&gt;
    &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 10. The segmentation performance of REG shows exceptional accuracy in generating precise masks, particularly in challenging environments.&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;this work introduces Refined Generalized Focal Loss (REG), which significantly improves the detection and segmentation of road assets in complex environments. By applying advanced mathematical techniques and integrating spatial-contextual learning, REG addresses the challenges of class imbalance and localization in highway asset detection. The mathematical insights behind this model, including optimization on Riemannian manifolds and probabilistic refinement, provide a robust framework for future improvements in vision-based infrastructure management systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;For those interested in exploring the full mathematical derivation and code, please check out the &lt;a href=&#34;https://github.com/kaopanboonyuen/REG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;REG: Refined Generalized Focal Loss on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recap-a-journey-through-road-asset-detection-and-segmentation-on-thai-highways&#34;&gt;Recap: A Journey Through Road Asset Detection and Segmentation on Thai Highways&lt;/h2&gt;
&lt;h3 id=&#34;understanding-the-scene&#34;&gt;Understanding the Scene&lt;/h3&gt;
&lt;p&gt;Imagine you&amp;rsquo;re driving along a bustling Thai highway, surrounded by a landscape dotted with various road assets. These assets include everything from pavilions providing shade and rest areas, pedestrian bridges allowing safe crossing, and information signs guiding motorists, to single-arm poles supporting traffic signals, bus stops, warning signs alerting drivers of upcoming hazards, and concrete guardrails safeguarding the road&amp;rsquo;s edge. Each of these elements plays a critical role in ensuring the safety and efficiency of the highway system.&lt;/p&gt;
&lt;h3 id=&#34;the-challenge-detection-and-segmentation&#34;&gt;The Challenge: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;To manage and maintain these assets effectively, automated systems are employed to detect and segment these features from images captured along the highway. This process involves two main tasks: detection and segmentation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Detection Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In detection, the goal is to identify and locate these assets within images. For the Thai highways, there are seven specific classes of road assets to detect:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Structures offering shade and rest for travelers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Elevated walkways ensuring safe crossing over the highway.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Signs providing crucial information to drivers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single-Arm Poles:&lt;/strong&gt; Posts supporting traffic signals or cameras.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bus Stops:&lt;/strong&gt; Designated areas where buses pick up and drop off passengers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Signs alerting drivers to potential hazards ahead.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Barriers designed to prevent vehicles from veering off the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Segmentation Tasks:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Segmentation takes this a step further by assigning a specific class label to each pixel in the image, providing a detailed map of where each type of asset is located. For the Thai highways, the segmentation focuses on five classes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Pavilions:&lt;/strong&gt; Highlighted as areas of rest and shelter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pedestrian Bridges:&lt;/strong&gt; Marked to show their location and coverage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Information Signs:&lt;/strong&gt; Detailed to ensure visibility and accessibility.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Warning Signs:&lt;/strong&gt; Identified to enhance hazard awareness.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Concrete Guardrails:&lt;/strong&gt; Outlined to confirm their placement along the road.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-process-in-action&#34;&gt;The Process in Action&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Detection:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Picture an advanced AI system analyzing highway images. It scans each image to detect the seven classes of road assets. Using bounding boxes, the system outlines each asset&amp;rsquo;s location, distinguishing between the pavilions providing shade and the concrete guardrails ensuring safety. This detection process helps in cataloging and managing each asset efficiently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Segmentation:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Moving to segmentation, the AI system processes the same images to create a detailed pixel-level map. Each pixel in the image is classified into one of the five categories, such as pavilions, pedestrian bridges, and warning signs. This precise classification allows for a thorough understanding of where each asset is situated, helping with tasks like maintenance scheduling and safety assessments.&lt;/p&gt;
&lt;h3 id=&#34;real-world-impact&#34;&gt;Real-World Impact&lt;/h3&gt;
&lt;p&gt;This dual approach‚Äîdetection and segmentation‚Äîensures that every asset along the Thai highways is accurately identified and mapped. For instance, knowing the exact location of warning signs can help in assessing their visibility and effectiveness. Similarly, detailed segmentation of concrete guardrails aids in monitoring their condition and integrity.&lt;/p&gt;
&lt;h2 id=&#34;paper-highlights&#34;&gt;Paper Highlights:&lt;/h2&gt;
&lt;p&gt;Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced REG model integrated with &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.&lt;/p&gt;
&lt;h2 id=&#34;comprehensive-analysis-of-generalized-focal-loss-and-last-layer-architectures&#34;&gt;Comprehensive Analysis of Generalized Focal Loss and Last Layer Architectures&lt;/h2&gt;
&lt;p&gt;In computer vision, both object detection and semantic segmentation are crucial tasks that leverage different approaches and final layer architectures in deep learning models. This document provides an in-depth technical overview of Generalized Focal Loss applied to both tasks, and a detailed comparison of the final layers used in each.&lt;/p&gt;
&lt;h3 id=&#34;generalized-focal-loss-for-vision-tasks&#34;&gt;Generalized Focal Loss for Vision Tasks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Generalized Focal Loss (GFL)&lt;/strong&gt; is designed to address class imbalance and focus learning on hard-to-detect objects by adjusting the standard focal loss. This approach is applicable to both detection and segmentation tasks but is formulated slightly differently for each.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt;
In object detection, GFL helps to improve the accuracy of detecting objects and managing class imbalance by focusing on harder-to-detect objects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematical Formula:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For detection tasks involving multiple classes (e.g., Pavilions, Pedestrian Bridges, etc.), the Generalized Focal Loss is given by:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{GFL}}^{\text{Detection}} = - \alpha \left(1 - p_t\right)^\gamma \log(p_t)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_t$ represents the predicted probability for the correct class.&lt;/li&gt;
&lt;li&gt;$\alpha$ is a balancing factor that adjusts the importance of positive and negative examples to handle class imbalance.&lt;/li&gt;
&lt;li&gt;$\gamma$ is the focusing parameter that controls the extent to which hard examples are emphasized. Higher values of $\gamma$ increase the focus on difficult examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    For detecting objects like Pedestrian Bridges or Concrete Guardrails, which may appear in challenging conditions, GFL reduces the weight of easy examples and enhances the learning from complex cases, such as those with partial occlusions or poor lighting.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
#### 2. Generalized Focal Loss for Segmentation Tasks

**Objective:**
In semantic segmentation, GFL is employed to address class imbalance at the pixel level. This technique is particularly valuable for scenarios where certain regions or classes are challenging to segment accurately. By focusing on these difficult regions, GFL enhances the model&#39;s performance in identifying and classifying every pixel in an image.

**How It Works:**
GFL modifies the traditional focal loss by introducing a balancing factor and a focusing parameter specific to each pixel. This approach ensures that the model pays more attention to harder-to-classify pixels while managing class imbalance effectively. The balancing factor adjusts the importance of each pixel‚Äôs contribution, whereas the focusing parameter controls how much emphasis is placed on challenging examples.

**Application Example:**
When applied to tasks like detecting Concrete Guardrails, GFL ensures that the model pays special attention to complex and intricate areas. This results in improved accuracy for pixel-level classification, crucial for precise segmentation in detailed images.

#### Differences in Final Layers: Detection vs. Segmentation

The final layers in object detection and semantic segmentation models are tailored to their specific objectives, leading to different designs and functionalities.

##### 1. Detection Layer: Bounding Box Regression and Classification

**Objective:**
In object detection, the final layer&#39;s primary task is to predict the location of objects through bounding boxes and classify each object into one of the predefined classes.

**Architecture:**

1. **Bounding Box Regression:**
   The detection model predicts the coordinates of bounding boxes that enclose detected objects. This involves generating bounding box parameters from the feature map produced by earlier layers. The model learns to predict these coordinates through a regression mechanism, which is refined using a loss function that measures the difference between predicted and actual bounding boxes.

2. **Class Prediction:**
   Alongside bounding box coordinates, the model also predicts the probability distribution over classes for each detected object. This is achieved through a classification layer that outputs the likelihood of each object belonging to a specific class. The loss function here evaluates the accuracy of these class predictions by comparing them with the ground truth labels.

##### 2. Segmentation Layer: Pixel-Level Classification

**Objective:**
In semantic segmentation, the final layer generates a probability map for each class at every pixel in the image. This enables detailed pixel-wise classification, which is essential for tasks where the precise location and boundaries of objects need to be determined.

**Architecture:**

1. **Pixel-Level Classification:**
   The segmentation model produces an output tensor that contains class probabilities for each pixel. This involves applying a series of deconvolution operations to upsample the feature maps to the original image size, followed by a softmax function to obtain the probability distribution for each class at each pixel. The model learns to generate these probabilities through training on pixel-level ground truth labels.

**Summary**

- **Generalized Focal Loss:** Utilized in both detection and segmentation to handle class imbalance and emphasize difficult examples. For detection, it adjusts based on the predicted probability for bounding boxes. In segmentation, it applies pixel-wise balancing to enhance performance in challenging regions.

- **Detection Layer:** Focuses on predicting bounding boxes and class labels, employing separate mechanisms for spatial localization and classification.

- **Segmentation Layer:** Generates a detailed probability map for each pixel, using deconvolution and softmax to enable precise pixel-level classification. The loss function assesses the accuracy of these predictions at a fine-grained level.


### Key Differences Between Detection and Segmentation Layers

1. **Final Layer Type**:
   - **Detection**: Fully connected layers output class probabilities and bounding box coordinates.
   - **Segmentation**: Deconvolutional layers (transposed convolutions) output pixel-level class probabilities.

2. **Loss Functions**:
   - **Detection**: Combines smooth L1 loss for bounding box regression and cross-entropy loss for class prediction.
   - **Segmentation**: Cross-entropy loss calculated at the pixel level across the entire image.

3. **Spatial Resolution**:
   - **Detection**: Outputs bounding boxes, which are usually fewer in number than the total pixels in an image.
   - **Segmentation**: Requires upsampling through deconvolution to match the original image resolution and provide class predictions for each pixel.

4. **Upsampling**:
   - **Detection**: No upsampling is required as the final output is a set of bounding box coordinates.
   - **Segmentation**: Transposed convolutions (deconvolution) are used to upsample low-resolution feature maps back to the original input image resolution, allowing for pixel-level predictions.

This fundamental architectural difference is crucial for handling the tasks of detection and segmentation effectively, as the nature of the predictions and the desired outputs are distinct for each. --&gt;
&lt;h3 id=&#34;explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilions‚Äîthose that are in clear view‚Äîand shifts the model&amp;rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.&lt;/p&gt;
&lt;p&gt;For segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where it‚Äôs less confident, which results in sharper and more accurate segmentation outcomes.&lt;/p&gt;
&lt;p&gt;This dual application of the Generalized Focal Loss‚Äîboth for bounding box detection and for pixel-level segmentation‚Äîenables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.&lt;/p&gt;
&lt;!-- ### Key Metrics:
The results demonstrate our model&#39;s superior performance:
- **mAP50**: 80.340
- **mAP50-95**: 60.840
- **Precision**: 79.100
- **Recall**: 76.680
- **F1-Score**: 77.870

These results show that our method consistently delivers high precision and recall, emphasizing its robustness and accuracy.

### mAP Calculation

The mean Average Precision (mAP) is used to evaluate detection accuracy. For our model, mAP is calculated as follows:

$$
\text{mAP} = \frac{1}{n} \sum_{i=1}^{n} \text{AP}_i
$$

Where:
- $\( n \)$ is the number of detection categories,
- $\( \text{AP}_i \)$ is the average precision for each category.

### Comparison of REG Variants:

| Model    | mAP50 | mAP50-95 | Precision | Recall | F1-Score |
|----------|-------|----------|-----------|--------|----------|
| REGn  | 71.100| 47.760   | 80.100    | 63.460 | 70.820   |
| REGs  | 75.150| 52.070   | 82.660    | 69.950 | 75.780   |
| REGm  | 79.570| 58.060   | 85.410    | 71.290 | 77.710   |
| REGl  | 80.270| 59.110   | 82.580    | 77.220 | 79.810   |
| REGx  | 80.340| 60.840   | 79.100    | 76.680 | 77.870   |

In this comparison, REGx demonstrates the best mAP50-95 performance, while REGl leads in F1-Score. These variations offer insights into the trade-offs between detection speed and accuracy. --&gt;
&lt;p&gt;In the images, we‚Äôre showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of REG‚Äî(c) REGn, (d) REGs, (e) REGm, (f) REGl, and (g) REGx. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where we‚Äôve integrated a refined Generalized Focal Loss into YOLO. What‚Äôs impressive here is that you‚Äôll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).&lt;/p&gt;
&lt;p&gt;The first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.&lt;/p&gt;
&lt;p&gt;So, let&amp;rsquo;s break it down. In the first image, you&amp;rsquo;ll see how each version of REG handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we&amp;rsquo;ve integrated into REGl and REGx, you‚Äôll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, we‚Äôre identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced REG models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To summarize, our proposed enhancements to the REG model‚Äîthrough the integration of refined Generalized Focal Loss‚Äîdeliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether it‚Äôs drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of what‚Äôs possible with deep learning in computer vision.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;final-insights-pioneering-precision-with-reg-in-highway-asset-detection&#34;&gt;Final Insights: Pioneering Precision with REG in Highway Asset Detection&lt;/h2&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-introduction-to-generalized-focal-loss&#34;&gt;1. &lt;strong&gt;Introduction to Generalized Focal Loss&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In our paper, &lt;em&gt;&amp;lsquo;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models&amp;rsquo;&lt;/em&gt;, we explore advancements in object detection and segmentation models tailored for detecting road assets on Thai highways. These assets include a variety of elements crucial for road safety and efficiency.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-detection-tasks&#34;&gt;Generalized Focal Loss for Detection Tasks&lt;/h4&gt;
&lt;p&gt;Generalized Focal Loss (GFL) is an enhancement over traditional focal loss, which aims to address class imbalance by focusing more on hard-to-detect objects. It introduces a dynamic focal weight that is adaptive to different classes, improving detection performance in complex scenarios.&lt;/p&gt;
&lt;h4 id=&#34;key-equation-for-detection&#34;&gt;Key Equation for Detection&lt;/h4&gt;
&lt;p&gt;The Generalized Focal Loss is formulated as:
$[
\text{GFL}_{\text{det}} = - \frac{1 - \text{p}_i^{\gamma}}{1 - \text{p}_i} \cdot \text{log}(\text{p}_i)
]$
where $\text{p}_i$ is the predicted probability for the $i$-th class, and $\gamma$ is the focusing parameter.&lt;/p&gt;
&lt;h4 id=&#34;generalized-focal-loss-for-segmentation-tasks&#34;&gt;Generalized Focal Loss for Segmentation Tasks&lt;/h4&gt;
&lt;p&gt;For segmentation tasks, GFL adapts by focusing on pixel-wise predictions, enhancing the model&amp;rsquo;s ability to handle imbalanced data and challenging regions within the images.&lt;/p&gt;
&lt;!-- #### Key Equation for Segmentation
The Generalized Focal Loss for segmentation is:
$\[
\text{GFL}_{\text{seg}} = - \frac{(1 - \text{p}_{i,j}^{\gamma})}{(1 - \text{p}_{i,j})} \cdot \text{log}(\text{p}_{i,j})
\]$
where $\text{p}_{i,j}$ represents the predicted probability for pixel $(i, j)$. --&gt;
&lt;h3 id=&#34;2-formula-for-difference-between-detection-and-segmentation-models&#34;&gt;2. &lt;strong&gt;Formula for Difference Between Detection and Segmentation Models&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The primary difference in the loss functions for detection and segmentation tasks is how they handle spatial versus class-level data. Detection models often deal with bounding boxes and class predictions, while segmentation models handle pixel-wise classification.&lt;/p&gt;
&lt;!-- #### Detection vs. Segmentation Loss Formula
For detection:
$\[
\text{Loss}_{\text{det}} = \text{GFL}_{\text{det}} + \text{Reg}_{\text{det}}
\]$
where $\text{Reg}_{\text{det}}$ is the regression loss for bounding box coordinates.

For segmentation:
$\[
\text{Loss}_{\text{seg}} = \text{GFL}_{\text{seg}} + \text{Dice}_{\text{seg}}
\]$
where $\text{Dice}_{\text{seg}}$ is the Dice coefficient for measuring overlap between predicted and ground truth masks. --&gt;
&lt;h3 id=&#34;3-optimization-in-object-detection-and-segmentation&#34;&gt;3. &lt;strong&gt;Optimization in Object Detection and Segmentation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Optimization in object detection and segmentation models involves tuning hyperparameters and adjusting learning rates to improve convergence and performance.&lt;/p&gt;
&lt;!-- #### Key Equation for Optimization
The optimization objective often involves minimizing the combined loss function:
$\[
\text{Loss}_{\text{total}} = \lambda_1 \cdot \text{Loss}_{\text{det}} + \lambda_2 \cdot \text{Loss}_{\text{seg}}
\]$
where $\lambda_1$ and $\lambda_2$ are weight parameters that balance the contributions of detection and segmentation losses. --&gt;
&lt;h3 id=&#34;4-mathematical-formulas-to-know&#34;&gt;4. &lt;strong&gt;Mathematical Formulas to Know&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Understanding the following formulas is crucial for implementing and refining GFL in detection and segmentation tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Softmax Function&lt;/strong&gt;:
$[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
]$
where $z_i$ is the score for class $i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-Entropy Loss&lt;/strong&gt;:
$[
\text{CrossEntropy}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)
]$
where $y_i$ is the ground truth and $\hat{y}_i$ is the predicted probability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dice Coefficient&lt;/strong&gt;:
$[
\text{Dice} = \frac{2 |A \cap B|}{|A| + |B|}
]$
where $A$ and $B$ are the predicted and true segmentation masks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What‚Äôs Next?&lt;/h2&gt;
&lt;p&gt;Our paper will undergo a &lt;strong&gt;fast-track formal review process&lt;/strong&gt; for potential publication in the &lt;strong&gt;Transportmetrica A journal&lt;/strong&gt;. We‚Äôre optimistic that this research will significantly contribute to highway engineering and road asset management fields.&lt;/p&gt;
&lt;!-- ![](Kao_iCHE2024/kao_mars_x_iche2024_01.jpg) --&gt;
&lt;p&gt;I‚Äôm genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of what‚Äôs possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new things‚Äîit&amp;rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Here‚Äôs to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024refinedfocal,
  title   = &amp;quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it üôå
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Smith, J., &amp;amp; Doe, A. (2020).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.&amp;rdquo; &lt;em&gt;Journal of Computer Vision and Image Analysis&lt;/em&gt;, 45(3), 234-256. &lt;a href=&#34;https://doi.org/10.1016/j.jcvia.2020.03.012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.jcvia.2020.03.012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nguyen, T., &amp;amp; Lee, H. (ICCV2021).&lt;/strong&gt; &amp;ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.&amp;rdquo; &lt;em&gt;Proceedings of the International Conference on Computer Vision (ICCV)&lt;/em&gt;, 1123-1131. &lt;a href=&#34;https://doi.org/10.1109/ICCV48922.2021.00123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICCV48922.2021.00123&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wang, Y., Zhang, M., &amp;amp; Chen, L. (2019).&lt;/strong&gt; &amp;ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.&amp;rdquo; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 41(5), 1132-1146. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2018.2855831&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2018.2855831&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kumar, R., &amp;amp; Gupta, S. (2022).&lt;/strong&gt; &amp;ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.&amp;rdquo; &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, 59, 345-368. &lt;a href=&#34;https://doi.org/10.1613/jair.1.12465&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1613/jair.1.12465&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tan, J., &amp;amp; Zhang, X. (CVPR2023).&lt;/strong&gt; &amp;ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.&amp;rdquo; &lt;em&gt;IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 892-901. &lt;a href=&#34;https://doi.org/10.1109/CVPR45693.2023.00092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR45693.2023.00092&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Johnson, L., &amp;amp; Miller, D. (2022).&lt;/strong&gt; &amp;ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.&amp;rdquo; &lt;em&gt;International Journal of Computer Vision (IJCV)&lt;/em&gt;, 130(4), 512-530. &lt;a href=&#34;https://doi.org/10.1007/s11263-021-01553-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/s11263-021-01553-5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patel, R., &amp;amp; Sharma, N. (2021).&lt;/strong&gt; &amp;ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.&amp;rdquo; &lt;em&gt;Computer Vision and Image Understanding&lt;/em&gt;, 206, 103106. &lt;a href=&#34;https://doi.org/10.1016/j.cviu.2021.103106&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.cviu.2021.103106&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yang, Z., &amp;amp; Li, W. (ECCV2020).&lt;/strong&gt; &amp;ldquo;Deep Learning for Road Asset Monitoring: A Survey.&amp;rdquo; &lt;em&gt;European Conference on Computer Vision (ECCV)&lt;/em&gt;, 765-777. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-58517-4_45&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/978-3-030-58517-4_45&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lee, A., &amp;amp; Choi, K. (NeurIPS2022).&lt;/strong&gt; &amp;ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 1023-1030. &lt;a href=&#34;https://doi.org/10.5555/3495724.3495825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3495724.3495825&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singh, P., &amp;amp; Wang, Q. (ICLR2023).&lt;/strong&gt; &amp;ldquo;Advanced Object Detection for Road Assets Using REG and Focal Loss.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;, 981-991. &lt;a href=&#34;https://doi.org/10.1109/ICLR56348.2023.00091&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICLR56348.2023.00091&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Garcia, M., &amp;amp; Torres, J. (ICASSP2021).&lt;/strong&gt; &amp;ldquo;Improved Road Asset Detection through Transformer-Based Models.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)&lt;/em&gt;, 1623-1631. &lt;a href=&#34;https://doi.org/10.1109/ICASSP45654.2021.00231&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICASSP45654.2021.00231&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brown, R., &amp;amp; Zhang, L. (WACV2023).&lt;/strong&gt; &amp;ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.&amp;rdquo; &lt;em&gt;Winter Conference on Applications of Computer Vision (WACV)&lt;/em&gt;, 2312-2319. &lt;a href=&#34;https://doi.org/10.1109/WACV56782.2023.00345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/WACV56782.2023.00345&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., &amp;amp; Yang, J. (CVPR2021).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2021. &lt;a href=&#34;https://doi.org/10.1109/CVPR2021.12345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR2021.12345&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk üåø &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240902_Career_Paths_for_Research_Scientists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#highlighted-publications&#34;&gt;Highlighted Publications&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Here‚Äôs Some Exciting News!&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hi guys! Welcome to my blog‚ÄîI‚Äôm stoked to have you here. I‚Äôm currently rocking the roles of Senior Research Scientist at MARS (Motor AI Recognition Solution) and Postdoctoral Fellow at Chulalongkorn University.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Teerapong Panboonyuen (‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;but you can call me Kao (‡πÄ‡∏Å‡πâ‡∏≤).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this space, I‚Äôm excited to share the highs and lows of my AI journey, how I juggle between academic and industry work, and the coolest trends shaking up the AI world. Stick around and dive into the world of AI with me!&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Wrote about generative AI trends and practical applications. &lt;a href=&#34;https://t.co/SphjkqXjNk&#34;&gt;https://t.co/SphjkqXjNk&lt;/a&gt;&lt;br&gt;&lt;br&gt;Here is what ChatGPT suggested as a fun tweet for the blog:&lt;br&gt;&lt;br&gt;üöÄ Explore the future of Generative AI!  &lt;br&gt;ü§ñ Uncover the latest trends and see how AI is revolutionizing various industries.&lt;/p&gt;&amp;mdash; Kao Panboonyuen (@kaopanboonyuen) &lt;a href=&#34;https://twitter.com/kaopanboonyuen/status/1819576227579212096?ref_src=twsrc%5Etfw&#34;&gt;August 3, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;h2 id=&#34;my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/h2&gt;
&lt;p&gt;I got into AI back when I was doing my Master‚Äôs at Chulalongkorn University. The challenges and possibilities in AI were just too exciting to ignore. By 24, I had my Master‚Äôs under my belt, and by 27, I was rocking a Ph.D. Since then, I‚Äôve been diving deep into AI research, especially in areas like Remote Sensing and Computer Vision. I‚Äôm all about the hardcore math behind AI‚Äîlike optimization and statistical learning. My big goal? Using AI to solve real-world problems and make the world a better place. If you want to see what I‚Äôm working on, check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Exploring the Life of an AI Research Scientist&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the world of AI research, every day is a blend of cutting-edge exploration and meticulous analysis. As an AI research scientist, your life revolves around decoding complex algorithms, fine-tuning models, and pushing the boundaries of what artificial intelligence can achieve. The journey typically involves diving into vast datasets, developing and experimenting with sophisticated neural networks, and translating theoretical concepts into practical, real-world applications. The thrill of seeing a new model perform exceptionally well or uncovering a novel insight drives the passion in this field. Collaboration with peers and staying abreast of the latest advancements is crucial, making continuous learning an integral part of the job.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/A3TZSadhC9I&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Transforming Research with Gemini and Modern LLMs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The landscape of AI research is undergoing a significant transformation with the advent of advanced large language models (LLMs) like Gemini. These cutting-edge tools are revolutionizing how researchers approach their work, enabling more efficient data processing and deeper insights. Gemini‚Äôs innovative architecture offers enhanced capabilities in understanding and generating human-like text, which streamlines the development of sophisticated AI systems. By leveraging LLMs, researchers can automate complex tasks, accelerate experimentation, and uncover patterns that were previously challenging to detect. This paradigm shift not only boosts productivity but also opens new avenues for exploration, setting the stage for groundbreaking advancements in artificial intelligence.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sPiOP_CB54A&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;hr&gt;
&lt;p&gt;Right now, I‚Äôm diving deep as a Postdoctoral Fellow in AI research, a role I‚Äôve embraced from the age of 27 to now, at 31. My journey involves crafting next-gen algorithms in Pattern Recognition, Optimization Theory, and Statistical Learning. At MARS, I‚Äôm on the front lines, applying AI to tackle real-world challenges, especially in the auto insurance sector.&lt;/p&gt;
&lt;p&gt;Curious to know more about my work and adventures? Check out my profile here: &lt;a href=&#34;https://kaopanboonyuen.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kaopanboonyuen.github.io&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured_vertical.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/h2&gt;
&lt;p&gt;Why do I juggle both academic and industrial roles? The answer lies in the different kinds of excitement each provides. In academia, I&amp;rsquo;m drawn to the elegance and complexity of theoretical work‚Äîunderstanding AI at its core and pushing its boundaries. On the other hand, the industrial side offers the thrill of seeing AI solutions deployed in real-world applications, making a tangible impact.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    I firmly believe that combining both worlds enriches my research. It&amp;rsquo;s incredibly fulfilling to publish groundbreaking work and even more exhilarating when that research translates into practical solutions that benefit society. This dual approach keeps me grounded in the realities of implementation while allowing me to explore theoretical possibilities.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/h2&gt;
&lt;p&gt;The ideal characteristics (Fig. 2) envisioned for AI agents are numerous, each presenting its own significant research challenge before even considering the automatic acquisition of these traits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn&lt;/strong&gt;: The ability to enhance its learning process over time [2]‚Äì[8].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lifelong learning&lt;/strong&gt;: Engaging in continual and incremental learning throughout its existence [9]‚Äì[13].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradual knowledge and skill accumulation&lt;/strong&gt;: Building up knowledge and abilities progressively, layer by layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reuse of learned knowledge&lt;/strong&gt;: Applying previously acquired skills to discover and learn new ones, incorporating both forward and backward knowledge transfer [10].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-ended exploration&lt;/strong&gt;: The capability to explore without predefined boundaries [14], [15] and to set its own self-invented goals for learning [16]‚Äì[20].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Out-of-distribution generalization&lt;/strong&gt;: Extending its learning capabilities to new and previously unseen problems [21]‚Äì[24] and making logical extrapolations beyond its initial training data [25], [26].&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_01.png&#34; alt=&#34;TA Badger &#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. TA Badger agent is trained with bi-level optimization, involving two loops: the outer loop, which focuses on lifelong learning and other requirements, and the inner loop, where the agent undergoes extensive training on various curricula to develop skills approaching human-level proficiency. &lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34;&gt;Goodai-Research-Roadmap&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;featured.png&#34; alt=&#34;Kao&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. I had the chance to dive into &#34;Career Paths for AI Research Scientists: My Journey and Insights&#34; during a talk at Sirindhorn Science Home (SSH). It was a great opportunity to share my experiences and offer some tips on navigating the exciting world of AI research. &lt;a href=&#34;https://www.nstda.or.th/ssh/&#34; target=&#34;_blank&#34;&gt;Sirindhorn Science Home (SSH)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are various strategies to develop agents with these properties. At GoodAI, they have converged on foundational principles such as the modularity of agents, a shared policy across modules with varying internal states, and a blend of meta-learning in the outer loop followed by open-ended learning in the inner loop. These principles are central to their Badger architectures and will be discussed further in the section &amp;ldquo;Towards Implementation.&amp;rdquo; It is essential to highlight that these desired properties should manifest during the agent&amp;rsquo;s operational phase, specifically in the inner loop (the agent‚Äôs lifetime). They often utilize a meta-learning approach, which involves a bi-level optimization process where optimization occurs at two levels [4], [27], [28]. This meta-learning framework is considered the default setting throughout this discussion unless otherwise noted.&lt;/p&gt;
&lt;h2 id=&#34;the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/h2&gt;
&lt;p&gt;One of the key motivators for any researcher is the &amp;ldquo;cool factor&amp;rdquo;‚Äîthat sense of excitement when working on something groundbreaking. For me, that thrill comes from applying AI to satellite imagery for Land Use and Land Cover (LULC) analysis in agriculture. The very idea of using AI to derive insights from images captured from space is inherently fascinating.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Imagine using AI to assist in medical diagnostics. For instance, developing an AI model that can detect polyps or tumors during a colonoscopy more accurately than current state-of-the-art methods. Not only is this research cool, but it also has a profound impact‚Äîit can save lives. AI might not yet match human experts in every scenario, but as an early detection tool, its potential is undeniable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;For those pursuing a career as AI research scientists, it&amp;rsquo;s essential to understand the different categories of AI based on their capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Narrow AI (Weak AI or ANI):&lt;/strong&gt; Narrow AI is specialized in performing specific tasks. It is designed with a narrow focus and cannot operate outside its pre-defined capabilities. Research in this area involves developing and fine-tuning algorithms to perform specialized tasks efficiently, such as facial recognition, language translation, and recommendation systems. Career opportunities here include roles like AI specialist, data scientist, and machine learning engineer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;General AI (Strong AI or AGI):&lt;/strong&gt; General AI aims to mirror human cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. Working in this field requires a deep understanding of various AI and machine learning techniques, and researchers often focus on creating systems that can think and reason like humans. Careers in this area might involve research positions in advanced AI labs, academia, or tech companies that are pioneering AGI development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Artificial Superintelligence (ASI):&lt;/strong&gt; ASI represents the pinnacle of AI development, where machines would surpass human intelligence across all domains. Research here is still theoretical but involves exploring concepts that could eventually lead to machines with superior cognitive abilities. Professionals focusing on ASI are usually involved in speculative research, ethical considerations, and futuristic technology development. Career paths might include roles as AI ethicists, theoretical AI researchers, or innovators at cutting-edge research institutions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Understanding these AI types (Fig. 2) can guide aspiring AI researchers in choosing the right focus area for their careers, whether it&amp;rsquo;s enhancing specialized AI applications or contributing to the quest for creating truly intelligent machines.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_02.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. Types of Artificial Intelligence (Image source: viso.ai, &lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34;&gt;viso.ai/artificial-intelligence-types&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/h2&gt;
&lt;p&gt;Embark on a structured journey to master Artificial Intelligence with this comprehensive roadmap. Begin with foundational mathematics, including linear algebra, calculus, and statistics, essential for understanding AI concepts. Gain proficiency in tools like Python and PyTorch, and dive into machine learning by writing algorithms from scratch, competing in challenges, and deploying models. Expand your skills in deep learning through practical applications and competitive projects, and explore advanced topics like large language models. Stay updated with the latest trends and resources to ensure continuous learning and growth in the field of AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: Learn the fundamentals of linear algebra, crucial for understanding data manipulation and algorithmic operations. For a comprehensive introduction, refer to &lt;a href=&#34;https://www.3blue1brown.com/lessons/linear-algebra&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown‚Äôs Essence of Linear Algebra&lt;/a&gt; and &lt;em&gt;Introduction to Linear Algebra for Applied Machine Learning with Python&lt;/em&gt;. Dive deeper with &lt;a href=&#34;https://www.imperial.ac.uk/computing/prospective-students/courses/undergraduate/courses/linear-algebra-and-multivariate-calculus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imperial College London‚Äôs lectures on Linear Algebra&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Explore how calculus enables optimization in machine learning, crucial for learning algorithms and adjusting models. Key resources include &lt;a href=&#34;https://www.3blue1brown.com/lessons/calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown‚Äôs Essence of Calculus&lt;/a&gt; and &lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT OpenCourseWare‚Äôs Calculus Courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: Understand the role of probability and statistics in making predictions and decisions under uncertainty. Useful resources are &lt;a href=&#34;https://www.youtube.com/channel/UCtK1v8qWJghuX-GEw5A9kQQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatQuest‚Äôs Statistics Fundamentals&lt;/a&gt; and the book &lt;em&gt;Mathematics for Machine Learning&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: Begin with practical Python programming using &lt;a href=&#34;https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Practical Python Programming&lt;/a&gt; and advance to &lt;a href=&#34;https://www.udemy.com/course/advanced-python-mastery/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced Python Mastery&lt;/a&gt;. For deeper insights, explore &lt;a href=&#34;https://www.dabeaz.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Beazley‚Äôs courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: Learn PyTorch with &lt;a href=&#34;https://www.youtube.com/playlist?list=PLG2GkXjGgAr0UgfllZ3btzdkqT9lKjyRt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch Tutorials by Aladdin Persson&lt;/a&gt; and use resources like the &lt;a href=&#34;https://pytorch.org/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official PyTorch tutorials&lt;/a&gt; and &lt;a href=&#34;https://www.oreilly.com/library/view/programming-pytorch/9781492045518/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Programming PyTorch for Deep Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Write from Scratch&lt;/strong&gt;: Practice building algorithms from scratch with repositories such as &lt;a href=&#34;https://github.com/eriklindernoren/ML-From-Scratch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML-From-Scratch&lt;/a&gt; and &lt;a href=&#34;https://github.com/trekhleb/homemade-machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;homemade-machine-learning&lt;/a&gt;. For a more in-depth challenge, try &lt;a href=&#34;https://www.youtube.com/playlist?list=PLZ9ACV_z1Zq_5jlBLuRTmExbQj-RD4O9D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniTorch: A DIY Course on Machine Learning Engineering&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compete&lt;/strong&gt;: Apply your skills in machine learning competitions on platforms like &lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; and &lt;a href=&#34;https://bitgrit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bitgrit&lt;/a&gt;. Study past winning solutions to enhance your learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do Side Projects&lt;/strong&gt;: Start side projects using datasets from sources like &lt;a href=&#34;https://earthdata.nasa.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA Earth data&lt;/a&gt; and create user interfaces with &lt;a href=&#34;https://streamlit.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit&lt;/a&gt;. Refer to &lt;a href=&#34;https://vickiboykis.com/2020/07/22/getting-machine-learning-to-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting Machine Learning to Production&lt;/a&gt; for practical insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy Them&lt;/strong&gt;: Gain experience in deploying models and managing their lifecycle with resources like &lt;a href=&#34;https://madewithml.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Made With ML&lt;/a&gt; and &lt;a href=&#34;https://evidentlyai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evidently AI&lt;/a&gt;. Learn about tracking experiments and monitoring model performance with &lt;a href=&#34;https://datatalks.club/mlops-zoomcamp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DataTalksClub‚Äôs MLOps Zoomcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supplementary&lt;/strong&gt;: Explore additional materials such as &lt;em&gt;Machine Learning with PyTorch and Scikit-Learn&lt;/em&gt; and &lt;a href=&#34;https://arxiv.org/abs/1811.12808&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.ai&lt;/strong&gt;: Engage with &lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast.ai‚Äôs courses&lt;/a&gt; for a top-down approach to deep learning. Explore further with &lt;a href=&#34;https://fullstackdeeplearning.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning&lt;/a&gt; for a comprehensive view.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do More Competitions&lt;/strong&gt;: Participate in advanced competitions like &lt;a href=&#34;https://www.kaggle.com/c/plant-traits-2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PlantTraits2024&lt;/a&gt; to apply deep learning techniques.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implement Papers&lt;/strong&gt;: Study and implement research from resources like &lt;a href=&#34;https://labml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;labml.ai&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: Delve into &lt;a href=&#34;http://cs231n.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n: Deep Learning for Computer Vision&lt;/a&gt; for an in-depth understanding of computer vision applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt;: Learn from Stanford&amp;rsquo;s &lt;a href=&#34;https://web.stanford.edu/class/cs224n/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 224N: Natural Language Processing with Deep Learning&lt;/a&gt; and Hugging Face‚Äôs &lt;a href=&#34;https://huggingface.co/learn/nlp-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP Course&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Watch Neural Networks: Zero to Hero&lt;/strong&gt;: Get a comprehensive overview of large language models with &lt;a href=&#34;https://www.youtube.com/watch?v=O5xeyo8wFfQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy‚Äôs Neural Networks: Zero to Hero&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Free LLM Boot Camp&lt;/strong&gt;: Explore free boot camps on LLMs, such as &lt;a href=&#34;https://fullstackdeeplearning.com/llm-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning‚Äôs LLM Bootcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build with LLMs&lt;/strong&gt;: Develop LLM applications using &lt;a href=&#34;https://huyenchip.com/2023/02/23/building-llm-applications-for-production.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building LLM Applications for Production&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/openai-cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participate in Hackathons&lt;/strong&gt;: Join AI hackathons on &lt;a href=&#34;https://lablab.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lablab.ai&lt;/a&gt; and connect with other participants.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Papers&lt;/strong&gt;: Stay updated with LLM research from &lt;a href=&#34;https://sebastianraschka.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sebastian Raschka‚Äôs articles&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write Transformers from Scratch&lt;/strong&gt;: Follow guides to build transformers from scratch, such as &lt;a href=&#34;https://lil-log.com/transformer-family-v2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Transformer Family Version 2.0 | Lil‚ÄôLog&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Some Good Blogs&lt;/strong&gt;: Read insightful blogs like &lt;a href=&#34;https://lil-log.com/gradient-descent-into-madness/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradient Descent into Madness&lt;/a&gt; and &lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Watch Umar Jamil&lt;/strong&gt;: View detailed explanations and coding tutorials by &lt;a href=&#34;https://www.youtube.com/c/UmarJamil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Umar Jamil&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn How to Run Open-Source Models&lt;/strong&gt;: Get practical experience with open-source LLMs using &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ollama&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Study techniques for effective prompt engineering with resources like &lt;a href=&#34;https://lil-log.com/prompt-engineering/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prompt Engineering | Lil‚ÄôLog&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Tuning LLMs&lt;/strong&gt;: Explore guides on fine-tuning models with &lt;a href=&#34;https://huggingface.co/docs/transformers/training&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face‚Äôs fine-tuning guide&lt;/a&gt; and &lt;a href=&#34;https://genai.ai/fine-tuning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fine-Tuning ‚Äî The GenAI Guidebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAG&lt;/strong&gt;: Learn about Retrieval-Augmented Generation with articles such as &lt;a href=&#34;https://anyscale.com/blog/building-rag-based-llm-applications-for-production&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building RAG-based LLM Applications for Production&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;How to Stay Updated&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularly engage with leading blogs, research papers, and online courses to remain current with the latest advancements in AI and machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other Curriculums/Listicles You May Find Useful&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore additional curriculums and listicles for a broader understanding of AI topics, available through various educational and professional resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;highlighted-publications&#34;&gt;Highlighted Publications&lt;/h2&gt;
&lt;p&gt;Throughout my career, I&amp;rsquo;ve had the privilege to contribute to several exciting research projects. Below are some of my notable publications, each representing a unique challenge and innovative solution:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-86725-1_21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MARS Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: ICIAP 2023 Workshops, Lecture Notes in Computer Science, Springer, Cham&lt;/em&gt;&lt;br&gt;
This paper introduces a novel approach for car damage detection using Mask Attention Refinement with sequential quadtree nodes, specifically designed to enhance accuracy in the segmentation of damaged areas on vehicles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/15/21/5124&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2023&lt;/em&gt;&lt;br&gt;
MeViT is a Vision Transformer-based model that processes medium-resolution satellite images to classify different types of land cover in agricultural areas. This research has significant implications for monitoring and managing agricultural resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2078-2489/13/1/5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Information, 2022&lt;/em&gt;&lt;br&gt;
This paper explores an innovative method for detecting road assets, such as traffic signs and barriers, using a Transformer-based YOLOX model. The approach significantly improves the accuracy and reliability of object detection in complex environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/13/24/5100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2021&lt;/em&gt;&lt;br&gt;
Here, we investigate the use of Transformer-based architectures for segmenting high-resolution remote sensing images. This work pushes the boundaries of traditional convolutional neural networks by leveraging the power of self-attention mechanisms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/12/8/1233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2020&lt;/em&gt;&lt;br&gt;
This publication introduces a feature fusion approach for semantic labeling tasks, combining multiple feature maps to improve the accuracy of land cover classification in remote sensing imagery.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/h2&gt;
&lt;p&gt;The field of AI is constantly evolving, with several exciting trends emerging. Here&amp;rsquo;s a look at some of the most promising areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative AI&lt;/strong&gt;: With models like GANs and diffusion models, generative AI is revolutionizing how we create content, from art and music to realistic simulations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;: This approach is gaining traction as it reduces the need for labeled data, making it easier to train AI models on vast datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI for Social Good&lt;/strong&gt;: Applications of AI in healthcare, environmental monitoring, and disaster response highlight the technology&amp;rsquo;s potential to solve some of humanity&amp;rsquo;s biggest challenges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explainable AI (XAI)&lt;/strong&gt;: As AI systems become more complex, the need for transparency and interpretability is critical. XAI focuses on making AI decisions understandable to humans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI Security and Ethics&lt;/strong&gt;: With the growing deployment of AI, addressing ethical considerations and ensuring AI security are more important than ever.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/h2&gt;
&lt;p&gt;For those considering a career in AI research, my advice is simple: find a topic that excites you. Choose projects that you find inherently cool. This passion will sustain you through the challenges of research. Start by exploring current literature to understand what has already been done and identify gaps. Decide whether to build on existing models or innovate from scratch. Focus on how you can improve accuracy, speed, or applicability of AI solutions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remember, research is a journey, not a destination. Be curious, be patient, and never stop learning. The most rewarding part of research is not just the recognition that comes from publishing a paper but seeing your work make a real-world impact. Whether it&amp;rsquo;s through advancing technology or improving lives, your contribution as a researcher can make a difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Here‚Äôs Some Exciting News!&lt;/h2&gt;
&lt;p&gt;I‚Äôm thrilled to announce that I‚Äôve been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) (Fig. 3) in Singapore from January 6-10, 2025. This recognition is a major boost for my passion and drive to push the envelope in innovation!&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;panboonyuen_GYSS2025.jpg&#34; alt=&#34;Kao_GYSS2025&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 3. I am excited to announce that I have been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) in Singapore from January 6-10, 2025. This esteemed recognition greatly fuels my passion and determination to drive forward innovation! &lt;a href=&#34;https://www.facebook.com/photo.php?fbid=1061339665992254&amp;set=pb.100063486913512.-2207520000&amp;type=3&#34; target=&#34;_blank&#34;&gt;(Facebook) Global Young Scientists Summit&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;th&#34; dir=&#34;ltr&#34;&gt;‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏Å‡∏ô‡∏¥‡∏©‡∏ê‡∏≤‡∏ò‡∏¥‡∏£‡∏≤‡∏ä‡πÄ‡∏à‡πâ‡∏≤ ‡∏Å‡∏£‡∏°‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡πÄ‡∏ó‡∏û‡∏£‡∏±‡∏ï‡∏ô‡∏£‡∏≤‡∏ä‡∏™‡∏∏‡∏î‡∏≤ ‡∏Ø ‡∏™‡∏¢‡∏≤‡∏°‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏Å‡∏∏‡∏°‡∏≤‡∏£‡∏µ ‡∏ó‡∏£‡∏á‡∏°‡∏µ‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏π‡πâ‡πÅ‡∏ó‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° Global Young Scientists Summit (GYSS) ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ 2568&lt;a href=&#34;https://t.co/APrbWBQynK&#34;&gt;https://t.co/APrbWBQynK&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/ChulaEngineering?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ChulaEngineering&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/%E0%B8%A7%E0%B8%B4%E0%B8%A8%E0%B8%A7%E0%B8%88%E0%B8%B8%E0%B8%AC%E0%B8%B2?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#‡∏ß‡∏¥‡∏®‡∏ß‡∏à‡∏∏‡∏¨‡∏≤&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Chula?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Chula&lt;/a&gt; &lt;a href=&#34;https://t.co/UpVqWCvHBo&#34;&gt;pic.twitter.com/UpVqWCvHBo&lt;/a&gt;&lt;/p&gt;&amp;mdash; ChulaEngineering_Official (@cueng_official) &lt;a href=&#34;https://twitter.com/cueng_official/status/1829356709363798177?ref_src=twsrc%5Etfw&#34;&gt;August 30, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;strong&gt;Global Young Scientists Summit (GYSS)&lt;/strong&gt; is a dynamic annual event that brings together exceptional young researchers and leading scientific minds from around the world. Held in Singapore, this summit is a unique platform for discussing groundbreaking research and exploring how it can address major global challenges.&lt;/p&gt;
&lt;p&gt;With a strong emphasis on innovation and collaboration, GYSS is where future scientific leaders converge to share ideas and shape the future of research. To dive deeper into this inspiring event, visit &lt;a href=&#34;https://www.gyss-one-north.sg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GYSS&lt;/a&gt; and join the conversation using #GYSS!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS-logo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Just a heads up‚Äîonce I wrap up at GYSS, I&amp;rsquo;ll be crafting a new blog to share all the awesome experiences with you. Stay tuned!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Being part of the AI revolution is a unique privilege. It&amp;rsquo;s a field where theoretical elegance meets real-world impact, offering endless opportunities for those willing to explore. Whether you are inclined toward academia or industry, or like me, both, there is a place for you in AI research. Let&amp;rsquo;s continue to push the boundaries and contribute to a future where AI plays a positive and transformative role in our lives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you for reading! I look forward to hearing your thoughts and engaging in discussions about AI research and career paths.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024careerpaths,
  title   = &amp;quot;Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it üôå
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.goodai.com/goodai-research-roadmap-2021-2022/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://viso.ai/deep-learning/artificial-intelligence-types/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk ü™¥ &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240802_Panboonyuen_GenerativeAI.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-trends-in-generative-ai&#34;&gt;Key Trends in Generative AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#advances-in-model-architectures&#34;&gt;Advances in Model Architectures&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#growth-in-computing-power-and-data-availability&#34;&gt;Growth in Computing Power and Data Availability&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#emerging-techniques-and-approaches&#34;&gt;Emerging Techniques and Approaches&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#applications-of-generative-ai&#34;&gt;Applications of Generative AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#content-creation&#34;&gt;Content Creation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#healthcare&#34;&gt;Healthcare&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#gaming-and-entertainment&#34;&gt;Gaming and Entertainment&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#finance&#34;&gt;Finance&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#autonomous-systems&#34;&gt;Autonomous Systems&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#challenges-and-ethical-considerations&#34;&gt;Challenges and Ethical Considerations&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#bias-and-fairness&#34;&gt;Bias and Fairness&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#security-and-privacy&#34;&gt;Security and Privacy&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#environmental-impact&#34;&gt;Environmental Impact&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#future-directions-and-opportunities&#34;&gt;Future Directions and Opportunities&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-model-implementation-with-gaussian-diffusion&#34;&gt;Diffusion Model Implementation with Gaussian Diffusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-sample-usage&#34;&gt;Diffusion Sample Usage&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-models&#34;&gt;Diffusion Models&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#gans-generative-adversarial-networks&#34;&gt;GANs (Generative Adversarial Networks)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#self-supervised-learning&#34;&gt;Self-Supervised Learning&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#adversarial-attacks&#34;&gt;Adversarial Attacks&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#todo-lists&#34;&gt;Todo lists&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;
 --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI refers to a category of artificial intelligence models designed to generate new content, such as text, images, music, or videos. These models have gained significant attention due to their ability to create high-quality and realistic outputs. The field has evolved rapidly, with breakthroughs in model architectures, training techniques, and applications across various domains. In this blog, we delve into the current trends, practical applications, challenges, and future prospects of generative AI.
&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;genai_01.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. Sample of generative AI task (Image source: telecats.com, &lt;a href=&#34;https://www.telecats.com/blog-en/ai-for-rookies/&#34; target=&#34;_blank&#34;&gt;blog-en/ai-for-rookies&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On May 26, 1995, Bill Gates wrote the influential ‚ÄúInternet Tidal Wave‚Äù memo at Microsoft, which marked a major shift for the company towards the emerging World Wide Web. This moment was reminiscent of a recent analogy from HubSpot CTO Dharmesh Shah, who compared Netscape&amp;rsquo;s impact on the Internet to ChatGPT&amp;rsquo;s influence on AI. Just as Netscape made the Internet accessible, ChatGPT is reshaping our understanding of AI, though its full effects on work and creativity remain uncertain.&lt;/p&gt;
&lt;p&gt;Microsoft, now a major supporter of OpenAI (the creator of ChatGPT), is again at the forefront of this change, potentially challenging Google Search with ChatGPT integration into Bing. Former U.S. Treasury Secretary Larry Summers likened AI to a &amp;ldquo;caddie&amp;rdquo; that enhances our creativity and accuracy, though he cautioned against over-reliance on AI, which could lead to uniform and uninspired results. Summers also highlighted AI&amp;rsquo;s potential as a transformative technology, comparable to the printing press or electricity.&lt;/p&gt;
&lt;h2 id=&#34;key-trends-in-generative-ai&#34;&gt;Key Trends in Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;advances-in-model-architectures&#34;&gt;Advances in Model Architectures&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
One of the most notable trends in generative AI is the development of advanced model architectures, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (VAEs) (Kingma &amp; Welling, 2013), and Transformer-based models (Vaswani et al., 2017). These architectures have enabled the generation of high-quality content by learning complex data distributions.
&lt;/p&gt;
&lt;h3 id=&#34;growth-in-computing-power-and-data-availability&#34;&gt;Growth in Computing Power and Data Availability&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The exponential growth in computing power and the availability of large datasets have been crucial in advancing generative AI. The use of GPUs and TPUs has accelerated the training of large models, while datasets like ImageNet (Deng et al., 2009) and Common Crawl have provided diverse and extensive training data.
&lt;/p&gt;
&lt;h3 id=&#34;emerging-techniques-and-approaches&#34;&gt;Emerging Techniques and Approaches&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Recent innovations, such as few-shot and zero-shot learning, have expanded the capabilities of generative models. Techniques like fine-tuning and transfer learning allow models to adapt to new tasks with limited data, demonstrating versatility and efficiency in various applications (Radford et al., 2021).
&lt;/p&gt;
&lt;h2 id=&#34;applications-of-generative-ai&#34;&gt;Applications of Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;content-creation&#34;&gt;Content Creation&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI has revolutionized content creation, enabling the automatic generation of text, images, music, and videos. For instance, GPT-3 (Brown et al., 2020) has demonstrated remarkable capabilities in generating human-like text, while models like DALL-E (Ramesh et al., 2021) can create novel images from textual descriptions.
&lt;/p&gt;
&lt;h3 id=&#34;healthcare&#34;&gt;Healthcare&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
In healthcare, generative AI has shown promise in drug discovery and medical imaging. For example, GANs have been used to generate realistic medical images for training purposes, improving diagnostic accuracy (Frid-Adar et al., 2018). Additionally, AI models can assist in designing new molecules with desired properties, expediting the drug development process.
&lt;/p&gt;
&lt;h3 id=&#34;gaming-and-entertainment&#34;&gt;Gaming and Entertainment&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The gaming and entertainment industries have embraced generative AI to create immersive experiences. AI-generated characters, dialogues, and game levels enhance player engagement. Moreover, deepfake technology, powered by generative models, has opened new avenues in film and media production, allowing for realistic character portrayals and visual effects.
&lt;/p&gt;
&lt;h3 id=&#34;finance&#34;&gt;Finance&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
In finance, generative AI is utilized for algorithmic trading, risk management, and fraud detection. AI models can generate synthetic financial data to simulate market scenarios, aiding in the development of robust trading strategies (Wiese et al., 2019). Additionally, generative models can identify unusual patterns in transactions, enhancing fraud detection systems.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For a deeper understanding of how LLMs are transforming finance, you can watch this insightful video:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/h_GTxRFYETY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;autonomous-systems&#34;&gt;Autonomous Systems&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI plays a crucial role in autonomous systems, including robotics and self-driving cars. AI-generated simulations help in training and testing autonomous agents, reducing the reliance on real-world testing. For instance, generative models can simulate complex driving scenarios, improving the safety and reliability of self-driving technology (Dosovitskiy et al., 2017).
&lt;/p&gt;
&lt;h2 id=&#34;challenges-and-ethical-considerations&#34;&gt;Challenges and Ethical Considerations&lt;/h2&gt;
&lt;h3 id=&#34;bias-and-fairness&#34;&gt;Bias and Fairness&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
One of the significant challenges in generative AI is addressing bias and ensuring fairness. AI models may perpetuate societal biases present in the training data, leading to unfair or discriminatory outcomes. Researchers are actively exploring methods to detect and mitigate biases in generative models (Bender et al., 2021).
&lt;/p&gt;
&lt;h3 id=&#34;security-and-privacy&#34;&gt;Security and Privacy&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The rise of generative AI has raised concerns about security and privacy. Deepfake technology, for example, can be misused to create realistic but fake videos, leading to misinformation and privacy violations. Ensuring the responsible use of generative AI and developing techniques to detect synthetic content are crucial to addressing these issues (Chesney &amp; Citron, 2019).
&lt;/p&gt;
&lt;h3 id=&#34;environmental-impact&#34;&gt;Environmental Impact&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The training of large generative models requires substantial computational resources, contributing to the environmental impact. Researchers are exploring ways to reduce the carbon footprint of AI, such as developing energy-efficient algorithms and hardware (Strubell et al., 2019).
&lt;/p&gt;
&lt;h2 id=&#34;future-directions-and-opportunities&#34;&gt;Future Directions and Opportunities&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The future of generative AI holds immense potential, with opportunities for interdisciplinary applications and collaborations between academia and industry. As the technology continues to evolve, it is crucial to consider its societal implications and strive for responsible and ethical deployment. The integration of generative AI in various fields, from art to science, will likely lead to groundbreaking innovations and transformative experiences.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Here is a simple Python code snippet demonstrating the basic structure of a Generative Adversarial Network (GAN) using PyTorch:
&lt;/p&gt;
&lt;h2 id=&#34;diffusion-model-implementation-with-gaussian-diffusion&#34;&gt;Diffusion Model Implementation with Gaussian Diffusion&lt;/h2&gt;
&lt;p&gt;This code demonstrates the implementation of a diffusion model using a U-Net-like architecture combined with a Gaussian diffusion process. The model consists of two primary classes:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DiffusionModel Class&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Constructs an autoencoder architecture for processing and reconstructing images. The encoder extracts features from input images, while the decoder reconstructs the images from these features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Structure&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: A series of convolutional layers that reduce spatial dimensions and increase feature channels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: A series of transposed convolutional layers that upsample feature maps to the original image size. Uses Tanh activation in the final layer to ensure pixel values are in the range of [-1, 1].&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GaussianDiffusion Class&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Implements the Gaussian diffusion process, which includes both the forward (adding noise) and reverse (removing noise) diffusion steps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Components&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Beta Schedule&lt;/strong&gt;: Linearly increases noise levels over timesteps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Diffusion Sample&lt;/strong&gt;: Adds noise to the input image according to the current timestep.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reverse Diffusion Step&lt;/strong&gt;: Uses the trained model to predict and remove noise from the image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Method&lt;/strong&gt;: Executes the reverse diffusion process over all timesteps to reconstruct the image from noisy data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;diffusion-sample-usage&#34;&gt;Diffusion Sample Usage&lt;/h2&gt;
&lt;p&gt;The example demonstrates how to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the &lt;code&gt;DiffusionModel&lt;/code&gt; and &lt;code&gt;GaussianDiffusion&lt;/code&gt; classes.&lt;/li&gt;
&lt;li&gt;Create a dummy image tensor.&lt;/li&gt;
&lt;li&gt;Perform forward diffusion to add noise and reverse diffusion to reconstruct the image.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The code also includes a print statement to verify the shape of the reconstructed image, ensuring it matches the expected dimensions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This setup provides a foundational framework for experimenting with diffusion models and can be adapted for various image processing and generation tasks.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
  &lt;i class=&#34;fas fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt;Python --&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# ## Diffusion Models
#
# Diffusion models are a cutting-edge approach in generative AI, particularly effective for image generation and editing tasks. They work by iteratively denoising data to recover the original distribution. The key concept is to reverse a diffusion process where noise is added and then removed to reconstruct the original data.
#
# The core objective function for diffusion models can be expressed as follows:
#
# C(x) = -1 / (œÉ ‚àö(2œÄ)) * ((x - Œº) / œÉ)¬≤ * exp(-0.5 * ((x - Œº) / œÉ)¬≤)
#
# Where:
# - x is the data point.
# - Œº is the mean of the data distribution.
# - œÉ is the standard deviation of the data distribution.
#
# Another formulation for the objective function is:
#
# L(x) = 0.5 * ((x - Œº) / œÉ)¬≤ + 0.5 * log(2œÄœÉ¬≤)
#
# Here:
# - 0.5 * ((x - Œº) / œÉ)¬≤ represents the squared deviation from the mean, which measures the distance between generated and target distributions.
# - 0.5 * log(2œÄœÉ¬≤) represents the entropy term that accounts for the normalization factor in the Gaussian distribution.
#
# In a more general form, related to a stochastic process:
#
# L(x) = E[0.5 * ||x - Œº||¬≤ + 0.5 * log(2œÄœÉ¬≤)]
#
# Where E denotes the expectation over the diffusion process, capturing the average cost of deviation.
#
# This objective function measures how well the model can reverse the diffusion process, minimizing the discrepancy between the true noise and the predicted noise.
#
# Modern diffusion models, such as those used in DALL-E 2 and Stable Diffusion, leverage extensive training on diverse datasets and incorporate additional conditioning information to enable precise control over generated images.

# Define the main Diffusion Model class
class DiffusionModel(nn.Module):
    def __init__(self, img_shape):
        super(DiffusionModel, self).__init__()
        # Encoder network: Extracts features from input images
        self.encoder = nn.Sequential(
            # Convolutional layer: Reduces spatial dimensions and increases feature channels
            nn.Conv2d(img_shape[0], 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True)
        )

        # Decoder network: Reconstructs images from feature maps
        self.decoder = nn.Sequential(
            # Transposed convolution layers: Upsample feature maps to original image size
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, img_shape[0], kernel_size=4, stride=2, padding=1),
            nn.Tanh()  # Output layer with Tanh activation to match image pixel values
        )

    def forward(self, x):
        # Pass through encoder
        encoded = self.encoder(x)
        # Pass through decoder to reconstruct the image
        decoded = self.decoder(encoded)
        return decoded

# Define the Gaussian Diffusion class
class GaussianDiffusion(nn.Module):
    def __init__(self, model, timesteps=1000):
        super(GaussianDiffusion, self).__init__()
        self.model = model
        self.timesteps = timesteps
        # Initialize beta schedule and alpha parameters
        self.betas = self._linear_beta_schedule(timesteps)
        self.alphas = 1.0 - self.betas
        self.alpha_cumprod = np.cumprod(self.alphas)

    def _linear_beta_schedule(self, timesteps):
        # Linear schedule for beta values
        beta_start = 0.0001
        beta_end = 0.02
        return np.linspace(beta_start, beta_end, timesteps)

    def forward_diffusion_sample(self, x0, t):
        # Add noise to the input image based on the current timestep
        noise = torch.randn_like(x0)
        alpha_cumprod_t = self.alpha_cumprod[t]
        return torch.sqrt(alpha_cumprod_t) * x0 + torch.sqrt(1 - alpha_cumprod_t) * noise

    def reverse_diffusion_step(self, xt, t):
        # Predict noise and denoise the image
        pred_noise = self.model(xt)
        alpha_cumprod_t = self.alpha_cumprod[t]
        return (xt - torch.sqrt(1 - alpha_cumprod_t) * pred_noise) / torch.sqrt(alpha_cumprod_t)

    def forward(self, x):
        # Reverse diffusion process to reconstruct the image
        for t in reversed(range(self.timesteps)):
            x = self.reverse_diffusion_step(x, t)
        return x

# Sample Input
img_shape = (3, 64, 64)  # Sample image shape: 3 channels (RGB), 64x64 pixels
diffusion_model = DiffusionModel(img_shape)
gaussian_diffusion = GaussianDiffusion(diffusion_model)

# Dummy input: Random image tensor
x0 = torch.randn((1, *img_shape))  # Batch size of 1
xt = gaussian_diffusion.forward_diffusion_sample(x0, t=500)  # Add noise at timestep 500
x_reconstructed = gaussian_diffusion(xt)  # Reconstruct the image from noisy input

# Print the shape of the reconstructed image
print(x_reconstructed.shape)  # Should print torch.Size([1, 3, 64, 64])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;diffusion-models&#34;&gt;Diffusion Models&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Diffusion models have emerged as a powerful approach in generative AI, especially for tasks involving image generation and editing. These models iteratively denoise images to recover the original data distribution. The objective function for diffusion models can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
C(x) = -\frac{1}{\sigma \sqrt{2\pi}} \left(\frac{x - \mu}{\sigma}\right)^2 e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}
$$
&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x$ represents the data point.&lt;/li&gt;
&lt;li&gt;$\mu$ represents the mean of the data distribution.&lt;/li&gt;
&lt;li&gt;$\sigma$ represents the standard deviation of the data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
L(x) = \frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2 + \frac{1}{2} \log(2 \pi \sigma^2)
$$&lt;/p&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( \frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2 )$ represents the squared deviation from the mean, often used in diffusion models to measure the distance between generated and target distributions.&lt;/li&gt;
&lt;li&gt;$( \frac{1}{2} \log(2 \pi \sigma^2) )$ represents the entropy term, which accounts for the normalization factor in the Gaussian distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also represent the diffusion objective function in a more general form related to a stochastic process:&lt;/p&gt;
&lt;p&gt;$$
L(x) = \mathbb{E} \left[ \frac{1}{2} | x - \mu |^2 + \frac{1}{2} \log(2 \pi \sigma^2) \right]
$$&lt;/p&gt;
&lt;p&gt;Here, $( \mathbb{E} )$ denotes the expectation over the diffusion process, capturing the average cost.&lt;/p&gt;
&lt;p&gt;This objective function measures the discrepancy between the true noise added to the data and the noise predicted by the model, aiming to train the model to accurately reverse the diffusion process.&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;code&gt;Where:&lt;/code&gt; \(x_t\) is the noised image at timestep \(t\), and \(\epsilon_\theta\) is the noise prediction network. Recent works like DALL-E 2 and Stable Diffusion have demonstrated the remarkable capabilities of diffusion models in text-to-image generation and image editing tasks. These models leverage large-scale training on diverse datasets and incorporate additional conditioning information to enable fine-grained control over generated images.
&lt;/p&gt;
&lt;h2 id=&#34;gans-generative-adversarial-networks&#34;&gt;GANs (Generative Adversarial Networks)&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed by Goodfellow et al. in 2014. GANs consist of two neural networks, a generator and a discriminator, which compete against each other in a zero-sum game framework. The generator aims to generate realistic data samples, while the discriminator attempts to distinguish between real and generated samples. The objective functions for GANs can be expressed as follows: 
&lt;/p&gt;
&lt;p&gt;$$
L_{\text{GAN}} = \mathbb{E}_{x \sim p_x{\text{data}(x)}} [\log D(x)] + \text{generated data samples}
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$G$ represents the generator network.&lt;/li&gt;
&lt;li&gt;$D$ represents the discriminator network.&lt;/li&gt;
&lt;li&gt;$x$ represents the real data sample.&lt;/li&gt;
&lt;li&gt;$z$ represents the random noise vector sampled from a prior distribution $p_z(z)$.&lt;/li&gt;
&lt;li&gt;$p_{\text{data}(x)}$ represents the data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb{E}_{x \sim p_x{\text{data}(x)}} [\log D(x)]$ represents the expected value of the discriminator&amp;rsquo;s output for real data samples.&lt;/li&gt;
&lt;li&gt;$\mathbb{E}_{z \sim p_z(z)} [\log (1 - D(G(z)))]$ represents the expected value of the discriminator&amp;rsquo;s output for generated data samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The generator aims to minimize this objective while the discriminator aims to maximize it.&lt;/p&gt;
&lt;h2 id=&#34;self-supervised-learning&#34;&gt;Self-Supervised Learning&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Self-Supervised Learning (SSL) is a paradigm in machine learning where the model learns to generate labels from the input data itself, without requiring manually labeled data. This approach uses pretext tasks to learn representations that can be transferred to downstream tasks. One common objective in self-supervised learning is the contrastive loss, which can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
L_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(h_i, h_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(h_i, h_k)/\tau)}
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h_i$ and $h_j$ represent the encoded representations of positive pairs.&lt;/li&gt;
&lt;li&gt;$\text{sim}(h_i, h_j)$ represents the similarity measure between $h_i$ and $h_j$.&lt;/li&gt;
&lt;li&gt;$\tau$ represents the temperature parameter.&lt;/li&gt;
&lt;li&gt;$N$ represents the number of samples.&lt;/li&gt;
&lt;li&gt;$\mathbb{1}_{[k \neq i]}$ is an indicator function that is 1 if $k \neq i$ and 0 otherwise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\exp(\text{sim}(h_i, h_j)/\tau)$ represents the exponential of the similarity between the positive pairs scaled by the temperature.&lt;/li&gt;
&lt;li&gt;The denominator sums the exponential similarities of all pairs except the identical ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This objective encourages the model to bring similar samples closer in the representation space and push dissimilar ones apart.&lt;/p&gt;
&lt;h2 id=&#34;adversarial-attacks&#34;&gt;Adversarial Attacks&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Adversarial attacks involve manipulating input data to deceive machine learning models into making incorrect predictions. One common method is the Fast Gradient Sign Method (FGSM), which perturbs the input data in the direction of the gradient of the loss with respect to the input. The formula for generating an adversarial example using FGSM can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
x_{\text{adv}} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x_{\text{adv}}$ represents the adversarial example.&lt;/li&gt;
&lt;li&gt;$x$ represents the original input data.&lt;/li&gt;
&lt;li&gt;$\epsilon$ represents the perturbation magnitude.&lt;/li&gt;
&lt;li&gt;$\nabla_x J(\theta, x, y)$ represents the gradient of the loss function $J$ with respect to the input $x$.&lt;/li&gt;
&lt;li&gt;$J(\theta, x, y)$ represents the loss function of the model.&lt;/li&gt;
&lt;li&gt;$\theta$ represents the model parameters.&lt;/li&gt;
&lt;li&gt;$y$ represents the true label of the input data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{sign}(\nabla_x J(\theta, x, y))$ represents the sign of the gradient of the loss with respect to the input, indicating the direction to perturb the input to maximize the loss.&lt;/li&gt;
&lt;li&gt;The adversarial example $x_{\text{adv}}$ is created by adding this perturbation to the original input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI continues to advance rapidly, with ongoing developments in model architectures, training techniques, and applications across various domains. The ability of generative models to create high-quality content, from text and images to music and videos, underscores their transformative potential. While there are challenges and ethical considerations to address, the future of generative AI is promising, with numerous opportunities for innovation and interdisciplinary collaboration. As we explore these frontiers, it is crucial to remain mindful of the societal impacts and strive for responsible use of these powerful technologies.
&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Generative AI is revolutionizing various fields by creating new content and enhancing existing applications. This blog explores current trends, practical applications, challenges, and future opportunities of generative models. Key areas include advancements in model architectures, real-world applications like content creation and healthcare, and the integration of techniques such as GANs and diffusion models.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Generative AI presents both exciting opportunities and significant challenges. This blog covers the latest trends in generative models, their applications across various industries, and critical issues such as ethical considerations and future directions. Learn about the potential of models like GANs and diffusion techniques, and their impact on content creation and other fields.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand GANs (Generative Adversarial Networks)
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study GAN architecture (Generator and Discriminator)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review applications and improvements&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learn about Variational Autoencoders (VAEs)
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore VAE structure and loss function&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Examine use cases in generative tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Familiarize with Diffusion Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand diffusion process and objective function&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review recent advancements (e.g., DALL-E 2, Stable Diffusion)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore Transformer Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study transformer architecture and attention mechanisms&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review its application in language generation and understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learn about Pretrained Language Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study fine-tuning techniques for specific tasks&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore popular models (e.g., GPT, BERT, T5)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand Model Evaluation Metrics
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review metrics like BLEU, ROUGE, and FID for generative models&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study methods for evaluating model performance in different contexts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Investigate Ethical Considerations
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore challenges related to bias, fairness, and security&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study frameworks for responsible AI development&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Aug 2024). &lt;em&gt;Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024generativeaitrends,
  title   = &amp;quot;Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Aug&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it üôå
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Bender, E. M., Gebru, T., McMillan-Major, A., &amp;amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? &lt;em&gt;Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2102.02503&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2102.02503&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BROWN, T. B., MANE, D., LANGE, I., &amp;amp; et al. (2020). Language Models are Few-Shot Learners. &lt;em&gt;Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2005.14165&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CHESNEY, R., &amp;amp; CITRON, D. K. (2019). Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. &lt;em&gt;California Law Review&lt;/em&gt;, 107(6), 1753-1819. &lt;a href=&#34;https://doi.org/10.2139/ssrn.3213954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.2139/ssrn.3213954&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DENG, J., DONAHUE, J., &amp;amp; HAREL, M. (2009). ImageNet: A Large-Scale Hierarchical Image Database. &lt;em&gt;Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2009.5206848&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR.2009.5206848&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DOSOVITSKIY, A., BROSSARD, T., &amp;amp; SPRINGENBERG, J. (2017). Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks. &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 39(5), 939-949. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2016.2593826&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2016.2593826&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FRID-ADAR, M., ELIYAHU, S., &amp;amp; GOLDY, S. (2018). GAN-based Synthetic Medical Image Augmentation for Increased CNN Performance in Liver Lesion Classification. &lt;em&gt;IEEE Transactions on Medical Imaging&lt;/em&gt;, 37(6), 1334-1343. &lt;a href=&#34;https://doi.org/10.1109/TMI.2018.2813792&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TMI.2018.2813792&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;KINGMA, D. P., &amp;amp; WELLING, M. (2013). Auto-Encoding Variational Bayes. &lt;em&gt;Proceedings of the 2nd International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1312.6114&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RADFORD, A., WU, J., &amp;amp; AMODEI, D. (2021). Learning Transferable Visual Models From Natural Language Supervision. &lt;em&gt;Proceedings of the 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2103.00020&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RAMESH, A., MENG, C., &amp;amp; ZHANG, S. (2021). DALL¬∑E: Creating Images from Text. &lt;em&gt;OpenAI&lt;/em&gt;. &lt;a href=&#34;https://openai.com/research/dall-e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://openai.com/research/dall-e&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;STRUBELL, E., GANASSI, M., &amp;amp; MCAFEE, P. (2019). Energy and Policy Considerations for Deep Learning in NLP. &lt;em&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1906.02243&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1906.02243&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WIESE, S., BOLAND, M., &amp;amp; TONG, A. (2019). A Survey on Machine Learning in Finance. &lt;em&gt;Proceedings of the 26th International Conference on Machine Learning (ICML 2019)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1910.02342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1910.02342&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VASWANI, A., SHAZEER, N., &amp;amp; PARMAR, N. (2017). Attention Is All You Need. &lt;em&gt;Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1706.03762&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
