[{"authors":null,"categories":null,"content":"My research focuses on Pattern Recognition‚Äîdeveloping cutting-edge algorithms with Optimization Theory and Statistical Learning to push AI\u0026rsquo;s limits. I work with advanced models like GANs and Diffusion Models, use Semi-Supervised Learning, and explore Adversarial Attacks and Large Language Models (LLMs) to revolutionize AI capabilities.\nI am currently a Senior AI Research Scientist at MARS (Motor AI Recognition Solution) and a Postdoctoral Fellow at Chulalongkorn University. I earned my Ph.D. in Computer Engineering from Chulalongkorn University, where I specialized in AI.\nMy passion lies in advancing AI Technologies to unlock human potential. I am particularly interested in Remote Sensing, where AI unveils transformative insights and redefines how we perceive and interact with our environment.\nYou can find summaries of my academic, industry, and teaching experience in my CV, and explore more about my personal life on my blog. Additionally, check out some of my music on SoundCloud.\nCall me Teerapong Panboonyuen, or just Kao (‡πÄ‡∏Å‡πâ‡∏≤) in Thai: ‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô.\nDownload my CV.\n","date":1722124800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1722124800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/teerapong-panboonyuen/","section":"authors","summary":"My research focuses on Pattern Recognition‚Äîdeveloping cutting-edge algorithms with Optimization Theory and Statistical Learning to push AI\u0026rsquo;s limits. I work with advanced models like GANs and Diffusion Models, use Semi-Supervised Learning, and explore Adversarial Attacks and Large Language Models (LLMs) to revolutionize AI capabilities.","tags":null,"title":"Teerapong Panboonyuen","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents What you will learn Program overview Courses in this program Meet your instructor FAQs What you will learn Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program Basic Python Build a foundation in Python. Visualization Learn how to visualize data with Plotly. Statistics Introduction to statistics for data science. Meet your instructor Teerapong Panboonyuen FAQs Are there prerequisites? There are no prerequisites for the first course.\nHow often do the courses run? Continuously, at your own pace.\nBegin the course ","date":1706054400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1706054400,"objectID":"9175b58bdbbc0b3316bae12a6c70af53","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/","publishdate":"2024-01-24T00:00:00Z","relpermalink":"/courses/learn-data-science/","section":"courses","summary":"An sample of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üöÄ Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n1-2 hours per week, for 8 weeks\nLearn Quiz What is the difference between lists and tuples? Lists\nLists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world'] Tuples\nTuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world') Is Python case-sensitive? Yes\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"e9ca400ea73091d69f8c17a54f939765","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/python/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Basic Python","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n1-2 hours per week, for 8 weeks\nLearn Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nWrite Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show() ","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"c8051497f5da2d7333291e1d5361c7b4","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/visualization/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\nThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$. Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"90a9a7e769286f383e6e8f7355492613","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/stats/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" ","date":1733058000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733058000,"objectID":"996883997cabd32d724ba8a17e5846a5","permalink":"https://kaopanboonyuen.github.io/talk/inspiring-the-future-of-ai-innovations-and-mastering-llm/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/talk/inspiring-the-future-of-ai-innovations-and-mastering-llm/","section":"event","summary":"I had the opportunity to give a final orientation speech to the undergraduate students of the Department of Electrical and Computer Engineering at KMUTNB. The focus of my speech was on the transformative impact of AI, particularly highlighting the advancements in Large Language Models (LLMs) like ChatGPT. I discussed how these models have revolutionized natural language processing, enabling sophisticated interactions and problem-solving capabilities. Emphasizing the importance of mastering AI tools, I encouraged students to develop strong prompting skills to effectively control and harness the potential of AI technologies. The future of AI holds immense possibilities, and by staying adept at these emerging trends, students can significantly contribute to the field and drive innovation.","tags":[],"title":"Inspiring the Future of AI Innovations and Mastering LLM","type":"event"},{"authors":["Teerapong Panboonyuen"],"categories":["remote-sensing","AI","machine-learning"],"content":"Table of Contents Introduction Understanding Large Language Models (LLMs) LLMs in LULC Classification Methodology Case Study: LULC Classification on Sentinel-2 Imagery Conclusion Introduction Remote sensing has revolutionized the way we observe and understand the Earth‚Äôs surface. With the advent of satellites like Sentinel, Landsat-8, and THEOS, we have access to a plethora of high-resolution imagery that can be used for various applications, including Land Use/Land Cover (LULC) classification and image classification. However, analyzing and interpreting this vast amount of data is a complex task. Enter Large Language Models (LLMs), which have shown promise in various domains, including natural language processing, computer vision, and remote sensing. In this blog, we will explore how LLMs can be applied to remote sensing, particularly in the domains of LULC and image classification. We will delve into the methodologies, algorithms, and techniques that can be utilized to harness the power of LLMs for these applications. Understanding Large Language Models (LLMs) Large Language Models, such as GPT-4, are deep learning models that have been trained on vast amounts of text data. They are capable of understanding and generating human-like text, making them highly versatile for various applications. In the context of remote sensing, LLMs can be used to analyze and interpret imagery data, aiding in tasks like LULC classification and image classification. LLMs in LULC Classification LULC classification involves categorizing different regions of an image into land use and land cover classes, such as forests, urban areas, water bodies, and agricultural land. Traditional methods for LULC classification include supervised and unsupervised learning techniques. LLMs, however, can enhance these methods by providing contextual understanding and improved feature extraction. Methodology The process of using LLMs for LULC classification can be summarized as follows: Data Preprocessing:\nCollect satellite imagery from sources like Sentinel, Landsat-8, and THEOS. Perform image correction and normalization to ensure consistency in the data. Feature Extraction:\nUse convolutional neural networks (CNNs) to extract features from the satellite images. Integrate LLMs to enhance feature extraction by incorporating contextual information from related text data (e.g., environmental reports, land use documentation). Model Training:\nTrain a classification model using the extracted features. The model can be a hybrid of CNNs and LLMs, where the CNN handles the spatial features and the LLM provides contextual understanding. Classification and Validation:\nApply the trained model to classify the satellite images into LULC categories. Validate the model using ground truth data and performance metrics like accuracy, precision, recall, and F1-score. Case Study: LULC Classification on Sentinel-2 Imagery To illustrate the application of LLMs in LULC classification, let‚Äôs consider a case study using Sentinel-2 imagery. Sentinel-2 provides high-resolution optical imagery, which is ideal for detailed LULC classification. Data Collection and Preprocessing We collected Sentinel-2 imagery for a region with diverse land cover types. The images were preprocessed to correct for atmospheric effects and normalize the reflectance values. Feature Extraction A pre-trained CNN, such as ResNet-50, was used to extract spatial features from the images. Simultaneously, a large corpus of environmental text data was fed into an LLM to extract contextual features. Model Training and Classification The extracted features were combined and fed into a hybrid classification model. The model was trained using labeled ground truth data. The results showed a significant improvement in classification accuracy compared to traditional methods. Conclusion The integration of LLMs in remote sensing, particularly for LULC and image classification, holds immense potential. By combining the spatial feature extraction capabilities of CNNs with the contextual understanding of LLMs, we can achieve more accurate and meaningful classifications. As remote sensing technology continues to evolve, the role of advanced AI models like LLMs will become increasingly crucial in unlocking new insights from satellite imagery. In this blog, we explored the methodologies and techniques for leveraging LLMs in remote sensing applications. The case study on Sentinel-2 imagery demonstrated the practical benefits of this approach. As we move forward, further research and development in this field will undoubtedly lead to more innovative and effective solutions for remote sensing challenges. ","date":1722124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722124800,"objectID":"81f92504ef13a550691c26befb1a616d","permalink":"https://kaopanboonyuen.github.io/blog/2024-07-29-leveraging-large-language-models-in-remote-sensing/","publishdate":"2024-07-28T00:00:00Z","relpermalink":"/blog/2024-07-29-leveraging-large-language-models-in-remote-sensing/","section":"blog","summary":"Exploring how LLMs can enhance remote sensing applications like LULC and image classification using Sentinel, Landsat-8, and THEOS imagery.","tags":["remote-sensing","LULC","image-classification","LLM","Sentinel","Landsat","THEOS"],"title":"Leveraging Large Language Models (LLMs) in Remote Sensing for Land Use/Land Cover (LULC) and Image Classification","type":"blog"},{"authors":[],"categories":null,"content":" ","date":1701435600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701435600,"objectID":"cd4a86c4c3997f9191260d8c073400f1","permalink":"https://kaopanboonyuen.github.io/talk/geospatial-big-data-analytics/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/talk/geospatial-big-data-analytics/","section":"event","summary":"Geospatial Data Analytics involves analyzing spatial and geographical data to gain insights and make informed decisions. Using PySpark, this process is accelerated through distributed computing, enabling the handling of large datasets efficiently. Distributed Machine Learning models further enhance the analysis by providing scalable and robust predictions. Visualization tools like Looker Studio present the analyzed data in an interactive and comprehensible format, facilitating better decision-making and strategic planning. This combination of technologies allows for comprehensive geospatial data analysis, uncovering patterns and trends that drive actionable insights.","tags":[],"title":"Geospatial Big Data Analytics","type":"event"},{"authors":["Teerapong Panboonyuen","C. Charoenphon","C. Satirapod"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"14c0ff27b66af589b9731863f8bf9193","permalink":"https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/mevit-a-medium-resolution-vision-transformer/","section":"publication","summary":"In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT's effectiveness in accurately segmenting Thai Landsat-8 data.","tags":["Remote Sensing","Landsat-8","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Vision Transformers","Transformer","Multi-branch Architectures","Mixed-scale Convolutional Feedforward Networks"],"title":"MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand","type":"publication"},{"authors":["Teerapong Panboonyuen","N. Nithisopa","P. Pienroj","L. Jirachuphun","C. Watthanasirikrit","N. Pornwiriyakul"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"ecd0bcce1705e21b780021bb64e0e45a","permalink":"https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/","section":"publication","summary":"Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at https://github.com/kaopanboonyuen/MARS.","tags":["Attention","Self-Attention","MARS","Sequential Quadtree Nodes","Mask R-CNN","PointRend","Mask Transfiner"],"title":"MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1669899600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669899600,"objectID":"8f68e458f32432396b1cad7a1a1de3bd","permalink":"https://kaopanboonyuen.github.io/talk/distributed-machine-learning-techniques-for-geospatial-data/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/talk/distributed-machine-learning-techniques-for-geospatial-data/","section":"event","summary":"I was invited to teach a course on distributed machine learning to the Geo-Informatics and Space Technology Development Agency (GISTDA). The curriculum covered fundamental concepts of PySpark, basic deep learning techniques, and practical applications of distributed training using TensorFlow. I also emphasized methods for leveraging Multi-GPU setups and implementing distributed training strategies, particularly in the context of geospatial data analytics, equipping participants with the skills needed to handle large-scale machine learning tasks efficiently.","tags":[],"title":"Distributed Machine Learning Techniques for Geospatial Data","type":"event"},{"authors":["Teerapong Panboonyuen","S. Thongbai","W. Wongweeranimit","P. Santitamnont","K. Suphan","C. Charoenphon"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"4adfc7217759e6906ac85e560b8c385f","permalink":"https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/object-detection-of-road-assets-using-transformer-based-yolox/","section":"publication","summary":"Detecting objects of varying sizes, like kilometer stones, remains a significant challenge and directly affects the accuracy of object counts. Transformers have shown remarkable success in natural language processing (NLP) and image processing due to their ability to model long-range dependencies. This paper proposes an enhanced YOLO (You Only Look Once) series with two key contributions, (i) We employ a pre-training objective to obtain original visual tokens from image patches of road assets, using a pre-trained Vision Transformer (ViT) backbone, which is then fine-tuned on downstream tasks with additional task layers. (ii) We incorporate Feature Pyramid Network (FPN) decoder designs into our deep learning network to learn the significance of different input features, avoiding issues like feature mismatch and performance degradation that arise from simple summation or concatenation. Our proposed method, Transformer-Based YOLOX with FPN, effectively learns general representations of objects and significantly outperforms state-of-the-art detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. It achieves a 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP on the test-dev dataset.","tags":["Attention","Self-Attention","MARS","Sequential Quadtree Nodes","Mask R-CNN","PointRend","Mask Transfiner"],"title":"Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama","type":"publication"},{"authors":["K. Thitisiriwech","Teerapong Panboonyuen","P. Kantavat","Y. Iwahori","B. Kijsirikul"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"9d0e53f94cc9f61ef1adaf4acad0f1a3","permalink":"https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/","section":"publication","summary":"In the modern era, urban design and sustainable development are vital topics for megacities, as they are important for the wellbeing of its residents. One of the effective key performance indices (KPIs) measuring the city plan‚Äôs efficiency in quantity and quality factors is Quality of Life (QOL), an index that policymakers can use as a critical KPI to measure the quality of urbanscape design. In the traditional approach, the researchers conduct the questionnaire survey and then analyze the gathered data to acquire the QOL index. The conventional process is costly and time-consuming, but the result of the evaluation area is limited. Moreover, it is difficult to embed in an application or system; we proposed artificial intelligence (AI) approaches to solve the limitation of the traditional method in Bangkok as a case study. There are two steps for our proposed method. First, in the knowledge extraction step, we apply deep convolutional neural networks (DCNNs), including semantic segmentation and object detection, to extract helpful information images. Second, we use a linear regression model for inferring the QOL score. We conducted various state-of-the-art (SOTA) models and public datasets to evaluate the performance of our method. The experiment results show that our novel approach is practical and can be considered for use as an alternative QOL acquisition method. We also gain some understanding of drivers‚Äô insights from the experiment result. Implementation codes and dataset are available at [https://kaopanboonyuen.github.io/bkkurbanscapes](https://kaopanboonyuen.github.io/bkkurbanscapes).","tags":["DeepLab","Sustainability","Quality of Life (QOL)","Bangkok Urbanscapes Dataset","Xception","Cityscapes"],"title":"Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1606827600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606827600,"objectID":"85d93937200093837b1d1a85f626df23","permalink":"https://kaopanboonyuen.github.io/talk/achieve-data-science-first-meet/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/talk/achieve-data-science-first-meet/","section":"event","summary":"I was invited to speak at the \"Achieve Data Science First Meet\" for a MOOC student project event, where I highlighted the growing recognition of data science, AI, and machine learning's importance across various industries. I advised that organizations, regardless of their size or sector, must effectively develop and implement data science capabilities to stay competitive in the era of big data, or risk falling behind.","tags":[],"title":"Achieve Data Science First Meet","type":"event"},{"authors":[],"categories":null,"content":" ","date":1596200400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596200400,"objectID":"d7024594cc6cb801b6ae9f39b7862ecf","permalink":"https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/","publishdate":"2020-07-31T00:00:00Z","relpermalink":"/talk/ph.d.-thesis-defense/","section":"event","summary":"My thesis defense at the Faculty of Engineering, Chulalongkorn University. This dissertation introduces a new architecture for remote sensing, featuring Global Convolutional Network (GCN), channel attention, domain-specific transfer learning, Feature Fusion (FF), and Depthwise Atrous Convolution (DA). Tests on Landsat-8 and ISPRS Vaihingen datasets show that this model significantly outperforms the baseline.","tags":[],"title":"Ph.D. Thesis Defense","type":"event"},{"authors":["Teerapong Panboonyuen","P. Rakwatin","K. Intarat"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"3cc38a178687941b0f68b09b6ba4eff2","permalink":"https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/enhanced-feature-pyramid-vision-transformert/","section":"publication","summary":"Semantic segmentation on Landsat-8 data is crucial in the integration of diverse data, allowing researchers to achieve more productivity and lower expenses. This research aimed to improve the versatile backbone for dense prediction without convolutions‚Äînamely, using the pyramid vision transformer (PRM-VS-TM) to incorporate attention mechanisms across various feature maps. Furthermore, the PRM-VS-TM constructs an end-to-end object detection system without convolutions and uses handcrafted components, such as dense anchors and non-maximum suspension (NMS). The present study was conducted on a private dataset, i.e., the Thailand Landsat-8 challenge. There are three baselines, DeepLab, Swin Transformer (Swin TF), and PRM-VS-TM. Results indicate that the proposed model significantly outperforms all current baselines on the Thailand Landsat-8 corpus, providing F1-scores greater than 80% in almost all categories. Finally, we demonstrate that our model, without utilizing pre-trained settings or any further post-processing, can outperform current state-of-the-art (SOTA) methods for both agriculture and forest classes.","tags":["Mon-Maximum Suspension","Transfer Learning","Vision Transformer","Remote Sensing","Landsat-8","Transformer"],"title":"Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus","type":"publication"},{"authors":["K. Thitisiriwech","Teerapong Panboonyuen","P. Kantavat","Y. Iwahori","B. Kijsirikul"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"dbdbb2d13060b56b1c4b922f50cd5ad4","permalink":"https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/the-bangkok-urbanscapes-dataset/","section":"publication","summary":"This paper addresses semantic segmentation for autonomous driving systems, focusing on self-driving cars in Thailand. We introduce DeepLab-V3-A1 with Xception, an enhanced version of DeepLab-V3+, and present the Bangkok Urbanscapes dataset. Our method improves segmentation accuracy by refining the decoder and modifying the Xception backbone. Experiments on four datasets, including CamVid, Cityscapes, IDD, and our proposed dataset, show our approach performs comparably to baseline methods. Our dataset includes 701 annotated images of various Bangkok driving environments, covering eleven semantic classes. The architecture and dataset aim to aid developers in improving autonomous driving systems for diverse urban conditions. Implementation codes and dataset are available at [https://kaopanboonyuen.github.io/bkkurbanscapes](https://kaopanboonyuen.github.io/bkkurbanscapes).","tags":["DeepLab","Bangkok Urbanscapes Dataset","Xception","Cityscapes"],"title":"The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"4428c44f98c396bbd30d518b58b80be8","permalink":"https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/transformer-based-decoder-designs-for-semantic-segmentation/","section":"publication","summary":"Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% ùêπ1 score), Thailand North Landsat-8 corpus (63.12% ùêπ1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.","tags":["Transformer","Semantic Segmentation","Decoder Design","Swin Transformer","Vision Transformer","Self-Attention","Global Convolutional Network"],"title":"Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"08bf283a12b51cb06a21958c8492cd7d","permalink":"https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/","section":"publication","summary":"My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.","tags":["Convolutional Neural Networks","Landsat-8","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Global Convolutional Network","Encoder-Decoder Networks","ISPRS Vaihingen"],"title":"Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"6e6a1251e5de03c8b0aadd7a8242a7b8","permalink":"https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/","section":"publication","summary":"This paper addresses improving semantic segmentation in remote sensing for aerial and satellite images, which is crucial for agriculture, map updates, route optimization, and navigation. We propose enhancements to the state-of-the-art Enhanced Global Convolutional Network (GCN152-TL-A) by introducing a High-Resolution Representation (HR) backbone for better feature extraction, Feature Fusion (FF) to capture low-level details, and Depthwise Atrous Convolution (DA) for refined multi-resolution features. Experiments on Landsat-8 and ISPRS Vaihingen datasets demonstrate our model's superior performance, achieving over 90% accuracy in F1 scores and outperforming baseline models.","tags":["Feature Fusion","Transfer Learning","Remote Sensing","ISPRS Vaihingen Dataset"],"title":"Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"05de61049c7b26380a42ed51f66573c2","permalink":"https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/","section":"publication","summary":"In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, e.g., river, building, forest, etc., on raster images. A deep convolutional encoder‚Äìdecoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second, ‚Äúchannel attention‚Äù is presented in our network in order to select the most discriminative filters (features). Third, ‚Äúdomain-specific transfer learning‚Äù is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets (i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of ùêπ1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively.","tags":["Global Convolutional Network","Transfer Learning","Channel Attention","Remote Sensing","Discriminative Filters"],"title":"Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning","type":"publication"},{"authors":["P. Kantavat","Y. Hayashi","G. City","B. Kijsirikul","Teerapong Panboonyuen","Y. Iwahori"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"52061b758c1cab61004baa1a27c5c001","permalink":"https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/","section":"publication","summary":"For an urban development, the Quality of Life (QOL) of people in the city is a vital issue that should be considered. There are many researches in QOL topics that use questionnaire survey approach. These studies yield very useful information for city development planning. As the Artificial Intelligence technologies are developed very fast recently, they are applied to solve many transportation problems. In this paper, we propose a method that automatically extract mobility indicators using two image recognition techniques, Semantic Segmentation and Object Recognition. Because the mobility is an important factor in QOL evaluation, our work can be used to enhance a performance and reduce a data gathering cost of the QOL evaluation.","tags":["YOLOv5","Quality of Life (QOL)","Semantic Segmentation","Object Detection","Image Recognition"],"title":"Transportation Mobility Factor Extraction Using Image Recognition Techniques","type":"publication"},{"authors":["I. Wichakam","Teerapong Panboonyuen","C. Udomcharoenchaikit","P. Vateekul"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"7ae277d2610b5783ef94eba07969563a","permalink":"https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/","section":"publication","summary":"Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.","tags":["Colorectal Cancer","Fully Convolutional Network","CVC-EndoSceneStill","Colonoscopy Video"],"title":"Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network","type":"publication"},{"authors":["S. Chantharaj","K. Pornratthanapong","P. Chitsinpchayakun","Teerapong Panboonyuen"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"426382c4e17472b69c94efc70af45de8","permalink":"https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/semantic-segmentation-on-medium-resolution-satellite-images/","section":"publication","summary":"Semantic Segmentation is a fundamental task in computer vision and remote sensing imagery. Many applications, such as urban planning, change detection, and environmental monitoring, require the accurate segmentation; hence, most segmentation tasks are performed by humans. Currently, with the growth of Deep Convolutional Neural Network (DCNN), there are many works aiming to find the best network architecture fitting for this task. However, all of the studies are based on very-high resolution satellite images, and surprisingly; none of them are implemented on medium resolution satellite images. Moreover, no research has applied geoinformatics knowledge. Therefore, we purpose to compare the semantic segmentation models, which are FCN, SegNet, and GSN using medium resolution images from Landsat-8 satellite. In addition, we propose a modified SegNet model that can be used with remote sensing derived indices. The results show that the model that achieves the highest accuracy RGB bands of medium resolution aerial imagery is SegNet. The overall accuracy of the model increases when includes Near Infrared (NIR) and Short-Wave Infrared (SWIR) band. The results showed that our proposed method (our modified SegNet model, named RGB-IR-IDX-MSN method) outperforms all of the baselines in terms of mean F1 scores.","tags":["Convolutional Neural Networks","Semantic Segmentation","Near Infrared (NIR)","Short-Wave Infrared (SWIR)","Remote Sensing"],"title":"Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"f6d05230dcdb2e8c944ff54140727202","permalink":"https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/road-segmentation-on-remote-sensing/","section":"publication","summary":"Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.","tags":["Remote Sensing","Road Segmentation","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Exponential Linear Unit","Conditional Random Fields","Landscape Metrics"],"title":"Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"fca7fa3fb9e06fccd9699f13b4e415fe","permalink":"https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/road-segmentation-on-aerial-imagery/","section":"publication","summary":"In this paper, we introduce an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Enhancements include the use of ELU (exponential linear unit) instead of ReLU, dataset augmentation with incrementally-rotated images to increase training data by eight times, and the use of landscape metrics to remove false road objects. Tested on the Massachusetts Roads dataset, our method outperformed the SegNet benchmark and other baselines in precision, recall, and F1 scores.","tags":["Remote Sensing","Road Segmentation","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Exponential Linear Unit"],"title":"An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b626b42b6e307c281ff5e448e98cd9b0","permalink":"https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/road-map-extraction-from-satellite-imagery/","section":"publication","summary":"Road map extraction is vital for GIS and underpins many location-based applications like GPS navigation, delivery route planning, tourist attraction locating, and location-based marketing. This research uses satellite imagery, though other remotely sensed images like aerial photographs, UAVs, or drones are also applicable. Despite various proposed methods focusing primarily on accuracy, completeness of results is equally important. We enhance accuracy by incorporating connected component analysis and improve completeness using landscape metrics, which describe spatial characteristics through shape and isolation indices. Evaluated on precision, recall, quality, and F1 scores, our method achieves over 90% performance in all criteria.","tags":["Road Segmentation","Connected Component Analysis","Image Processing"],"title":"Road map extraction from satellite imagery using connected component analysis and landscape metrics","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"50982ca008f9ff19109ec4bea6c187cd","permalink":"https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/publication/image-vectorization-of-road-satellite-data-sets/","section":"publication","summary":"Data extraction of geo-spatial objects from satellite images is a crucial step in facilitating analysis of geo-spatial or spatio-temporal data, typically involving line (road) and polygon (area) layers. This paper introduces a method for transforming satellite data (raster images) containing roads from pixel form into spatial objects comprising lines and polygons. Our algorithm consists of three primary steps. First, roads are isolated from other objects using k-means clustering. Second, line extraction is performed on the road areas by applying morphological operations to skeletonize the image, followed by enhancement using the Ramer-Douglas-Peucker algorithm. Finally, land-cover classification is applied to non-road objects to extract polygons. Experimental results demonstrate that both lines (road networks) and polygons (areas) can be accurately extracted from satellite imagery simultaneously.","tags":["Remote Sensing","Road Segmentation","Spatio-Temporal Data","High-Resolution Imagery","Aerial Imagery","K-Means Clustering","Ramer-Douglas-Peucker"],"title":"Image Vectorization of Road Satellite Data Sets","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://kaopanboonyuen.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]