[{"authors":null,"categories":null,"content":"My research focuses on Machine Vision with Deep Learning‚Äîdeveloping algorithms that leverage Optimization Theory and Statistical Learning to enhance AI capabilities. I work extensively with Stochastic Gradient Descent (SGD) and Attention Mechanisms in Transformer architectures, and have a deep understanding of Backpropagation and Generative Pre-Trained Transformers (GPT), including concepts like Autoregressive Modeling and Self-Attention.\nI am currently a Senior AI Research Scientist at MARS (Motor AI Recognition Solution) and a Postdoctoral Fellow at Chulalongkorn University. I earned my Ph.D. in Computer Engineering from Chulalongkorn University, where I specialized in AI.\nMy passion lies in advancing AI technologies to amplify human potential, with a focus on deep learning and computer vision. I am particularly interested in applications in remote sensing, where AI can offer new insights into our world.\nYou can find summaries of my academic, industry, and teaching experience in my CV, and explore more about my personal life on my blog. Additionally, check out some of my music on SoundCloud.\nCall me Teerapong Panboonyuen (Kao), or just Kao (‡πÄ‡∏Å‡πâ‡∏≤) in Thai: ‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô.\nDownload my CV.\n","date":1682899200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1682899200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/teerapong-panboonyuen/","section":"authors","summary":"My research focuses on Machine Vision with Deep Learning‚Äîdeveloping algorithms that leverage Optimization Theory and Statistical Learning to enhance AI capabilities. I work extensively with Stochastic Gradient Descent (SGD) and Attention Mechanisms in Transformer architectures, and have a deep understanding of Backpropagation and Generative Pre-Trained Transformers (GPT), including concepts like Autoregressive Modeling and Self-Attention.","tags":null,"title":"Teerapong Panboonyuen","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents What you will learn Program overview Courses in this program Meet your instructor FAQs What you will learn Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program Python basics Build a foundation in Python. Visualization Learn how to visualize data with Plotly. Statistics Introduction to statistics for data science. Meet your instructor Teerapong Panboonyuen FAQs Are there prerequisites? There are no prerequisites for the first course.\nHow often do the courses run? Continuously, at your own pace.\nBegin the course ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://kaopanboonyuen.github.io/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n1-2 hours per week, for 8 weeks\nLearn Quiz What is the difference between lists and tuples? Lists\nLists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world'] Tuples\nTuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world') Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://kaopanboonyuen.github.io/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n1-2 hours per week, for 8 weeks\nLearn Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nWrite Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show() ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://kaopanboonyuen.github.io/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\nThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$. Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://kaopanboonyuen.github.io/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" ","date":1701435600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701435600,"objectID":"cd4a86c4c3997f9191260d8c073400f1","permalink":"https://kaopanboonyuen.github.io/talk/geospatial-big-data-analytics/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/talk/geospatial-big-data-analytics/","section":"event","summary":"Geospatial Data Analytics involves analyzing spatial and geographical data to gain insights and make informed decisions. Using PySpark, this process is accelerated through distributed computing, enabling the handling of large datasets efficiently. Distributed Machine Learning models further enhance the analysis by providing scalable and robust predictions. Visualization tools like Looker Studio present the analyzed data in an interactive and comprehensible format, facilitating better decision-making and strategic planning. This combination of technologies allows for comprehensive geospatial data analysis, uncovering patterns and trends that drive actionable insights.","tags":[],"title":"Geospatial Big Data Analytics","type":"event"},{"authors":["Teerapong Panboonyuen","C. Charoenphon","C. Satirapod"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"14c0ff27b66af589b9731863f8bf9193","permalink":"https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/mevit-a-medium-resolution-vision-transformer/","section":"publication","summary":"In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT's effectiveness in accurately segmenting Thai Landsat-8 data.","tags":["Remote Sensing","Landsat-8","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Vision Transformers","Transformer","Multi-branch Architectures","Mixed-scale Convolutional Feedforward Networks"],"title":"MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand","type":"publication"},{"authors":["Teerapong Panboonyuen","N. Nithisopa","P. Pienroj","L. Jirachuphun","C. Watthanasirikrit","N. Pornwiriyakul"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"ecd0bcce1705e21b780021bb64e0e45a","permalink":"https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/","section":"publication","summary":"Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at https://github.com/kaopanboonyuen/MARS.","tags":["Attention","Self-Attention","MARS","Sequential Quadtree Nodes","Mask R-CNN","PointRend","Mask Transfiner"],"title":"MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation","type":"publication"},{"authors":["Teerapong Panboonyuen","S. Thongbai","W. Wongweeranimit","P. Santitamnont","K. Suphan","C. Charoenphon"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"4adfc7217759e6906ac85e560b8c385f","permalink":"https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/object-detection-of-road-assets-using-transformer-based-yolox/","section":"publication","summary":"Detecting varying-sized objects, such as kilometer stones, is challenging and impacts accuracy. This paper enhances YOLO with two main contributions, using a pre-trained Vision Transformer (ViT) to fine-tune model weights for road asset images and incorporating Feature Pyramid Network (FPN) decoders to handle different input features. Our method, Transformer-Based YOLOX with FPN, outperforms state-of-the-art detectors, achieving 61.5% AP on the Thailand highway corpus, surpassing YOLOv5L by 2.56% AP.","tags":["Attention","Self-Attention","MARS","Sequential Quadtree Nodes","Mask R-CNN","PointRend","Mask Transfiner"],"title":"Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1606827600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606827600,"objectID":"85d93937200093837b1d1a85f626df23","permalink":"https://kaopanboonyuen.github.io/talk/achieve-data-science-first-meet/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/talk/achieve-data-science-first-meet/","section":"event","summary":"I was invited to speak at the \"Achieve Data Science First Meet\" for a MOOC student project event, where I highlighted the growing recognition of data science, AI, and machine learning's importance across various industries. I advised that organizations, regardless of their size or sector, must effectively develop and implement data science capabilities to stay competitive in the era of big data, or risk falling behind.","tags":[],"title":"Achieve Data Science First Meet","type":"event"},{"authors":[],"categories":null,"content":" ","date":1596200400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596200400,"objectID":"d7024594cc6cb801b6ae9f39b7862ecf","permalink":"https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/","publishdate":"2020-07-31T00:00:00Z","relpermalink":"/talk/ph.d.-thesis-defense/","section":"event","summary":"My thesis defense at the Faculty of Engineering, Chulalongkorn University. This dissertation introduces a new architecture for remote sensing, featuring Global Convolutional Network (GCN), channel attention, domain-specific transfer learning, Feature Fusion (FF), and Depthwise Atrous Convolution (DA). Tests on Landsat-8 and ISPRS Vaihingen datasets show that this model significantly outperforms the baseline.","tags":[],"title":"Ph.D. Thesis Defense","type":"event"},{"authors":["Teerapong Panboonyuen","P. Rakwatin","K. Intarat"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"3cc38a178687941b0f68b09b6ba4eff2","permalink":"https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/enhanced-feature-pyramid-vision-transformert/","section":"publication","summary":"Semantic segmentation on Landsat-8 data is crucial in the integration of diverse data, allowing researchers to achieve more productivity and lower expenses. This research aimed to improve the versatile backbone for dense prediction without convolutions‚Äînamely, using the pyramid vision transformer (PRM-VS-TM) to incorporate attention mechanisms across various feature maps. Furthermore, the PRM-VS-TM constructs an end-to-end object detection system without convolutions and uses handcrafted components, such as dense anchors and non-maximum suspension (NMS). The present study was conducted on a private dataset, i.e., the Thailand Landsat-8 challenge. There are three baselines, DeepLab, Swin Transformer (Swin TF), and PRM-VS-TM. Results indicate that the proposed model significantly outperforms all current baselines on the Thailand Landsat-8 corpus, providing F1-scores greater than 80% in almost all categories. Finally, we demonstrate that our model, without utilizing pre-trained settings or any further post-processing, can outperform current state-of-the-art (SOTA) methods for both agriculture and forest classes.","tags":["Mon-Maximum Suspension","Transfer Learning","Vision Transformer","Remote Sensing","Landsat-8","Transformer"],"title":"Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus","type":"publication"},{"authors":["K. Thitisiriwech","Teerapong Panboonyuen","P. Kantavat","Y. Iwahori","B. Kijsirikul"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"dbdbb2d13060b56b1c4b922f50cd5ad4","permalink":"https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/the-bangkok-urbanscapes-dataset/","section":"publication","summary":"This paper addresses semantic segmentation for autonomous driving systems, focusing on self-driving cars in Thailand. We introduce DeepLab-V3-A1 with Xception, an enhanced version of DeepLab-V3+, and present the Bangkok Urbanscapes dataset. Our method improves segmentation accuracy by refining the decoder and modifying the Xception backbone. Experiments on four datasets, including CamVid, Cityscapes, IDD, and our proposed dataset, show our approach performs comparably to baseline methods. Our dataset includes 701 annotated images of various Bangkok driving environments, covering eleven semantic classes. The architecture and dataset aim to aid developers in improving autonomous driving systems for diverse urban conditions. Implementation codes and dataset are available at [https://kaopanboonyuen.github.io/bkkurbanscapes](https://kaopanboonyuen.github.io/bkkurbanscapes).","tags":["DeepLab","Bangkok Urbanscapes Dataset","Xception","Cityscapes"],"title":"The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"4428c44f98c396bbd30d518b58b80be8","permalink":"https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/transformer-based-decoder-designs-for-semantic-segmentation/","section":"publication","summary":"Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% ùêπ1 score), Thailand North Landsat-8 corpus (63.12% ùêπ1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.","tags":["Transformer","Semantic Segmentation","Decoder Design","Swin Transformer","Vision Transformer","Self-Attention","Global Convolutional Network"],"title":"Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"08bf283a12b51cb06a21958c8492cd7d","permalink":"https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/","section":"publication","summary":"My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.","tags":["Convolutional Neural Networks","Landsat-8","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Global Convolutional Network","Encoder-Decoder Networks","ISPRS Vaihingen"],"title":"Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"6e6a1251e5de03c8b0aadd7a8242a7b8","permalink":"https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/","section":"publication","summary":"This paper addresses improving semantic segmentation in remote sensing for aerial and satellite images, which is crucial for agriculture, map updates, route optimization, and navigation. We propose enhancements to the state-of-the-art Enhanced Global Convolutional Network (GCN152-TL-A) by introducing a High-Resolution Representation (HR) backbone for better feature extraction, Feature Fusion (FF) to capture low-level details, and Depthwise Atrous Convolution (DA) for refined multi-resolution features. Experiments on Landsat-8 and ISPRS Vaihingen datasets demonstrate our model's superior performance, achieving over 90% accuracy in F1 scores and outperforming baseline models.","tags":["Feature Fusion","Transfer Learning","Remote Sensing","ISPRS Vaihingen Dataset"],"title":"Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"05de61049c7b26380a42ed51f66573c2","permalink":"https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/","section":"publication","summary":"In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, e.g., river, building, forest, etc., on raster images. A deep convolutional encoder‚Äìdecoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second, ‚Äúchannel attention‚Äù is presented in our network in order to select the most discriminative filters (features). Third, ‚Äúdomain-specific transfer learning‚Äù is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets (i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of ùêπ1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively.","tags":["Global Convolutional Network","Transfer Learning","Channel Attention","Remote Sensing","Discriminative Filters"],"title":"Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning","type":"publication"},{"authors":["I. Wichakam","Teerapong Panboonyuen","C. Udomcharoenchaikit","P. Vateekul"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"7ae277d2610b5783ef94eba07969563a","permalink":"https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/","section":"publication","summary":"Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.","tags":["Colorectal Cancer","Fully Convolutional Network","CVC-EndoSceneStill","Colonoscopy Video"],"title":"Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"fca7fa3fb9e06fccd9699f13b4e415fe","permalink":"https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/road-segmentation-on-aerial-imagery/","section":"publication","summary":"In this paper, we introduce an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Enhancements include the use of ELU (exponential linear unit) instead of ReLU, dataset augmentation with incrementally-rotated images to increase training data by eight times, and the use of landscape metrics to remove false road objects. Tested on the Massachusetts Roads dataset, our method outperformed the SegNet benchmark and other baselines in precision, recall, and F1 scores.","tags":["Remote Sensing","Road Segmentation","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Exponential Linear Unit"],"title":"An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b626b42b6e307c281ff5e448e98cd9b0","permalink":"https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/road-map-extraction-from-satellite-imagery/","section":"publication","summary":"Road map extraction is vital for GIS and underpins many location-based applications like GPS navigation, delivery route planning, tourist attraction locating, and location-based marketing. This research uses satellite imagery, though other remotely sensed images like aerial photographs, UAVs, or drones are also applicable. Despite various proposed methods focusing primarily on accuracy, completeness of results is equally important. We enhance accuracy by incorporating connected component analysis and improve completeness using landscape metrics, which describe spatial characteristics through shape and isolation indices. Evaluated on precision, recall, quality, and F1 scores, our method achieves over 90% performance in all criteria.","tags":["Road Segmentation","Connected Component Analysis","Image Processing"],"title":"Road map extraction from satellite imagery using connected component analysis and landscape metrics","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://kaopanboonyuen.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]