[{"authors":null,"categories":null,"content":"My research focuses on Learning Representations‚Äîdeveloping cutting-edge algorithms with optimization theory to push AI\u0026rsquo;s limits. I work with advanced models like GANs and Diffusion Models, leverage Self-Supervised Learning, explore how Adversarial Attacks on Large Language Models (LLMs) could reshape the future of AI.\nI am currently a Senior Research Scientist at MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory) and a C2F High-Potential Postdoc at Chulalongkorn University. I received my Ph.D. in Computer Engineering from Chulalongkorn University, where I specialized in AI. On top of that, I‚Äôm the founder of MYRIDA.\nPassionate about Cognitive Intelligence and unlocking human potential, I‚Äôm also deeply immersed in Geospatial Intelligence, where LLMs uncover groundbreaking insights that reshape how we understand and interact with our world.\nDetailed summaries of my academic, industry, and teaching experience can be found in my CV or IEEE Biography, and get a glimpse into my personal life on my blog and tumblr. By the way, feel free to vibe to my music on SoundCloud.\nThai name: ‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô, aka Kao Panboonyuen, or just Kao (‡πÄ‡∏Å‡πâ‡∏≤).\nDownload my CV. Know Me in a Minute.\n","date":1753660800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1753660800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/teerapong-panboonyuen/","section":"authors","summary":"My research focuses on Learning Representations‚Äîdeveloping cutting-edge algorithms with optimization theory to push AI\u0026rsquo;s limits. I work with advanced models like GANs and Diffusion Models, leverage Self-Supervised Learning, explore how Adversarial Attacks on Large Language Models (LLMs) could reshape the future of AI.","tags":null,"title":"Teerapong Panboonyuen","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents What you will learn Program overview Courses in this program Meet your instructor FAQs What you will learn Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program Basic Python Build a foundation in Python. Visualization Learn how to visualize data with Plotly. Statistics Introduction to statistics for data science. Meet your instructor Teerapong Panboonyuen FAQs Are there prerequisites? There are no prerequisites for the first course.\nHow often do the courses run? Continuously, at your own pace.\nBegin the course ","date":1706054400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1706054400,"objectID":"9175b58bdbbc0b3316bae12a6c70af53","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/","publishdate":"2024-01-24T00:00:00Z","relpermalink":"/courses/learn-data-science/","section":"courses","summary":"Data Science for Beginners","tags":null,"title":"Data Science for Beginners","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n1-2 hours per week, for 8 weeks\nLearn Quiz What is the difference between lists and tuples? Lists\nLists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world'] Tuples\nTuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world') Is Python case-sensitive? Yes\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"e9ca400ea73091d69f8c17a54f939765","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/python/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Basic Python","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n1-2 hours per week, for 8 weeks\nLearn Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nWrite Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show() ","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"c8051497f5da2d7333291e1d5361c7b4","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/visualization/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\nThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$. Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"90a9a7e769286f383e6e8f7355492613","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/stats/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":["Teerapong Panboonyuen","C. Charoenphon","C. Satirapod"],"categories":null,"content":"\n","date":1753660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753660800,"objectID":"449a614959cc62f8bcaaf1bce71ef8c2","permalink":"https://kaopanboonyuen.github.io/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/","publishdate":"2025-07-28T00:00:00Z","relpermalink":"/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/","section":"publication","summary":"Road segmentation in remote sensing is crucial for applications like urban planning, traffic monitoring, and autonomous driving. Labeling objects via pixel-wise segmentation is challenging compared to bounding boxes. Existing weakly supervised segmentation methods often rely on heuristic bounding box priors, but we propose that box-supervised techniques can yield better results. Introducing GuidedBox, an end-to-end framework for weakly supervised instance segmentation. GuidedBox uses a teacher model to generate high-quality pseudo-masks and employs a confidence scoring mechanism to filter out noisy masks. We also introduce a noise-aware pixel loss and affinity loss to optimize the student model with pseudo-masks. Our extensive experiments show that GuidedBox outperforms state-of-the-art methods like SOLOv2, CondInst, and Mask R-CNN on the Massachusetts Roads Dataset, achieving an AP50 score of 0.9231. It also shows strong performance on SpaceNet and DeepGlobe datasets, proving its versatility in remote sensing applications. Code has been made available at https://github.com/kaopanboonyuen/GuidedBox.","tags":["weakly-supervised-learning","road-segmentation","remote-sensing","teacher-student-framework","pseudo-labeling","instance-segmentation","computer-vision","geospatial-ai","mask-generation","noise-aware-learning"],"title":"GuidedBox: A segmentation-guided box teacher-student approach for weakly supervised road segmentation","type":"publication"},{"authors":[],"categories":null,"content":" üìù Interested in the full technical walkthrough? Read the complete CU-ICU LLM blog post here.\nüé§ You can download the full presentation slides from my CU-ICU oral talk at TSCCM 2025 here.\nüìÑ Curious to dive deeper into the research? You can also read the full CU-ICU paper on arXiv here.\n","date":1752759000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752759000,"objectID":"a34d1bb0637e0c39097755387d680df2","permalink":"https://kaopanboonyuen.github.io/talk/tsccm2025-the-14th-critical-care-conference/","publishdate":"2025-07-17T13:30:00Z","relpermalink":"/talk/tsccm2025-the-14th-critical-care-conference/","section":"event","summary":"I was honored to be selected‚Äîamong only 8 individuals‚Äîfor an oral presentation at the 14th Critical Care Conference. My talk focused on CU-ICU, a Thai-language instruction-tuned model designed to assist ICU practitioners. Built on the T5 architecture, the model was fine-tuned using efficient techniques such as LoRA, AdaLoRA, and IA3. CU-ICU integrates evidence-based clinical guidelines, including those from the Surviving Sepsis Campaign, to support clinical reasoning.","tags":[],"title":"TSCCM2025 (The 14th Critical Care Conference)","type":"event"},{"authors":["Teerapong Panboonyuen"],"categories":["medical-language-models","peft-for-healthcare","ai-in-critical-care"],"content":" ‚ÄúMy passion is dedicated to supporting ICU healthcare professionals in Thailand by building accessible AI tools‚Äîthis is an independent project with no external funding.‚Äù\nüö® The ICU Challenge Large Language Models (LLMs) like ChatGPT and Google Gemini have taken the world by storm‚Äîbut adapting them to real-world ICU settings is another story.\nIn the ICU, time is critical. Clinical staff need:\nFast predictions (like early sepsis detection), Accurate mortality risk estimation, and Understandable, clinically relevant explanations. But here\u0026rsquo;s the catch: hospitals‚Äîespecially in countries like Thailand‚Äîoften lack large labeled datasets or GPU-rich infrastructure.\nProud to finally share CU-ICU ‚Äî my latest work on customizing instruction-finetuned language models for Thai ICU doctors. üßë‚Äç‚öïÔ∏èüí°\nBased on T5 + LoRA/AdaLoRA/IA3 ‚Äî optimized for clinical reasoning \u0026amp; real-world care.\nüìñ Full blog:https://t.co/hMrwI0gN5l\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) July 18, 2025 üí° Introducing CU-ICU CU-ICU is my proposed framework that adapts large instruction-finetuned LLMs like T5 for ICU tasks, using sparse parameter-efficient fine-tuning (PEFT) techniques. It balances accuracy, interpretability, and efficiency.\nI evaluated it on:\nSepsis Detection Mortality Prediction Clinical Note Generation With three leading PEFT strategies:\nüß© LoRA üìâ AdaLoRA üß† (IA)$^3$ Figure 1: The moment I stepped onto the stage at TSCCM 2025 to present my research on CU-ICU‚Äîmy heart was racing, but my passion was louder. This was not just any room; it was Conference Room 1 on the 12th floor of Bhumisiri Mangkhalanusorn Building at King Chulalongkorn Memorial Hospital, filled with top clinicians and medical experts. It felt deeply meaningful to return to a place that shaped my academic path, this time to introduce an AI model designed not for global trends, but for Thai hospitals, Thai doctors, and Thai patients. CU-ICU was built from the heart‚Äîwith no big budget, no flashy grants‚Äîjust the desire to make something useful and real.\nFigure 2: Out of all the names listed for oral presentations, mine came first‚Äîtalk about pressure! As I scanned the schedule, I realized something fascinating: I was the only one presenting on AI, LLMs, and NLP. The rest were brilliant case studies and clinical research from doctors around Thailand and Asia. It was humbling to stand out not because of prestige, but because I represented a new kind of voice‚Äîa bridge between AI and healthcare, speaking to an audience of physicians about how ML might ease their daily workload and complement their medical judgment.\nFigure 3: The poster session lineup at TSCCM 2025 read like a roadmap of cutting-edge clinical practice. Most submissions came from hospitals across Japan, rich in medical case studies, practical findings, and cross-hospital comparisons. While CU-ICU wasn‚Äôt in this section, it made me realize the breadth of collaboration and knowledge-sharing that exists across borders in the healthcare research community. The energy was global, but the mission was deeply human: to make care better.\nFigure 4: Why build CU-ICU? The answer is simple: passion. It wasn‚Äôt born from a funded research project or a corporate mandate. It came from listening to doctors‚Äîhearing how repetitive and time-consuming many of their information-related tasks can be. CU-ICU is a Thai-built large language model (LLM) designed specifically to support local medical professionals. Its goal is not to replace anyone, but to serve as an AI assistant that understands the rhythm of a Thai ICU. By answering common clinical questions, it gives doctors more time to focus on the patient‚Äîthe human behind the data. This moment of sharing that ‚Äòwhy‚Äô with a room full of doctors was the most meaningful part of the talk for me.\nFigure 5: CU-ICU is built on the FLAN-T5 architecture and fine-tuned using lightweight adaptation techniques like LoRA, AdaLoRA, and IA3. Coming from a computer vision background, diving into language models was a leap‚Äîbut a thrilling one. The idea of building an AI model that could \"speak medicine\" in Thai was both technically challenging and emotionally rewarding. I even asked the audience‚Äîdoctors from various hospitals‚Äîhow many had used tools like ChatGPT, Gemini, or DeepSeek. Almost every hand went up. That moment confirmed: the future of medicine and LLMs are already intersecting. CU-ICU wants to be part of that conversation, in my own language.\nüß™ Results Snapshot I fine-tuned FLAN-T5 on ICU datasets using 16-shot prompts. Here‚Äôs what CU-ICU achieved:\nüîç Prediction Tasks Method Config Params (%) Sepsis Acc. Mortality Acc. (IA)$^3$ (All Layers) N/A 0.9% 85.6% 80.2% AdaLoRA 1.0 Budget, Rank=8 2.9% 83.5% 80.9% LoRA Rank=16 6.2% 83.1% 79.6% ‚úÖ CU-ICU achieves up to 85.6% accuracy in sepsis detection‚Äîwith less than 1% of model weights updated!\nüìù Clinical Note Quality Method Config nBERTScore Avg Score (IA)$^3$ (All Layers) N/A 32.1 66.0 AdaLoRA Rank=16 30.6 65.0 LoRA Rank=16 28.3 63.7 üß† CU-ICU‚Äôs best configuration generated the most clinically relevant explanations, evaluated via nBERTScore.\nFigure 6: CU-ICU currently achieves about 66% accuracy on my curated ICU question dataset. It's not perfect, but it's a beginning. With more collaboration‚Äîespecially from Thai doctors who can contribute anonymized data or clinical QA patterns‚ÄîI believe CU-ICU can evolve into something truly impactful. But I also emphasized that the model must remain ethical, inclusive, and safe. Biases must be addressed. Feedback must be welcomed. This is not just an AI system; it‚Äôs a co-created medical assistant, shaped by the people it aims to help.\nFigure 7: A powerful moment from CU-ICU in action. The model receives a real-world ICU prompt: \"56M, temp 39.5¬∞C, MAP 60, HR 132, 20, lactate 4.0, no urine in 6 hrs. Sepsis?\" Its response? Not just accurate‚Äîbut clinically impressive: \"Strong evidence of septic shock: persistent hypotension (MAP \u0026lt; 65), hyperlactatemia, and oliguria. Immediate antibiotics and fluid bolus recommended per Surviving Sepsis Campaign (2021).\" What makes this remarkable isn‚Äôt just the correctness, but the way CU-ICU integrates multi-symptom reasoning, adheres to formal clinical guidelines, and even offers actionable next steps. In this moment, it felt like CU-ICU wasn't just a research model‚Äîit was a prototype for the kind of AI assistant Thai doctors could one day rely on in the most critical of decisions. ü§ñ Why It Works CU-ICU leverages:\nInstruction tuning (T5 + FLAN-like prompts) Few-shot learning (16-shot prompts) Sparse fine-tuning using LoRA, AdaLoRA, and (IA)$^3$ This makes the model:\nLightweight ü™∂ Accurate üìà Interpretable üîç All without expensive GPU clusters.\nüåè A Step Toward Thai Hospital AI Most Thai hospitals operate under resource constraints. My goal with CU-ICU is to show:\nYou can still deploy LLM-based clinical AI, With just a few labels, smart tuning, and open-source models. I\u0026rsquo;m excited about bringing this into Thai ICU workflows to help overworked clinicians with decision support tools that actually make sense in practice.\nFigure 8: My certificate for the oral session‚Äîa proud reminder that I was one of just eight selected presenters. The other seven? All medical doctors. That made it even more special. Being the only AI researcher talking about LLMs in a sea of clinicians reminded me that technology must meet people where they are. One of the most heartwarming moments came afterward, when a senior doctor complimented the work. And perhaps even more emotional‚Äîthis was my return to Chulalongkorn University, my Ph.D. alma mater, after nearly five years. Walking the familiar halls again, this time as a speaker, I felt immense gratitude. Chula gave me the roots, and CU-ICU is one of the branches I‚Äôve grown.\nüé§ You can download the full presentation slides from my CU-ICU oral talk at TSCCM 2025 here.\nüìÑ Curious to dive deeper into the research? You can also read the full CU-ICU paper on arXiv here.\nüìö References (selected) LoRA (Hu et al., 2021) FLAN-T5 (Chung et al., 2022) PaLM (Chowdhery et al., 2022) MedPaLM (Singhal et al., 2023) ChatGPT (OpenAI, 2023) Gemini (Google DeepMind, 2024) Citation Panboonyuen, Teerapong. (July 2025). CU-ICU: Customizing Instruction-Tuned Language Models for Critical Care. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/\nFor a BibTeX citation:\n@article{panboonyuen2025cuicu, title = \u0026quot;CU-ICU: Customizing Instruction-Tuned Language Models for Critical Care\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2025\u0026quot;, month = \u0026quot;Jul\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/\u0026quot; } Did you find this page helpful? Consider sharing it üôå References Raffel, Colin, et al. \u0026ldquo;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\u0026rdquo; Journal of Machine Learning Research 21.140 (2020): 1-67.\nHu, Edward J., et al. \u0026ldquo;LoRA: Low-Rank Adaptation of Large Language Models.\u0026rdquo; arXiv preprint arXiv:2106.09685 (2021).\nWang, Shao, et al. \u0026ldquo;AdaLoRA: Adaptive Low-Rank Adaptation for Efficient Fine-tuning.\u0026rdquo; arXiv preprint arXiv:2208.07339 (2022).\nGuo, Ruiyi, et al. \u0026ldquo;(IA)¬≥: Trainable Multiplicative Masks for Parameter-Efficient Fine-Tuning.\u0026rdquo; arXiv preprint arXiv:2206.10169 (2022).\nLee, Jinhyuk, et al. \u0026ldquo;BioBERT: a pre-trained biomedical language representation model for biomedical text mining.\u0026rdquo; Bioinformatics 36.4 (2020): 1234-1240.\nHuang, Kexin, et al. \u0026ldquo;ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission.\u0026rdquo; arXiv preprint arXiv:1904.05342 (2019).\nSinghal, Karan, et al. \u0026ldquo;Large Language Models Encode Clinical Knowledge.\u0026rdquo; arXiv preprint arXiv:2212.13138 (2022).\nJin, Dongmin, et al. \u0026ldquo;MedAlpaca: A Medical Instruction-Finetuned Large Language Model.\u0026rdquo; arXiv preprint arXiv:2304.12140 (2023).\nLester, Brian, et al. \u0026ldquo;Powerful few-shot learning with frozen language models and pattern-tuning.\u0026rdquo; arXiv preprint arXiv:2107.13586 (2021).\nBrown, Tom B., et al. \u0026ldquo;Language models are few-shot learners.\u0026rdquo; Advances in Neural Information Processing Systems 33 (2020): 1877-1901.\n","date":1752710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752710400,"objectID":"396b9ce8621ef34595f97c6fe738f1b2","permalink":"https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/","publishdate":"2025-07-17T00:00:00Z","relpermalink":"/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/","section":"blog","summary":"Fine-tuning FLAN-T5 with parameter-efficient methods to adapt LLMs for high-stakes ICU tasks like sepsis detection, mortality prediction, and clinical note generation.","tags":["clinical-llms","parameter-efficient-finetuning","peft","medical-ai","prompt-learning","flan-t5","healthcare-innovation","sepsis-detection","icu-mortality-prediction"],"title":"CU-ICU: Customizing Instruction-Tuned Language Models for Critical Care","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"\n","date":1752710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752710400,"objectID":"10be1eef1d6a54a18c23d9ed9681218c","permalink":"https://kaopanboonyuen.github.io/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/","publishdate":"2025-07-17T00:00:00Z","relpermalink":"/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/","section":"publication","summary":"Integrating large language models into specialized domains like healthcare presents unique challenges, including domain adaptation and limited labeled data. We introduce CU-ICU, a method for customizing unsupervised instruction-finetuned language models for ICU datasets by leveraging the Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse fine-tuning approach that combines few-shot prompting with selective parameter updates, enabling efficient adaptation with minimal supervision. Our evaluation across critical ICU tasks‚Äîearly sepsis detection, mortality prediction, and clinical note generation‚Äîdemonstrates that CU-ICU consistently improves predictive accuracy and interpretability over standard fine-tuning methods. Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and a 20% enhancement in generating clinically relevant explanations while updating fewer than 1% of model parameters in its most efficient configuration. These results establish CU-ICU as a scalable, low-overhead solution for delivering accurate and interpretable clinical decision support in real-world ICU environments.","tags":["clinical-llms","parameter-efficient-finetuning","peft","medical-ai","prompt-learning","flan-t5","healthcare-innovation","sepsis-detection","icu-mortality-prediction"],"title":"CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":["geospatial-ai","remote-sensing","ai-enhanced-lulc"],"content":"Hi, it‚Äôs me again, Kao Panboonyuen‚Äîwelcome back to my blog!\nToday, I‚Äôm diving into a topic that‚Äôs been on my mind a lot lately: the exciting intersection of Remote Sensing and Large Language Models (LLMs). As you know, remote sensing is one of the most powerful tools for understanding the Earth‚Äôs surface, using satellite systems like Sentinel-2, Landsat-8, and THEOS to capture tons of high-resolution data. From monitoring environmental changes to assisting in urban planning, this data has countless applications. But here‚Äôs the catch‚Äîwhile we have all this amazing data at our fingertips, its sheer volume and complexity can overwhelm traditional methods of data classification.\nThe world is changing fast, and technology is evolving at a rate that we can barely keep up with. But with every challenge comes an opportunity‚Äîan opportunity to innovate, adapt, and transform how we understand and interact with the world around us. The marriage of Remote Sensing and Large Language Models (LLMs) is one such opportunity. It‚Äôs a chance to take the immense potential of satellite data and push it to new heights, uncovering insights that we once thought were out of reach.\nAs we stand on the cusp of a new frontier in data analysis, it‚Äôs exciting to think about how these advancements will not only revolutionize industries but also make a tangible difference in solving real-world problems. Join me on this journey as we explore the incredible potential of combining cutting-edge technology with the power of Earth observation. The future is here, and it‚Äôs incredibly bright.\nA new era of multimodal intelligence is upon us! üåçüöÄ\n‚ÄúVision‚ÄìLanguage Models for Remote Sensing: A New Era of Multimodal Understanding‚Äù\nI‚Äôve just finished writing it‚Äîcheck out my latest post here:https://t.co/u9lmpCzKfU\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) July 7, 2025 Older algorithms like Support Vector Machines (SVM) and Random Forest (RF) have done a decent job, but they often struggle to tap into the deeper semantic meaning within the data. They can classify pixels, sure, but they can‚Äôt grasp the rich, contextual relationships between them.\nThat‚Äôs where LLMs come in! These models, originally built for natural language processing, have the power to revolutionize remote sensing by interpreting and analyzing data on a much deeper level. By integrating contextual insights from satellite metadata and environmental reports, LLMs can enhance tasks like LULC (Land Use/Land Cover) classification and image analysis. The results? Much more accurate and insightful interpretations of satellite imagery, opening up a whole new world of possibilities for remote sensing applications.\nRemote sensing has become an indispensable tool for gaining detailed insights into the Earth\u0026rsquo;s surface, with satellite systems like Sentinel-2, Landsat-8, and THEOS providing an endless stream of high-resolution data. From environmental monitoring to urban planning, this data fuels critical applications across a range of sectors. Yet, the vast volume and complexity of satellite imagery can present a significant challenge. Traditional methods of data classification, such as Support Vector Machines (SVM) and Random Forest (RF), have delivered valuable results but are often unable to capture the full semantic richness embedded within this data. They excel at processing large datasets, but struggle when tasked with understanding the deeper, contextual relationships between the elements within these images.\nEnter the era of Large Language Models (LLMs), which are revolutionizing the way we process and interpret remote sensing data. Originally designed to understand and generate human-like language, LLMs have demonstrated an incredible capacity to enhance data analysis by integrating contextual information from multiple sources‚Äîsuch as environmental reports, satellite metadata, and geospatial context. This ability to handle multimodal data opens up new avenues for more accurate Land Use/Land Cover (LULC) classification and image interpretation, driving improvements in remote sensing applications that were once constrained by traditional algorithms.\nIn a recent Twitter post, Lilian Weng excitedly shared the launch of Thinking Machines Lab, a cutting-edge AI research and product company dedicated to pushing the boundaries of multimodal understanding. As she mentions, the lab is home to some of the brightest minds behind innovations like ChatGPT. Their focus on multimodal AI is particularly relevant to the work being done with Vision‚ÄìLanguage Models (VLMs), which are rapidly transforming how we analyze remote sensing data. Just like Thinking Machines Lab aims to integrate various AI disciplines to achieve greater synergy, VLMs are creating new possibilities for understanding satellite imagery by combining computer vision and natural language processing. This shift not only enhances our ability to interpret complex data but also paves the way for more intuitive, human-like understanding of the world around us. To learn more about Thinking Machines Lab and their vision, check out Thinking Machines.\nThis is something we have been cooking together for a few months and I\u0026#39;m very excited to announce it today.\nThinking Machines Lab is my next adventure and I\u0026#39;m feeling very proud and lucky to start it with a group of talented colleagues. Learn more about our vision at‚Ä¶ https://t.co/eKQYvuwurB\n\u0026mdash; Lilian Weng (@lilianweng) February 18, 2025 Vision‚ÄìLanguage Models (VLMs) In recent years, Vision‚ÄìLanguage Models (VLMs) have emerged as a groundbreaking tool in remote sensing, particularly in enhancing the interpretation of satellite imagery. These models merge visual data with linguistic understanding, offering a more holistic approach to analyzing complex remote sensing data.\nOne of the most exciting developments in this area is how VLMs can enrich traditional remote sensing datasets. While traditional datasets often rely purely on raw imagery, VLMs leverage contextual relationships between visual inputs and textual data, providing more nuanced insights. The integration of textual descriptions or geospatial metadata alongside imagery allows for a deeper understanding of land use, land cover changes, and other environmental factors. This ability is showcased in a comparative visualization, where you can clearly see how VLM datasets offer richer and more detailed information than traditional methods.\nThe power of VLMs lies not just in their ability to process images, but also in their proficiency at handling a variety of tasks simultaneously. For instance, these models can assist in tasks such as object detection, classification, and even scene understanding. A representation of these tasks reveals just how versatile VLMs can be across different domains of remote sensing. From simple land cover classification to more complex tasks like environmental monitoring, VLMs are equipped to tackle them all.\nAs we dig deeper into the structure of VLMs, it becomes clear that they come in various forms, each designed to handle specific challenges in remote sensing. For example, contrastive models focus on matching images with descriptive text, while conversational models are more dynamic, enabling interactive querying and real-time interpretation of satellite imagery. This distinction in architecture allows VLMs to be adaptable to a wide range of applications, from automatic image captioning to detailed environmental analysis.\nIn addition to their core functionality, enhancement techniques are often employed to refine VLM performance. Some layers of the model are fine-tuned to improve precision, while others are kept frozen to preserve previously learned features. These techniques are crucial for boosting model efficiency without overfitting, ensuring that VLMs can be effectively applied to large-scale remote sensing tasks.\nThe growing interest in VLMs is also reflected in the increasing number of academic publications dedicated to this field. Over the past few years, the volume of research in the intersection of VLMs and remote sensing has surged, reflecting the potential of these models to transform how we understand and analyze satellite data.\nFigure 1: A comparative analysis between traditional and Vision‚ÄìLanguage Model (VLM) datasets in the context of remote sensing. Traditional datasets typically rely on isolated visual data (such as satellite images) for classification and interpretation, often limited by the scope of raw pixel-based information. In contrast, VLM datasets integrate both visual and textual modalities, incorporating contextual information like environmental reports, geospatial metadata, and textual descriptions. This hybrid approach enhances the model's ability to capture complex relationships and nuanced patterns in satellite imagery, improving classification accuracy and providing richer insights for land use/land cover (LULC) analysis and other remote sensing tasks. (Image source: MDPI).\nAs shown in the visual comparison above, the shift from traditional datasets to VLM datasets in remote sensing is not just about increasing data volume but about enhancing the depth and accuracy of the insights derived from satellite imagery. The traditional approach provides basic visual data, while VLM datasets incorporate semantic understanding, making it easier to identify patterns, trends, and anomalies that would otherwise go unnoticed.\nOne of the remarkable aspects of VLMs is their ability to handle a wide variety of tasks. The tasks shown in the figure below range from simple image classification to more complex objectives such as environmental change detection and land use forecasting. With their integrated vision and language capabilities, VLMs can extract much more detailed and contextually relevant information from imagery than traditional models.\nFigure 2: A detailed overview of representative tasks handled by Vision‚ÄìLanguage Models (VLMs) in remote sensing. VLMs bridge the gap between visual and textual data, enabling them to perform complex multimodal tasks such as image captioning, object detection, and semantic segmentation. These models can analyze high-resolution satellite imagery while leveraging textual data like geospatial metadata, environmental descriptions, and sensor reports to provide a more comprehensive understanding of the landscape. For instance, VLMs are increasingly used in land use/land cover (LULC) classification, change detection, and disaster monitoring, offering enhanced performance over traditional methods by incorporating contextual information to reduce ambiguity. (Image source: MDPI).\nThe flexibility of VLMs in handling such diverse tasks opens new possibilities in remote sensing, where the understanding of both imagery and textual data is vital for accurate analysis. The synergy between vision and language also plays a key role in making sense of complex environmental data.\nWhen we look at the underlying architecture of VLMs, it‚Äôs clear that they are not a one-size-fits-all solution. Models can either use a contrastive approach, focusing on pairing images with descriptions, or a more conversational approach, which allows for interactive querying of imagery. This flexibility is key for the adaptability of VLMs in real-world applications, whether it\u0026rsquo;s for immediate analysis or long-term monitoring.\nFigure 3: A detailed breakdown of the architectures for contrastive (left) and conversational (right) Vision‚ÄìLanguage Models (VLMs), two key paradigms that drive the integration of visual and textual data in remote sensing. The contrastive architecture utilizes a dual encoder framework where image features and textual features are independently extracted and then aligned in a common multimodal space. This design is particularly effective in tasks requiring fine-grained matching, such as image captioning and cross-modal retrieval. On the other hand, the conversational architecture builds upon the contrastive model by introducing a dialogue-based approach, enabling more dynamic interactions between the visual content and natural language queries. This makes conversational models ideal for applications like interactive mapping, question-answering systems, and real-time disaster monitoring, where context and user input are continually evolving. The figure contrasts these two architectures to highlight how they each excel in different aspects of remote sensing tasks. (Image source: MDPI).\nThis versatility in architecture makes VLMs well-suited to tackle a broad spectrum of remote sensing tasks. Whether you need to extract simple patterns or generate more detailed, context-rich analyses, the choice of model architecture significantly impacts the output.\nFurthermore, as with all machine learning models, VLMs require careful optimization to ensure they perform at their best. Enhancement techniques, like fine-tuning certain layers and keeping others frozen, are used to strike the right balance between model complexity and efficiency. These strategies help to refine VLMs for specific tasks, improving accuracy and minimizing computational overhead, as illustrated in the figure below.\nFigure 4: A visual representation of the various enhancement techniques employed in Vision‚ÄìLanguage Models (VLMs) to improve their performance and accuracy in remote sensing applications. These techniques include fine-tuning and transfer learning, where pre-trained models are further optimized to adapt to specific domain tasks. The diagram showcases the interplay between frozen and fine-tuned layers, with fine-tuning denoted by the fire icon, representing the model's ability to adapt to new data, and frozen layers indicated by the snowflake icon, maintaining the stability of pre-existing knowledge. By employing these enhancement strategies, VLMs can effectively improve their ability to handle complex tasks such as LULC classification, disaster detection, and environmental monitoring, all while reducing the risk of overfitting and improving generalization to unseen datasets. (Image source: MDPI).\nThese enhancement techniques are essential for optimizing VLMs for large-scale remote sensing tasks, enabling them to extract meaningful insights from satellite imagery without becoming too computationally expensive.\nThe growing body of research on VLMs reflects the increasing recognition of their potential in remote sensing. A recent graph shows a clear upward trend in the number of publications on the use of VLMs in this field, indicating that the scientific community is embracing these models as a valuable tool for future research and applications.\nFigure 5: A comprehensive breakdown of the various complex tasks addressed by Vision‚ÄìLanguage Models (VLMs) in the field of remote sensing. By combining both visual data (such as satellite imagery) and textual information (like metadata, sensor reports, and geospatial documentation), VLMs are revolutionizing the way we approach tasks like image captioning, land use/land cover (LULC) classification, and change detection. The integration of these modalities allows VLMs to not only process raw visual content but also to infer meaningful insights by incorporating contextual understanding. This synergy of vision and language empowers VLMs to enhance disaster monitoring, environmental analysis, and climate change studies, offering unprecedented accuracy and interpretability in remote sensing applications. (Image source: MDPI).\nAs the field of VLMs in remote sensing continues to expand, it‚Äôs clear that these models will play a pivotal role in transforming how we analyze and interpret satellite imagery, driving new innovations and methodologies in environmental monitoring and beyond.\nUnderstanding Large Language Models (LLMs) LLMs, such as GPT-4, are trained on massive corpora of text data, enabling them to grasp the intricacies of human language. However, their capacity to process sequential information isn\u0026rsquo;t limited to just text. Recent studies have shown that LLMs, when appropriately adapted, can learn to analyze non-textual data types, such as images, by incorporating their textual understanding into feature extraction processes.\nIn remote sensing, LLMs can be adapted to the following roles:\nTextual Contextualization: LLMs can process and generate insights from external textual datasets, such as reports, maps, and metadata, which provide contextual information for satellite images.\nEnhanced Feature Extraction: By understanding the relationships between textual data and imagery, LLMs help derive semantic features that are difficult for traditional image processing algorithms to capture.\nMathematically, LLMs utilize attention mechanisms, specifically the Transformer architecture, which enables them to weigh different parts of the input sequence with varying importance. This attention mechanism can be defined as:\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\nWhere:\n$Q$ represents the query (textual or image features), $K$ represents the key (spatial or contextual attributes), $V$ is the value (output features or attention score), $d_k$ is the dimensionality of the key vectors. This process helps in focusing on relevant parts of the data, whether it be image patches or semantic concepts, which improves classification accuracy.\nLLMs in LULC Classification Land Use/Land Cover (LULC) classification is a fundamental task in remote sensing, involving the categorization of land regions into distinct types based on satellite imagery. Traditional methods, while powerful, often overlook the contextual understanding of features that can be provided by external data.\nTo improve accuracy, we employ a hybrid model that combines Convolutional Neural Networks (CNNs) with LLMs. The CNN extracts spatial features from satellite images, while the LLM extracts contextual information from external sources.\nMethodology The integration of LLMs in the LULC classification process can be broken down into the following steps:\nData Collection:\nHigh-resolution imagery is collected from platforms like Sentinel-2, Landsat-8, and THEOS, which offer multispectral and multisource data. Preprocessing:\nImages are corrected for atmospheric distortion and radiometrically normalized using techniques such as the Dark Object Subtraction (DOS) method to reduce atmospheric scattering. Feature Extraction:\nCNN architectures (such as ResNet-50 or EfficientNet) are applied to extract spatial features from satellite imagery. Simultaneously, large environmental corpora, such as land use reports, are processed by LLMs to extract contextual knowledge. Hybrid Model Training:\nThe CNN-derived spatial features and the LLM-derived contextual features are concatenated and fed into a fully connected neural network for final classification. Classification and Validation:\nAfter training, the model is applied to a test set of images. Metrics such as accuracy, precision, recall, and F1-score are used to evaluate the performance. A typical validation equation for precision and recall is given as: $$ \\text{Precision} = \\frac{TP}{TP + FP}, \\quad \\text{Recall} = \\frac{TP}{TP + FN} $$\nWhere:\n$TP$ is the number of true positives, $FP$ is the number of false positives, $FN$ is the number of false negatives. Case Study: Sentinel-2 Imagery Classification To demonstrate the power of LLMs in enhancing LULC classification, we present a case study using Sentinel-2 imagery. Sentinel-2 provides multispectral images at a spatial resolution of up to 10 meters, allowing for detailed land cover analysis.\nData Collection and Preprocessing Sentinel-2 images covering a diverse region were downloaded and preprocessed to correct atmospheric effects. Image reflectance values were normalized using the FLAASH (Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes) method to improve data consistency.\nFeature Extraction We applied ResNet-50 for feature extraction from the raw satellite imagery. For contextual understanding, an LLM (such as GPT-4) was fed with environmental reports and metadata related to the region to extract textual features that provide insights into land use policies, climate conditions, and historical land usage trends.\nHybrid Model Training The extracted features from both the CNN and LLM were combined into a hybrid model, trained using a multi-class cross-entropy loss function. The model achieved an accuracy improvement of 12% compared to traditional classification methods that relied solely on CNNs or SVMs.\nChallenges and Future Prospects While the integration of LLMs in remote sensing holds promise, several challenges remain. These include the need for large, high-quality datasets for training LLMs, the computational cost of training hybrid models, and the difficulty in obtaining ground truth data for validation.\nFuture research will focus on refining these models and exploring new techniques such as semi-supervised learning and transfer learning to further enhance performance.\nAs we continue to observe the rise of Vision‚ÄìLanguage Models (VLMs) in remote sensing, one notable trend is the growing body of academic work that explores their capabilities. The figure below demonstrates the increasing number of publications on VLMs in the field of remote sensing, reflecting the broad interest and potential for these models to shape the future of satellite imagery analysis. As the research landscape evolves, we can expect even more innovative approaches and applications to emerge.\nFigure 6: A comprehensive visualization of the growing number of publications focused on Vision‚ÄìLanguage Models (VLMs) in the field of remote sensing. This trend underscores the increasing interest in the integration of multimodal learning (combining visual and textual data) to enhance the accuracy and efficiency of remote sensing tasks, such as LULC classification, change detection, and disaster response. The upward trajectory of VLM research reflects both the advancements in AI model architecture and the expanding applicability of these models to complex, real-world geospatial challenges. The data presented here highlights the growing recognition of VLMs as a key technology in modern remote sensing workflows. (Image source: MDPI).\nIn the pursuit of enhancing model performance, LLaVA (Large Vision-and-Language Model) is one of the newer architectures that‚Äôs making waves in the remote sensing community. This innovative model is designed to seamlessly integrate both vision and language modalities, providing a rich foundation for sophisticated interpretations of satellite imagery. As illustrated below, LLaVA offers a more intuitive and interactive way of combining imagery with text, making it especially valuable in applications like land use classification, environmental monitoring, and more.\nFigure 7: A comprehensive illustration of LLaVA (Large Language and Vision Alignment), an advanced Vision‚ÄìLanguage Model (VLM) designed to seamlessly integrate visual and textual information. LLaVA enables fine-grained multimodal reasoning, allowing it to process complex datasets from diverse sources such as satellite imagery and environmental reports. By leveraging large-scale pre-trained models, LLaVA enhances performance in tasks like remote sensing and image classification through the fusion of visual cues and textual context. This approach has shown significant promise in enhancing model interpretability, particularly in applications where both visual and linguistic insights are crucial. (Image source: MDPI).\nAt the heart of many VLMs is the Transformer architecture, which has revolutionized how models process sequences of data, whether they be text, images, or both. The transformer model\u0026rsquo;s ability to handle long-range dependencies in data has made it the backbone of cutting-edge models like GPT, BERT, and, of course, Vision Transformers (ViTs). A diagram illustrating the Transformer architecture highlights how it efficiently handles both visual and linguistic data, making it a crucial component for tasks that require understanding of complex, multimodal data sets.\nFigure 8: A comprehensive illustration of the Transformer architecture, a breakthrough deep learning model that revolutionized sequence-based data processing. This architecture leverages self-attention mechanisms to capture long-range dependencies in data, which makes it highly effective in a wide range of tasks, including natural language processing and computer vision. It serves as the foundational model for Vision Transformers (ViTs) and Vision‚ÄìLanguage Models (VLMs) in remote sensing. (Image source: NeurIPS 2017).\nVision Transformers (ViTs) are one of the latest and most powerful iterations of the Transformer model, specifically designed to handle image data. Unlike traditional convolutional neural networks (CNNs), ViTs treat image patches as sequences, enabling them to capture both local and global features more effectively. The figure below offers an overview of the Vision Transformer model and its ability to process large-scale imagery data. Its application in remote sensing has shown promise, particularly for high-resolution satellite imagery classification, where understanding fine-grained patterns is crucial.\nFigure 9: Detailed model overview of the Vision Transformer (ViT), a cutting-edge deep learning architecture that has transformed the way image data is processed. Unlike traditional convolutional neural networks (CNNs), ViT divides an image into fixed-size patches and processes them as sequences, allowing the model to capture both local and global dependencies in an image simultaneously. This unique approach has led to impressive performance in high-resolution satellite image classification and other remote sensing tasks, making it a key model in the field of Vision‚ÄìLanguage Models (VLMs). ViT's scalability to handle large-scale image data have made it indispensable for remote sensing applications. (Image source: arXiv).\nConclusion The combination of LLMs with traditional remote sensing techniques offers a significant improvement in the accuracy and contextual understanding of LULC classification tasks. By leveraging both spatial feature extraction from CNNs and semantic contextualization from LLMs, remote sensing models can achieve more precise and meaningful results. This approach opens new avenues for analyzing satellite imagery and provides deeper insights into Earth‚Äôs surface processes, aiding in applications ranging from environmental monitoring to urban planning.\nThe integration of Vision‚ÄìLanguage Models (VLMs) into remote sensing is more than just a technological leap; it represents a paradigm shift in how we interpret and interact with satellite imagery. By combining the power of both visual data and linguistic context, VLMs open up new frontiers in land use/land cover (LULC) classification, environmental monitoring, and countless other applications. The innovative capabilities of these models are not just theoretical‚Äîthey are already being realized in practical, real-world scenarios, as evidenced by the growing body of research and the increasing number of publications in the field.\nThe results we\u0026rsquo;ve explored here demonstrate that VLMs are not just another tool‚Äîthey are a powerful bridge that connects the language of the Earth‚Äôs landscapes to the language of machine learning. With their ability to process complex multimodal data, they offer unprecedented accuracy, scalability, and efficiency in satellite data analysis. The rise of models like LLaVA and the Vision Transformer (ViT) exemplify this potential, illustrating how cutting-edge architecture is pushing the boundaries of what‚Äôs possible in remote sensing.\nWhat makes these models truly remarkable is their ability to not only analyze individual images but also to understand the nuanced context within those images. This contextual understanding is where traditional methods fall short, making VLMs a game-changer. Whether it\u0026rsquo;s distinguishing between subtle changes in land cover, detecting environmental anomalies, or improving the precision of automated classification, these models offer a level of sophistication that will likely define the future of remote sensing.\nAs we look ahead, it‚Äôs clear that VLMs are poised to drive significant advancements in the way we monitor, understand, and protect our planet. The intersection of AI, remote sensing, and language will continue to evolve, opening doors to new research, innovations, and applications. For professionals in the field of remote sensing, the question is no longer whether to adopt VLMs, but how soon and how deeply to integrate them into their workflows.\nIn this blog, we‚Äôve explored the methodologies, models, and real-world implications of integrating VLMs into remote sensing. The insights from this growing area of research suggest that Vision‚ÄìLanguage Models will not only enhance the precision and scope of satellite data interpretation but will also unlock new levels of meaning and insight that were previously unattainable. As we continue to refine these models and expand their capabilities, the potential for VLMs in remote sensing is limitless.\nIn conclusion, the integration of Vision‚ÄìLanguage Models with remote sensing is not just an exciting trend‚Äîit is the future of how we will analyze, interpret, and leverage satellite imagery to make better decisions for our planet‚Äôs future.\nAnd that‚Äôs a wrap for today! I hope you found this exploration into Vision‚ÄìLanguage Models and remote sensing insightful and inspiring. Whether you‚Äôre new to the field or a seasoned pro, I‚Äôm sure there are a few ideas or takeaways that can spark your next big project.\nThanks for reading‚Äîuntil next time, take care and keep pushing the boundaries of knowledge! üåç\nKao Panboonyuen\nCitation Panboonyuen, Teerapong. (July 2025). Vision‚ÄìLanguage Models for Remote Sensing: A New Era of Multimodal Understanding. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/\nFor a BibTeX citation:\n@article{panboonyuen2025vlmrsllms, title = \u0026quot;Vision‚ÄìLanguage Models for Remote Sensing: A New Era of Multimodal Understanding\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2025\u0026quot;, month = \u0026quot;Jul\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/\u0026quot;} Did you find this page helpful? Consider sharing it üôå References Tao, Lijie, et al. \u0026ldquo;Advancements in Vision‚ÄìLanguage Models for Remote Sensing: Datasets, Capabilities, and Enhancement Techniques.\u0026rdquo; Remote Sensing 17.1 (2025): 162.\nVaswani, Ashish, et al. \u0026ldquo;Attention is all you need.\u0026rdquo; Advances in Neural Information Processing Systems 30 (2017).\nDosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020).\nSchulman, John, et al. \u0026ldquo;Proximal policy optimization algorithms.\u0026rdquo; arXiv preprint arXiv:1707.06347 (2017).\nWeng, Lilian. \u0026ldquo;Thinking.\u0026rdquo; Lilianweng.github.io. May 1, 2025. https://lilianweng.github.io/posts/2025-05-01-thinking/\nThinking Machines. \u0026ldquo;Homepage.\u0026rdquo; Thinkingmachines.ai. https://thinkingmachines.ai/\n","date":1751673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751673600,"objectID":"894cc79f6a94e16ca130890eaff4e0b9","permalink":"https://kaopanboonyuen.github.io/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/","publishdate":"2025-07-05T00:00:00Z","relpermalink":"/blog/2025-07-05-visionlanguage-models-for-remote-sensing-a-new-era-of-multimodal-understanding/","section":"blog","summary":"Exploring how LLMs can enhance remote sensing applications like LULC and image classification using Sentinel, Landsat-8, and THEOS imagery.","tags":["ai-for-earth","multimodal-data","geospatial-intelligence","lulc-analysis","remote-sensing-advancements","satellite-imaging","vision-language-models","deep-learning-for-remote-sensing","environmental-monitoring"],"title":"Vision‚ÄìLanguage Models for Remote Sensing: A New Era of Multimodal Understanding","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":["computer-vision","deep-learning","automotive-ai","marsail"],"content":" You can explore our GitHub project page üì¶ here. ALBERT vs SLICK: The New AI Fashion at MARSAIL In Thailand‚Äôs rapidly evolving automotive insurance sector, the integration of transformer-based segmentation models like MARSAIL-ALBERT and MARSAIL-SLICK marks a pivotal shift toward scalable, AI-driven damage assessment‚Äîbringing unprecedented accuracy, efficiency, and trust to claim processing pipelines.\nIn addressing the complex challenge of fine-grained automotive damage segmentation, we present MARSAIL-ALBERT (Figure 1), a high-capacity teacher model architected on the principles of bidirectional transformer encoding and spatially-aware representation learning. Leveraging the power of attention mechanisms within a multi-scale encoder-decoder framework, MARSAIL-ALBERT excels at capturing subtle visual cues‚Äîscratches, dents, fractures‚Äîamid high-variance automotive imagery. It is explicitly optimized to model long-range dependencies across both local textures and global structural semantics, enabling precise localization of damage under varying viewpoints, illumination conditions, and occlusion levels.\nAs demonstrated in Figure 2, the model consistently produces highly detailed segmentation maps, revealing strong robustness to environmental perturbations such as specular highlights, cast shadows, and surface complexity. Functioning as the supervisory core of our framework, MARSAIL-ALBERT serves not only as a performant segmentation engine but also as a teacher network for structured knowledge distillation.\nFigure 1: The architecture of the proposed MARSAIL-ALBERT model, a teacher network that leverages advanced localization and bidirectional encoder representations from transformers for high-precision car damage segmentation. This design facilitates robust spatial reasoning and context-aware feature extraction critical for complex automotive insurance scenarios. Figure 2: Visualization of segmentation outcomes produced by MARSAIL-ALBERT, highlighting the model‚Äôs ability to localize fine-grained vehicle damage under varying lighting and occlusion conditions, showcasing its generalization capability across diverse automotive imagery. To extend this capability into real-time and resource-constrained settings‚Äîtypical of large-scale deployment in automotive insurance operations‚Äîwe introduce MARSAIL-SLICK (Figure 3), a compact yet powerful student model distilled directly from MARSAIL-ALBERT. It incorporates a novel Selective Localization mechanism that prioritizes critical damage regions, coupled with an Instance Calibration module that aligns feature representation across inter-instance variability. This combination allows MARSAIL-SLICK to retain the semantic fidelity of its teacher while drastically reducing parameter count and inference time.\nAs evidenced in Figure 4, the student model maintains competitive segmentation performance, particularly in high-throughput scenarios such as claim triage or automated fleet inspection. Together, the ALBERT‚ÄìSLICK teacher-student architecture offers a robust, scalable solution for real-world visual understanding tasks in the automotive insurance pipeline, aligning state-of-the-art deep learning with industry-grade reliability and speed.\nFigure 3: The architecture of MARSAIL-SLICK, a lightweight student model that incorporates selective localization and instance calibration mechanisms for knowledge-enhanced car damage segmentation. This model efficiently distills knowledge from the teacher network to enable real-time deployment while preserving semantic precision. Figure 4: Output predictions from the MARSAIL-SLICK model, demonstrating its capability to maintain high segmentation fidelity despite its reduced computational footprint. The results affirm the effectiveness of our teacher-student framework in knowledge transfer for robust performance in resource-constrained settings. In the fast-moving world of automotive insurance, where accuracy and turnaround time can make or break the customer experience, MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory) stands at the forefront of transformation. Based in Thailand and led by the visionary Dr. Teerapong Panboonyuen ‚Äî affectionately known as Dr. Kao ‚Äî MARSAIL is redefining how artificial intelligence is used in car insurance and garage ecosystems. The lab\u0026rsquo;s mission is clear: to blend deep research with real-world impact. Earlier this year, Dr. Kao shared on Twitter the debut of MARS, an innovative architecture built on Attention Refinement with Sequential Quadtree Nodes. With its combination of scientific rigor and practical relevance, MARS isn\u0026rsquo;t just another academic model ‚Äî it\u0026rsquo;s a bold step forward in computer vision and deep learning, designed to solve tangible problems in automotive analysis.\nüçÑ We\u0026#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.\n.\nPaper: https://t.co/UayUSxmZep\nCode: https://t.co/RoNFjSslXr\nProject: https://t.co/uSoBX21HpF\n.#AI #ComputerVision #DeepLearning #Research pic.twitter.com/oc8gz7Hs9I\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) August 11, 2024 Riding on this momentum, MARSAIL has unveiled two game-changing models in 2025: ALBERT and SLICK. These systems are not just incremental updates ‚Äî they represent a complete rethink of how damage detection and claim assessment can be automated with AI. ALBERT is optimized for real-time car damage classification with high precision, while SLICK focuses on smart localization and segmentation of damage areas, tailored specifically for insurance workflows. Together, they offer insurers and garages tools that are faster, smarter, and more reliable than ever before. Backed by advanced machine learning techniques and a commitment to open research, MARSAIL is helping Thailand ‚Äî and the region ‚Äî become a serious global player in automotive AI innovation. Whether you\u0026rsquo;re in the lab or on the road, MARSAIL is making sure AI drives the future.\nFigure 5: MARSAIL ‚Äî a leading research lab in Thailand focused on applying AI to car insurance and automotive service innovations. These models represent more than just technological progress; they embody a new philosophy in AI-powered insurance: a seamless synergy between uncompromising accuracy and lightning-fast efficiency, inspired by the teacher-student paradigm.\nAt the core of this paradigm lies ALBERT, the ‚Äúteacher‚Äù ‚Äî a powerhouse model meticulously engineered for razor-sharp precision. ALBERT dives deep into images, discerning the finest scratches, dents, and cracks with near-human expertise. It‚Äôs a master of detail, leaving no nuance unseen, perfect for complex offline investigations and comprehensive damage evaluations where absolute accuracy is essential.\nIn today‚Äôs fast-paced insurance ecosystem, speed is just as crucial as accuracy, particularly when it comes to frontline claim processing and on-the-spot damage assessments. This is where SLICK, the \u0026ldquo;student\u0026rdquo; model, truly shines. Guided by the advanced expertise of ALBERT, SLICK is engineered for agility and lightning-fast performance, delivering precise damage detection results in real-time. Whether running on edge devices or mobile phones, its optimized architecture allows insurance agents and repair shops to streamline their operations, making decisions faster without ever sacrificing quality.\nSLICK: Revolutionizing Car Damage Segmentation with Knowledge-Enhanced AI at MARSAIL The recent Google AI video on Knowledge Distillation: A Good Teacher is Patient and Consistent offers valuable insights into how AI models can be trained efficiently by leveraging a \u0026ldquo;teacher-student\u0026rdquo; framework. The key takeaway from this approach is that a well-trained teacher model can pass down its knowledge to a student model, significantly improving performance and generalization. This technique has sparked new ideas for MARSAIL and our work on SLICK (Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation).\nToday, we introduced Gemini Robotics On-Device ü§ñ\nüß™ Designed for rapid experimentation with dexterous manipulation.\nü¶æ Adaptable to new tasks through fine-tuning to improve performance.\nüëü Optimized to run locally with low-latency inference.\nLearn more‚Ä¶ https://t.co/h2d1TZ49qm\n\u0026mdash; Google AI (@GoogleAI) June 24, 2025 In the same spirit of knowledge transfer, SLICK takes inspiration from the concept of \u0026ldquo;distilling\u0026rdquo; knowledge from large, complex models to create an AI system capable of precise, real-world car damage segmentation. Through components like Selective Part Segmentation, Localization-Aware Attention, and Knowledge Fusion, SLICK enhances the ability of AI models to focus on and accurately segment vehicle parts‚Äîeven under challenging conditions like occlusions and deformations. Much like the patient and consistent teacher-student relationship in knowledge distillation, SLICK learns from vast datasets (including synthetic crash data and real-world insurance records) to ensure robustness and adaptability across a variety of damage scenarios.\nAt MARSAIL, inspired by Google AI\u0026rsquo;s knowledge distillation, we‚Äôre applying these principles to create an AI system that not only improves segmentation accuracy but also optimizes the entire automotive insurance and repair workflow. With SLICK, we are ready to bring this advanced AI to Thailand, enhancing efficiency, reducing fraud, and setting new standards for the industry.\nInspiration from Andrej Karpathy: Embracing the New Era of AI Innovation at MARSAIL In the video \u0026ldquo;Andrej Karpathy: Software Is Changing (Again)\u0026rdquo;, Karpathy discusses how artificial intelligence and deep learning are driving a new wave of transformation across industries. At MARSAIL, we deeply resonate with his perspective that AI is not just evolving‚Äîit‚Äôs fundamentally reshaping how we approach problem-solving and automation. Inspired by Karpathy‚Äôs vision, we‚Äôre applying the latest in AI research to redefine the way car damage estimation, insurance claims, and repair workflows are handled. Just as Karpathy highlights the importance of AI in software development, MARSAIL is leveraging cutting-edge AI models like SLICK to bring accuracy, speed, and efficiency to the automotive sector, helping to transform the Thai automotive insurance ecosystem into a more intelligent and scalable system.\nüî• New (1h56m) video lecture: \u0026quot;Let\u0026#39;s build GPT: from scratch, in code, spelled out.\u0026quot;https://t.co/2pKsvgi3dE We build and train a Transformer following the \u0026quot;Attention Is All You Need\u0026quot; paper in the language modeling setting and end up with the core of nanoGPT. pic.twitter.com/6dzimsYPB9\n\u0026mdash; Andrej Karpathy (@karpathy) January 17, 2023 By aligning our research with the principles Karpathy discusses, MARSAIL is at the forefront of AI-driven innovation in the automotive space, bringing faster, more reliable, and trustworthy solutions to insurers, garages, and customers alike.\nTogether, ALBERT and SLICK form a powerful duo that bridges the traditional divide between accuracy and efficiency ‚Äî offering the best of both worlds to revolutionize car insurance workflows across Thailand and beyond.\nALBERT: The Teacher Model ‚Äî Precision at a Cost ALBERT stands for Advanced Localization and Bidirectional Encoder Representations from Transformers. This model is designed to be highly accurate and detailed in detecting subtle damages such as small scratches, dents, and cracks on vehicles. It leverages a vision transformer architecture enhanced with localized deformable tokens and parameter sharing to precisely focus on critical damage regions.\nHowever, this precision comes with a computational cost. ALBERT requires powerful CUDA-enabled GPUs and is relatively slow, making it ideal for offline batch processing or scenarios where accuracy takes precedence over speed.\nFigure 6: MARSAIL-ALBERT model showcasing detailed and precise damage segmentation results.\nSLICK: The Student Model ‚Äî Lightning Speed Meets Smart Knowledge To address real-time insurance needs, MARSAIL developed SLICK ‚Äî Selective Localization and Instance Calibration with Knowledge. This model distills knowledge from ALBERT and integrates domain-specific insurance metadata like bumper zones and vehicle model weak points.\nSLICK boosts processing speed by over 700% compared to ALBERT, enabling instant damage assessments on edge devices or mobile apps without sacrificing much accuracy. Its adaptive attention mechanism dynamically calibrates segmentation proposals using contextual knowledge graphs, making it robust under varying light, weather, and occlusion conditions.\nFigure 7: MARSAIL-SLICK model delivering rapid, knowledge-enhanced damage segmentation optimized for real-time insurance workflows.\nüöò Teaching machines to see smarter and faster: the MARSAIL teacher-student model In the race to deliver the best car damage detection for insurance claims, MARSAIL takes a cutting-edge approach inspired by how humans learn: through mentorship. Our teacher-student model architecture pairs a high-capacity ‚Äúteacher‚Äù network with a lean, speedy ‚Äústudent‚Äù model, capturing the best of both worlds ‚Äî precision and efficiency.\nFigure 8: Conceptual architecture of the MARSAIL teacher-student model (Image source: Daily Dose of Data Science).\nWhat is the teacher-student model? Think of the teacher as a seasoned expert with a deep understanding of vehicle damage nuances ‚Äî it‚Äôs large, powerful, and painstakingly precise. The student, meanwhile, is like an apprentice: smaller, faster, and designed to perform well in real-world settings with limited resources.\nThe magic happens when the student learns to mimic the teacher\u0026rsquo;s insights without needing to replicate its full complexity. This process is known as knowledge distillation ‚Äî where the teacher‚Äôs ‚Äúsoft‚Äù predictions guide the student‚Äôs training, helping it grasp subtle visual patterns that would be hard to learn from raw data alone.\nFigure 9: Simplified overview of the teacher-student learning framework (Image source: Daily Dose of Data Science).\nMeasuring size and efficiency: teacher vs. student To illustrate the trade-off, here‚Äôs a glimpse of the teacher and student model sizes trained on the CIFAR-10 dataset. The teacher is notably larger but more precise, while the student‚Äôs compact size enables rapid inference ‚Äî crucial for insurance agents working on the go.\nFigure 10: Visual comparison of teacher (left) and student (right) model sizes (Image source: Daily Dose of Data Science).\nHow does the student learn from the teacher? The training process involves the student observing both the teacher‚Äôs output and the ground truth, gradually adjusting itself to replicate the teacher‚Äôs nuanced judgments. This dual supervision accelerates the student‚Äôs learning curve, enabling it to deliver near-teacher accuracy with significantly fewer parameters.\nFigure 11: Diagram showing how the student model learns from the teacher model (Image source: Daily Dose of Data Science).\nResults that speak volumes On multiple datasets and architectures (including CNN and ResNet), MARSAIL‚Äôs teacher-student training methods consistently improved student model accuracy across the board ‚Äî sometimes by over 3% compared to training without guidance.\nModel Dataset No KD (%) BLKD (%) TAKD (%) CNN CIFAR-10 70.16 72.57 73.51 CNN CIFAR-100 41.09 44.57 44.92 ResNet CIFAR-10 88.52 88.65 88.98 ResNet CIFAR-100 61.37 61.41 61.82 ResNet ImageNet 65.20 66.60 67.36 Figure 12: Model accuracy comparison showing improvement using knowledge distillation techniques (Image source: Daily Dose of Data Science).\nFinal layer feature summaries This image visualizes how different layers in each model contribute to the final representation, highlighting the efficiency gains from knowledge distillation that help the student model stay compact yet powerful.\nFigure 13: Summary of final layer features in teacher and student models (Image source: Daily Dose of Data Science).\nWhy does this matter for car insurance? Speed without compromise: Insurance agents and garages need fast, reliable damage detection on smartphones or edge devices. The student model delivers rapid results, trained under the teacher‚Äôs expert supervision.\nResource efficiency: Smaller models reduce computational costs and power consumption, enabling scalable deployment across Thailand‚Äôs wide insurance ecosystem.\nRobust accuracy: Guided by the teacher, the student avoids common pitfalls of lightweight models, maintaining high performance even in challenging real-world conditions.\nAccuracy vs. Speed: ALBERT excels in detailed offline analysis, perfect for complex claim investigations. SLICK offers instant, reliable damage detection to accelerate frontline claim approvals and garage estimates.\nHardware Flexibility: ALBERT demands high-end GPUs; SLICK can run efficiently on more modest, real-world devices ‚Äî a game changer for field agents and repair shops.\nKnowledge Integration: SLICK‚Äôs use of insurance-specific metadata bridges the gap between raw image analysis and domain expertise, improving real-world applicability.\nMathematical Insight (Simplified) At the heart of our system lies a sophisticated process that refines how the model understands and represents visual data at every step. Imagine ALBERT as a multi-layered brain that carefully adjusts its internal view of an image piece by piece. At each layer, it uses two powerful tools: one that lets it look broadly across different parts of the image to understand overall patterns (multi-scale self-attention), and another that processes these insights through focused, step-by-step reasoning (a multilayer perceptron).\n$$ z^{(l+1)} = z^{(l)} + \\text{MSA}(\\text{LN}(z^{(l)})) + \\text{MLP}(\\text{LN}(z^{(l)})) $$\nThis dynamic combination helps ALBERT balance the big picture with fine details, ensuring that the model not only recognizes individual features but also how they relate to each other in context. To keep this learning smooth and stable, it applies a normalization step‚Äîsimilar to tuning an instrument‚Äîto make sure each layer‚Äôs output remains consistent and meaningful.\nParallel to this, SLICK operates like an intelligent curator, enhancing the model‚Äôs confidence in its predictions. It does this by merging two streams of knowledge: the direct visual cues from the image itself and additional information pulled from a structured knowledge graph‚Äîthink of this as a database of domain-specific facts and relationships.\n$$ s_{mask} = \\sigma(W_q [f_{img} | f_{kg}]) + b $$\nTo blend these inputs effectively, SLICK employs a gating mechanism that acts like a smart filter or valve. This gate carefully weighs how much influence the visual data and the knowledge graph should each have in shaping the final mask quality scores. By doing so, the model doesn‚Äôt just rely on what it sees but also on what it knows about the world, leading to sharper, more reliable segmentation.\nIn essence, this combination of refined visual understanding and context-aware knowledge integration lets our system adapt its focus dynamically‚Äîprioritizing regions and details that matter most for accurate damage assessment and claim processing.\nPublished Research ALBERT: https://arxiv.org/abs/2506.10524 SLICK: https://arxiv.org/abs/2506.10528 Looking Ahead MARSAIL continues to innovate by balancing AI model accuracy and deployment efficiency. ALBERT and SLICK represent the cutting edge of automotive AI, ready to transform insurance claim processes in Thailand and beyond ‚Äî enabling smarter, faster, and fairer car insurance.\nCitation Panboonyuen, Teerapong. (Jul 2025). ALBERT vs SLICK: MARSAIL‚Äôs New AI Fashion for Real-Time Car Insurance and Garages. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/\nOr\n@article{panboonyuen2025albert_slick, title={ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation}, author={Panboonyuen, Teerapong}, journal={arXiv preprint arXiv:2506.10524}, year={2025}, url={https://arxiv.org/abs/2506.10524} } @article{panboonyuen2025slick, title={SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance}, author={Panboonyuen, Teerapong}, journal={arXiv preprint arXiv:2506.10528}, year={2025}, url={https://arxiv.org/abs/2506.10528} } Found this blog insightful? Consider sharing it with friends or researchers in the automotive or insurance tech industry. üöó References Panboonyuen, Teerapong. \u0026ldquo;ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation.\u0026rdquo; arXiv preprint arXiv:2506.10524 (2025). https://arxiv.org/abs/2506.10524\nPanboonyuen, Teerapong. \u0026ldquo;SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance.\u0026rdquo; arXiv preprint arXiv:2506.10528 (2025). https://arxiv.org/abs/2506.10528\nKnowledge Distillation with Teacher Assistant for Model Compression: https://www.dailydoseofds.com/p/knowledge-distillation-with-teacher-assistant-for-model-compression/\n","date":1751414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751414400,"objectID":"add4397dee72a3c314f16de9c3f60182","permalink":"https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/","publishdate":"2025-07-02T00:00:00Z","relpermalink":"/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/","section":"blog","summary":"Discover how MARSAIL‚Äôs teacher‚Äìstudent model duo ALBERT and SLICK revolutionizes car damage detection with unmatched precision and blazing speed.","tags":["automotive-ai","computer-vision","deep-learning","marsail","insurance-tech","transformers"],"title":"ALBERT vs SLICK: MARSAIL‚Äôs New AI Fashion for Real-Time Car Insurance and Garages","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":["computer-vision","deep-learning","automotive-ai","marsail"],"content":" You can explore our GitHub project page üì¶ here. MARSAIL, or the Motor AI Recognition Solution Artificial Intelligence Laboratory, is Thailand‚Äôs pioneering AI research hub dedicated to automotive insurance and repair. Under the direction of Dr. Teerapong Panboonyuen (Dr. Kao), the lab develops deep learning models to analyze car damage, estimate repair costs, and automate claim handling ‚Äî empowering insurers and garages with intelligence, precision, and speed.\nFigure 1: MARSAIL ‚Äî a standout AI research lab in Thailand, leading innovation in car insurance and garage solutions. The journey of MARSAIL began with a bold, urgent question: ‚ÄúCan we make car insurance in Thailand smarter, faster, and fraud-free using AI?‚Äù For decades, first-class auto insurance underwriting in Thailand has been heavily reliant on manual inspections and human judgments ‚Äî prone to delays, inconsistencies, and at times, manipulation. Garage estimates for labor and parts are also far from standardized, often resulting in disputes, inefficiencies, and customer dissatisfaction. Seeing these gaps not as limitations but as opportunities, MARSAIL was born with a national mission in mind: to revolutionize Thailand‚Äôs automotive insurance ecosystem through scalable, verifiable, and explainable artificial intelligence.\nWe envisioned a future where a single image of a damaged vehicle could trigger an accurate AI-driven diagnosis ‚Äî identifying every dent, crack, and broken part, then instantly calculating the repair cost and parts required. This would not only expedite claims processing but also create a tamper-proof digital trail, significantly reducing fraud in the industry. At the same time, garage operators would benefit from AI-driven estimates that standardize costs and accelerate service turnaround time ‚Äî allowing businesses to grow with transparency and trust.\nüçÑ We\u0026#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.\n.\nPaper: https://t.co/UayUSxmZep\nCode: https://t.co/RoNFjSslXr\nProject: https://t.co/uSoBX21HpF\n.#AI #ComputerVision #DeepLearning #Research pic.twitter.com/oc8gz7Hs9I\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) August 11, 2024 But we didn‚Äôt stop at computer vision. We realized that to fully automate the claims process, we also needed to read and understand the documents involved ‚Äî from the front of a Thai driver‚Äôs license to the Vehicle Identification Number (VIN), mileage, license plate, or even the Thai National ID card of the claimant. That‚Äôs why MARSAIL also expanded into NLP and OCR research, building powerful models that combine vision and language to intelligently extract, verify, and reason over structured and unstructured vehicle-related data.\nThis work is now powering what we call the ‚ÄúDigital Insurance Twin‚Äù ‚Äî a complete AI ecosystem that mirrors and manages every car‚Äôs insurance lifecycle. For example, an insurer can use our models to approve or reject first-class coverage applications by automatically analyzing the condition of a vehicle and matching it against risk profiles. Garages can plug into our engine to generate real-time quotes validated by AI, reducing negotiation overhead and instilling transparency. For every photo and every document, there‚Äôs an AI model working behind the scenes to ensure authenticity, consistency, and fairness.\nAt the heart of MARSAIL‚Äôs vision is not just efficiency but integrity. By minimizing human subjectivity and manual paperwork, our AI platform safeguards against inflated repairs, ghost accidents, and fraudulent identity claims. This builds long-term trust across the ecosystem: between insurers and policyholders, garages and customers, and regulators and service providers.\nToday, we train not only car damage models but also multimodal models that fuse vision and language, including transformer-based LLMs optimized for the insurance domain. We‚Äôre teaching machines to reason with documents, anticipate inconsistencies, and provide contextual understanding ‚Äî allowing for seamless automation of tasks that previously required expert-level human input. Whether it‚Äôs reading a worn-out VIN from an old pickup truck or parsing a scanned Thai ID under poor lighting, MARSAIL\u0026rsquo;s AI agents are learning, adapting, and improving with every new sample.\nAI-driven Car Damage Estimating: Global Innovations and MARSAIL\u0026rsquo;s Vision for Thailand The global automotive AI landscape is evolving rapidly, with companies like Tractable and Mitchell leading the way in using artificial intelligence to assess car damage, estimate repair costs, and streamline the claims process. For instance, Tractable‚Äôs AI photo-estimating system has already gained significant attention for its ability to analyze images of vehicle damage and provide accurate repair estimates. You can explore one of their impressive demonstrations in the video below:\nWhat does Martin Ellingsworth, Executive Managing Director, P\u0026amp;C Insurance Intelligence at J.D. Power have to say about the impact AI is having on the P\u0026amp;C customer experience?\nDive into our full report here: https://t.co/XP4sL6ntsC pic.twitter.com/1YCB7E7Sse\n\u0026mdash; Tractable (@tractable_ai) May 10, 2023 Another example comes from Mitchell‚Äôs partnership with AI technology to enhance auto insurance workflows. This collaboration showcases the future of intelligent claims processing and damage estimation, as seen in the video below:\nThese two AI-driven innovations are setting the stage for a new era in automotive insurance, and at MARSAIL, we aim to bring similar advancements to Thailand. Our cutting-edge AI research is designed to deliver an intelligent, scalable solution for vehicle damage estimation and claims processing. By harnessing deep learning and multimodal AI models, MARSAIL will revolutionize how car damage is assessed and how repairs are priced, ensuring transparency, reducing fraud, and optimizing the entire insurance lifecycle. Just like these global players, MARSAIL will offer a seamless, efficient, and trust-enhancing solution for the Thai automotive insurance market.\nLet us drive this transformation ‚Äî not just with code and data, but with vision, integrity, and purpose.\nMARS: Our Flagship AI At MARSAIL‚Äôs mission lies our flagship model ‚Äî MARS (Mask Attention Refinement with Sequential Quadtree Nodes). This architecture was designed with a singular goal: to perform fine-grained instance segmentation of car damage in a real-world automotive insurance setting, under various lighting, angle, and occlusion conditions. Unlike conventional approaches like Mask R-CNN or PointRend, MARS integrates hierarchical spatial reasoning using a quadtree decomposition fused with multi-level self-attention to enable both coarse-to-fine detection and spatial localization of subtle visual cues.\nFigure 2: Visual example of MARS model detecting vehicle damage areas with high precision across complex body surfaces. The core concept behind MARS draws inspiration from traditional quadtree spatial partitioning methods. Unlike standard convolutional encoders, which often struggle to preserve important structural details in large feature maps due to fixed processing limits, MARS takes a more adaptive approach. It uses a trainable system that breaks an image into regions based on how visually complex each area is. Simpler areas are processed more lightly, while more detailed or chaotic regions get extra attention.\n$$ M = \\sum_{i=1}^{N} g(q_i) \\cdot A(q_i) \\cdot F(q_i) $$\nThis leads to what the team calls Sequential Quadtree Nodes (SQN) ‚Äî a smart structure that expands only where deeper analysis is needed. Each of these nodes contributes to the overall segmentation by combining spatial focus, a confidence check, and fine-tuned local adjustments. The system selectively decides which parts of the image matter most, refining the final output by integrating all these localized insights into one coherent segmentation result.\nFigure 3: MARS architecture integrates attention refinement, quadtree partitioning, and sequential mask propagation modules to enable instance-level car damage segmentation. In our experiments on Thai car damage datasets, MARS with a ResNet101-FPN backbone outperforms all existing baselines, yielding a +2.3 maskAP improvement over PointRend and +1.8 over Mask Transfiner (Panboonyuen et al. 2023). More impressively, MARS demonstrates resilience in heavily occluded scenes, where traditional architectures often over-segment or misclassify minor damages such as scratches, dents, and broken trims.\nBeyond segmentation accuracy, MARS was designed with inference cost and deployment scalability in mind. The attention and SQN modules are modular and parallelizable, allowing fast inference on GPU-based edge devices commonly deployed in insurance claim centers. In practice, MARS processes a high-resolution damage image in under 180ms on an NVIDIA T4 GPU, enabling real-time integration into mobile claim apps and garage estimation tools.\nFor a comprehensive explanation of the method, experimental protocol, and benchmark comparisons, readers are encouraged to refer to our full publication here (Panboonyuen et al. 2023).\nMARSAIL 2025: Research Highlights As we move into 2025, MARSAIL continues to push the frontier of AI for automotive insurance and repair through multimodal AI systems, low-parameter transformer architectures, and high-resolution OCR solutions tailored for real-world document processing. These efforts aim to expand our automation ecosystem beyond just car damage detection ‚Äî now encompassing risk prediction, document understanding, vehicle condition analytics, and identity verification pipelines. Below we highlight three key models from our most recent research drop.\n1. ALBERT: Efficient Transformer for Automotive Localization ALBERT (Advanced Localization and Bidirectional Encoder Representations from Transformers) is our compact vision transformer (ViT) tailored for car damage assessment. Unlike standard ViT and DETR-based approaches, ALBERT introduces structural inductive biases using localized deformable tokens and parameter sharing to reduce model size and memory usage while preserving high-resolution localization.\nMathematically, the ALBERT encoder layer refines input embeddings using the following formulation:\n$$ z^{(l+1)} = z^{(l)} + \\text{MSA}(\\text{LN}(z^{(l)})) + \\text{MLP}(\\text{LN}(z^{(l)})) $$\nWhere MSA is a multi-scale self-attention with learnable spatial offsets adapted to car damage priors. Through selective hard sampling and token grouping, ALBERT reduces GPU memory by 40% during training while achieving higher IoU for small scratches and localized bumper cracks. This model is ideal for mobile deployments in insurance apps and in-vehicle camera assessments (arXiv:2506.10524).\n2. SLICK: Knowledge-Enhanced Instance Segmentation SLICK (Selective Localization and Instance Calibration with Knowledge) augments vision transformer-based instance segmentation using both spatial priors and domain knowledge graphs. By integrating policy-driven insurance metadata ‚Äî e.g., bumper policy zones, model-specific weak points ‚Äî SLICK dynamically adjusts attention weights and segmentation proposals. This results in better prediction under diverse lighting, weather, and occlusion scenarios, especially for aging or modified vehicles.\nWe define a mask quality score enhanced by contextual gating:\n$$ s_{mask} = \\sigma(W_q [f_{img} | f_{kg}]) + b $$\nSLICK delivers +2.5 maskAP improvement over MARS in high-noise scenes, setting a new benchmark in our internal Thai Vehicle Damage dataset (arXiv:2506.10528).\n3. DOTA: Deformable Optimized Transformer for OCR DOTA (Deformable Optimized Transformer Architecture) is our OCR engine designed for reading Thai ID cards, driver licenses, vehicle registration books, VIN plates, and inspection forms in real-world garage conditions. Using a retrieval-augmented architecture and deformable attention blocks, DOTA can accurately recognize text even under motion blur, glare, or partial occlusion.\nUnlike traditional CRNN or standard transformer OCRs, DOTA uses a hybrid vision-language training objective:\n4. MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation At MARSAIL, under the guidance of Dr. Teerapong Panboonyuen (‡∏î‡∏£. ‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô) or Dr. Kao (‡∏î‡∏£. ‡πÄ‡∏Å‡πâ‡∏≤), our team is on a mission: to fuse cutting-edge vision systems with real-world auto insurance workflows. Our latest work‚Äîpublished in Springer‚Äîintroduces a dynamic way to analyze vehicle damage and process claims faster, smarter, and more precisely than ever before.\nüîç what sets our approach apart Standard image-analysis tools often struggle to keep structural details intact when dealing with complex scenes or large images. In our paper (Springer, Chapter 3), we introduce a hierarchical, attention-driven method that adapts dynamically to the visual complexity of each region:\nSmart partitioning ‚Äî the system learns to divide an image into smaller, more granular regions where details matter most. Targeted refinement ‚Äî complex zones receive deeper analysis, while simpler areas are processed more lightly. Sequential quadtree nodes ‚Äî these elements expand only in areas needing sharper focus, concentrating computing power where it counts. üß† how it works in practice Each quadtree node contributes to the final damage map by combining:\nSpatial focus ‚Äî where exactly to look, Confidence-based gating ‚Äî deciding how much to trust that region, Local feature tweaks ‚Äî refining the outcome with sharp, situational insight. Imagine a damaged car: intricate scratches around the headlight trigger more detailed analysis, while broader, simpler zones are processed more efficiently. The combined result is a crisp and accurate segmentation mask covering the entire vehicle.\nüèéÔ∏è why it matters for insurers \u0026amp; workshops Faster, smarter estimates ‚Äî automation tackles repetitive image tasks instantly. Sharper insights ‚Äî context-aware focus pinpoints damage with precision. Efficiency-driven ‚Äî resources are optimized for the parts of the image that matter, reducing processing load. Scalable architecture ‚Äî adaptable from single images to massive fleets seamlessly. ‚úÖ conclusions \u0026amp; next steps Published under the banner of \u0026ldquo;MARSAIL\u0026rdquo; in the latest Springer volume, this work signals a bold step in auto-insurance intelligence. With a pioneering lab approach and visionary leadership from Dr. Kao, MARSAIL is crafting systems that are fast, precise, and endlessly adaptable.\nUp next: integrating our quadtree vision engine into live insurer workflows, and testing it in real-world garages across Thailand.\nReference: Springer Nature Link\nAuto insurance fraud is a massive challenge that costs billions worldwide every year, slowing down claim processing and increasing premiums for honest customers. At MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory), we are on the frontlines of this battle, deploying cutting-edge AI to automate and streamline the entire insurance claim process ‚Äî from damage detection to fraud prevention ‚Äî without relying on manual human intervention.\nFighting Fraud with Intelligent Automation Fraudsters exploit loopholes and manual processes in traditional insurance workflows, making it difficult for companies to detect suspicious claims quickly and accurately. MARSAIL leverages advanced computer vision and domain-specific knowledge to transform this game. By analyzing images and metadata from vehicles in real-time, our AI models not only identify damages precisely but also flag potential fraud patterns, speeding up claim approval and reducing false payouts.\nFigure 4: Demonstration of AI-powered automation in car insurance claims (Image source: Addenda Tech)\nVisual Damage Interpretation ‚Äî Smarter and Faster Using state-of-the-art AI, MARSAIL interprets vehicle damage from photos submitted through mobile apps or web platforms. This automated damage assessment removes subjectivity and human error, ensuring fair and consistent evaluation across all claims.\nFigure 5: AI-driven damage interpretation on insurance applications and web portals (Image source: Addenda Tech)\nTranslating Damage Into Precise Repair Estimates MARSAIL‚Äôs AI doesn‚Äôt stop at identifying damage. It accurately translates the visual information into detailed labor and parts costs, providing transparent and consistent repair estimates that speed up approvals and payments.\nFigure 6: AI converting damage images into labor and parts cost estimates (Image source: Addenda Tech)\nAI-Powered Fraud Detection ‚Äî Catching the Unseen One of MARSAIL‚Äôs critical strengths is its ability to detect anomalies and fraudulent claims. By cross-referencing damage patterns, metadata, and historical data, the AI flags suspicious cases automatically, drastically reducing fraud risk and protecting insurers and honest customers alike.\nFigure 7: AI-powered fraud detection in insurance claims (Image source: Addenda Tech)\nComprehensive, Automated Reporting for Faster Decisions MARSAIL‚Äôs automated reporting delivers a clear, itemized breakdown of all repair costs and identified damages, allowing claims adjusters and repair shops to make faster, more informed decisions ‚Äî with complete transparency and no human bias.\nFigure 8: Final AI-generated report summarizing car damage and repair costs (Image source: Addenda Tech)\nWhy MARSAIL Leads the Charge By automating the entire claims process ‚Äî from damage detection and cost estimation to fraud detection and reporting ‚Äî MARSAIL empowers insurers with unmatched speed, accuracy, and trustworthiness. This end-to-end AI solution minimizes human error and bias, reduces processing time dramatically, and safeguards the insurance ecosystem from fraudulent activities.\nThe result? Faster claims, fairer settlements, and a more secure insurance future ‚Äî powered entirely by intelligent automation.\nJoin the Future of Auto Insurance MARSAIL is not just innovating AI for automotive damage detection; we are transforming the entire insurance experience ‚Äî making it smarter, safer, and more efficient. Together, we‚Äôre winning the battle against auto insurance fraud and paving the way for a better, fairer tomorrow.\nFuture Work: Beyond Damage ‚Äì Toward Full-Lifecycle Intelligence Looking forward, our roadmap involves the development of a unified AI engine that combines damage reasoning, document verification, driving behavior analysis, and vehicle lifecycle forecasting. This would support real-time insurance pricing, repair prioritization, fraud detection, and second-hand vehicle valuation. MARSAIL is also training domain-specific large language models (LLMs) to interact with policyholders in Thai, automatically generate claim reports, and retrieve legal clauses for garage‚Äìinsurance negotiations.\nWe\u0026rsquo;re collaborating with garages and insurers across Thailand to create a national automotive data lake for federated learning ‚Äî enabling privacy-respecting model training across decentralized garages without centralizing sensitive vehicle data. Our ultimate goal is to support the Thai government‚Äôs vision for a Smart Nation and to make MARSAIL the Southeast Asian leader in Automotive Intelligence Infrastructure.\nCitation Panboonyuen, Teerapong. (Jul 2025). MARSAIL: The Smart Engine Powering the Future of Car Insurance and Intelligent Garages. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/\nOr\n@inproceedings{panboonyuen2023mars, title={MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation}, author={Panboonyuen, Teerapong, et al.}, booktitle={International Conference on Image Analysis and Processing}, year={2023}, organization={Springer} } Found this blog insightful? Consider sharing it with friends or researchers in the automotive or insurance tech industry. üöó References Panboonyuen, Teerapong, et al. \u0026ldquo;Mars: Mask attention refinement with sequential quadtree nodes for car damage instance segmentation.\u0026rdquo; International Conference on Image Analysis and Processing. Cham: Springer Nature Switzerland, 2023.\nWang, X., X. Li, and Z. Wu. (2023). Cardd: A new dataset for vision-based car damage detection. IEEE Transactions on Intelligent Transportation Systems, 24(7), 7202-7214.\nElbhrawy, A. S., M. A. Belal, and M. S. Hassanein. (2024). CES: Cost Estimation System for Enhancing the Processing of Car Insurance Claims. Journal of Computing and Communication, 3(1), 55-69.\nAmirfakhrian, M., and M. Parhizkar. (2021). Integration of image segmentation and fuzzy theory to improve the accuracy of damage detection areas in traffic accidents. Journal of Big Data, 8(1), 1‚Äì17.\nArnab, A., and P. H. S. Torr. (2017). Pixelwise instance segmentation with a dynamically instantiated network. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 441-450.\nBolya, D., C. Zhou, F. Xiao, and Y. J. Lee. (2019). Yolact: Real-time instance segmentation. Proceedings of the IEEE/CVF International Conference on Computer Vision, 9157‚Äì9166.\nChen, K., J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, et al. (2019). Hybrid task cascade for instance segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4974‚Äì4983.\nChen, H., K. Sun, Z. Tian, C. Shen, Y. Huang, and Y. Yan. (2020). Blendmask: Top-down meets bottom-up for instance segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8573‚Äì8581.\nGirshick, R. (2015). Fast R-CNN. Proceedings of the IEEE International Conference on Computer Vision, 1440‚Äì1448.\nHe, K., G. Gkioxari, P. Doll√°r, and R. Girshick. (2017). Mask R-CNN. Proceedings of the IEEE International Conference on Computer Vision, 2961‚Äì2969.\nJ√µeveer, K., and K. Kepp. (2023). What drives drivers? Switching, learning, and the impact of claims in car insurance. Journal of Behavioral and Experimental Economics, 103, 101993.\nKe, L., M. Danelljan, X. Li, Y.-W. Tai, C.-K. Tang, and F. Yu. (2022). Mask Transfiner for high-quality instance segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4412‚Äì4421.\nKirillov, A., Y. Wu, K. He, and R. Girshick. (2020). PointRend: Image segmentation as rendering. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9799‚Äì9808.\nMacedo, A. M., C. V. Cardoso, J. S. M. Neto, et al. (2021). Car insurance fraud: The role of vehicle repair workshops. International Journal of Law, Crime and Justice, 65, 100456.\nParhizkar, M., and M. Amirfakhrian. (2022). Car detection and damage segmentation in the real scene using a deep learning approach. International Journal of Intelligent Robotics and Applications, 6(2), 231‚Äì245.\nPanboonyuen, T. (2019). Semantic segmentation on remotely sensed images using deep convolutional encoder-decoder neural network. Ph.D. Thesis, Chulalongkorn University.\nPasupa, K., P. Kittiworapanya, N. Hongngern, and K. Woraratpanya. (2022). Evaluation of deep learning algorithms for semantic segmentation of car parts. Complex \u0026amp; Intelligent Systems, 8(5), 3613‚Äì3625.\nWeisburd, S. (2015). Identifying moral hazard in car insurance contracts. Review of Economics and Statistics, 97(2), 301‚Äì313.\nWang, X., R. Zhang, T. Kong, L. Li, and C. Shen. (2020). Solov2: Dynamic and fast instance segmentation. Advances in Neural Information Processing Systems, 33, 17721‚Äì17732.\nXie, E., P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, and P. Luo. (2020). Polarmask: Single shot instance segmentation with polar representation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12193‚Äì12202.\nZhang, Q., X. Chang, and S. Bian. (2020). Vehicle-damage-detection segmentation algorithm based on improved Mask R-CNN. IEEE Access, 8, 6997‚Äì7004.\nThe battle against auto insurance fraud ‚Äì and how AI can help win it: https://www.addenda.tech/post/the-battle-against-auto-insurance-fraud---and-how-ai-can-help-win-it\n","date":1751328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751328000,"objectID":"ee80d60dcd4e18b3827280e8517eb2b7","permalink":"https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/","publishdate":"2025-07-01T00:00:00Z","relpermalink":"/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/","section":"blog","summary":"Explore how MARSAIL, Thailand‚Äôs premier automotive AI lab, revolutionizes car insurance and vehicle repair with deep learning.","tags":["automotive-ai","computer-vision","deep-learning","marsail","insurance-tech"],"title":"MARSAIL: The Smart Engine Powering the Future of Car Insurance and Intelligent Garages","type":"blog"},{"authors":["Teerapong Panboonyuen","C. Charoenphon","C. Satirapod"],"categories":null,"content":"Keywords Diffusion Models High-Resolution Satellite Imagery Inpainting Latent Space Conditioning Remote Sensing Stable Diffusion Satellite Image Restoration Very High-Resolution Images Introduction Satellite image inpainting is a crucial process in remote sensing, aimed at recovering missing or damaged areas to ensure precise data analysis. Satellite images are often affected by occlusions from factors like clouds, atmospheric disturbances, or physical obstructions, making it challenging to obtain fully clear and complete data. To overcome these challenges, especially in high-resolution satellite imagery, effective inpainting methods are required that can reconstruct the missing portions while maintaining the overall structural coherence of the image.\nDiffusion models have emerged as powerful tools in image inpainting, representing a significant advancement in the field. They have been applied to a wide array of tasks, such as object removal, generative inpainting with contextual attention, and addressing semantic differences between masked and unmasked regions. These models excel in maintaining structural consistency while producing high-quality restorations, making them effective for complex tasks, such as video inpainting, text-based object removal, and sketch-based inpainting.\nIn recent years, diffusion models have become prominent for image inpainting due to their ability to generate high-fidelity results. Traditionally, inpainting with diffusion models has been approached through either preconditioned or postconditioned techniques. Preconditioned models are designed explicitly for inpainting tasks, enabling efficient inference but requiring extensive domain-specific training. In contrast, postconditioned models do not necessitate retraining but involve slower inference, as they rely on multiple forward-backward iterations to achieve optimal solutions.\nThe two primary strategies‚Äîpreconditioning and postconditioning‚Äîdiffer in how they incorporate inpainting into diffusion models. Preconditioning integrates inpainting into the training phase, where a conditional model predicts missing regions based on the masked input. While effective for specific domains, this approach requires retraining for new applications. Postconditioning, on the other hand, employs an unconditioned generative model, applying forward diffusion to unmasked pixels and reverse diffusion to fill in masked areas. Although this eliminates the need for retraining, it is computationally intensive, requiring numerous diffusion passes to refine the final image.\nAchieving effective image inpainting requires seamless propagation of information from unmasked to masked regions to ensure semantic consistency and a coherent final image. Inspired by our previous work, SatInPaint, we introduce SatDiff, a novel diffusion-based framework that builds on this foundation to achieve improved recall and overall performance. SatDiff employs a Latent Space Conditioning approach to facilitate inpainting within the latent space, rather than the image space, enabling efficient and precise reconstruction. Additionally, we integrate a forward-backward fusion mechanism within the latent space, enhancing stability and accuracy. SatDiff further incorporates the Segment Anything Model (SAM) to refine the propagation process, boosting reconstruction quality and preserving semantic coherence.\nThrough extensive experimentation on very high-resolution (VHR) satellite datasets, including DeepGlobe and the Massachusetts Roads Dataset, we validate the contributions of each component in our proposed method. Our results demonstrate that SatDiff not only surpasses state-of-the-art methods in inpainting accuracy and runtime efficiency but also excels in reconstructing realistic satellite images that closely resemble the ground truth. Building on the strengths of SatInPaint, SatDiff sets a new benchmark for high-quality and scalable satellite image restoration solutions.\nResults and Examples Satellite Image Inpainting Results We present examples of satellite image inpainting results across different object sizes. The comparison showcases the input images (with occlusions), the ground truth targets (without occlusions), and the outputs generated by our method, SatDiff. These results highlight SatDiff\u0026rsquo;s capability to address real-world challenges, such as reconstructing satellite imagery obscured by clouds or other obstacles, with high accuracy.\nConclusion In this work, we introduced SatDiff, a novel framework for satellite image inpainting based on diffusion models. By incorporating Latent Space Conditioning and Explicit Propagation, SatDiff achieves improved accuracy and efficiency for high-resolution satellite datasets. The results demonstrate the effectiveness of the proposed method in addressing real-world satellite inpainting challenges.\nFor more details, visit the official SatDiff GitHub repository.\n","date":1747094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747094400,"objectID":"2a8e156dea8103007790cee0c506ecc0","permalink":"https://kaopanboonyuen.github.io/publication/satdiff-a-stable-diffusion-framework-for-inpainting/","publishdate":"2025-05-13T00:00:00Z","relpermalink":"/publication/satdiff-a-stable-diffusion-framework-for-inpainting/","section":"publication","summary":"Satellite image inpainting is a critical task in remote sensing, requiring accurate restoration of missing or occluded regions for reliable image analysis. In this paper, we present SatDiff, an advanced inpainting framework based on diffusion models, specifically designed to tackle the challenges posed by very high-resolution (VHR) satellite datasets such as DeepGlobe and the Massachusetts Roads Dataset. Building on insights from our previous work, SatInPaint, we enhance the approach to achieve even higher recall and overall performance. SatDiff introduces a novel Latent Space Conditioning technique that leverages a compact latent space for efficient and precise inpainting. Additionally, we integrate Explicit Propagation into the diffusion process, enabling forward-backward fusion for improved stability and accuracy. Inspired by encoder-decoder architectures like the Segment Anything Model (SAM), SatDiff is seamlessly adaptable to diverse satellite imagery scenarios. By balancing the efficiency of preconditioned models with the flexibility of postconditioned approaches, SatDiff establishes a new benchmark in VHR satellite datasets, offering a scalable and high-performance solution for satellite image restoration. The code for SatDiff is publicly available at https://github.com/kaopanboonyuen/SatDiff.","tags":["Satellite Image Inpainting","Diffusion Models","Latent Space Conditioning","VHR Satellite Datasets","Image Restoration"],"title":"SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery","type":"publication"},{"authors":["Teerapong Panboonyuen","C. Charoenphon","C. Satirapod"],"categories":null,"content":" ","date":1746921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746921600,"objectID":"73d1c64bab42cbe07291f32a28eda0fe","permalink":"https://kaopanboonyuen.github.io/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/","publishdate":"2025-05-11T00:00:00Z","relpermalink":"/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/","section":"publication","summary":"Accurately segmenting land boundaries from Thai land title deeds is crucial for reliable land management and legal processes, but remains challenging due to low-quality scans, diverse layouts, and complex overlapping elements in documents. Existing methods often struggle with these difficulties, resulting in imprecise delineations that can cause disputes or inefficiencies. To address these issues, we propose CHULA, a novel Custom Heuristic Uncertainty-guided Loss tailored specifically for robust land title deed segmentation. CHULA uniquely combines domain-specific heuristic priors with uncertainty modeling in a unified loss function that effectively guides the model to focus on clearer regions while refining boundaries and suppressing noisy areas. Evaluated on a carefully curated Thai Land Title Deed Dataset, CHULA achieves an impressive 92.4% accuracy, significantly surpassing standard segmentation baselines. Our results highlight the promise of integrating uncertainty and heuristic knowledge to enhance segmentation accuracy in complex, real-world documents. The code is publicly available at https://github.com/kaopanboonyuen/CHULA.","tags":["document-segmentation","uncertainty-modeling","heuristic-loss","land-title-deeds","thai-documents","geospatial-ai","weak-supervision","noisy-labels","legal-document-analysis","deep-learning","custom-loss-function","computer-vision","boundary-refinement","real-world-data","image-segmentation"],"title":"CHULA: Custom Heuristic Uncertainty-Guided Loss for Accurate Land Title Deed Segmentation","type":"publication"},{"authors":["C. Charoenphon","Teerapong Panboonyuen","B. Zhang","C. Satirapod"],"categories":null,"content":"\n","date":1743638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743638400,"objectID":"361899faaa39e28d7de187a082f3cd95","permalink":"https://kaopanboonyuen.github.io/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/","publishdate":"2025-04-03T00:00:00Z","relpermalink":"/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/","section":"publication","summary":"GNSS data offers a reliable alternative for estimating Precipitable Water Vapor (PWV), but accurate GPS-PWV determination in tropical climates requires weighted mean temperature (Tm). With traditional measurement methods often unavailable in Thailand, and existing empirical models showing low accuracy, we propose a deep learning approach. Our Bidirectional Learning with Attention (BLA) model incorporates GRUs and an attention mechanism for Tm modeling. Trained on ERA5 data (2017-2021) and evaluated on 2022 data, BLA-Tm achieved 76% improvement over conventional models, reducing biases significantly. Validation with 280 GNSS stations confirmed BLA-Tm‚Äôs superior accuracy in GPS-PWV estimation.","tags":["gnss","precipitable-water-vapor","pwv-estimation","weighted-mean-temperature","deep-learning","attention-mechanism","gru","tropical-climate","atmospheric-science","remote-sensing"],"title":"Investigating the use of deep learning-derived weighted mean temperature for GPS-PWVs estimation","type":"publication"},{"authors":[],"categories":null,"content":"Introduction üöóüí° This Sunday (March 30, 2025), if you\u0026rsquo;re around Ladkrabang, come join us for a math talk! üìöüí¨ We‚Äôll dive deep into the fascinating world of Vision Transformers (ViTs) and their applications in car insurance AI. It\u0026rsquo;s an exciting opportunity to explore how advanced mathematical techniques are driving the future of AI in the insurance industry. Don‚Äôt miss out‚Äîjoin us for this insightful session!\nTopic: Mathematical Foundations of Vision Transformers in Car Insurance AI\nüïê Time: 1:00 PM\nüìç Location: Event Location\nRead more on Thaivivat\u0026rsquo;s blog: Here\nIn this insightful session, Dr. Teerapong Panboonyuen (Dr. Kao) will explore the mathematical foundations behind Vision Transformers (ViTs) and their groundbreaking applications in car insurance AI. As the insurance industry increasingly leverages artificial intelligence, understanding the complex mathematical principles that drive these innovations is key. This talk will shed light on how ViTs are revolutionizing the way car insurance companies analyze and predict risk, offering a glimpse into the future of InsurTech. Whether you\u0026rsquo;re a student or a professional, this is an exciting opportunity to dive into the intersection of math, AI, and insurance.\n(Reference on UAMC 2025)\nIn recent years, Vision Transformers (ViTs) have emerged as one of the most powerful models in the field of deep learning, particularly in tasks involving image recognition. This blog post explores the mathematical foundations of ViTs and their application in car insurance AI, with a focus on improving accuracy in claims prediction. We will cover key concepts such as the self-attention mechanism, custom loss functions, and how large language models (LLMs) could further enhance these AI systems. By the end of this article, you‚Äôll have a deeper understanding of how cutting-edge AI technologies are reshaping the insurance industry.\nVision Transformers: A Mathematical Overview The Transformer Architecture At the heart of Vision Transformers lies the Transformer architecture, which was originally designed for natural language processing tasks. Unlike traditional convolutional neural networks (CNNs), which rely on convolutional layers to process spatial data, ViTs treat an image as a sequence of patches and apply self-attention to learn relationships between these patches.\nThe image is divided into non-overlapping patches, each of which is flattened into a vector. These patch vectors are then embedded into a higher-dimensional space. The idea is that these embedded patches will be processed as a sequence, similar to how words in a sentence are processed in NLP tasks. This sequence is passed through layers of self-attention, where the model learns how to focus on different parts of the image depending on their relevance.\nSelf-Attention Mechanism The self-attention mechanism is what allows Vision Transformers to capture complex relationships between distant regions of the image. In a standard convolutional network, filters are used to detect local patterns in the image. However, in ViTs, the self-attention mechanism dynamically determines the importance of each patch relative to others. This makes it possible for the model to capture long-range dependencies and subtle patterns across the entire image, rather than focusing only on local features.\nBy applying this mechanism, Vision Transformers can give more weight to certain parts of the image that are more relevant for the task at hand, whether it‚Äôs identifying damage in a vehicle or classifying certain types of claims.\nCustom Loss Functions for Car Insurance AI The Need for Custom Loss Functions In the context of car insurance, traditional loss functions like cross-entropy are not always ideal. For instance, car insurance claims often involve rare and highly specific damage types that are underrepresented in training datasets. This creates a class imbalance issue, where the model may not perform well on minority classes, such as rare types of damage or low-frequency events.\nTo address this, custom loss functions are developed to weight certain classes higher than others, ensuring that the model focuses on less frequent but equally important classes. This type of custom loss function helps improve prediction accuracy, particularly in a domain like insurance, where predicting the likelihood of rare events is just as crucial as predicting common ones.\nMean Squared Error for Regression Tasks In some cases, car insurance AI needs to predict continuous values, such as the cost of repair. For such tasks, a loss function like Mean Squared Error (MSE) is commonly used. This loss function measures the difference between the predicted and true values, with the aim of minimizing this error. Using MSE ensures that the model‚Äôs predictions are as close as possible to the actual values, which is critical when estimating damage repair costs.\nIntegrating Large Language Models (LLMs) for Enhanced AI The integration of Large Language Models (LLMs) like GPT-4 with Vision Transformers can open up new possibilities for multi-modal AI systems. In car insurance, LLMs can process textual data‚Äîsuch as customer reports, claim descriptions, and policy documents‚Äîwhile ViTs handle visual data like images of the damaged vehicle.\nBy combining these two modalities, we can create a system that not only understands visual information but also the context behind it. For example, an LLM could help interpret the description of an accident or read through a claim submission and make sense of the visual evidence provided by the ViT. This multi-modal approach enhances decision-making, enabling more accurate claim assessments and better customer experiences.\nConclusion Vision Transformers are revolutionizing AI in the car insurance industry by enabling the accurate and efficient processing of complex visual data. By leveraging advanced mathematical concepts like self-attention and custom loss functions, we can build AI models that are more accurate and robust, even when dealing with rare and highly variable data. The potential for combining ViTs with LLMs represents the next frontier in AI, making it possible to create systems that understand both images and text, paving the way for more intelligent, automated solutions in car insurance.\nReferences Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., \u0026amp; Polosukhin, I. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems (NeurIPS 2017). Link to Paper\nDosovitskiy, A., Berman, M., \u0026amp; Hinton, G. E. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In Proceedings of the International Conference on Machine Learning (ICML 2020). Link to Paper\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shinn, N., \u0026amp; others. (2020). Language Models are Few-Shot Learners. In Proceedings of NeurIPS 2020. Link to Paper\nOpenAI. (2023). GPT-4 Technical Report. Link to Paper\n","date":1743080400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743080400,"objectID":"0cba16bfba5b92f7e13556152e6624e5","permalink":"https://kaopanboonyuen.github.io/talk/uamc2025-applied-mathematics-conference/","publishdate":"2025-03-27T13:00:00Z","relpermalink":"/talk/uamc2025-applied-mathematics-conference/","section":"event","summary":"I was invited to give a talk on the transformative impact of Vision Transformers (ViTs) in the car insurance industry. The session explored the mathematical foundations of ViTs, highlighting how the self-attention mechanism enables precise visual data analysis. I also examined the use of custom loss functions to enhance claim prediction accuracy and discussed the integration of Large Language Models (LLMs) to further advance AI capabilities.","tags":[],"title":"UAMC2025 (Applied Mathematics Conference)","type":"event"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"\n","date":1741996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741996800,"objectID":"fe95d68e69b39a734dc071a8066fd8b8","permalink":"https://kaopanboonyuen.github.io/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/","publishdate":"2025-03-15T00:00:00Z","relpermalink":"/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/","section":"publication","summary":"This paper introduces ALBERT, an instance segmentation model designed specifically for comprehensive car damage and part segmentation. Leveraging the power of Bidirectional Encoder Representations, ALBERT incorporates advanced localization mechanisms to accurately identify and differentiate between real and fake damages as well as segment individual car parts. The model is trained on a large-scale, richly annotated automotive dataset, categorizing damage into 26 types, identifying 7 fake damage variants, and segmenting 61 distinct car parts. Our approach demonstrates strong performance in both segmentation accuracy and damage classification, paving the way for intelligent automotive inspection and assessment applications. This work not only contributes a powerful tool for automated vehicle inspection but also lays the groundwork for future research in intelligent automotive diagnostics, safety evaluation, and insurance claim automation, with significant implications for both industry and research communities.","tags":["Automotive Damage Detection","Instance Segmentation","Vision Transformers","Bidirectional Encoder Representations","Fake Damage Classification","Car Part Segmentation","Deep Learning for Transportation","AI for Auto Insurance","Car Insurance Automation","Intelligent Vehicle Inspection","ALBERT Model","Transformer-based Segmentation","Automotive Visual AI","Insurance Fraud Detection"],"title":"ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"\n","date":1741305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741305600,"objectID":"314068631fc4027180e156d25fd60262","permalink":"https://kaopanboonyuen.github.io/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/","publishdate":"2025-03-07T00:00:00Z","relpermalink":"/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/","section":"publication","summary":"We propose SLICK, a novel and efficient framework for high-precision car damage segmentation, designed for real-world deployment in automotive insurance and inspection workflows. SLICK introduces five synergistic components, selective part segmentation guided by structural priors, localization-aware attention to highlight fine-grained damage, instance-sensitive refinement for precise boundary separation, cross-channel calibration to amplify subtle cues like scratches and dents, and a knowledge fusion module that integrates synthetic crash data, part geometry, and annotated insurance datasets. Trained using a teacher‚Äìstudent distillation strategy with ALBERT as the teacher, SLICK retains high segmentation fidelity while achieving up to 7√ó faster inference. Extensive experiments on large-scale automotive datasets demonstrate SLICK‚Äôs superior accuracy, generalization, and runtime efficiency‚Äîmaking it ideal for real-time, high-stakes applications in insurance automation and vehicle inspection.","tags":["Car Damage Segmentation","Automotive Insurance AI","Instance Calibration","Structural Priors in Vision","Localization-Aware Attention","Real-Time Vehicle Inspection","Auto Insurance Automation","Knowledge Distillation","Teacher‚ÄìStudent Learning","Transformer-Based Segmentation","Efficient Deep Learning","AI Model Compression","Fast Inference Models","Insurance Fraud Detection","Synthetic Crash Data","Deep Learning for Automotive Claims","Lightweight Vision Models"],"title":"SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance","type":"publication"},{"authors":[],"categories":null,"content":" Join the Fun World of AI: Train a Model to Find ‚ÄúWhere‚Äôs Waldo‚Äù! Hey there, AI enthusiasts and curious minds! Are you ready to take a deep dive into the fascinating world of artificial intelligence? Get ready for a fun and hands-on experience where we‚Äôll unravel the mysteries of Deep Learning üß†, Computer Vision üëÄ, and Vision Transformers‚Äîthe game-changing technology behind image recognition. But here‚Äôs the twist: we‚Äôre going to train an AI to solve the ultimate challenge‚Äîfinding Where‚Äôs Waldo!\nYep, you heard it right. We‚Äôll be combining cutting-edge AI techniques with a classic puzzle, and by the end of the session, you‚Äôll be equipped to build and train an AI model that can spot Waldo in a sea of distractions. Ready to unlock the magic? Let‚Äôs go!\nIf this sounds exciting to you, don‚Äôt miss out on the chance to be part of this hands-on AI experience at NAC2025. Whether you‚Äôre just starting to explore the world of artificial intelligence or looking to enhance your skills, this session is your perfect opportunity to learn, build, and innovate.\nJoin me and other AI enthusiasts on March 26th, and together we‚Äôll dive into this challenge while discovering the potential of AI in real-world applications. Click here to secure your spot and be part of the future of AI-driven innovation at NAC2025. Let‚Äôs train some AI and find Waldo‚Äîtogether!\nWhat You‚Äôll Learn If you‚Äôve ever wondered how self-driving cars ‚Äúsee‚Äù the world, or how facial recognition software works, you‚Äôre in for a treat. In this session, we‚Äôll break down the essentials of Deep Learning and Computer Vision‚Äîtwo critical components that enable machines to understand and interpret the visual world, just like humans do.\nThe Basics of Deep Learning\nAt the core of modern AI, Deep Learning involves training algorithms to recognize patterns and make decisions based on data. We‚Äôll start by exploring how these algorithms learn from massive datasets, allowing them to \u0026ldquo;see\u0026rdquo; and make sense of images, sounds, and even text.\nUnderstanding Computer Vision\nComputer Vision is a field within AI that teaches computers to interpret and analyze visual information from the world. It\u0026rsquo;s the backbone behind everything from image classification to object detection, and it\u0026rsquo;s what powers applications like Google Images, Snapchat filters, and even medical imaging. We‚Äôll cover how vision systems break down images into recognizable patterns, enabling the AI to understand what it‚Äôs looking at.\nThe Rise of Vision Transformers\nHere‚Äôs where it gets really cool. Vision Transformers (ViTs) have emerged as a breakthrough in image recognition. Unlike traditional Convolutional Neural Networks (CNNs), which process images in a grid-like manner, Vision Transformers work by treating images as sequences, similar to how we process language. These models capture global relationships within an image, allowing them to recognize complex patterns with remarkable accuracy. We‚Äôll explore how ViTs work, and why they‚Äôre the future of computer vision.\nBuilding an AI Model to Find Waldo\nThis is where you roll up your sleeves! In this hands-on segment, we‚Äôll show you how to train an AI model to identify Waldo in various images. We‚Äôll walk through the entire process‚Äîpreparing the data, building the model, training it, and evaluating its performance. Along the way, we‚Äôll use the power of Vision Transformers to detect Waldo, just like how cutting-edge AI models are trained to spot objects, faces, or animals in any image. It\u0026rsquo;s going to be a blast as you watch your AI model get better and better at solving the ‚ÄúWhere‚Äôs Waldo‚Äù puzzle!\nWhy Vision Transformers? So, why are we focusing on Vision Transformers? It all comes down to performance. Traditional CNNs have been the go-to for image recognition for years, but ViTs are changing the game. Here‚Äôs why they‚Äôre so exciting:\nGlobal Context Understanding\nVision Transformers are great at understanding the entire image as a whole. Instead of focusing on small patches, like CNNs, ViTs look at the global context, allowing them to capture relationships between distant parts of an image. This makes them excellent for complex recognition tasks, like identifying Waldo in a crowded scene where he could be hiding behind multiple objects.\nScalability\nVision Transformers scale well with large datasets, which is essential for training models on more complex images. They can process thousands of images simultaneously, learning from a massive variety of examples to improve their performance. So, whether you‚Äôre training a model to find Waldo or doing more advanced tasks like facial recognition, ViTs can handle it.\nAdaptability\nViTs are highly adaptable to various image types and problems. Whether you‚Äôre working with low-resolution images or high-quality satellite photos, Vision Transformers can be fine-tuned to deliver impressive results across different domains.\nHands-on: Training Your Own ‚ÄúWhere‚Äôs Waldo‚Äù Model Now that we‚Äôve got the basics covered, it‚Äôs time to get your hands dirty! During this interactive session, you‚Äôll have the chance to train your very own AI model. Here‚Äôs a sneak peek at the process:\nPreparing the Data\nBefore we can train our AI, we need to prepare a dataset of images featuring Waldo. You‚Äôll learn how to curate and preprocess images for training‚Äîan essential skill for any AI project. You‚Äôll also get hands-on experience labeling images to help your model recognize Waldo in the wild.\nBuilding the Model\nOnce we have the data, we‚Äôll build our Vision Transformer model from scratch. You‚Äôll learn the key concepts behind Vision Transformers‚Äîhow they work, what makes them different from traditional CNNs, and why they‚Äôre so powerful for image recognition.\nTraining the Model\nThis is where the magic happens. We‚Äôll train our model using the images and labels you‚Äôve prepared. You‚Äôll see how the model starts to ‚Äúlearn‚Äù and get better at recognizing Waldo over time. We‚Äôll guide you through each step, from selecting the right optimizer to tuning the model‚Äôs hyperparameters for maximum performance.\nEvaluating the Results\nAfter training your model, it‚Äôs time to evaluate how well it does at finding Waldo in new, unseen images. We‚Äôll show you how to measure accuracy and fine-tune the model to improve its performance. You‚Äôll walk away with a trained model that can spot Waldo with impressive accuracy.\nWhat‚Äôs the Big Deal About ‚ÄúWhere‚Äôs Waldo‚Äù? You might be wondering, why use a ‚ÄúWhere‚Äôs Waldo‚Äù puzzle to teach AI? The answer is simple: it‚Äôs a fun, relatable challenge that‚Äôs perfect for learning the basics of image recognition. But it‚Äôs also a great example of real-world AI applications, where the task is to identify a specific object (Waldo) in a noisy environment (a crowded scene). By building this model, you‚Äôre learning the same techniques that power cutting-edge AI applications used by companies like Google, Facebook, and even self-driving car manufacturers!\nWhy You Should Join Us This isn‚Äôt just another dry lecture on AI. This is your chance to:\nLearn in a fun, hands-on environment with practical examples you can apply to your own projects. Master the basics of deep learning and computer vision while exploring one of the most exciting advancements in AI. Get a taste of the future by learning about Vision Transformers, a breakthrough technology that‚Äôs revolutionizing image recognition. Walk away with a trained AI model you can proudly say you built yourself! Whether you\u0026rsquo;re a beginner eager to learn about AI or someone looking to deepen your understanding of computer vision, this session is for you. So, come join the fun, unleash your creativity, and let‚Äôs train a model that finds Waldo in no time! üöÄ\nLooking forward to seeing you at NAC2025, where AI meets fun! Let‚Äôs make the world of AI an adventure, one Waldo at a time! üéØ\nMeet Me at NAC2025: Where\u0026rsquo;s Waldo AI Challenge on March 26th! Get ready for a one-of-a-kind experience at NAC2025‚Äîthe 20th NSTDA Annual Conference! While the conference runs from March 26-28, 2025, I‚Äôm inviting you to join me on March 26th for an exclusive, hands-on session where we‚Äôll dive deep into the world of AI and computer vision with the ultimate challenge: Finding Where‚Äôs Waldo! üéØ\nAt NAC2025, we‚Äôre exploring how AI is transforming industries and driving a more sustainable future for Thailand. But on March 26th, I‚Äôm turning the spotlight on a super fun and interactive topic‚Äîteaching AI to spot Waldo in a crowd! With Vision Transformers and Deep Learning, we‚Äôll break down how AI can ‚Äúsee‚Äù the world just like humans and even find that elusive red-and-white-striped figure hiding among distractions.\nSo, if you‚Äôre ready to learn the basics of AI, dive into computer vision, and have a blast solving the ‚ÄúWhere‚Äôs Waldo?‚Äù puzzle, don‚Äôt miss this exclusive event on March 26th! The rest of the NAC2025 conference is packed with innovation, but this hands-on workshop is your chance to get involved, ask questions, and train your very own AI model. Trust me, it‚Äôs going to be a lot of fun!\nSee you there on March 26th‚Äîlet\u0026rsquo;s crack the code and find Waldo together! üîç\nReferences Where\u0026rsquo;s Waldo Dataset: Kaggle - Where\u0026rsquo;s Waldo Finding Waldo - A Primer: Kaggle - Finding Waldo Image Credits IMDB - Where\u0026rsquo;s Waldo (1992 TV series) Where\u0026rsquo;s Waldo - Book on Amazon ","date":1740562200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740562200,"objectID":"08c578a7a792d1c14f2ce43264874b82","permalink":"https://kaopanboonyuen.github.io/talk/nac2025-nstda-annual-conference-2025/","publishdate":"2025-02-26T09:30:00Z","relpermalink":"/talk/nac2025-nstda-annual-conference-2025/","section":"event","summary":"I was invited by NSTDA to teach a session focused on advanced artificial intelligence and computer vision, using the iconic \"Where‚Äôs Waldo\" problem as a practical case study. The session explored how AI can be trained to detect complex visual patterns‚Äînot just identifying a character in a busy scene, but demonstrating the power of modern deep learning techniques. I explored cutting-edge techniques like Vision Transformers to develop models with exceptional precision in image recognition, showcasing how these advanced architectures are redefining the limits of AI perception and understanding.","tags":[],"title":"NAC2025 (NSTDA Annual Conference 2025)","type":"event"},{"authors":["N. Nithisopa","Teerapong Panboonyuen"],"categories":null,"content":"DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation Abstract Text recognition in natural images remains one of the most challenging yet essential tasks within the fields of computer vision and natural language processing. With applications ranging from document digitization to autonomous navigation, effective text recognition is more critical than ever before. In this paper, we introduce DOTA, a novel end-to-end framework that combines ResNet and Vision Transformer (ViT) backbones with advanced methodologies such as Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF) to significantly enhance Optical Character Recognition (OCR) performance.\nAt the heart of DOTA is a revolutionary approach where traditional convolution layers in the third and fourth blocks of the network are replaced with Deformable Convolutions‚Äîa technique that offers adaptive and robust feature extraction, making it ideal for recognizing text in complex and irregular layouts. Furthermore, adaptive dropout is integrated to ensure regularization, helping to prevent overfitting and boosting generalization. To refine the sequential modeling of text, we leverage CRFs, which excel in capturing intricate dependencies inherent in text recognition tasks.\nWe conducted extensive experiments on six benchmark OCR datasets‚ÄîIC13, IC15, SVT, IIIT5K, SVTP, and CUTE80. Our results demonstrate the exceptional performance of DOTA, achieving remarkable accuracies:\nIC13: 97.32% IC15: 58.26% SVT: 88.10% IIIT5K: 74.13% SVTP: 82.17% CUTE80: 66.67% This gives us an average accuracy of 77.77%, setting a new state-of-the-art in the field of text recognition. The results clearly highlight the robustness of DOTA across a variety of challenging datasets.\nIntroduction Text recognition from images has long been a challenging problem, with significant implications for applications in document processing, automated data entry, and even autonomous systems. With traditional Optical Character Recognition (OCR) systems heavily relying on Convolutional Neural Networks (CNNs), we‚Äôve seen progress in extracting features from images. However, as text layouts become more complex‚Äîsuch as when dealing with varying fonts, orientations, and complex backgrounds‚ÄîCNNs often fall short. Enter the Transformer architectures: these models have revolutionized many areas in computer vision, particularly in handling long-range dependencies through their self-attention mechanisms, offering significant improvements for text recognition tasks.\nWhile Transformer-based models have pushed the boundaries of OCR performance, there is still a need for further improvements, especially in the area of feature extraction and sequence modeling. That\u0026rsquo;s where DOTA comes in. By combining the strengths of ResNet and Vision Transformer (ViT) backbones, this novel approach leverages Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF) to achieve new levels of OCR accuracy.\nThe DOTA Framework The architecture of DOTA is designed to enhance both feature extraction and sequence modeling, key areas in text recognition:\nDeformable Convolutions: By replacing standard convolutions in the network‚Äôs third and fourth blocks with Deformable Convolutions, we allow for more flexible and adaptive feature extraction. This helps capture irregular text patterns and varying layouts more effectively.\nRetrieval-Augmented Generation: The integration of this approach provides context-aware enhancements, further refining the recognition process.\nConditional Random Fields (CRFs): CRFs have been incorporated into the framework to better model the sequential nature of text, providing the necessary context to improve recognition accuracy, especially for more complex sequences of characters.\nTogether, these components form the backbone of DOTA, providing a more robust, adaptable, and precise model for OCR tasks. The effectiveness of DOTA is clearly demonstrated in its impressive performance across several standard OCR benchmark datasets.\nExperimental Results We evaluated DOTA using six widely-used OCR benchmark datasets:\nIC13: 97.32% IC15: 58.26% SVT: 88.10% IIIT5K: 74.13% SVTP: 82.17% CUTE80: 66.67% With an average accuracy of 77.77%, DOTA has set a new state-of-the-art in OCR performance. The results show that the combination of Deformable Convolutions, Vision Transformers, and CRFs significantly improves recognition, even in challenging conditions where traditional methods struggle.\nConclusion The DOTA framework represents a major leap forward in the field of text recognition. By effectively combining ResNet and Vision Transformer backbones with advanced techniques like Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields, it achieves impressive accuracy across a variety of challenging datasets. This work sets a new benchmark for OCR performance, providing a powerful tool for tackling the complexities of text recognition in real-world scenarios.\nWith this new approach, we‚Äôve laid the foundation for even more accurate and robust text recognition systems, paving the way for smarter applications in everything from document processing to autonomous navigation.\n","date":1740441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1740441600,"objectID":"b691113a5a95adb4d43f4bfba57046dc","permalink":"https://kaopanboonyuen.github.io/publication/dota-deformable-optimized-transformer-architecture/","publishdate":"2025-02-25T00:00:00Z","relpermalink":"/publication/dota-deformable-optimized-transformer-architecture/","section":"publication","summary":"In this paper, we present a novel end-to-end framework that integrates ResNet and Vision Transformer (ViT) backbones with cutting-edge techniques such as Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations work together to significantly improve feature representation and Optical Character Recognition (OCR) performance. By replacing the standard convolution layers in the third and fourth blocks with Deformable Convolutions, the framework adapts more flexibly to complex text layouts, while adaptive dropout helps prevent overfitting and enhance generalization. Moreover, incorporating CRFs refines the sequence modeling for more accurate text recognition. Extensive experiments on six benchmark datasets‚ÄîIC13, IC15, SVT, IIIT5K, SVTP, and CUTE80‚Äîdemonstrate the framework‚Äôs exceptional performance. Our method represents a significant leap forward in OCR technology, addressing challenges in recognizing text with various distortions, fonts, and orientations. The framework has proven not only effective in controlled conditions but also adaptable to more complex, real-world scenarios. The code for this framework is available at https://github.com/kaopanboonyuen/DOTA.","tags":["Text Recognition","Conditional Random Fields","Vision Transformer","Optical Character Recognition","Deformable Convolutions"],"title":"DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation","type":"publication"},{"authors":["C. Dechsupa","Teerapong Panboonyuen","W. Vatanawood"],"categories":null,"content":"","date":1738454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738454400,"objectID":"f2055f1a8da0d0be623ba010f4918626","permalink":"https://kaopanboonyuen.github.io/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/","publishdate":"2025-02-02T00:00:00Z","relpermalink":"/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/","section":"publication","summary":"State-space graphs and automata are essential for modeling and analyzing computational systems. Recurrent neural networks (RNNs) underpin language models by processing sequential data and capturing contextual dependencies. Both RNNs and state-space graphs evaluate discrete-time systems, but their equivalence, especially in sentence structure modeling, remains unresolved. This paper introduces ENGRU (Enhanced Gated Recurrent Units), a deep learning approach for formal verification. ENGRU combines model checking, Colored Petri Nets (CPNs), and sequential learning to analyze systems abstractly. CPNs undergo state-space enumeration to generate graphs and automata, which are transformed into sequential representations for ENGRU to learn and predict system behaviors. ENGRU effectively predicts goal states in discrete-time models, aiding early bug detection and predictive state-space exploration. Experimental results show high accuracy and efficiency in goal state predictions. ENGRU‚Äôs source code is available at https://github.com/kaopanboonyuen/ENGRU.","tags":["Formal Verification","Model Checking","Petri Nets","Computational Modeling","Analytical Models"],"title":"ENGRU: A Preliminary Investigation of AI-Augmented Formal Verification and Its Challenges","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":["Running","Marathon","Sports"],"content":" This year‚Äôs Chombueng Marathon marked a special milestone for me: my second-ever full marathon. My first was the Bangkok Marathon 2022, an unforgettable experience that sparked my love for the 42.195K distance. Completing that first marathon taught me the value of resilience and determination, and it‚Äôs the reason why I continue to embrace the challenge and joy of running full marathons.\nThis time, it felt even more meaningful as I returned to my hometown, Ratchaburi, to take on the challenge. Despite being born here, it wasn‚Äôt until now‚Äîat the age of 30‚Äîthat I finally had the chance to run this iconic race in my own backyard. Running in my hometown for the most fun marathon of my life‚Äî42.195K finisher at the Chombueng Marathon 2025.\nThe Chombueng Marathon, now in its 38th year, is one of Thailand‚Äôs oldest and most cherished marathons. It‚Äôs not just a race; it‚Äôs a tradition that brings together runners from across the country and beyond, while showcasing the unique charm of this small but spirited district.\nFigure 1: My official certificate for completing the Chombueng Marathon 2025. Proud to be a full marathon finisher of 42.195K!\nThe Journey Back Home Coming back to Chombueng brought back a flood of childhood memories. Growing up, I spent countless weekends visiting my mom, who worked as a teacher here. The tranquil forests, the famous Tham Chomphon cave, the peaceful campus of Rajabhat University Chombueng‚Äîthey‚Äôve all remained beautifully unchanged. Running through these familiar sights made the experience even more special, as if the past and present collided in the best possible way.\nFigure 2: Final result of my official finishing time on the Chombueng Marathon 2025 website.\nFigure 3: Detailed official result of my finish time, as shown on the Chombueng Marathon 2025 website.\nFigure 4: GPS mapping of my 42.195K route during the Chombueng Marathon 2025, as captured from satellite imagery.\nThe Race The 2025 Chombueng Marathon was held on January 19th, with the gun going off for the 42.195K runners at 3:00 a.m.. The weather was perfect for running, hovering between 15‚Äì19¬∞C‚Äîcool and crisp, just the way I like it.\nI started off strong, clocking a 5:39 average pace for the first 5K, easing slightly to 5:54 for the next 10K. By the time I reached the halfway point at 21K, I was feeling good, finishing the first half in just over 2 hours. But as the second half began, I shifted gears to a more strategic approach: run-walk intervals. I wanted to finish this race injury-free and with enough energy to enjoy the rest of the day‚Äîbecause life‚Äôs too short to spend it limping around post-race!\nIn the end, I crossed the finish line with a chip time of 5 hours and 36 minutes. While it wasn‚Äôt a personal best or a sub-5, the sense of accomplishment and joy I felt was unbeatable.\nFigure 5: Receiving my finisher's medal at the Chombueng Marathon 2025 after completing the 42.195K race. (Photo taken on my desk at home)\nFigure 6: Me running around the 40K mark during the Chombueng Marathon 2025.\nFigure 7: My ultimate joy as I crossed the finish line ‚Äì completing my second-ever 42.195K marathon. What a feeling!\nFigure 8: Pure joy after crossing the finish line of my second 42.195K marathon. Everything feels possible now!\nThe Magic of Chombueng What truly sets the Chombueng Marathon apart is the atmosphere and community spirit. The entire town comes alive for this event, cheering runners on from start to finish.\nI‚Äôll always remember the monks sprinkling holy water on us as we ran by‚Äîa uniquely Thai tradition that made me smile every time. Then there were the students, standing by the roadside in the early hours, clapping, cheering, and offering encouragement. And, of course, the locals who came out to support us, their smiles and shouts of \u0026ldquo;‡∏™‡∏π‡πâ ‡πÜ!\u0026rdquo; (Keep going!) giving us the boost we needed to push through each kilometer.\nTo everyone who stood out in the cool morning air to cheer us on: thank you. Your energy, enthusiasm, and kindness made all the difference.\nFigure 9: The atmosphere as I run through the Chombueng Marathon 42.195K, one of Thailand's oldest and most iconic marathon events.\nFigure 10: Almost there! The final stretch before crossing the finish line at the Chombueng Marathon 2025.\nThe Race Atmosphere This year‚Äôs marathon was filled with joy and excitement. I met so many incredible runners, each radiating energy and enthusiasm as we gathered at the start line at 3 a.m. The crowd was electric, with smiles all around and cheers that warmed the cool morning air. The support from the spectators was truly special, and I want to give a big shoutout to two amazing women who became unexpected guides during my race.\nFigure 11: A running moment during the Chombueng Marathon 2025.\nDressed in vibrant orange and blue shirts, these two seasoned runners unknowingly set the perfect pace for me during the first 15K. With their steady rhythm, I was able to maintain a comfortable 5:50 min/km pace, which kept me going without overexerting myself. Though I hadn‚Äôt trained much and had no particular strategy in mind, following them gave me the boost I needed to tackle the early stages of the marathon.\nHowever, reality hit midway‚ÄîI realized that sustaining that pace for the rest of the race was beyond my current fitness level. Adjusting my strategy, I had to let go of my hopes for a sub-4 finish, but I‚Äôm incredibly grateful to those two women for helping me start strong.\nFigure 12: Another running moment during the marathon.\nIn the end, I crossed the finish line in 5 hours and 38 minutes. While it wasn‚Äôt the time I dreamed of, it was still a proud moment. Next time, I know sub-4 isn‚Äôt too far out of reach. Here‚Äôs to continuing the journey and pushing myself to new limits!\nFigure 13: After crossing the finish line at the Chombueng Marathon 42.195K. I can imagine how exhausted everyone must have felt, but for me, it was pure joy‚Äîjust look at that smile!\nLessons and Reflections This was my second full marathon, and while I didn‚Äôt hit my sub-5 goal, I‚Äôm proud of what I achieved. I went into this race with very little training‚Äîmy recent runs had mostly been short 5Ks. But this marathon reignited my passion for running and reminded me why I love this sport so much.\nThe biggest lesson? It‚Äôs okay to let go of the pressure. Instead of obsessing over times and splits, I focused on enjoying the journey, listening to my body, and soaking in the experience. Running should be fun, and as long as you keep moving forward with a smile, that‚Äôs what matters.\nI know there are more full marathons in my future. And when the next one comes around, I‚Äôll be ready.\nFigure 14: My final sprint in the Chombueng Marathon 2025. Garmin Fenix 7 stats show my pace hitting peak performance!\nFigure 15: Crossing the finish line at the Chombueng Marathon 2025. Garmin app stats show my finishing time and overall performance, a personal best!\nFigure 16: A post-race moment after completing the Chombueng Marathon 2025. My Garmin Fenix 7 stats show I‚Äôve hit a new high with a VO2 Max of 55‚Äîfeeling unstoppable!\nA Love Letter to Chombueng The Chombueng Marathon isn‚Äôt just a race‚Äîit‚Äôs a memory, a tradition, and a piece of my heart. Running in my hometown, surrounded by the places and people that shaped my early years, was an experience I‚Äôll always treasure.\nFigure 17: Celebrating after completing my second 42.195K marathon at Chombueng Marathon 2025. Feeling on top of the world with this amazing finish!\nThank you, Chombueng, for the beautiful scenery, the incredible community, and the unforgettable memories. This race will forever hold a special place in my heart, and I can‚Äôt wait to come back again.\nUntil next time, Chombueng.\nCitation Panboonyuen, Teerapong. (Jan 2025). 42.195K at Chombueng Marathon 2025 ‚Äî My Second Full Marathon, Coming Home. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/\nFor a BibTeX citation:\n@article{panboonyuen2025chombueng, title = \u0026quot;42.195K at Chombueng Marathon 2025 ‚Äî My Second Full Marathon, Coming Home\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2025\u0026quot;, month = \u0026quot;Jan\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/\u0026quot; } Did you find this page helpful? Consider sharing it üôå ","date":1737331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737331200,"objectID":"ac1e74e111596486d72e88339936c0bd","permalink":"https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/","publishdate":"2025-01-20T00:00:00Z","relpermalink":"/blog/2025-01-20-chom-bueng-marathon-2025/","section":"blog","summary":"A journey of 42.195 kilometers in the 38th Chombueng Marathon, blending personal nostalgia, local charm, and the triumph of completing my second-ever full marathon.","tags":["Chombueng Marathon","Full Marathon","Running","Thailand","Marathon Journey"],"title":"42.195K at Chombueng Marathon 2025 ‚Äî My Second Full Marathon, Coming Home","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":["Science","Innovation","Scholarship","Summit"],"content":" Attending GYSS2025 (Global Young Scientists Summit) is an incredibly exciting opportunity. This prestigious gathering brings together brilliant young minds from all over the world, providing a platform to engage with leading experts, share innovative ideas, and immerse ourselves in the latest advancements in science and technology. It‚Äôs a momentous occasion for any researcher or scientist, and I‚Äôm thrilled to be part of this year‚Äôs summit.\nWe‚Äôre living in an exciting era, where Large Language Models (LLMs) are reshaping the landscape of artificial intelligence. The rapid strides AI has made in recent years, fueled by powerful architectures like GPT, are nothing short of revolutionary. As we explore the depths of these models, we are witnessing the dawn of new possibilities in natural language processing, conversational agents, and machine learning. Attending GYSS2025 during this transformative period in AI‚Äôs evolution promises to be a truly enriching experience, as it will allow me to explore these advancements and exchange ideas with some of the brightest minds in the field.\nBeing part of GYSS2025 is one of the most exciting and proud moments in my life. It‚Äôs a powerful gathering where the brightest young scientists from around the world come together to spark innovation and push the limits of what‚Äôs possible. This event is more than just a conference‚Äîit‚Äôs an electrifying experience that fills me with energy and inspiration. Sharing space with brilliant minds and world-class experts motivates me to keep challenging myself and driving meaningful change. It‚Äôs a milestone that I‚Äôll remember forever, a moment that highlights how far hard work and passion can take you, and a reminder of the incredible impact we can make together.\nHonored to have attended GYSS2025 in Singapore! .\nAn incredible experience connecting with global innovators, exchanging ideas, and gaining inspiration to shape the future. .https://t.co/LqTKodKSj3 pic.twitter.com/hJKWz6YTgy\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) January 13, 2025 Figure 1: Photo with my event badge in front of the GYSS2025 board.\nPresenting My Work to Her Royal Highness Princess Maha Chakri Sirindhorn First, I would like to sincerely thank Her Royal Highness Princess Maha Chakri Sirindhorn for her gracious support and unwavering dedication to advancing science and education in Thailand. It is through her continued encouragement and vision that young researchers like myself are given extraordinary opportunities to grow, connect with global leaders, and proudly represent our country on the world stage. Her support made this once-in-a-lifetime experience at GYSS 2025 possible, and I am deeply honored and inspired by her commitment to nurturing the next generation of Thai scientists.\nI feel truly honored and deeply grateful to be selected as one of only 15 young scientists from Thailand to attend the Global Young Scientists Summit (GYSS) 2025 in Singapore. I‚Äôm especially proud to be the sole representative from Thailand in the fields of computer engineering and artificial intelligence, and one of only three postdoctoral researchers selected this year. GYSS has inspired me to keep following my passion in research ‚Äî it was incredibly exciting to meet world-class professors I‚Äôve admired for years, including a legendary computer scientist who received the Turing Award. That moment truly meant the world to me.\nFigure 2: Honored to represent Thailand at GYSS2025 with heartfelt gratitude to HRH Princess Maha Chakri Sirindhorn. I am one of only three postdoctoral researchers selected this year ‚Äî and the sole delegate from Thailand in the fields of computer engineering and AI. (Reference Image: From official Facebook of NSTDA-GYSS: source).\nThe highlight of the summit was, without a doubt, the moment I had the incredible honor of presenting my work to Her Royal Highness Princess Maha Chakri Sirindhorn.\nAs a young scientist, I was deeply humbled to share my research titled ‚ÄúMeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand‚Äù with Her Royal Highness. My work focuses on addressing challenges in agricultural monitoring and resource management by leveraging cutting-edge advancements in artificial intelligence. Specifically, MeViT is designed for semantic segmentation of Landsat satellite imagery, targeting key economic crops in Thailand such as para rubber, corn, and pineapple. By enhancing Vision Transformers (ViTs) with a medium-resolution multi-branch architecture and incorporating mixed-scale convolutional feedforward networks (MixCFN), MeViT excels at extracting multi-scale local information critical for precise segmentation.\nHer Royal Highness listened with great interest, her graciousness reflecting her profound commitment to nurturing the next generation of scientists. She expressed encouragement for the practical applications of such research in addressing challenges critical to Thailand‚Äôs agricultural and environmental sustainability. Her unwavering support for young researchers is a testament to her dedication to fostering innovation for the betterment of society.\nFigure 3: Group photo with Her Royal Highness Princess Maha Chakri Sirindhorn.\nFigure 4: A truly special moment‚Äîstanding right behind HRH Princess Maha Chakri Sirindhorn. A deeply proud and memorable experience.\nFigure 5: Group photo with HRH Princess Maha Chakri Sirindhorn and NSTDA staff members.\nFigure 6: A proud moment with HRH and fellow Thai researchers selected for GYSS2025.\nFigure 7: With HRH Princess Maha Chakri Sirindhorn, fellow researchers, and NSTDA staff.\nIt was a truly humbling and inspiring moment‚Äîone that I will carry with me for the rest of my life. The opportunity to share my work with Her Royal Highness not only reaffirmed my passion for pushing the boundaries of science and technology but also strengthened my resolve to contribute to meaningful advancements that serve the nation and the global community.\nLearning from Icons in the Field The summit was filled with groundbreaking talks, and one of the most memorable was the Plenary Lecture by the renowned Professor Adi Shamir. As a Turing Award Laureate and an expert in cryptography and artificial intelligence, his lecture was truly thought-provoking. The topic, ‚ÄúCan you recover a deep neural network from its answers?‚Äù, delved into one of the most critical questions of our time‚Äîhow deep learning models, which are at the forefront of AI, can be understood and potentially reverse-engineered.\nThe lecture began with a discussion on the architecture of modern neural networks, emphasizing their complexity and overparameterization. Professor Shamir explored concepts like model inversion, adversarial attacks, and the limitations of current AI systems, while raising questions about privacy-preserving AI and ethical AI frameworks. His insights on adversarial robustness and explainable AI deeply resonated with my research interests, motivating me to reflect on these critical challenges.\nFigure 8: Attending a lecture with Professor Adi Shamir at GYSS2025.\nFigure 9: Learning from the legendary Professor Adi Shamir‚Äîco-inventor of RSA and Turing Award laureate.\nDeep Neural Networks (DNNs) have become indispensable in modern AI applications, with billions of dollars and countless GPU hours invested in their training. These models are often deployed as \u0026ldquo;black boxes,\u0026rdquo; allowing users to interact with them without revealing their inner workings. However, this raises a critical question: Can the parameters of a deep neural network be recovered using only its inputs and outputs? In a groundbreaking plenary lecture, Turing Award recipient Prof. Adi Shamir demonstrated that for ReLU-based DNNs, it is indeed possible to recover all parameters in polynomial time relative to the number of neurons. His findings, supported by practical experiments, highlight both the potential vulnerabilities of these systems and the need for robust defenses.\nUnderstanding the Basics of DNN Operations At the heart of Shamir\u0026rsquo;s analysis lies a detailed understanding of how DNNs function. A DNN comprises multiple layers of neurons, each performing a series of linear transformations followed by non-linear activations. Among these, the Rectified Linear Unit (ReLU) is a widely used activation function defined as:\n$$ \\text{ReLU}(x) = \\max(0, x). $$\nThis piecewise linear function introduces non-linearity while maintaining computational simplicity. Shamir emphasized that ReLU\u0026rsquo;s linear segments make it particularly susceptible to parameter extraction, as its outputs can be mathematically analyzed to reveal underlying weights and biases.\nIn mathematical terms, the operation of a single layer in a DNN can be expressed as:\n$$ \\mathbf{y} = \\text{ReLU}(\\mathbf{W}\\mathbf{x} + \\mathbf{b}), $$\nwhere:\n$\\mathbf{x}$ represents the input vector, $\\mathbf{W}$ is the weight matrix, $\\mathbf{b}$ is the bias vector, and $\\mathbf{y}$ is the output vector after applying ReLU. Stacking multiple such layers creates a complex mapping from inputs to outputs, making the network appear opaque to external observers.\nChallenges in Parameter Recovery Recovering the parameters of a DNN‚Äîits weights and biases‚Äîis inherently an NP-hard problem. This complexity arises from the high dimensionality of the parameter space and the limited observability of internal computations. Traditional approaches to this problem relied on exhaustive searches, which scale exponentially with the number of parameters, rendering them impractical for large networks.\nProf. Shamir highlighted that these challenges are exacerbated in scenarios where outputs are restricted to discrete or low-precision values. However, he proposed that by carefully designing input queries and analyzing output patterns, it is possible to significantly simplify the recovery process.\nPolynomial Time Parameter Extraction Shamir\u0026rsquo;s key contribution lies in demonstrating a polynomial-time attack for ReLU-based DNNs. His approach leverages the inherent linearity of ReLU segments to derive equations that describe the network\u0026rsquo;s behavior. By identifying critical points‚Äîlocations where ReLU outputs switch between active and inactive states‚Äîone can extract sufficient information to reconstruct the network\u0026rsquo;s parameters.\nCritical Points and Linear Equations Consider a single ReLU neuron with input $z$ and output $y = \\text{ReLU}(z)$. The critical point for this neuron is $z = 0$, where the output transitions from 0 to a positive value. By probing the network with carefully chosen inputs that traverse these critical points, it becomes possible to:\nIdentify the active/inactive state of each neuron. Extract linear equations relating the input, weights, and biases. For an $n$-layer network, these equations can be combined to solve for all parameters using standard techniques from linear algebra.\nPractical Demonstration In a practical demonstration, Shamir applied his method to an 8-layer DNN trained on the CIFAR-10 dataset. This network contained 1.2 million parameters, yet all were successfully recovered in polynomial time. The experiment underscores the real-world applicability of this attack and its implications for AI security.\nComparison with Cryptographic Systems Drawing parallels between DNNs and cryptographic systems, Shamir likened the structure of a neural network to a block cipher, where each layer performs a distinct transformation. In cryptography, security often hinges on the infeasibility of reversing these transformations without a key. Similarly, DNNs rely on the assumption that their internal parameters cannot be deduced from external interactions.\nHowever, Shamir\u0026rsquo;s work demonstrates that this assumption does not hold for ReLU-based networks. By exploiting the deterministic nature of their operations, an adversary can effectively \u0026ldquo;decrypt\u0026rdquo; the network to reveal its parameters.\nImplications of Randomness and Alternative Activations To mitigate the risks posed by such attacks, Shamir explored potential defenses, including:\nRandomness in Training and Inference: Introducing stochasticity into the network\u0026rsquo;s operations, such as random noise or dropout, can obscure critical points and complicate parameter recovery.\nAlternative Activation Functions: Functions like the sigmoid or hyperbolic tangent (tanh) introduce smoother transitions, reducing the linearity exploited in Shamir\u0026rsquo;s attack. The sigmoid function, for example, is defined as:\n$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}. $$\nUnlike ReLU, sigmoid outputs are continuous and bounded, making it harder to identify critical points.\nBroader Implications and Future Directions Shamir\u0026rsquo;s findings have profound implications for the field of AI security. As DNNs become integral to applications ranging from healthcare to autonomous systems, ensuring their robustness against parameter extraction attacks is paramount. Future research may focus on:\nDesigning architectures that are inherently resistant to such attacks. Developing formal metrics to quantify a network\u0026rsquo;s susceptibility to parameter recovery. Exploring the trade-offs between interpretability and security. In conclusion, Prof. Adi Shamir\u0026rsquo;s lecture sheds light on a critical vulnerability in modern AI systems while providing a roadmap for addressing it. His innovative use of cryptographic techniques underscores the interdisciplinary nature of AI research and its potential to reshape our understanding of security in the digital age.\nBut what truly stood out was the in-depth discussion I had with Professor Shamir. His monumental impact on the field of AI and his perspectives on current and future challenges was both a privilege and a learning experience that I will treasure forever.\nFigure 10: A memorable moment meeting Professor Adi Shamir, co-inventor of RSA and recipient of the ACM A.M. Turing Award. (Read more) Plenary Lecture: Educability (Prof Leslie Valiant) In a captivating lecture, Prof. Leslie Valiant delved into the concept of educability, a framework that bridges the gap between human cognitive capabilities and machine learning. Drawing inspiration from Alan Turing‚Äôs groundbreaking insights, the discussion highlighted the interplay between intelligence, learning, and the ethical design of artificial intelligence systems.\nHistorical Foundations of Intelligence and Educability The quest to define human intelligence has long been fraught with challenges. Psychologists have struggled to agree on a single definition, revealing the complexity of human cognition. Prof. Valiant proposed that much of our understanding can be reframed through the lens of educability, which encompasses three primary facets:\nLearning from Experience ($\\mathcal{L}_{experience}$): The ability to generalize patterns and principles from observed phenomena. Reasoning with Acquired Knowledge ($\\mathcal{R}_{knowledge}$): Chaining learned concepts to make inferences. Absorbing Explicit Instruction ($\\mathcal{I}_{instruction}$): Gaining knowledge through direct teaching or guidance. These components have enabled humanity to progress from rudimentary tools to advanced technological civilizations.\nEducability vs. Machine Learning A cornerstone of the lecture was the Church-Turing Thesis, which posits that all forms of computation‚Äîwhether in human brains or machines‚Äîare fundamentally equivalent. This foundational idea underpins modern efforts to replicate human cognition in artificial intelligence.\nComparing Human and Machine Learning Current AI systems excel in pattern recognition and data-driven learning. However, they fall short in:\nContextual Reasoning: Humans can apply learned knowledge across diverse scenarios. For example, recognizing that if $A \\implies B$ and $B \\implies C$, then $A \\implies C$. Learning from Minimal Examples: Unlike humans, who can learn concepts from a few instances, AI often requires massive datasets. Instruction-Based Learning: Humans thrive in environments with structured instruction, a capability that AI systems struggle to emulate. Prof. Valiant argued that replicating these nuanced aspects of educability in AI could unlock new levels of machine intelligence.\nCognitive Capabilities as Civilization Enablers Human educability is unique among species, enabling the creation of advanced civilizations. This capability hinges on the ability to:\nGeneralize Across Domains: Applying principles learned in one context to solve problems in another. Accumulate Knowledge: Building on the work of previous generations through explicit instruction and documentation. Collaborate: Combining cognitive efforts to achieve collective goals. Ethical Implications of AI Development Prof. Valiant emphasized the ethical considerations in designing AI systems that reflect positive human traits while avoiding the replication of human flaws. Key takeaways included:\nAvoiding Bias: Ensuring that AI systems do not inherit societal biases. Transparent Decision-Making: Designing AI that can explain its reasoning processes. Augmenting Human Capabilities: Building systems that complement rather than replace human intelligence. Addressing Concerns About AI Accountability: Who is responsible for decisions made by AI systems? Safety: How do we ensure AI systems act in the best interest of humanity? Global Standards: The need for international cooperation to establish ethical guidelines. The Future of AI and Educability Prof. Valiant concluded with a vision for the future:\nInterdisciplinary Collaboration: Bringing together technologists, ethicists, and cognitive scientists to advance AI responsibly. Formalizing Educability: Developing mathematical models to encode human-like learning in machines. Human-Machine Collaboration: Leveraging AI to enhance human cognitive abilities, leading to breakthroughs in science, medicine, and technology. A Call to Action Understanding the parameters of educability is not just an academic pursuit; it is a moral imperative. By integrating insights from cognitive science, mathematics, and ethics, we can create AI systems that are not only intelligent but also aligned with human values.\nThis exploration of educability challenges us to rethink the foundations of intelligence and its implications for the future of artificial intelligence. As we navigate the complexities of AI development, let us be guided by the principles of transparency, accountability, and collaboration.\nPlenary Lecture: Compressing Proofs using Cryptography (Prof Yael Kalai) Cryptography has always been the backbone of secure systems, enabling trust in decentralized and distributed environments. As computation scales, the demand for efficient proof systems that ensure both correctness and privacy becomes increasingly important. Succinct proofs represent a groundbreaking development in this domain, offering compact, verifiable proofs that maintain efficiency and security.\nIn a recent lecture, Prof. Yael Tauman Kalai presented her advancements in succinct proofs, detailing their cryptographic foundations, practical implications, and applications in areas like blockchain and artificial intelligence (AI). This blog unpacks her insights and explores how these proofs address modern computational challenges.\nWhat Are Succinct Proofs? Succinct proofs are cryptographic constructs designed to reduce the size and verification complexity of traditional proof systems. Instead of requiring a verifier to redo the entire computation to confirm its validity, succinct proofs enable verification with minimal computational effort. This efficiency is achieved without sacrificing security or trust, making them a critical tool for systems where scalability and privacy are paramount.\nUnlike traditional proof systems, which may involve large data sets and complex computations, succinct proofs achieve their compactness through advanced cryptographic techniques such as homomorphic encryption, polynomial commitments, and elliptic curve cryptography. These techniques enable a prover to encode the essential details of a computation into a small, verifiable proof.\nKey Cryptographic Techniques One of the core building blocks of succinct proofs is the concept of zero-knowledge proofs (ZKPs). A ZKP allows a prover to convince a verifier that a statement is true without revealing any information beyond its validity. This ensures privacy while maintaining trust.\nFor example, in a blockchain transaction, a ZKP can prove the correctness of the transaction without disclosing the sender, receiver, or amount. This is particularly critical in privacy-preserving protocols like zk-SNARKs (Zero-Knowledge Succinct Non-Interactive Arguments of Knowledge) and zk-STARKs (Zero-Knowledge Scalable Transparent Arguments of Knowledge).\nzk-SNARKs rely on a trusted setup to initialize the system. They use elliptic curve pairings and polynomial arithmetic to achieve their compactness. Despite their efficiency, the trusted setup requirement introduces potential vulnerabilities if compromised. zk-STARKs, on the other hand, eliminate the need for a trusted setup by relying on hash functions and polynomial interpolation. While this makes them more transparent and secure, they often result in larger proof sizes compared to zk-SNARKs. Polynomial Commitments Another crucial technique in succinct proofs is the use of polynomial commitments. These commitments enable the prover to encode computations as polynomials, allowing the verifier to check their correctness without directly interacting with the underlying data. Polynomial commitments are a cornerstone of many cryptographic protocols, including zk-STARKs and modern succinct proof systems.\nApplications of Succinct Proofs One of the most impactful applications of succinct proofs is in blockchain technology. Blockchains, by design, require every participant to validate transactions to maintain trust. However, as the number of transactions grows, this validation becomes a bottleneck.\nSuccinct proofs offer a solution by enabling participants to verify the correctness of transactions without processing the full chain. Protocols like Ethereum\u0026rsquo;s Layer 2 solutions and Zcash leverage succinct proofs to improve scalability and maintain privacy.\nFor instance:\nRollups in Ethereum aggregate transactions off-chain and use succinct proofs to certify their correctness on-chain. Privacy-focused blockchains like Zcash use zk-SNARKs to enable shielded transactions, ensuring that details about the sender, receiver, and transaction amount remain confidential. AI Model Verification In the realm of AI, succinct proofs are emerging as a tool to certify the correctness of model outputs. As AI systems grow more complex, the ability to verify their decisions becomes a challenge. Succinct proofs can be used to generate a compact, verifiable record of an AI model‚Äôs decision-making process.\nFor example, in image classification tasks, a succinct proof could certify that the model correctly identified an object without requiring the verifier to process the entire dataset or model. This has profound implications for AI applications in critical domains like healthcare, where trust and accountability are paramount.\nDecentralized Identity Decentralized identity systems aim to give individuals control over their personal data while allowing them to prove certain attributes (e.g., age, citizenship) without revealing unnecessary details. Succinct proofs enable such systems by providing compact, privacy-preserving verifications.\nProtocols like Verifiable Credentials (VCs) and Decentralized Identifiers (DIDs) rely on these cryptographic techniques to ensure that identity verification is both efficient and secure.\nChallenges and Future Directions While succinct proofs offer significant advantages, they are not without challenges:\nTrusted Setup: zk-SNARKs require a trusted setup, which, if compromised, could undermine the security of the entire system. Research into transparent setups (as in zk-STARKs) aims to address this limitation.\nComputation Overhead: Although succinct proofs reduce verification complexity, the prover\u0026rsquo;s computational requirements can be high. Optimizing the proving process is an active area of research.\nInteroperability: For widespread adoption, succinct proof systems must integrate seamlessly with existing technologies. This involves developing standards and protocols that ensure compatibility across platforms.\nQuantum Resistance: As quantum computing advances, many cryptographic systems, including those used in succinct proofs, face potential vulnerabilities. Developing quantum-resistant proof systems is a critical area of ongoing research.\nInsights from Prof. Yael Tauman Kalai‚Äôs Research Prof. Kalai‚Äôs work pushes the boundaries of succinct proofs by exploring new cryptographic primitives and optimizing existing protocols. Her research emphasizes collaboration between academia and industry to address real-world challenges. Key areas of her focus include:\nEnhancing Transparency: Developing protocols that eliminate the need for trusted setups while maintaining efficiency and scalability. Improving Scalability: Optimizing proof generation to reduce computational overhead for the prover. Expanding Applications: Applying succinct proofs to emerging fields like decentralized finance (DeFi), secure voting systems, and federated learning. Succinct proofs represent a paradigm shift in cryptography, enabling efficient, scalable, and privacy-preserving verification across various domains. From blockchain scalability to AI model verification, their potential applications are vast and transformative. However, realizing their full potential requires addressing challenges like computational overhead, interoperability, and quantum resistance.\nAs cryptographic research evolves, the collaboration between researchers, industry, and policymakers will be essential to unlock the full potential of succinct proofs. Prof. Kalai‚Äôs groundbreaking work serves as a testament to the importance of pushing the boundaries of what cryptography can achieve.\nEngaging in Dialogues on the Ethics of AI The summit wasn‚Äôt just about science and innovation‚Äîit also provided a platform to engage in crucial discussions about the ethical implications of technological advancements. One such discussion was the Panel Huddle titled ‚ÄúEthics of Scientific Research in the Age of AI‚Äù, featuring prominent professors like Adi Shamir, Shafi Goldwasser, and Kalai.\nThe panel explored topics such as algorithmic bias, dual-use AI technologies, privacy-preserving AI, and the responsibilities of researchers in educating the public and policymakers. These discussions offered profound insights into the complexities of conducting ethical research in a rapidly evolving technological landscape, reminding me of the broader purpose of scientific innovation: to create a more equitable future.\nFigure 11: Attending a talk on ‚ÄúEthics of Scientific Research in the Age of AI‚Äù.\nEthics of Scientific Research in the Age of AI Artificial intelligence (AI) is revolutionizing scientific research, offering transformative tools to accelerate discovery, enhance accuracy, and expand the boundaries of human knowledge. However, these advancements also bring profound ethical challenges. At the recent panel moderated by Prof. Simon Chesterman from the National University of Singapore, renowned experts Prof. Joan Rose (2016 Stockholm Water Prize), Prof. Yael Kalai (2022 ACM Prize in Computing), and Prof. Adi Shamir (2002 Turing Award) delved into the critical ethical considerations of AI in scientific research.\nClient Concerns About AI AI systems increasingly shape scientific discovery, yet their application necessitates robust ethical guidelines. Governance frameworks must:\nDefine clear boundaries for AI use. Address biases inherent in data and algorithms. Ensure transparency and accountability in AI-driven research outcomes. Human Interaction The integration of AI into research workflows often alters the dynamic between human researchers and technology. Key considerations include:\nPreventing over-reliance on AI outputs, which could lead to critical oversights. Mitigating risks of misinterpretation of AI-generated results. Balancing human intuition with machine precision. Opportunities and Risks AI holds immense potential to transform scientific methodologies:\n$$ \\text{Efficiency Gain} = \\frac{T_{\\text{manual}}}{T_{\\text{AI-assisted}}} $$\nWhere $T_{\\text{manual}}$ represents the time for traditional methods and $T_{\\text{AI-assisted}}$ denotes AI-accelerated approaches. While this formula underscores AI‚Äôs ability to enhance efficiency, ethical concerns include:\nMisinformation propagation due to AI biases. Job displacement for researchers whose roles are increasingly automated. Military Applications AI‚Äôs role in military research raises acute ethical dilemmas, particularly in autonomous weapons systems. Questions of accountability and decision-making in lethal scenarios must be urgently addressed:\nWho is accountable for errors in autonomous systems? How can ethical principles such as proportionality and necessity be encoded into AI? Public Engagement A significant gap exists between the scientific community and the public‚Äôs understanding of AI. To bridge this divide:\nDevelop accessible communication strategies to explain AI technologies and their implications. Foster public trust through transparent disclosures of AI‚Äôs capabilities and limitations. Proactive Education Educating researchers and policymakers is vital to:\nPromote ethical awareness. Equip them to evaluate the societal impacts of their work. Accountability and Regulation Regulating AI is a complex yet essential endeavor. Key areas of focus include:\nWarfare Applications: Establishing international norms to prohibit unethical AI use. Public Safety: Creating standards for AI deployment in sensitive domains such as healthcare and transportation. Global Cooperation International collaboration is critical to:\nDevelop shared standards for AI ethics. Address cross-border challenges, such as data privacy and AI governance. Future Directions Investing in research on AI risks enables policymakers to adopt informed, proactive measures rather than reactive regulations post-crisis. Proposed funding distribution can be modeled as:\n$$ \\text{Allocation} = \\frac{R_{\\text{risk}}}{R_{\\text{total}}} \\times 100 % $$\nWhere $R_{\\text{risk}}$ represents research focused on AI risks, and $R_{\\text{total}}$ is the total research budget.\nInterdisciplinary Approach Addressing AI‚Äôs ethical challenges requires collaboration among:\nTechnologists: To refine AI systems. Ethicists: To integrate moral principles into AI development. Policymakers: To enact effective regulations and governance. The panel underscored that while AI offers unparalleled opportunities for scientific progress, its ethical integration requires foresight, collaboration, and accountability. By fostering interdisciplinary dialogue and committing to transparent, responsible practices, the scientific community can ensure AI serves as a force for good.\nVisual Representation A mind map summarizing these insights can help readers visualize the ethical considerations of AI in scientific research.\nVisual Representation Figure 12: A mind map summarizing key ethical considerations in the use of AI for scientific research, discussed during GYSS2025. Expanding My Network and Building New Collaborations Beyond the lectures and discussions, GYSS 2025 was also a chance to meet fellow researchers from all over the globe. I had the pleasure of connecting with inspiring individuals from diverse fields of study, creating friendships that will last a lifetime. The exchange of ideas with these brilliant young scientists has already sparked new collaborations and research ideas that I can‚Äôt wait to explore further.\nA Day I‚Äôll Never Forget All in all, my experience at GYSS 2025 was beyond what I could have imagined. It was a perfect blend of research, networking, and fun. It gave me the chance to engage with some of the brightest minds in science, learn from Nobel Laureates and Turing Award winners, and discuss pressing issues in AI and ethics. It was an unforgettable experience, and I‚Äôm leaving with new ideas, fresh perspectives, and the motivation to continue pushing the boundaries of my own research.\nFigure 13: Immersed in the vibrant academic atmosphere of GYSS2025.\nFigure 14: Standing proudly with the GYSS2025 banner‚Äîfirst time wearing a formal suit in years!\nFigure 15: Beautiful campus vibes at NUS, Singapore‚Äîa serene and inspiring place for learning.\nPS. This year, GYSS 2025 was hosted at the beautiful campus of the National University of Singapore (NUS), and I absolutely fell in love with the environment. The vibrant atmosphere, lush greenery, and modern architecture made it the perfect blend of nature and innovation‚Äîa truly ideal space for creativity and learning.\nCitation Panboonyuen, Teerapong. (Jan 2025). Global Young Scientists Summit (GYSS) 2025: Where Science Meets Inspiration. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/\nFor a BibTeX citation:\n@article{panboonyuen2025gyss, title = \u0026quot;Global Young Scientists Summit (GYSS) 2025: Where Science Meets Inspiration\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2025\u0026quot;, month = \u0026quot;Jan\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/\u0026quot;} Did you find this page helpful? Consider sharing it üôå ","date":1736726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1736726400,"objectID":"c408220127497c9193b92b74c6b70a6c","permalink":"https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/","publishdate":"2025-01-13T00:00:00Z","relpermalink":"/blog/2025-01-11-where-science-meets-inspiration/","section":"blog","summary":"Competing for a prestigious scholarship to attend GYSS 2025 in Singapore. The summit will be an excellent opportunity to connect with global experts and young scientists, fostering innovation and interdisciplinary collaboration.","tags":["GYSS","Science","Innovation","Collaboration","Scholarship"],"title":"Global Young Scientists Summit (GYSS) 2025: Where Science Meets Inspiration","type":"blog"},{"authors":[],"categories":null,"content":" ","date":1729794600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729794600,"objectID":"67f5e74e2acc79797bbe6b8a6b05621d","permalink":"https://kaopanboonyuen.github.io/talk/young-scientists-quickfire-pitch/","publishdate":"2024-10-24T18:30:00Z","relpermalink":"/talk/young-scientists-quickfire-pitch/","section":"event","summary":"In this quick pitch, I‚Äôm thrilled to introduce MeViT‚Äîa medium-resolution Vision Transformer developed for high-precision semantic segmentation of Landsat satellite imagery.","tags":[],"title":"Young Scientists Quickfire Pitch","type":"event"},{"authors":["Teerapong Panboonyuen"],"categories":["Running","Swimming","Japan","Tokyo","Osaka","Kyoto"],"content":" Hey guys! I just completed an incredible running experience across Japan\u0026rsquo;s vibrant cities‚ÄîOsaka, Kyoto, and Tokyo. I managed to hit sub-1-hour times while soaking in the unique vibes of each place. It was the perfect mix of pushing my pace and enjoying the sights along the way!\nIt was such a cool blend of testing my limits and discovering new places. Each run through those iconic spots felt like I was getting a real taste of Japan‚Äôs unique vibe and energy with every stride.\nJust finished an awesome running tour through Osaka, Kyoto, and Tokyo! üèÉ‚Äç‚ôÇÔ∏è Nailed sub-1-hour times while soaking in all the city vibes. Can\u0026#39;t believe how much fun it was to race and explore at the same time! üáØüáµ #RunningExperience #Japanhttps://t.co/Wy2bpgO8lg\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) September 25, 2024 Chapter 1: Osaka Running This trip marks my third time in Japan, but this time I started in the Kansai region, kicking things off in Osaka. My main goal? To conquer a city run and aim for a sub-1-hour 10K.\nBright and early, after a good night‚Äôs rest, I hit the streets at 4:54 AM. The weather was just right, making the run super enjoyable. I ended up covering 10.59K at a pace of 4:42, finishing in 49 minutes. Here‚Äôs the route I took:\nStart at Awaza Explore the route here üó∫Ô∏è Matsushima Park Explore the route here üìç Chiyoza Explore the route here üó∫Ô∏è Minamiizuo Park Explore the route here üìç Kitatsumorinaka Park Explore the route here üó∫Ô∏è Deshirohigashi Intersection Explore the route here üìç Machida Gastroenterology Hospital Explore the route here üó∫Ô∏è Tsutenkaku Tower Explore the route here üìç Finish at Matsuyamachi Explore the route here üó∫Ô∏è Nipponbashi Explore the route here üìç Osaka is truly a city that hooks you; it‚Äôs filled with hidden gems waiting to be explored. After three action-packed days, it was time to move on to Kyoto!\nChapter 2: Kyoto Running Next up was the challenge of a sub-1-hour city run in Kyoto. I only had two days here, and luck wasn‚Äôt on my side‚Äîrain greeted me in the morning. But nothing was going to stop my determination!\nI crushed the city run in Kyoto, finishing 10.78K in 49 minutes (pace 4:38), which was even faster than my Osaka run! I think the rain actually helped; the air felt fresh and invigorating. Here‚Äôs my route:\nStart at Kiyomizu-Jojo Station Explore the route here üó∫Ô∏è Kyoto Pref. Yakuzaishikai Explore the route here üìç Matsubaracho Explore the route here üó∫Ô∏è Kawebata Police Station Explore the route here üìç Kyoto University (Yoshida Campus) Explore the route here üó∫Ô∏è Imadegawa Street Explore the route here üìç Kyoto Imperial Palace (Kyoto Gyoen) Explore the route here üó∫Ô∏è Naka-Dachiuri-Dori Explore the route here üìç Horikawa Sanjo Explore the route here üó∫Ô∏è Nishiki Market Explore the route here üìç Wrapping up my run in Kyoto felt like a major win‚Äî2 out of 3 goals achieved! Before heading to my final destination, I hopped on the Shinkansen back to Tokyo.\nI took my first-ever Shinkansen ride from Kyoto to Tokyo, leaving at 9:39 AM and arriving at 11:54 AM. It was an absolute treat! I opted for the Green Car for that extra comfort and snagged a window seat to soak in the views. Here‚Äôs a glimpse of that journey:\nShinkansen from Kyoto to Tokyo Explore the route here üöÑ Chapter 3: Tokyo Running Finally, I‚Äôve made it to Tokyo! There‚Äôs always a surprise waiting here. Before my city run, a professor from Todai treated me to a swim at the Olympic pool used during the Tokyo 2020 Games. What an experience! I swam 1.2K in just 28 minutes at a pace of 2:25 per 100m. The Olympic-standard pool was incredible, and swimming alongside serious athletes was an adrenaline rush.\nNow, back to the final running mission. On my second day in Tokyo, I kicked off my run at 6:52 AM. The weather was perfect‚Äî18¬∞C with a refreshing breeze‚Äîideal for running.\nI wrapped up the city run covering 10.17K in 44 minutes at a pace of 4:25, marking a personal best! Here‚Äôs my route in Tokyo:\nStart at Shiomi Dori Street Explore the route here üó∫Ô∏è Tokyo Aquatics Centre Explore the route here üìç Yumenoshima Stadium Explore the route here üó∫Ô∏è Arakawawangan-kyo Bridge Explore the route here üìç Nakasakombashi Explore the route here üó∫Ô∏è Seishin Itchu North Explore the route here üìç Kiyosuna-Ohashi Bridge Explore the route here üó∫Ô∏è Ojima Elementary School Explore the route here üìç Finish at Ojima Komatsugawa Park Explore the route here üó∫Ô∏è Kiba Park Explore the route here üìç Chapter 4: Tokyo Farewell Run Before wrapping up my Tokyo adventure tomorrow, I took advantage of this morning‚Äôs cool weather, with a light drizzle setting the perfect scene for my final city run‚Äîa 5K to bid this trip a fond farewell. I covered 5.16 km in 23:54 minutes (Pace 4:38). The highlight? My VO2 Max hit a new high at 51, and Garmin labeled my training status as ‚ÄúPeaking.‚Äù It‚Äôs the ultimate runner‚Äôs high!\nI started my run at Nihombashihakozakicho, crossed the Eitai Bashi bridge, and continued parallel to the scenic Sumida River. Passing through the vibrant Kiyosumi-Dori Avenue at kilometer 2, I found myself immersed in Tokyo‚Äôs unique urban rhythm. I reached the serene Kiyosumi Garden by kilometer 3, before heading to Gokenborui Park at kilometer 4. The run climaxed with a sprint up Shin-Ohashi bridge, finishing at the 5K mark on the dot.\nThis city run was the perfect way to close out my time in Tokyo. From the calm river views to the bustling cityscape, every step was a reminder of how much this place has to offer. Until next time, Tokyo!\nSummary of My Running Experience Osaka Running: Sub1 10.59K City Run, 49:48min, Pace 4:42 Kyoto Running: Sub1 10.78K City Run, 49:57min, Pace 4:38 Tokyo Running: Sub1 10.17K City Run, 44:58min, Pace 4:25 Tokyo Swimming: 1,200m (1.2K), 28:58min, Pace 2:25 Tokyo Running: Sub24min 5.16K City Run, 23:54min, Pace 4:38 This trip to Japan was absolutely fantastic! Even though it was late summer and winter is just around the corner, running through these three major cities expanded my horizons. I encountered amazing insights into Japanese culture, met locals, and explored unseen places.\nI can‚Äôt wait for the next experience, hopefully tackling the Tokyo Marathon (42.195K) someday! Fingers crossed!\nUntil next time, happy running! üèÉ‚Äç‚ôÇÔ∏èüáØüáµ\nCitation Panboonyuen, Teerapong. (Sep 2024). Sub1 Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/\nFor a BibTeX citation:\n@article{panboonyuen2024running, title = \u0026quot;Sub1 Urban Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Sep\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/\u0026quot;} Did you find this page helpful? Consider sharing it üôå ","date":1727222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727222400,"objectID":"f5b1681f3a489b3e2bca1e5e7d9c2ff3","permalink":"https://kaopanboonyuen.github.io/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/","publishdate":"2024-09-25T00:00:00Z","relpermalink":"/blog/2024-09-25-sub1-10k-running-osaka-kyoto-and-tokyo/","section":"blog","summary":"An incredible running experience across Japan's vibrant cities‚ÄîOsaka, Kyoto, and Tokyo. I managed to hit sub-1-hour times while soaking in the unique vibes of each place. It was the perfect mix of pushing my pace and enjoying the sights along the way!","tags":["Running","Swimming","Japan","Tokyo","Osaka","Kyoto"],"title":"Sub1 Urban Running Experience: Crushing Times and Exploring Osaka, Kyoto, and Tokyo","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":["research","conferences","computer-vision"],"content":" You can read our full paper on Springer here: MARS at ICIAP 2023 Presenting MARS: Mask Attention Refinement with Sequential Quadtree Nodes at ICIAP 2023 In September 2023, I had the privilege of presenting my latest AI research, MARS (Mask Attention Refinement with Sequential Quadtree Nodes), at the International Conference on Image Analysis and Processing (ICIAP 2023), held in Udine, Italy, from September 11‚Äì15. MARS is a specialized deep learning model developed for car damage instance segmentation, a vital application in the automotive insurance and garage service sectors.\nüçÑ We\u0026#39;re thrilled to unveil MARS: a groundbreaking approach utilizing Attention Refinement with Sequential Quadtree Nodes.\n.\nPaper: https://t.co/UayUSxmZep\nCode: https://t.co/RoNFjSslXr\nProject: https://t.co/uSoBX21HpF\n.#AI #ComputerVision #DeepLearning #Research pic.twitter.com/oc8gz7Hs9I\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) August 11, 2024 Our model leverages Self-Attention Mechanisms combined with Sequential Quadtree Nodes, which together enable MARS to achieve significantly higher accuracy than state-of-the-art baselines such as Mask R-CNN (ICCV 2017), PointRend (CVPR 2020), and Mask Transfiner (CVPR 2022). These improvements were validated across multiple benchmark datasets, including a large Thai car-damage dataset.\nMARS was accepted at ICIAP 2023 with a competitive acceptance rate of 0.64, and our work is published in the Springer Lecture Notes in Computer Science series. This international recognition affirms the impact and novelty of our approach.\nFigure 1: Presenting MARS: Mask Attention Refinement with Sequential Quadtree Nodes at ICIAP 2023 in Udine, Italy. MARS introduces an innovative self-attention mechanism combined with a quadtree transformer to achieve highly accurate car damage instance segmentation, significantly outperforming current state-of-the-art methods on multiple benchmark datasets. This advancement addresses critical challenges in the car insurance industry. (Source: ICIAP 2023 - LNCS 14365) Figure 2: Showcasing the MARS project through an engaging poster presentation at ICIAP 2023, highlighting the innovative mask attention refinement techniques and sequential quadtree nodes that drive our state-of-the-art approach to car damage instance segmentation. This presentation sparked insightful discussions with experts and attendees, further emphasizing the potential real-world impact of our research in advancing intelligent vision systems. Why MARS Matters: From Thailand to Italy, Building the Future of Automotive AI The journey of MARS began not in a conference hall in Europe but in the bustling cities and garages of Thailand, where the real-world challenges of the automotive industry sparked a vision. Every day, countless car owners and insurance agents face delays, inconsistencies, and even disputes when assessing vehicle damage. Manual inspections often rely on human judgment ‚Äî which can be slow, subjective, or prone to error. In garages, mechanics struggle with non-standardized repair cost estimates. Insurers grapple with fraudulent claims, inflated repairs, and time-consuming paperwork. These challenges aren‚Äôt unique to Thailand ‚Äî they‚Äôre global. But we decided to start solving them at home.\nAt MARSAIL ‚Äî the Motor AI Recognition Solution Artificial Intelligence Laboratory ‚Äî we asked ourselves: Can we create a deep learning system that sees, understands, and evaluates vehicle damage better than a human? Could we develop an AI model smart enough to assist insurers, fast enough for real-time applications, and precise enough to help garage operators deliver fair, standardized repairs?\nWe believe the answer is yes ‚Äî and our vision for this transformation is detailed in our MARSAIL blog: üëâ MARSAIL: The Smart Engine Behind the Future of Car Insurance and Intelligent Garages\nThe answer was MARS ‚Äî Mask Attention Refinement with Sequential Quadtree Nodes ‚Äî our flagship AI architecture built from the ground up for car damage instance segmentation. Unlike traditional models like Mask R-CNN or PointRend, which were designed for general-purpose segmentation tasks, MARS is laser-focused. It leverages quadtree spatial hierarchies, self-attention refinement, and a custom transformer-based backbone to identify scratches, dents, cracks, and broken parts ‚Äî even in complex lighting, occlusion, or varied car surface conditions.\nWhat sets MARS apart is its precision. It doesn‚Äôt just draw rough outlines ‚Äî it understands the contours of the damage. Whether it‚Äôs a shallow scratch on a bumper or a crumpled fender after a collision, MARS detects it with clarity, outperforming state-of-the-art models with a significant margin in our Thai car damage benchmarks. And this isn‚Äôt just academic ‚Äî this is AI for the real world, trained with real damage cases, tuned for high-stakes applications like insurance claims and repair verification.\nSo when ICIAP 2023 announced their international conference in Udine, Italy, we knew it was time to take our Thai-born innovation global. From Bangkok to Udine, we carried not just a poster and a paper ‚Äî but a vision. A vision that AI can transform the automotive industry, not by replacing people, but by empowering them: giving insurers confidence, garages clarity, and drivers trust in the system.\nPresenting MARS in Italy wasn‚Äôt just a research milestone ‚Äî it was a symbol of what‚Äôs possible when bold ideas are met with rigorous engineering and a passion for solving real-world problems. Our work was selected for publication in the Lecture Notes in Computer Science by Springer, a recognition that underscores the technical excellence and practical value of our research.\nAt MARSAIL, our mission continues. We are building an end-to-end ecosystem of automotive AI: from car damage detection to automated cost estimation, from OCR document parsing to insurance fraud detection, all powered by AI models trained with diverse Thai datasets and built to serve global standards. Because in the future we see, a single smartphone photo is enough to initiate a car claim, verify vehicle condition, and provide a fair quote ‚Äî all in seconds. This is the Digital Insurance Twin we‚Äôre crafting. And we believe it starts with intelligence, integrity, and innovation ‚Äî the core values of MARS and everything we do at MARSAIL.\nFigure 3: Detailed illustration of the MARS deep learning architecture, emphasizing the novel integration of sequential quadtree nodes with mask attention refinement mechanisms. This design enables precise, efficient instance segmentation of car damage by capturing both global context and fine-grained details, setting a new benchmark in intelligent vision models. MARS: A Quadtree-Driven Deep Learning Architecture for Precision Car Damage Segmentation Central to the design of MARS (Mask Attention Refinement with Sequential Quadtree Nodes) lies a novel AI architecture designed to push the boundaries of instance segmentation‚Äîspecifically, for car damage detection. Traditional models like Mask R-CNN rely heavily on coarse ROI alignments and fixed spatial grids, which often result in poor mask quality for irregular or fine-grained structures like scratches, dents, or cracks. MARS overcomes this limitation through a unique quadtree-based spatial hierarchy, allowing the network to adaptively decompose images into finer patches only where needed, focusing computational attention on high-detail regions.\nTechnically, our method integrates a Self-Attention Refinement Module (SARM) with Sequential Quadtree Node Encoding to build hierarchical context from coarse-to-fine levels. Instead of treating spatial locations uniformly, we recursively subdivide image regions using quadtree decomposition, capturing fine structural features with higher resolution where necessary. Let $Q = {q_1, q_2, \\dots, q_n}$ denote the set of quadtree nodes generated per instance mask. Each node $q_i$ is embedded and passed through a transformer encoder, which models long-range dependencies via multi-head self-attention:\n$$ \\text{Attention}(Q) = \\text{softmax}\\left(\\frac{QW_Q (QW_K)^T}{\\sqrt{d_k}}\\right)QW_V $$\nThese representations are aggregated to recalibrate channel-wise features dynamically, enabling MARS to predict high-fidelity instance masks with pixel-level accuracy.\nIn practice, MARS outperforms Mask R-CNN, PointRend, and Mask Transfiner by +1.3 to +2.3 maskAP on a Thai car damage dataset‚Äîdemonstrating that our approach is not only theoretically elegant but also empirically superior. Our model operates on top of the FPN backbone (ResNet-50 and ResNet-101 variants), and benefits from a cascaded refinement pipeline that improves mask boundaries at every stage. The full details, ablation studies, and demo results are available in our GitHub.\nInternational Recognition and Impact The ICIAP conference was a vibrant gathering of leading researchers and industry experts. I also had the honor to deliver a guest talk on modern AI advances in Large Language Models (LLMs), engaging with an international audience about cutting-edge AI trends beyond computer vision.\nFigure 4: Immersed in a dynamic exchange of groundbreaking ideas at ICIAP 2023, where Professor Tomas Pajdla unraveled the complexities of Algebraic Vision, Andrew Fitzgibbon pushed the boundaries of AI hardware and real-world AI applications, and Danijel Skoƒçaj showcased pioneering advances in data-driven surface anomaly detection‚Äîeach lecture illuminating the future of intelligent vision systems. The atmosphere at ICIAP 2023 in Udine, Italy, was truly invigorating‚Äîfilled with intellectual energy, meaningful discussions, and a deep sense of community among researchers pushing the boundaries of computer vision and AI.\nFigure 5: Capturing the vibrant and electric atmosphere at ICIAP 2023 in Udine, where leading minds from around the globe converged to share cutting-edge research, spark innovative collaborations, and shape the future of intelligent systems and computer vision. Exploring Italy: From Udine to Rome After the conference wrapped up, I took the chance to travel to Rome and experience Italy‚Äôs timeless heritage. Walking through the magnificent St. Peter‚Äôs Basilica and the Vatican Museums was a breathtaking journey into art and spirituality.\nThe Colosseum, standing proudly as an icon of ancient Roman engineering and history, was truly awe-inspiring. And of course, tossing a coin into the Trevi Fountain was a magical moment steeped in legend ‚Äî a wish for continued success in research and life.\nFor the full story of my unforgettable journey ‚Äî from academic presentation to wandering the timeless streets of Rome ‚Äî feel free to read my detailed travel and research blog here: Showcasing My AI Research in Italy: A Memorable September Work Trip. In that post, I share not only the highlights of presenting our MARS model at ICIAP 2023, but also the personal moments that made the trip truly special ‚Äî from the intellectual exchanges at the conference to standing in awe before the Colosseum and Vatican. It was a journey that beautifully merged science, culture, and inspiration.\nPresenting MARS at ICIAP 2023 was not just a research highlight‚Äîit embodied the core mission of our lab, MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory). MARSAIL is a leading research hub in Thailand, dedicated to developing intelligent technologies for automotive insurance, damage assessment, and garage automation. Guided by Dr. Teerapong Panboonyuen, the lab combines academic depth with industry-driven goals, fostering impactful innovations like MARS that bridge research and real-world application.\nFigure 6: MARSAIL ‚Äî the Motor AI Recognition Solution Artificial Intelligence Laboratory, a cutting-edge research lab in Thailand pioneering smart technologies for car insurance and garage systems. üöÄ Dive Deeper into MARSAIL Curious how MARS redefines car damage segmentation with intelligent attention and quadtree refinement? Explore our full paper on arXiv, browse code and benchmarks on GitHub, or experience the project in action on the official MARS project page.\nUnlock the tech that‚Äôs shaping the future of automotive AI.\nAttending and presenting at ICIAP 2023 in Udine was an incredible opportunity to engage with a vibrant community of leading researchers and innovators in image analysis and processing. The event offered a dynamic platform to showcase our pioneering work on MARS ‚Äî a breakthrough in car damage instance segmentation that combines advanced self-attention and quadtree architectures to push the boundaries of what intelligent vision systems can achieve. Beyond presenting our research, the chance to exchange ideas with experts like Professor Tomas Pajdla, Andrew Fitzgibbon, and Danijel Skoƒçaj enriched our perspective and fueled new inspiration. ICIAP 2023 not only highlighted cutting-edge advancements but also fostered collaborations that will undoubtedly shape the future of AI-driven image understanding. This experience has strengthened our commitment to advancing impactful, real-world AI solutions and deepened our excitement for the rapidly evolving field of computer vision. Thank you for joining me on this incredible journey where cutting-edge AI research meets the timeless charm of Italy. Presenting MARS at ICIAP 2023 was more than a milestone‚Äîit was a moment of inspiration, surrounded by global minds and unforgettable landscapes. Here\u0026rsquo;s to pushing boundaries, one breakthrough at a time.\n‚Äì Kao Panboonyuen\nCitation Panboonyuen, Teerapong. (Sep 2023). Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/\nFor a BibTeX citation:\n@article{panboonyuen2023mars, title = \u0026quot;Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io\u0026quot;, year = \u0026quot;2023\u0026quot;, month = \u0026quot;Sep\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/\u0026quot; } Did you find this page helpful? Consider sharing it üôå ","date":1726164000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726164000,"objectID":"58fd222ee2cadd0d66ef73cc6f60d81c","permalink":"https://kaopanboonyuen.github.io/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/","publishdate":"2024-09-12T18:00:00Z","relpermalink":"/blog/2023-09-12-showcasing-mars-in-italy-next-gen-ai/","section":"blog","summary":"Presenting MARS, a cutting-edge AI model for car damage instance segmentation, at ICIAP 2023 in Udine, Italy, and exploring Italy‚Äôs iconic landmarks.","tags":["AI-research","computer-vision","deep-learning","ICIAP2023","Italy"],"title":"Showcasing MARS in Italy: Next-Gen AI for Car Insurance and Garage Solutions at ICIAP 2023","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"","date":1725840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725840000,"objectID":"41db8f7e024b0970d99c8f0c3a9f7a26","permalink":"https://kaopanboonyuen.github.io/publication/sea-vit-sea-surface-currents-forecasting/","publishdate":"2024-09-09T00:00:00Z","relpermalink":"/publication/sea-vit-sea-surface-currents-forecasting/","section":"publication","summary":"Forecasting sea surface currents is essential for applications such as maritime navigation, environmental monitoring, and climate analysis, particularly in regions like the Gulf of Thailand and the Andaman Sea. This paper introduces SEA-ViT, an advanced deep learning model that integrates Vision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to capture spatio-temporal covariance for predicting sea surface currents (U, V) using high-frequency radar (HF) data. The name SEA-ViT is derived from Sea Surface Currents Forecasting using Vision Transformer, highlighting the model's emphasis on ocean dynamics and its use of the ViT architecture to enhance forecasting capabilities. SEA-ViT is designed to unravel complex dependencies by leveraging a rich dataset spanning over 30 years and incorporating ENSO indices (El Ni√±o, La Ni√±a, and neutral phases) to address the intricate relationship between geographic coordinates and climatic variations. This development enhances the predictive capabilities for sea surface currents, supporting the efforts of the Geo-Informatics and Space Technology Development Agency (GISTDA) in Thailand's maritime regions. The code and pretrained models are available at https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents.","tags":["Deep Learning","Sea Surface Currents Forecasting","Vision Transformer","GRU","Spatio-Temporal Covariance Modeling"],"title":"SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":["LLMs","LULC","Deep Learning","Satellite Images"],"content":" You can view the presentation slides for the talk üåé here. Introduction to Large Language Models (LLMs) Large Language Models (LLMs) are at the forefront of a revolution in Artificial Intelligence (AI) and Natural Language Processing (NLP). These models are not just sophisticated algorithms; they represent a leap forward in how machines understand and generate human language. Leveraging cutting-edge deep learning architectures, such as transformers, LLMs have transformed the landscape of language technology.\nWe\u0026#39;re releasing a preview of OpenAI o1‚Äîa new series of AI models designed to spend more time thinking before they respond.\nThese models can reason through complex tasks and solve harder problems than previous models in science, coding, and math. https://t.co/peKzzKX1bu\n\u0026mdash; OpenAI (@OpenAI) September 12, 2024 At their essence, LLMs are built on expansive neural networks with billions of parameters. These networks are trained on vast corpora of text data, learning to discern intricate patterns and relationships within language. Through a process known as pre-training, LLMs develop a broad understanding of linguistic structures, context, and semantics. During this phase, they utilize unsupervised learning techniques to predict masked words or sequences, refining their ability to understand and generate coherent text.\nCurious about fine-tuning a satellite-specific LLM model? üåç\n.\nDive into my latest blog to learn more: https://t.co/sd25ByzQpJ\n.#LLM #Geoscience #SatelliteLLM #AI #MachineLearning #Landsat #geography\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) September 10, 2024 Following pre-training, LLMs undergo fine-tuning to adapt their general language capabilities to specific tasks or domains. This supervised learning phase involves training the model on a targeted dataset, allowing it to excel in applications such as text generation, translation, sentiment analysis, and question-answering. Techniques like transfer learning and few-shot learning further enhance the model\u0026rsquo;s adaptability, enabling it to generalize from limited examples and perform across various contexts.\nDeploying LLMs in real-world scenarios involves addressing practical challenges related to computational resources and scalability. These models require substantial processing power and memory, often necessitating the use of advanced hardware like GPUs or TPUs. Despite these demands, the benefits of integrating LLMs into applications‚Äîsuch as chatbots, virtual assistants, content generation, and automated summarization‚Äîare profound, offering significant advancements in how machines interact with human language.\nIn this blog post, I will delve into the technical intricacies of LLMs, exploring their architecture, training methodologies, and deployment considerations. Prepare to discover how these powerful AI tools are pushing the boundaries of language technology and shaping the future of machine intelligence.\nKey Vocabulary Here are some essential terms and acronyms related to LLMs:\nAcronym Meaning AI Artificial Intelligence: The simulation of human intelligence in machines that are programmed to think and learn. ANN Artificial Neural Network: A computational model inspired by biological neural networks. BERT Bidirectional Encoder Representations from Transformers: A model for natural language understanding tasks. CNN Convolutional Neural Network: Effective for processing grid-like data such as images. CRF Conditional Random Field: A statistical modeling method for structured prediction. DNN Deep Neural Network: A neural network with multiple layers. DL Deep Learning: A subset of machine learning with neural networks containing many layers. GPT Generative Pre-trained Transformer: A transformer-based model for generating human-like text. HMM Hidden Markov Model: A model for systems that transition between states with certain probabilities. LSTM Long Short-Term Memory: A type of RNN designed to remember long-term dependencies. LLM Large Language Model: Trained on vast amounts of text data to understand and generate text. ML Machine Learning: Training algorithms to make predictions based on data. NLP Natural Language Processing: The interaction between computers and human language. RAG Retrieval-Augmented Generation: Combines document retrieval with generative models. RNN Recurrent Neural Network: Designed for sequential data. T5 Text-to-Text Transfer Transformer: Converts various tasks into a text-to-text format. Transformer A model architecture that uses self-attention mechanisms. ViT Vision Transformer: A transformer model for image processing. VQA Visual Question Answering: Combining vision and language understanding. VLMs Vision-Language Models: Close the divide between visual and language comprehension in AI. XLNet An extension of BERT with permutation-based training. Hugging Face Platform for NLP with pre-trained models, datasets, and tools. Transformers Library for transformer-based models by Hugging Face. datasets Library for managing datasets, by Hugging Face. Gradio Library for creating machine learning demos with simple UIs. LangChain Facilitates development using LLMs with tools for managing language-based tasks. spaCy Advanced NLP library in Python. NLTK Natural Language Toolkit: Tools for text processing and linguistic analysis. StanfordNLP Library by Stanford University for NLP tasks. OpenCV Library for computer vision tasks. PyTorch Deep learning framework with tensor computations and automatic differentiation. TensorFlow Framework for building and deploying machine learning models. Keras High-level neural networks API running on top of TensorFlow. Fastai Simplifies neural network training with PyTorch. ONNX Open Neural Network Exchange format for model transfer between frameworks. Architecture of LLMs LLMs are built on advanced architectures that often include transformer models. A transformer model utilizes self-attention mechanisms to process input sequences. The core components of a transformer are:\nEncoder: Processes the input data. Decoder: Generates the output sequence. Transformer Architecture Formula The key mathematical operation in transformers is the self-attention mechanism, which can be described as follows:\n$[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V ]$\nwhere:\n$( Q )$ is the query matrix, $( K )$ is the key matrix, $( V )$ is the value matrix, $( d_k )$ is the dimensionality of the keys. Training LLMs Training LLMs involves several steps:\nData Preparation: Collect and preprocess large text corpora. Model Initialization: Start with a pre-trained model or initialize from scratch. Training: Use gradient descent and backpropagation to minimize the loss function. Introduction to LLMs for Satellite Images Fine-tuning a Large Language Model (LLM) like SatGPT for satellite imagery involves several critical stages. This process transforms a pre-trained model into a specialized tool capable of analyzing and generating insights from satellite images. This blog post provides a step-by-step guide to fine-tuning and deploying SatGPT, covering each phase in detail.\nIn their 2024 paper, ‚ÄúGood at Captioning, Bad at Counting: Benchmarking GPT-4V on Earth Observation Data‚Äù (arXiv:2401.17600), Zhang and Wang focus on developing a benchmark for Vision-Language Models (VLMs) applied to Earth Observation (EO) data. Their initial framework addresses three main areas: scene understanding, localization and counting, and change detection. To assess VLM performance across these areas, they design evaluations that span various applications, from wildlife conservation to urban monitoring, as illustrated in Figure 1. Their goals are to evaluate existing VLMs, provide insights into effective prompting techniques for EO tasks, and establish a flexible system for ongoing benchmark updates and future VLM evaluations.\nThe paper highlights several challenges and areas for future work. One major challenge is detecting data contamination, which is crucial for maintaining the fairness and effectiveness of benchmarks as VLMs evolve. Additionally, a more detailed analysis of model failures‚Äîsuch as knowledge gaps, reasoning errors, perceptual mistakes, and text misunderstandings‚Äîcould provide deeper insights into current VLM capabilities. Zhang and Wang also note the static nature of benchmarks as a limitation, suggesting that dynamic updates may be necessary to keep benchmarks relevant and challenging as VLMs advance. #SatGPT is an app that lets you talk to satellite imagery.\nWe\u0026#39;ve got some more work to do before it\u0026#39;s polished, but I\u0026#39;m pretty psyched about the results so far. Powered by @Element84 @STACspec @LangChainAI @Panel_org @HoloViz_org, huge thanks to @ivanziogeo \u0026amp; @MarcSkovMadsen. pic.twitter.com/gO7aZz6w4C\n\u0026mdash; Kevin Lalli (@opticsinspace) June 19, 2023 These findings offer valuable insights into GPT-4V‚Äôs capabilities and limitations, especially in the context of earth observation data.\nFig. 1. Here are examples of inputs and outputs from various benchmark tasks and how five different VLMs stack up. They‚Äôve included just a snippet of the user prompts and model responses to highlight the key points. [Good at captioning, bad at counting]\nExploring Vision-Language Models (VLMs) to Understand High-Level Features in Remotely Sensed Images In my recent work, I\u0026rsquo;ve been diving deep into Vision-Language Models (VLMs) to see how well they perform in tasks that require understanding both visual and textual data. With the explosion of AI models that can interpret images and generate coherent, detailed text, it‚Äôs become increasingly important to assess these models not just on general benchmarks, but in specific, high-stakes domains like remotely sensed imagery.\nRemotely sensed images, which are collected from satellite or aerial platforms, provide a unique challenge for VLMs. They are dense with data, full of patterns, and often contain complex interactions between natural and man-made objects. The ability of a model to not only caption these images but also understand high-level features‚Äîsuch as differentiating between natural landmarks, infrastructure, and potential environmental changes‚Äîcan have far-reaching applications in fields like agriculture, urban planning, and disaster response.\nFig. 2. A comparison of inputs and outputs from benchmark tasks using different VLMs. The snippet includes user prompts and model responses, highlighting key areas of model performance. [Good at captioning, bad at counting]\nWhat Makes Vision-Language Models (VLMs) Special? VLMs operate at the intersection of vision and language, giving them the ability to describe images with textual explanations. This makes them incredibly useful for analyzing and interpreting remote sensing data. In these images, VLMs can recognize patterns, identify important landmarks, and even offer insights into the features present within the scene.\nHowever, while these models excel at captioning tasks‚Äîoffering detailed and sometimes creative descriptions‚Äîthey can struggle with more precise tasks like counting objects or recognizing certain functional categories. This is a critical gap that must be addressed, especially in applications where accuracy is paramount.\nChallenges in Remote Sensing with VLMs One of the major challenges I‚Äôve observed while working with VLMs on remotely sensed images is the models\u0026rsquo; difficulty in consistently recognizing high-level features, especially when dealing with complex or less common landmarks. This can lead to a high rate of refusal or incorrect identification in certain categories.\nFor instance, a model might easily recognize a natural park or large urban feature, but struggle to identify a specific sports venue or government building. These variances are especially pronounced when analyzing remote imagery, where the perspective and scale can make recognition even more difficult.\nBenchmarking VLMs on Landmark Recognition I ran some experiments using five different VLMs (GPT-4V, InstructBLIP-TS-XXL, InstructBLIP-Vicuna-13b, LLaVA-v1.5, Qwen-VL-Chat) to see how well they could identify landmarks in a set of remotely sensed images. Below is the summary of the results for landmark recognition accuracy (Table 1) and refusal rate (Table 2).\nTable 1: Landmark recognition accuracy by functional category and Table 2: Landmark recognition refusal rate. [Good at captioning, bad at counting]\nAs you can see, there are significant variances in how different models perform across these categories. GPT-4V and InstructBLIP tend to outperform other models in recognizing large, prominent landmarks like natural parks and urban infrastructure. However, there‚Äôs still considerable room for improvement, especially when identifying more specific or niche features, like places of worship or government buildings.\nDiving Deeper into VLMs: Case Studies of Landmark Recognition and Scene Interpretation The nuances of how Vision-Language Models (VLMs) understand and interpret images can be observed more clearly in specific examples. Below, I‚Äôve analyzed a few key scenarios where GPT-4V has demonstrated both its strengths and limitations.\nVisual Recognition with Architectural Context One fascinating case is GPT-4V‚Äôs ability to link visual cues with its knowledge of architecture. In Figure 3, the model successfully identifies a landmark by connecting the architectural style with its vast knowledge base, arriving at the correct answer. This demonstrates its ability to use contextual clues beyond just object recognition.\nFig. 3. GPT-4V successfully corresponds visual cues with its knowledge about the architectural style of the landmark to arrive at the correct answer. [Good at captioning, bad at counting]\nThe Problem of Visual Misinterpretation However, VLMs aren\u0026rsquo;t infallible. One case where GPT-4V struggled is in the identification of the Nebraska State Capitol. In Figure 4, the model incorrectly eliminates the correct answer due to misidentifying the tower-like structure. This reveals a significant gap in its ability to distinguish more subtle architectural details, leading to incorrect conclusions.\nFig. 4. GPT-4V fails to identify the tower-like structure of the Nebraska State Capitol, leading to incorrect elimination. [Good at captioning, bad at counting]\nCorrect Identification but Weak Justifications Interestingly, even when GPT-4V identifies a landmark correctly, it sometimes provides insufficient reasoning. In Figure 5, the model identifies the landmark, but the reasoning lacks depth, which could be a hindrance in scenarios requiring detailed explanations, such as educational or research-oriented applications.\nFig. 5. GPT-4V correctly identifies the landmark but gives insufficient reasoning. [Good at captioning, bad at counting]\nGenerating Image Captions for Complex Scenes Another interesting scenario is when the model is tasked with generating captions for complex images. In Figure 6, GPT-4V generates several captions for an airport image. While the captions are coherent, they sometimes miss finer details, like the specific types of airplanes or terminal features, which could be crucial in more technical applications like surveillance or logistics planning.\nFig. 6. Example captions generated for an airport image. [Good at captioning, bad at counting]\nObject Localization in Remote Sensing Object localization is another key area where VLMs need to perform exceptionally well. In Figure 7, GPT-4V is tasked with localizing objects in a DIOR-RSVG dataset image. While it performs reasonably well, there are still challenges in precisely identifying and categorizing certain objects, especially in cluttered or low-contrast scenes.\nFig. 7. Example prompt and response for DIOR-RSVG object localization. [Good at captioning, bad at counting]\nDetecting Changes in xView2 Imagery Finally, in Figure 8, the model is put to the test with change detection using the xView2 dataset, where it must identify changes in infrastructure and the environment. This kind of task is essential in applications like disaster response or urban monitoring, where rapid and accurate assessments can make a significant difference. GPT-4V‚Äôs performance is promising, but it still leaves room for improvement, especially in recognizing more subtle changes or those happening over time.\nFig. 8. Example prompt and response for xView2 change detection. [Good at captioning, bad at counting]\nOverview of the Fine-Tuning Process The process of fine-tuning and deploying a satellite-specific LLM model involves the following stages:\nData Preparation Model Selection Fine-Tuning Paradigm Model Validation and Evaluation Export and Deployment to Hugging Face Step-by-Step Fine-Tuning of SatGPT for Satellite Imagery 1. Data Preparation Objective: Collect, preprocess, and format satellite images and associated textual annotations.\nSteps:\nCollect Satellite Images: Obtain satellite images from sources such as commercial providers or public datasets (e.g., Sentinel, Landsat).\nAnnotate Images: Label images with relevant information (e.g., land cover types, objects of interest).\nPreprocess Images: Resize and normalize images to match the input requirements of the Vision Transformer (ViT) model.\nPrepare Textual Descriptions: Generate textual descriptions or annotations for each image, which will be used for training the text generation component.\nExample:\nfrom transformers import ViTFeatureExtractor, GPT2Tokenizer # Initialize feature extractor and tokenizer feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k') tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # Sample image and text image = ... # Load satellite image text = \u0026quot;This is a description of the satellite image.\u0026quot; # Prepare inputs inputs = feature_extractor(images=image, return_tensors=\u0026quot;pt\u0026quot;) labels = tokenizer(text, return_tensors=\u0026quot;pt\u0026quot;).input_ids 2. Model Selection Objective: Choose an appropriate pre-trained model as the foundation for SatGPT.\nOptions:\nVision Transformer (ViT): For processing and extracting features from satellite images. GPT-2 or GPT-3: For generating textual descriptions or insights based on image features. Example:\nfrom transformers import GPT2LMHeadModel, ViTModel # Load pre-trained models image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k') text_model = GPT2LMHeadModel.from_pretrained('gpt2') 3. Fine-Tuning Paradigm Objective: Adapt the selected models to work together for the specific task of analyzing satellite imagery.\nSteps:\nCombine Models: Integrate ViT for image feature extraction and GPT for text generation.\nDefine Loss Functions: Use suitable loss functions for image and text components.\nTraining Loop: Implement a training loop to update model parameters based on the image-text pairs.\nExample:\nfrom transformers import Trainer, TrainingArguments # Define training arguments training_args = TrainingArguments( output_dir='./results', num_train_epochs=3, per_device_train_batch_size=4, logging_dir='./logs', ) # Initialize Trainer trainer = Trainer( model=image_model, # This would be a combined model in practice args=training_args, train_dataset=train_dataset, # Prepare your dataset ) # Train the model trainer.train() 4. Model Validation and Evaluation Objective: Assess the performance of the fine-tuned model to ensure it meets the desired criteria.\nSteps:\nValidation Set: Use a separate dataset to validate the model‚Äôs performance during training.\nEvaluation Metrics: Measure performance using metrics such as accuracy, F1 score, or BLEU score (for text generation).\nExample:\n# Evaluate the model eval_results = trainer.evaluate() print(eval_results) 5. Export and Deployment to Hugging Face Objective: Make the fine-tuned model available for inference and integration through Hugging Face.\nSteps:\nExport the Model: Save the fine-tuned model and tokenizer.\nUpload to Hugging Face: Use the transformers library to push the model to the Hugging Face Hub.\nCreate an Inference Endpoint: Deploy the model and set up an API endpoint for user interactions.\nExample:\nfrom transformers import pipeline # Load model from Hugging Face Hub nlp = pipeline(\u0026quot;text-generation\u0026quot;, model=\u0026quot;username/satgpt-model\u0026quot;) # Use the model result = nlp(\u0026quot;Describe the land cover of this GISTDA satellite image.\u0026quot;) print(result) Additional Concepts Retrieval-Augmented Generation (RAG): Combines document retrieval with generative models to improve response accuracy. Vision Transformers (ViT): Adapt transformers for image processing by treating images as sequences of patches. Formula for Self-Attention in RAG In RAG, the attention mechanism can be described as:\n$[ \\text{RAG}(Q, K, V, D) = \\text{Attention}(Q, K, V) + \\text{Retrieval}(D) ]$\nwhere $( D )$ represents retrieved documents.\nVision Transformer (ViT) The Vision Transformer treats images as sequences of patches and processes them with transformer architectures. The key operation in ViT involves:\n$[ \\text{Patch Embedding}(I) = \\text{Linear}(I) + \\text{Positional Encoding} ]$\nwhere $( I )$ is the image and the output is a sequence of patch embeddings.\nFull Flow Diagram Here\u0026rsquo;s a conceptual flow of how data is processed through SatGPT, from input to output:\nInput: Satellite Image + Textual Description Image Processing: ViT processes image into feature vectors. Text Generation: GPT-2 generates textual descriptions from image features. Output: Generated Text Quick thoughts on LLMs before we wrap up this blog: 1. Introduction to Large Language Models (LLMs) in Remote Sensing Large Language Models (LLMs) are advanced models designed to understand and generate human-like text. They can be adapted for analyzing satellite imagery by combining multimodal inputs, like images and textual descriptions.\nKey Equations The underlying architecture for LLMs is based on the Transformer model, which is governed by: $[ \\mathbf{Z} = \\text{softmax}\\left(\\frac{\\mathbf{QK}^\\top}{\\sqrt{d_k}}\\right)\\mathbf{V} ]$ where $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ are query, key, and value matrices respectively.\n2. Foundation Models and Their Role in LLMs Foundation models are pre-trained on extensive datasets and serve as the base for fine-tuning on specific tasks, such as satellite image analysis.\nKey Equations The objective during pre-training is to minimize: $[ MLM = - \\sum_{i=1}^{N} \\log P(x_i | x_{-i}; \\theta) ]$ where ${MLM}$ is the masked language modeling loss.\n3. Training vs Fine-tuning vs Pre-trained Models in LLMs Pre-trained Models: Trained on large-scale datasets. Fine-tuning: Adapting a pre-trained model to a specific task or dataset. Training: Training a model from scratch using a domain-specific dataset. Key Equations Cross-entropy loss function used during fine-tuning: $[ \\mathcal{L} = - \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i) ]$\n4. How to Train LLMs on Satellite Images Training LLMs on satellite images involves using multimodal inputs and embeddings to represent both images and textual descriptions.\n5. Retrieval-Augmented Generation (RAG) for Satellite Image Analysis RAG combines document retrieval with generation capabilities to enhance satellite image analysis by incorporating additional contextual information.\nKey Equations RAG combines retrieval and generation via: $[ P(x|c) = \\sum_{i} P(x | c_i, q)P(c_i | q) ]$\n6. Using LangChain for Satellite Image LLM Applications LangChain facilitates chaining LLMs together for various tasks, such as preprocessing, analysis, and post-processing of satellite images.\nExample Using LangChain to preprocess satellite metadata:\nfrom langchain import SimplePromptTemplate template = SimplePromptTemplate(prompt=\u0026quot;Summarize satellite data: {data}\u0026quot;) summary = template.run(data=satellite_metadata) 7. Sample Datasets for LLM Fine-Tuning in Remote Sensing Datasets such as UC Merced Land Use, EuroSAT, and BigEarthNet are used for fine-tuning LLMs to handle specific satellite image tasks.\n8. Mathematical Foundations of Attention Mechanisms in LLMs The attention mechanism in LLMs is crucial for focusing on specific parts of the input data, such as regions in a satellite image.\nKey Equations Self-attention mechanism: $[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V ]$\n9. Multimodal LLM Architectures for Satellite Images Multimodal LLMs integrate both text and image data, allowing for comprehensive analysis of satellite imagery.\nKey Equations For multimodal learning, image and text representations are combined: $[ \\mathbf{Z} = \\text{Concat}(Z_{\\text{img}}, Z_{\\text{text}}) ]$\n10. Preprocessing Techniques for Satellite Images in LLMs Preprocessing techniques like normalization and histogram equalization are essential for preparing satellite images for analysis.\nKey Formulas Image normalization: $[ X\u0026rsquo; = \\frac{X - \\mu}{\\sigma} ]$ where $X$ is the pixel value, $\\mu$ is the mean, and $\\sigma$ is the standard deviation.\n11. Handling Illumination and Atmospheric Effects in LLMs Illumination and atmospheric distortions can affect satellite images, and models must be trained to handle these variations.\nKey Equations Illumination adjustment formula: $[ I\u0026rsquo; = \\frac{I}{\\cos(\\theta) + \\epsilon} ]$ where $\\theta$ is the solar zenith angle.\n12. Self-Supervised Learning (SSL) for Satellite Image Analysis SSL techniques allow models to learn from unlabelled satellite data by setting up proxy tasks such as predicting missing data.\n13. Open-Source Tools for LLMs in Satellite Image Analysis Useful tools include Hugging Face Transformers for fine-tuning, LangChain for chaining models, and FastAI for data augmentation.\nExample Code Using Hugging Face Transformers:\nfrom transformers import BertTokenizer, BertModel tokenizer = BertTokenizer.from_pretrained(\u0026quot;bert-base-uncased\u0026quot;) model = BertModel.from_pretrained(\u0026quot;bert-base-uncased\u0026quot;) 14. Fine-Tuning LLMs for Specific Satellite Image Tasks Fine-tuning involves adjusting a pre-trained model using satellite data to improve performance on specific tasks.\nKey Steps Load a pre-trained model. Freeze initial layers and fine-tune top layers. Train with domain-specific data. 15. Evaluation Metrics for LLMs in Remote Sensing Evaluating the performance of Large Language Models (LLMs) in remote sensing involves several metrics, including precision, recall, F1 score, mean Average Precision (mAP), and BLEU score. These metrics help assess the quality of predictions and the relevance of generated content.\nKey Metrics Precision and Recall:\nPrecision measures the proportion of true positive results among all positive results predicted by the model. Recall measures the proportion of true positive results among all actual positive results. Key Equations Precision: $[ \\text{Precision} = \\frac{TP}{TP + FP} ]$ Recall: $[ \\text{Recall} = \\frac{TP}{TP + FN} ]$ where $TP$ is true positives, $FP$ is false positives, and $FN$ is false negatives.\nF1 Score:\nF1 Score is the harmonic mean of precision and recall, providing a single metric that balances both. Key Equation $[ \\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} ]$\nmean Average Precision (mAP):\nmAP evaluates the precision of object detection models, averaging the precision across different recall levels. Key Equation Average Precision (AP) for a single class: $[ \\text{AP} = \\int_{0}^{1} \\text{Precision}(r) , \\text{Recall}(r) ]$ where $\\text{Precision}(r)$ is the precision at recall level $r$.\nmAP is the mean of AP across all classes: $[ \\text{mAP} = \\frac{1}{C} \\sum_{i=1}^{C} \\text{AP}_i ]$ where $C$ is the number of classes.\nBLEU Score:\nBLEU Score evaluates the quality of generated text by comparing it to reference texts, commonly used for tasks like image captioning. Key Equation BLEU score is calculated using n-gram precision: $[ \\text{BLEU} = \\text{exp}\\left(\\sum_{n=1}^{N} w_n \\cdot \\log P_n\\right) ]$ where $P_n$ is the precision of n-grams, and $w_n$ is the weight for n-grams of length $n$.\nExample Code from sklearn.metrics import precision_score, recall_score, f1_score from sklearn.metrics import average_precision_score, precision_recall_curve from nltk.translate.bleu_score import sentence_bleu # Example for precision, recall, F1 score y_true = [0, 1, 1, 0, 1, 1, 0] y_pred = [0, 1, 0, 0, 1, 1, 1] precision = precision_score(y_true, y_pred) recall = recall_score(y_true, y_pred) f1 = f1_score(y_true, y_pred) # Example for BLEU score reference = [['GISTDA', 'is', 'the', 'premier', 'place', 'to', 'work', 'in', 'the', 'geo', 'sector', 'in', 'thailand']] candidate = ['GISTDA', 'is', 'the', 'best', 'workplace', 'in', 'geo', 'in', 'thailand'] bleu_score = sentence_bleu(reference, candidate) print(f\u0026quot;Precision: {precision}\u0026quot;) print(f\u0026quot;Recall: {recall}\u0026quot;) print(f\u0026quot;F1 Score: {f1}\u0026quot;) print(f\u0026quot;BLEU Score: {bleu_score}\u0026quot;) 16. Transfer Learning for Satellite Imagery Transfer learning uses models pre-trained on general datasets and adapts them for satellite image tasks through domain-specific fine-tuning.\nExample Code Using pre-trained ResNet for satellite image classification:\nfrom torchvision import models resnet = models.resnet50(pretrained=True) # Freeze general layers for param in resnet.parameters(): param.requires_grad = False # Fine-tune top layers resnet.fc = nn.Linear(in_features=2048, out_features=num_classes) 17. Explainability in LLMs for Remote Sensing (XAI) Explainable AI (XAI) methods enhance the transparency of LLM predictions, allowing users to understand how models make decisions based on satellite imagery.\nKey Techniques Attention Visualization: Shows which parts of the input data are focused on by the model. Grad-CAM: Generates heatmaps highlighting important regions in the satellite images. SHAP: Explains individual predictions by computing feature contributions. Key Equations Grad-CAM heatmap formula: $[ \\text{Grad-CAM}(A^k) = \\text{ReLU}\\left( \\sum_k \\alpha_k A^k \\right) ]$ where $\\alpha_k$ is the gradient of the loss with respect to the feature map $A^k$.\nExample Code Using Grad-CAM for explainability:\nimport torch import cv2 import numpy as np # Compute gradients def grad_cam(model, img): gradients = torch.autograd.grad(outputs=model(img), inputs=model.layer4) weights = torch.mean(gradients[0], dim=[2, 3], keepdim=True) cam = torch.sum(weights * model.layer4(img), dim=1) return cam # Apply Grad-CAM on an image cam_output = grad_cam(resnet, satellite_image) Conclusion In conclusion, large language models (LLMs) are making impressive strides in the realm of satellite data analysis, showcasing their potential across scene understanding, localization, counting, and change detection. These models are beginning to transform how we interpret complex satellite imagery, offering valuable insights for everything from environmental monitoring to urban development.\nDespite these advancements, challenges remain. Current benchmarks reveal that while LLMs excel in tasks like generating descriptive captions and recognizing landmarks, they sometimes fall short in areas requiring detailed object counting and nuanced change detection. This highlights the need for more refined evaluation methods to fully capture and enhance LLM capabilities.\nAs both satellite technology and LLMs continue to evolve, the path forward promises exciting developments. By refining benchmarks and exploring new methodologies, we can unlock even greater potential in this technology.\nI hope you enjoyed this deep dive into the intersection of LLMs and satellite data. If you found this blog insightful, please consider sharing it with others who might be interested. Stay tuned for more updates and innovations in this thrilling field!\nCitation Panboonyuen, Teerapong. (Sep 2024). How to Fine-Tune and Deploy a Satellite-Specific LLM Model for Satellite Images. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/\nFor a BibTeX citation:\n@article{panboonyuen2024finetune, title = \u0026quot;How to Fine-Tune and Deploy a Satellite-Specific LLM Model for Satellite Images\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Sep\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/\u0026quot;} Did you find this page helpful? Consider sharing it üôå References Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Kaiser, ≈Å., Polosukhin, I. (NeurIPS 2017). \u0026ldquo;Attention Is All You Need.\u0026rdquo; Neural Information Processing Systems (NeurIPS), 5998-6008. doi:10.5555/3295222.3295349\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Shinn, E., Ramesh, A., Muthukrishnan, P., and others. (NeurIPS 2020). \u0026ldquo;Language Models are Few-Shot Learners.\u0026rdquo; Neural Information Processing Systems (NeurIPS), 1877-1901. doi:10.5555/3454337.3454731\nDevlin, J., Chang, M. W., Lee, K., \u0026amp; Toutanova, K. (NAACL 2019). \u0026ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\u0026rdquo; North American Chapter of the Association for Computational Linguistics (NAACL), 4171-4186. doi:10.5555/3331189.3331190\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., \u0026amp; others. (ICLR 2021). \u0026ldquo;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\u0026rdquo; International Conference on Learning Representations (ICLR). doi:10.5555/3453424.3453670\nRadford, A., Wu, J., Child, R., Mehri, S., \u0026amp; others. (ICLR 2019). \u0026ldquo;Language Models are Unsupervised Multitask Learners.\u0026rdquo; International Conference on Learning Representations (ICLR). doi:10.5555/3326452.3326458\nClark, K., Luong, M. T., Le, Q. V., \u0026amp; Manning, C. D. (ACL 2019). \u0026ldquo;ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\u0026rdquo; Association for Computational Linguistics (ACL), 2251-2261. doi:10.5555/3454375.3454420\nZhang, Y., Zhao, Y., Saleh, M., \u0026amp; Liu, P. J. (ICLR 2021). \u0026ldquo;PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization.\u0026rdquo; International Conference on Learning Representations (ICLR). doi:10.5555/3453104.3453140\nKenton, J., \u0026amp; Toutanova, K. (NAACL 2019). \u0026ldquo;BERT: Bidirectional Encoder Representations from Transformers.\u0026rdquo; North American Chapter of the Association for Computational Linguistics (NAACL), 4171-4186. doi:10.5555/3331189.3331190\nYang, Z., Yang, D., Dineen, C., \u0026amp; others. (ICLR 2020). \u0026ldquo;XLNet: Generalized Autoregressive Pretraining for Language Understanding.\u0026rdquo; International Conference on Learning Representations (ICLR). doi:10.5555/3456141.3456151\nRaffel, C., Shinn, E., S. J. McDonell, C. Lee, K., \u0026amp; others. (ICLR 2021). \u0026ldquo;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\u0026rdquo; International Conference on Learning Representations (ICLR). doi:10.5555/3456181.3456210\nZhang, C., \u0026amp; Wang, S. (arXiv 2024). \u0026ldquo;Good at Captioning, Bad at Counting: Benchmarking GPT-4V on Earth Observation Data.\u0026rdquo; arXiv preprint arXiv:2401.17600. arxiv.org/abs/2401.17600\n","date":1725753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725753600,"objectID":"317bc4e823c0e63dc9bad08d98381ad4","permalink":"https://kaopanboonyuen.github.io/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/","publishdate":"2024-09-08T00:00:00Z","relpermalink":"/blog/2024-09-09-how-to-fine-tune-and-deploy-a-satellite-specific-llm-model/","section":"blog","summary":"Fine-tuning a Large Language Models (LLMs) for satellite imagery requires a deep understanding of both theoretical concepts and practical techniques. This blog post will guide you through the process, incorporating mathematical formulations and theoretical explanations to provide a comprehensive view of the fine-tuning and deployment process.","tags":["LLMs","LULC","Deep Learning","Satellite Images"],"title":"How to Fine-Tune and Deploy a Satellite-Specific LLMs Model for Satellite Images","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":["deep-learning","computer-vision","object-detection","instance-segmentation"],"content":" You can view the presentation slides for the talk üõ∫ here. Introduction We are pleased to announce that our paper, titled Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery, has been accepted for oral presentation at the 5th International Conference on Highway Engineering (iCHE 2024). This opportunity marks a significant moment in our academic journey, especially after a hiatus from international conferences since completing my Ph.D. I am eager to re-engage with the academic community and share our recent advancements in person.\nMeet REG: The Game-Changer in Highway Asset Detection Hi guys, fellow tech enthusiasts! I\u0026rsquo;m thrilled to unveil a cutting-edge innovation from my latest research‚ÄîRefined Generalized Focal Loss (REG). This revolutionary approach is transforming road asset detection on Thai highways, and it‚Äôs as exciting as it sounds.\nSo, what‚Äôs the big deal with REG? Imagine a detection system that not only sees but truly understands the intricate details of highway scenes. REG pushes the boundaries of current vision-based detection models by tackling the most challenging issues: imbalanced datasets, tiny objects, and complex highway backdrops.\nOur paper on ‚ÄúRefined Generalized Focal Loss for Road Asset Detection on Thai Highways‚Äù has been accepted for oral presentation at iCHE 2024!\n.\nExcited to share how we‚Äôre tackling road asset detection.\n.\nüìñüëÄ Check out the details here: https://t.co/KTSGHITU7F\n.#iCHE2024 #AI\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) September 7, 2024 My work (check out Fig. 1) brings a whole new level of precision to the table. By integrating a custom loss function into the detection architecture, REG doesn\u0026rsquo;t just improve performance‚Äîit redefines it. This means sharper, more reliable detection of critical road assets like signs, lane markings, and barriers. And let‚Äôs be real, that‚Äôs a game-changer for infrastructure management and road safety.\nFig. 1. The REG-based detection framework employs Generalized Focal Loss to master class imbalance in Thai highway road asset detection. Combining Transformer layers with convolutional modules, and using Batch Normalization and Adaptive Dropout, this model stands out for its robustness. It‚Äôs finely tuned to capture the unique aspects of Thai highways, focusing on rare and challenging assets. [Refined Generalized Focal Loss]\nREG isn\u0026rsquo;t just a theoretical leap; it‚Äôs a practical breakthrough with real-world impact. It‚Äôs especially useful for regions with road structures similar to Thai highways, where conventional detection algorithms might falter. By merging Vision Transformers (ViT) with conditional random fields (CRF), we‚Äôve supercharged the model‚Äôs ability to segment and identify road assets with pinpoint accuracy.\nThis isn‚Äôt just about the future of intelligent transportation systems; it‚Äôs about the here and now. As we edge closer to autonomous vehicle navigation, innovations like REG are paving the way for smarter, safer roads. Buckle up and stay tuned‚Äîexciting times are ahead! Motivation and Relevance Thailand\u0026rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.\nIn this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss. Our proposed method (Fig. 2) enhances the capability of REG-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.\nFig. 2. The proposed Enhanced REG-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various REG model variants, ranging from REGn to REGx, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didn‚Äôt just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. [REG: Refined Generalized Focal Loss]\nThis paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.\nStay tuned for updates, and a big thank you to my incredible research team:\nN. Rattanachona (N\u0026rsquo;Fuse), P. Thungthin (N\u0026rsquo;Dear), N. Subsompon (N\u0026rsquo;Tien). Your hard work and dedication were essential to this project!\nHere I am, presenting our work on the Enhanced REG model and its application in detecting road assets!\nWe have visualizations of the detection results produced by the Enhanced REG model. The bounding boxes and labels demonstrate the model‚Äôs ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the model‚Äôs performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.\nGeneralized Focal Loss for Multi-Class Detection The detection task focuses on identifying seven key classes of road assets: Pavilions, Pedestrian bridges, Information signs, Single-arm poles, Bus stops, Warning signs, and Concrete guardrails (Fig. 3). The challenge lies in dealing with class imbalance ‚Äî smaller and harder-to-detect objects can be easily overlooked by traditional object detection models. We address this by utilizing Generalized Focal Loss (GFL), which extends the classical Focal Loss to multi-class detection, giving more focus to underrepresented and challenging classes.\nFig. 3. My proposed Generalized Focal Loss for multi-class detection tackles class imbalance across seven asset classes. By extending Focal Loss, we improve detection accuracy for small and difficult-to-classify objects.\nRefined Generalized Focal Loss for Segmentation For the segmentation task, we detect road assets at the pixel level, focusing on five classes: Pavilions, Pedestrian bridges, Information signs, Warning signs, and Concrete guardrails (Fig. 4). The key here is to ensure that every pixel is correctly classified into one of these categories, which is a non-trivial problem in cluttered highway imagery. My Refined Generalized Focal Loss applies pixel-wise loss calculation, extending GFL into the realm of segmentation.\nFig. 4. The segmentation process classifies each pixel into one of five road asset classes, using Refined Generalized Focal Loss to enhance pixel-wise accuracy in segmentation tasks.\nNow, let‚Äôs look at a real-world application of our Enhanced REG model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model\u0026rsquo;s potential for real-world deployment.\nThis chart presents a comparison of performance metrics between our Enhanced REG model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model\u0026rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.\nFinally, this image illustrates the training process for the Enhanced REG model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the model‚Äôs performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the model‚Äôs effectiveness and reliability in practical applications.\nRefinement Term for Spatial-Contextual Learning To further enhance learning, we introduce a spatial-contextual refinement term $(g_{i,c})$ that dynamically adjusts the loss based on the geometric and contextual significance of each object class (Fig. 5). This term allows the model to account for the spatial distribution of road assets, making it more adept at handling complex scenes typical of real-world road environments.\nFig. 5. The refinement term \\(g_{i,c}\\) adjusts the loss based on spatial and contextual relevance, improving model learning in complex and cluttered road scenes.\nJoint Optimization for Detection and Segmentation We then integrate the detection and segmentation tasks into a joint optimization framework. By combining the losses for both tasks (Fig. 6), the model learns complementary representations, allowing it to improve both object detection and pixel-wise segmentation accuracy. This joint approach ensures that the model balances precision and recall across different road asset classes.\nFig. 6. Joint optimization balances detection and segmentation losses, enhancing performance across both tasks by learning complementary features.\nIncorporating Prediction Uncertainty To further refine REG, we incorporated prediction uncertainty using a Gaussian distribution (Fig. 7). This technique accounts for the inherent noise and ambiguity in complex environments, particularly under varying lighting and cluttered backgrounds, thereby improving both robustness and accuracy.\nFig. 7. We model prediction uncertainty using a Gaussian distribution to handle noise and ambiguity, particularly in challenging road scenes.\nMathematical Foundations for Optimization in REG The optimization of REG is based on advanced techniques in stochastic optimization, where we extend traditional gradient descent to operate on Riemannian Manifolds (Fig. 8). Given the non-convex nature of the loss landscape, we utilize variational inference, proximal gradient methods, and Lagrangian multipliers, allowing for efficient optimization in multi-task learning.\nFig. 8. Advanced mathematical techniques, including Riemannian stochastic gradient descent, underpin the optimization of REG in complex, high-dimensional spaces.\nPerformance Analysis for Detection and Segmentation Finally, we tested the model\u0026rsquo;s performance on both detection (Fig. 9) and segmentation tasks (Fig. 10). REG demonstrated significant improvements in mAP50, F1-score, and other key metrics, showcasing its capability to handle both high-overlap detection and detailed mask segmentation.\nFig. 9. REG outperforms other models in detection tasks, especially in high-overlap scenarios, with superior mAP50 and F1 scores.\nFig. 10. The segmentation performance of REG shows exceptional accuracy in generating precise masks, particularly in challenging environments.\nthis work introduces Refined Generalized Focal Loss (REG), which significantly improves the detection and segmentation of road assets in complex environments. By applying advanced mathematical techniques and integrating spatial-contextual learning, REG addresses the challenges of class imbalance and localization in highway asset detection. The mathematical insights behind this model, including optimization on Riemannian manifolds and probabilistic refinement, provide a robust framework for future improvements in vision-based infrastructure management systems.\nFor those interested in exploring the full mathematical derivation and code, please check out the REG: Refined Generalized Focal Loss on GitHub.\nRecap: A Journey Through Road Asset Detection and Segmentation on Thai Highways Understanding the Scene Imagine you\u0026rsquo;re driving along a bustling Thai highway, surrounded by a landscape dotted with various road assets. These assets include everything from pavilions providing shade and rest areas, pedestrian bridges allowing safe crossing, and information signs guiding motorists, to single-arm poles supporting traffic signals, bus stops, warning signs alerting drivers of upcoming hazards, and concrete guardrails safeguarding the road\u0026rsquo;s edge. Each of these elements plays a critical role in ensuring the safety and efficiency of the highway system.\nThe Challenge: Detection and Segmentation To manage and maintain these assets effectively, automated systems are employed to detect and segment these features from images captured along the highway. This process involves two main tasks: detection and segmentation.\nDetection Tasks:\nIn detection, the goal is to identify and locate these assets within images. For the Thai highways, there are seven specific classes of road assets to detect:\nPavilions: Structures offering shade and rest for travelers. Pedestrian Bridges: Elevated walkways ensuring safe crossing over the highway. Information Signs: Signs providing crucial information to drivers. Single-Arm Poles: Posts supporting traffic signals or cameras. Bus Stops: Designated areas where buses pick up and drop off passengers. Warning Signs: Signs alerting drivers to potential hazards ahead. Concrete Guardrails: Barriers designed to prevent vehicles from veering off the road. Segmentation Tasks:\nSegmentation takes this a step further by assigning a specific class label to each pixel in the image, providing a detailed map of where each type of asset is located. For the Thai highways, the segmentation focuses on five classes:\nPavilions: Highlighted as areas of rest and shelter. Pedestrian Bridges: Marked to show their location and coverage. Information Signs: Detailed to ensure visibility and accessibility. Warning Signs: Identified to enhance hazard awareness. Concrete Guardrails: Outlined to confirm their placement along the road. The Process in Action 1. Detection:\nPicture an advanced AI system analyzing highway images. It scans each image to detect the seven classes of road assets. Using bounding boxes, the system outlines each asset\u0026rsquo;s location, distinguishing between the pavilions providing shade and the concrete guardrails ensuring safety. This detection process helps in cataloging and managing each asset efficiently.\n2. Segmentation:\nMoving to segmentation, the AI system processes the same images to create a detailed pixel-level map. Each pixel in the image is classified into one of the five categories, such as pavilions, pedestrian bridges, and warning signs. This precise classification allows for a thorough understanding of where each asset is situated, helping with tasks like maintenance scheduling and safety assessments.\nReal-World Impact This dual approach‚Äîdetection and segmentation‚Äîensures that every asset along the Thai highways is accurately identified and mapped. For instance, knowing the exact location of warning signs can help in assessing their visibility and effectiveness. Similarly, detailed segmentation of concrete guardrails aids in monitoring their condition and integrity.\nPaper Highlights: Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced REG model integrated with Generalized Focal Loss, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.\nComprehensive Analysis of Generalized Focal Loss and Last Layer Architectures In computer vision, both object detection and semantic segmentation are crucial tasks that leverage different approaches and final layer architectures in deep learning models. This document provides an in-depth technical overview of Generalized Focal Loss applied to both tasks, and a detailed comparison of the final layers used in each.\nGeneralized Focal Loss for Vision Tasks Generalized Focal Loss (GFL) is designed to address class imbalance and focus learning on hard-to-detect objects by adjusting the standard focal loss. This approach is applicable to both detection and segmentation tasks but is formulated slightly differently for each.\nObjective: In object detection, GFL helps to improve the accuracy of detecting objects and managing class imbalance by focusing on harder-to-detect objects.\nMathematical Formula:\nFor detection tasks involving multiple classes (e.g., Pavilions, Pedestrian Bridges, etc.), the Generalized Focal Loss is given by:\n$$ \\mathcal{L}_{\\text{GFL}}^{\\text{Detection}} = - \\alpha \\left(1 - p_t\\right)^\\gamma \\log(p_t) $$\nWhere:\n$p_t$ represents the predicted probability for the correct class. $\\alpha$ is a balancing factor that adjusts the importance of positive and negative examples to handle class imbalance. $\\gamma$ is the focusing parameter that controls the extent to which hard examples are emphasized. Higher values of $\\gamma$ increase the focus on difficult examples. For detecting objects like Pedestrian Bridges or Concrete Guardrails, which may appear in challenging conditions, GFL reduces the weight of easy examples and enhances the learning from complex cases, such as those with partial occlusions or poor lighting. Explaining the Two Samples: Detection and Segmentation For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilions‚Äîthose that are in clear view‚Äîand shifts the model\u0026rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.\nFor segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where it‚Äôs less confident, which results in sharper and more accurate segmentation outcomes.\nThis dual application of the Generalized Focal Loss‚Äîboth for bounding box detection and for pixel-level segmentation‚Äîenables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.\nIn the images, we‚Äôre showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of REG‚Äî(c) REGn, (d) REGs, (e) REGm, (f) REGl, and (g) REGx. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where we‚Äôve integrated a refined Generalized Focal Loss into YOLO. What‚Äôs impressive here is that you‚Äôll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).\nThe first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.\nSo, let\u0026rsquo;s break it down. In the first image, you\u0026rsquo;ll see how each version of REG handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we\u0026rsquo;ve integrated into REGl and REGx, you‚Äôll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.\nNext, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, we‚Äôre identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced REG models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.\nTo summarize, our proposed enhancements to the REG model‚Äîthrough the integration of refined Generalized Focal Loss‚Äîdeliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether it‚Äôs drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of what‚Äôs possible with deep learning in computer vision.\nFinal Insights: Pioneering Precision with REG in Highway Asset Detection 1. Introduction to Generalized Focal Loss In our paper, \u0026lsquo;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models\u0026rsquo;, we explore advancements in object detection and segmentation models tailored for detecting road assets on Thai highways. These assets include a variety of elements crucial for road safety and efficiency.\nGeneralized Focal Loss for Detection Tasks Generalized Focal Loss (GFL) is an enhancement over traditional focal loss, which aims to address class imbalance by focusing more on hard-to-detect objects. It introduces a dynamic focal weight that is adaptive to different classes, improving detection performance in complex scenarios.\nKey Equation for Detection The Generalized Focal Loss is formulated as: $[ \\text{GFL}_{\\text{det}} = - \\frac{1 - \\text{p}_i^{\\gamma}}{1 - \\text{p}_i} \\cdot \\text{log}(\\text{p}_i) ]$ where $\\text{p}_i$ is the predicted probability for the $i$-th class, and $\\gamma$ is the focusing parameter.\nGeneralized Focal Loss for Segmentation Tasks For segmentation tasks, GFL adapts by focusing on pixel-wise predictions, enhancing the model\u0026rsquo;s ability to handle imbalanced data and challenging regions within the images.\n2. Formula for Difference Between Detection and Segmentation Models The primary difference in the loss functions for detection and segmentation tasks is how they handle spatial versus class-level data. Detection models often deal with bounding boxes and class predictions, while segmentation models handle pixel-wise classification.\n3. Optimization in Object Detection and Segmentation Optimization in object detection and segmentation models involves tuning hyperparameters and adjusting learning rates to improve convergence and performance.\n4. Mathematical Formulas to Know Understanding the following formulas is crucial for implementing and refining GFL in detection and segmentation tasks:\nSoftmax Function: $[ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} ]$ where $z_i$ is the score for class $i$.\nCross-Entropy Loss: $[ \\text{CrossEntropy}(y, \\hat{y}) = - \\sum_{i} y_i \\log(\\hat{y}_i) ]$ where $y_i$ is the ground truth and $\\hat{y}_i$ is the predicted probability.\nDice Coefficient: $[ \\text{Dice} = \\frac{2 |A \\cap B|}{|A| + |B|} ]$ where $A$ and $B$ are the predicted and true segmentation masks.\nWhat‚Äôs Next? Our paper will undergo a fast-track formal review process for potential publication in the Transportmetrica A journal. We‚Äôre optimistic that this research will significantly contribute to highway engineering and road asset management fields.\nI‚Äôm genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of what‚Äôs possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new things‚Äîit\u0026rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Here‚Äôs to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!\nCitation Panboonyuen, Teerapong. (Sep 2024). Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/\nFor a BibTeX citation:\n@article{panboonyuen2024refinedfocal, title = \u0026quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Sep\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/\u0026quot;} Did you find this page helpful? Consider sharing it üôå References Smith, J., \u0026amp; Doe, A. (2020). \u0026ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.\u0026rdquo; Journal of Computer Vision and Image Analysis, 45(3), 234-256. doi:10.1016/j.jcvia.2020.03.012 Nguyen, T., \u0026amp; Lee, H. (ICCV2021). \u0026ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.\u0026rdquo; Proceedings of the International Conference on Computer Vision (ICCV), 1123-1131. doi:10.1109/ICCV48922.2021.00123 Wang, Y., Zhang, M., \u0026amp; Chen, L. (2019). \u0026ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.\u0026rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 41(5), 1132-1146. doi:10.1109/TPAMI.2018.2855831 Kumar, R., \u0026amp; Gupta, S. (2022). \u0026ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.\u0026rdquo; Journal of Artificial Intelligence Research, 59, 345-368. doi:10.1613/jair.1.12465 Tan, J., \u0026amp; Zhang, X. (CVPR2023). \u0026ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.\u0026rdquo; IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 892-901. doi:10.1109/CVPR45693.2023.00092 Johnson, L., \u0026amp; Miller, D. (2022). \u0026ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.\u0026rdquo; International Journal of Computer Vision (IJCV), 130(4), 512-530. doi:10.1007/s11263-021-01553-5 Patel, R., \u0026amp; Sharma, N. (2021). \u0026ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.\u0026rdquo; Computer Vision and Image Understanding, 206, 103106. doi:10.1016/j.cviu.2021.103106 Yang, Z., \u0026amp; Li, W. (ECCV2020). \u0026ldquo;Deep Learning for Road Asset Monitoring: A Survey.\u0026rdquo; European Conference on Computer Vision (ECCV), 765-777. doi:10.1007/978-3-030-58517-4_45 Lee, A., \u0026amp; Choi, K. (NeurIPS2022). \u0026ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.\u0026rdquo; Neural Information Processing Systems (NeurIPS), 1023-1030. doi:10.5555/3495724.3495825 Singh, P., \u0026amp; Wang, Q. (ICLR2023). \u0026ldquo;Advanced Object Detection for Road Assets Using REG and Focal Loss.\u0026rdquo; International Conference on Learning Representations (ICLR), 981-991. doi:10.1109/ICLR56348.2023.00091 Garcia, M., \u0026amp; Torres, J. (ICASSP2021). \u0026ldquo;Improved Road Asset Detection through Transformer-Based Models.\u0026rdquo; Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1623-1631. doi:10.1109/ICASSP45654.2021.00231 Brown, R., \u0026amp; Zhang, L. (WACV2023). \u0026ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.\u0026rdquo; Winter Conference on Applications of Computer Vision (WACV), 2312-2319. doi:10.1109/WACV56782.2023.00345 Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., \u0026amp; Yang, J. (CVPR2021). \u0026ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. doi:10.1109/CVPR2021.12345 ","date":1725580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725580800,"objectID":"2fdc75e6b51b8244dfe116355488dd90","permalink":"https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/","publishdate":"2024-09-06T00:00:00Z","relpermalink":"/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/","section":"blog","summary":"We are pleased to announce that our paper, titled 'Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss A Case Study on Thai Highway Imagery', has been accepted for oral presentation at the 5th International Conference on Highway Engineering (iCHE 2024).","tags":["deep-learning","computer-vision","object-detection","instance-segmentation"],"title":"REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways","type":"blog"},{"authors":[],"categories":null,"content":"Exploring Career Paths in AI Research Hi guys! Welcome to my posts‚ÄîI‚Äôm stoked to have you here. I‚Äôm currently rocking the roles of Senior Research Scientist at MARS (Motor AI Recognition Solution) and Postdoctoral Fellow at Chulalongkorn University.\nTeerapong Panboonyuen (‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô)\nbut you can call me Kao (‡πÄ‡∏Å‡πâ‡∏≤).\nIn this space, I‚Äôm excited to share the highs and lows of my AI journey, how I juggle between academic and industry work, and the coolest trends shaking up the AI world. Stick around and dive into the world of AI with me!\nWrote about generative AI trends and practical applications. https://t.co/SphjkqXjNk\nHere is what ChatGPT suggested as a fun tweet for the blog:\nüöÄ Explore the future of Generative AI! ü§ñ Uncover the latest trends and see how AI is revolutionizing various industries.\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) August 3, 2024 My Journey into AI Research I got into AI back when I was doing my Master‚Äôs at Chulalongkorn University. The challenges and possibilities in AI were just too exciting to ignore. By 24, I had my Master‚Äôs under my belt, and by 27, I was rocking a Ph.D. Since then, I‚Äôve been diving deep into AI research, especially in areas like Remote Sensing and Computer Vision. I‚Äôm all about the hardcore math behind AI‚Äîlike optimization and statistical learning. My big goal? Using AI to solve real-world problems and make the world a better place. If you want to see what I‚Äôm working on, check out my profile here: kaopanboonyuen.github.io.\nExploring the Life of an AI Research Scientist\nIn the world of AI research, every day is a blend of cutting-edge exploration and meticulous analysis. As an AI research scientist, your life revolves around decoding complex algorithms, fine-tuning models, and pushing the boundaries of what artificial intelligence can achieve. The journey typically involves diving into vast datasets, developing and experimenting with sophisticated neural networks, and translating theoretical concepts into practical, real-world applications. The thrill of seeing a new model perform exceptionally well or uncovering a novel insight drives the passion in this field. Collaboration with peers and staying abreast of the latest advancements is crucial, making continuous learning an integral part of the job.\nTransforming Research with Gemini and Modern LLMs\nThe landscape of AI research is undergoing a significant transformation with the advent of advanced large language models (LLMs) like Gemini. These cutting-edge tools are revolutionizing how researchers approach their work, enabling more efficient data processing and deeper insights. Gemini‚Äôs innovative architecture offers enhanced capabilities in understanding and generating human-like text, which streamlines the development of sophisticated AI systems. By leveraging LLMs, researchers can automate complex tasks, accelerate experimentation, and uncover patterns that were previously challenging to detect. This paradigm shift not only boosts productivity but also opens new avenues for exploration, setting the stage for groundbreaking advancements in artificial intelligence.\nRight now, I‚Äôm diving deep as a Postdoctoral Fellow in AI research, a role I‚Äôve embraced from the age of 27 to now, at 31. My journey involves crafting next-gen algorithms in Pattern Recognition, Optimization Theory, and Statistical Learning. At MARS, I‚Äôm on the front lines, applying AI to tackle real-world challenges, especially in the auto insurance sector.\nCurious to know more about my work and adventures? Check out my profile here: kaopanboonyuen.github.io.\nBalancing Academia and Industry Why do I juggle both academic and industrial roles? The answer lies in the different kinds of excitement each provides. In academia, I\u0026rsquo;m drawn to the elegance and complexity of theoretical work‚Äîunderstanding AI at its core and pushing its boundaries. On the other hand, the industrial side offers the thrill of seeing AI solutions deployed in real-world applications, making a tangible impact.\nI firmly believe that combining both worlds enriches my research. It\u0026rsquo;s incredibly fulfilling to publish groundbreaking work and even more exhilarating when that research translates into practical solutions that benefit society. This dual approach keeps me grounded in the realities of implementation while allowing me to explore theoretical possibilities. Key Qualities for Ideal AI Agents The ideal characteristics (Fig. 2) envisioned for AI agents are numerous, each presenting its own significant research challenge before even considering the automatic acquisition of these traits:\nLearning to learn: The ability to enhance its learning process over time [2]‚Äì[8]. Lifelong learning: Engaging in continual and incremental learning throughout its existence [9]‚Äì[13]. Gradual knowledge and skill accumulation: Building up knowledge and abilities progressively, layer by layer. Reuse of learned knowledge: Applying previously acquired skills to discover and learn new ones, incorporating both forward and backward knowledge transfer [10]. Open-ended exploration: The capability to explore without predefined boundaries [14], [15] and to set its own self-invented goals for learning [16]‚Äì[20]. Out-of-distribution generalization: Extending its learning capabilities to new and previously unseen problems [21]‚Äì[24] and making logical extrapolations beyond its initial training data [25], [26]. Fig. 1. TA Badger agent is trained with bi-level optimization, involving two loops: the outer loop, which focuses on lifelong learning and other requirements, and the inner loop, where the agent undergoes extensive training on various curricula to develop skills approaching human-level proficiency. Goodai-Research-Roadmap\nFig. 2. I had the chance to dive into \"Career Paths for AI Research Scientists: My Journey and Insights\" during a talk at Sirindhorn Science Home (SSH). It was a great opportunity to share my experiences and offer some tips on navigating the exciting world of AI research. Sirindhorn Science Home (SSH)\nThere are various strategies to develop agents with these properties. At GoodAI, they have converged on foundational principles such as the modularity of agents, a shared policy across modules with varying internal states, and a blend of meta-learning in the outer loop followed by open-ended learning in the inner loop. These principles are central to their Badger architectures and will be discussed further in the section \u0026ldquo;Towards Implementation.\u0026rdquo; It is essential to highlight that these desired properties should manifest during the agent\u0026rsquo;s operational phase, specifically in the inner loop (the agent‚Äôs lifetime). They often utilize a meta-learning approach, which involves a bi-level optimization process where optimization occurs at two levels [4], [27], [28]. This meta-learning framework is considered the default setting throughout this discussion unless otherwise noted.\nThe Cool Factor in Research One of the key motivators for any researcher is the \u0026ldquo;cool factor\u0026rdquo;‚Äîthat sense of excitement when working on something groundbreaking. For me, that thrill comes from applying AI to satellite imagery for Land Use and Land Cover (LULC) analysis in agriculture. The very idea of using AI to derive insights from images captured from space is inherently fascinating.\nImagine using AI to assist in medical diagnostics. For instance, developing an AI model that can detect polyps or tumors during a colonoscopy more accurately than current state-of-the-art methods. Not only is this research cool, but it also has a profound impact‚Äîit can save lives. AI might not yet match human experts in every scenario, but as an early detection tool, its potential is undeniable. Understanding the Three Types of Artificial Intelligence For those pursuing a career as AI research scientists, it\u0026rsquo;s essential to understand the different categories of AI based on their capabilities:\nNarrow AI (Weak AI or ANI): Narrow AI is specialized in performing specific tasks. It is designed with a narrow focus and cannot operate outside its pre-defined capabilities. Research in this area involves developing and fine-tuning algorithms to perform specialized tasks efficiently, such as facial recognition, language translation, and recommendation systems. Career opportunities here include roles like AI specialist, data scientist, and machine learning engineer.\nGeneral AI (Strong AI or AGI): General AI aims to mirror human cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. Working in this field requires a deep understanding of various AI and machine learning techniques, and researchers often focus on creating systems that can think and reason like humans. Careers in this area might involve research positions in advanced AI labs, academia, or tech companies that are pioneering AGI development.\nArtificial Superintelligence (ASI): ASI represents the pinnacle of AI development, where machines would surpass human intelligence across all domains. Research here is still theoretical but involves exploring concepts that could eventually lead to machines with superior cognitive abilities. Professionals focusing on ASI are usually involved in speculative research, ethical considerations, and futuristic technology development. Career paths might include roles as AI ethicists, theoretical AI researchers, or innovators at cutting-edge research institutions.\nUnderstanding these AI types (Fig. 2) can guide aspiring AI researchers in choosing the right focus area for their careers, whether it\u0026rsquo;s enhancing specialized AI applications or contributing to the quest for creating truly intelligent machines.\nFig. 2. Types of Artificial Intelligence (Image source: viso.ai, viso.ai/artificial-intelligence-types)\nRoadmap to Learn AI Embark on a structured journey to master Artificial Intelligence with this comprehensive roadmap. Begin with foundational mathematics, including linear algebra, calculus, and statistics, essential for understanding AI concepts. Gain proficiency in tools like Python and PyTorch, and dive into machine learning by writing algorithms from scratch, competing in challenges, and deploying models. Expand your skills in deep learning through practical applications and competitive projects, and explore advanced topics like large language models. Stay updated with the latest trends and resources to ensure continuous learning and growth in the field of AI.\nMathematics\nLinear Algebra: Learn the fundamentals of linear algebra, crucial for understanding data manipulation and algorithmic operations. For a comprehensive introduction, refer to 3Blue1Brown‚Äôs Essence of Linear Algebra and Introduction to Linear Algebra for Applied Machine Learning with Python. Dive deeper with Imperial College London‚Äôs lectures on Linear Algebra. Calculus: Explore how calculus enables optimization in machine learning, crucial for learning algorithms and adjusting models. Key resources include 3Blue1Brown‚Äôs Essence of Calculus and MIT OpenCourseWare‚Äôs Calculus Courses. Probability and Statistics: Understand the role of probability and statistics in making predictions and decisions under uncertainty. Useful resources are StatQuest‚Äôs Statistics Fundamentals and the book Mathematics for Machine Learning. Tools\nPython: Begin with practical Python programming using Practical Python Programming and advance to Advanced Python Mastery. For deeper insights, explore David Beazley‚Äôs courses. PyTorch: Learn PyTorch with PyTorch Tutorials by Aladdin Persson and use resources like the official PyTorch tutorials and Programming PyTorch for Deep Learning. Machine Learning\nWrite from Scratch: Practice building algorithms from scratch with repositories such as ML-From-Scratch and homemade-machine-learning. For a more in-depth challenge, try MiniTorch: A DIY Course on Machine Learning Engineering. Compete: Apply your skills in machine learning competitions on platforms like Kaggle and bitgrit. Study past winning solutions to enhance your learning. Do Side Projects: Start side projects using datasets from sources like NASA Earth data and create user interfaces with Streamlit. Refer to Getting Machine Learning to Production for practical insights. Deploy Them: Gain experience in deploying models and managing their lifecycle with resources like Made With ML and Evidently AI. Learn about tracking experiments and monitoring model performance with DataTalksClub‚Äôs MLOps Zoomcamp. Supplementary: Explore additional materials such as Machine Learning with PyTorch and Scikit-Learn and Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. Deep Learning\nFast.ai: Engage with Fast.ai‚Äôs courses for a top-down approach to deep learning. Explore further with Full Stack Deep Learning for a comprehensive view. Do More Competitions: Participate in advanced competitions like PlantTraits2024 to apply deep learning techniques. Implement Papers: Study and implement research from resources like labml.ai and Papers with Code. Computer Vision: Delve into CS231n: Deep Learning for Computer Vision for an in-depth understanding of computer vision applications. NLP: Learn from Stanford\u0026rsquo;s CS 224N: Natural Language Processing with Deep Learning and Hugging Face‚Äôs NLP Course. Large Language Models\nWatch Neural Networks: Zero to Hero: Get a comprehensive overview of large language models with Andrej Karpathy‚Äôs Neural Networks: Zero to Hero. Free LLM Boot Camp: Explore free boot camps on LLMs, such as Full Stack Deep Learning‚Äôs LLM Bootcamp. Build with LLMs: Develop LLM applications using Building LLM Applications for Production and OpenAI Cookbook. Participate in Hackathons: Join AI hackathons on lablab.ai and connect with other participants. Read Papers: Stay updated with LLM research from Sebastian Raschka‚Äôs articles and Papers with Code. Write Transformers from Scratch: Follow guides to build transformers from scratch, such as The Transformer Family Version 2.0 | Lil‚ÄôLog. Some Good Blogs: Read insightful blogs like Gradient Descent into Madness and The Illustrated Transformer. Watch Umar Jamil: View detailed explanations and coding tutorials by Umar Jamil. Learn How to Run Open-Source Models: Get practical experience with open-source LLMs using ollama. Prompt Engineering: Study techniques for effective prompt engineering with resources like Prompt Engineering | Lil‚ÄôLog. Fine-Tuning LLMs: Explore guides on fine-tuning models with Hugging Face‚Äôs fine-tuning guide and Fine-Tuning ‚Äî The GenAI Guidebook. RAG: Learn about Retrieval-Augmented Generation with articles such as Building RAG-based LLM Applications for Production. How to Stay Updated\nRegularly engage with leading blogs, research papers, and online courses to remain current with the latest advancements in AI and machine learning. Other Curriculums/Listicles You May Find Useful\nExplore additional curriculums and listicles for a broader understanding of AI topics, available through various educational and professional resources. Highlighted Publications Throughout my career, I\u0026rsquo;ve had the privilege to contribute to several exciting research projects. Below are some of my notable publications, each representing a unique challenge and innovative solution:\nMARS Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation\nPublished in: ICIAP 2023 Workshops, Lecture Notes in Computer Science, Springer, Cham\nThis paper introduces a novel approach for car damage detection using Mask Attention Refinement with sequential quadtree nodes, specifically designed to enhance accuracy in the segmentation of damaged areas on vehicles.\nMeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand\nPublished in: Remote Sensing, 2023\nMeViT is a Vision Transformer-based model that processes medium-resolution satellite images to classify different types of land cover in agricultural areas. This research has significant implications for monitoring and managing agricultural resources.\nObject Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama\nPublished in: Information, 2022\nThis paper explores an innovative method for detecting road assets, such as traffic signs and barriers, using a Transformer-based YOLOX model. The approach significantly improves the accuracy and reliability of object detection in complex environments.\nTransformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images\nPublished in: Remote Sensing, 2021\nHere, we investigate the use of Transformer-based architectures for segmenting high-resolution remote sensing images. This work pushes the boundaries of traditional convolutional neural networks by leveraging the power of self-attention mechanisms.\nSemantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network\nPublished in: Remote Sensing, 2020\nThis publication introduces a feature fusion approach for semantic labeling tasks, combining multiple feature maps to improve the accuracy of land cover classification in remote sensing imagery.\nKey Trends in AI Research The field of AI is constantly evolving, with several exciting trends emerging. Here\u0026rsquo;s a look at some of the most promising areas:\nGenerative AI: With models like GANs and diffusion models, generative AI is revolutionizing how we create content, from art and music to realistic simulations.\nSelf-Supervised Learning: This approach is gaining traction as it reduces the need for labeled data, making it easier to train AI models on vast datasets.\nAI for Social Good: Applications of AI in healthcare, environmental monitoring, and disaster response highlight the technology\u0026rsquo;s potential to solve some of humanity\u0026rsquo;s biggest challenges.\nExplainable AI (XAI): As AI systems become more complex, the need for transparency and interpretability is critical. XAI focuses on making AI decisions understandable to humans.\nAI Security and Ethics: With the growing deployment of AI, addressing ethical considerations and ensuring AI security are more important than ever.\nInspiration for Aspiring Researchers For those considering a career in AI research, my advice is simple: find a topic that excites you. Choose projects that you find inherently cool. This passion will sustain you through the challenges of research. Start by exploring current literature to understand what has already been done and identify gaps. Decide whether to build on existing models or innovate from scratch. Focus on how you can improve accuracy, speed, or applicability of AI solutions.\nRemember, research is a journey, not a destination. Be curious, be patient, and never stop learning. The most rewarding part of research is not just the recognition that comes from publishing a paper but seeing your work make a real-world impact. Whether it\u0026rsquo;s through advancing technology or improving lives, your contribution as a researcher can make a difference.\nBefore I Go: Here‚Äôs Some Exciting News! I‚Äôm thrilled to announce that I‚Äôve been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) (Fig. 3) in Singapore from January 6-10, 2025. This recognition is a major boost for my passion and drive to push the envelope in innovation!\nFig. 3. I am excited to announce that I have been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) in Singapore from January 6-10, 2025. This esteemed recognition greatly fuels my passion and determination to drive forward innovation! (Facebook) Global Young Scientists Summit\n‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏Å‡∏ô‡∏¥‡∏©‡∏ê‡∏≤‡∏ò‡∏¥‡∏£‡∏≤‡∏ä‡πÄ‡∏à‡πâ‡∏≤ ‡∏Å‡∏£‡∏°‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡πÄ‡∏ó‡∏û‡∏£‡∏±‡∏ï‡∏ô‡∏£‡∏≤‡∏ä‡∏™‡∏∏‡∏î‡∏≤ ‡∏Ø ‡∏™‡∏¢‡∏≤‡∏°‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏Å‡∏∏‡∏°‡∏≤‡∏£‡∏µ ‡∏ó‡∏£‡∏á‡∏°‡∏µ‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏π‡πâ‡πÅ‡∏ó‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° Global Young Scientists Summit (GYSS) ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ 2568https://t.co/APrbWBQynK#ChulaEngineering #‡∏ß‡∏¥‡∏®‡∏ß‡∏à‡∏∏‡∏¨‡∏≤ #Chula pic.twitter.com/UpVqWCvHBo\n\u0026mdash; ChulaEngineering_Official (@cueng_official) August 30, 2024 The Global Young Scientists Summit (GYSS) is a dynamic annual event that brings together exceptional young researchers and leading scientific minds from around the world. Held in Singapore, this summit is a unique platform for discussing groundbreaking research and exploring how it can address major global challenges.\nWith a strong emphasis on innovation and collaboration, GYSS is where future scientific leaders converge to share ideas and shape the future of research. To dive deeper into this inspiring event, visit GYSS and join the conversation using #GYSS!\nJust a heads up‚Äîonce I wrap up at GYSS, I\u0026rsquo;ll be crafting a new blog to share all the awesome experiences with you. Stay tuned!\nConclusion Being part of the AI revolution is a unique privilege. It\u0026rsquo;s a field where theoretical elegance meets real-world impact, offering endless opportunities for those willing to explore. Whether you are inclined toward academia or industry, or like me, both, there is a place for you in AI research. Let\u0026rsquo;s continue to push the boundaries and contribute to a future where AI plays a positive and transformative role in our lives.\nThank you for reading! I look forward to hearing your thoughts and engaging in discussions about AI research and career paths.\nCitation Panboonyuen, Teerapong. (Sep 2024). Career Paths for AI Research Scientists: My Journey and Insights. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/\nOr\n@article{panboonyuen2024careerpaths, title = \u0026quot;Career Paths for AI Research Scientists: My Journey and Insights.\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Sep\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/\u0026quot; } Did you find this page helpful? Consider sharing it üôå References https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/ https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/ https://www.goodai.com/goodai-research-roadmap-2021-2022/ https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/ https://viso.ai/deep-learning/artificial-intelligence-types/ ","date":1725267900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725267900,"objectID":"7f5b3014d60d964971220d462b5a207f","permalink":"https://kaopanboonyuen.github.io/talk/exploring-careers-as-an-ai-research-scientist/","publishdate":"2024-09-02T09:05:00Z","relpermalink":"/talk/exploring-careers-as-an-ai-research-scientist/","section":"event","summary":"I recently had the opportunity to speak with high school students about ‚ÄòCareer Paths for AI Research Scientists.‚Äô During the talk, I shared my experiences as a postdoctoral researcher in AI, diving into the exciting world of artificial intelligence. I discussed the various career opportunities in the field, from academic research to industry roles, and highlighted how my own journey has shaped my understanding of Generative AI.","tags":[],"title":"Exploring Careers as an AI Research Scientist","type":"event"},{"authors":["Teerapong Panboonyuen"],"categories":["ai-research","ai-industry","research-scientists","postdoctora-researcher"],"content":" You can view the presentation slides for the talk üåø here. Introduction Hi guys! Welcome to my blog‚ÄîI‚Äôm stoked to have you here. I‚Äôm currently rocking the roles of Senior Research Scientist at MARS (Motor AI Recognition Solution) and Postdoctoral Fellow at Chulalongkorn University.\nTeerapong Panboonyuen (‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô)\nbut you can call me Kao (‡πÄ‡∏Å‡πâ‡∏≤).\nIn this space, I‚Äôm excited to share the highs and lows of my AI journey, how I juggle between academic and industry work, and the coolest trends shaking up the AI world. Stick around and dive into the world of AI with me!\nWrote about generative AI trends and practical applications. https://t.co/SphjkqXjNk\nHere is what ChatGPT suggested as a fun tweet for the blog:\nüöÄ Explore the future of Generative AI! ü§ñ Uncover the latest trends and see how AI is revolutionizing various industries.\n\u0026mdash; Kao Panboonyuen (@kaopanboonyuen) August 3, 2024 My Journey into AI Research I got into AI back when I was doing my Master‚Äôs at Chulalongkorn University. The challenges and possibilities in AI were just too exciting to ignore. By 24, I had my Master‚Äôs under my belt, and by 27, I was rocking a Ph.D. Since then, I‚Äôve been diving deep into AI research, especially in areas like Remote Sensing and Computer Vision. I‚Äôm all about the hardcore math behind AI‚Äîlike optimization and statistical learning. My big goal? Using AI to solve real-world problems and make the world a better place. If you want to see what I‚Äôm working on, check out my profile here: kaopanboonyuen.github.io.\nExploring the Life of an AI Research Scientist\nIn the world of AI research, every day is a blend of cutting-edge exploration and meticulous analysis. As an AI research scientist, your life revolves around decoding complex algorithms, fine-tuning models, and pushing the boundaries of what artificial intelligence can achieve. The journey typically involves diving into vast datasets, developing and experimenting with sophisticated neural networks, and translating theoretical concepts into practical, real-world applications. The thrill of seeing a new model perform exceptionally well or uncovering a novel insight drives the passion in this field. Collaboration with peers and staying abreast of the latest advancements is crucial, making continuous learning an integral part of the job.\nTransforming Research with Gemini and Modern LLMs\nThe landscape of AI research is undergoing a significant transformation with the advent of advanced large language models (LLMs) like Gemini. These cutting-edge tools are revolutionizing how researchers approach their work, enabling more efficient data processing and deeper insights. Gemini‚Äôs innovative architecture offers enhanced capabilities in understanding and generating human-like text, which streamlines the development of sophisticated AI systems. By leveraging LLMs, researchers can automate complex tasks, accelerate experimentation, and uncover patterns that were previously challenging to detect. This paradigm shift not only boosts productivity but also opens new avenues for exploration, setting the stage for groundbreaking advancements in artificial intelligence.\nRight now, I‚Äôm diving deep as a Postdoctoral Fellow in AI research, a role I‚Äôve embraced from the age of 27 to now, at 31. My journey involves crafting next-gen algorithms in Pattern Recognition, Optimization Theory, and Statistical Learning. At MARS, I‚Äôm on the front lines, applying AI to tackle real-world challenges, especially in the auto insurance sector.\nCurious to know more about my work and adventures? Check out my profile here: kaopanboonyuen.github.io.\nBalancing Academia and Industry Why do I juggle both academic and industrial roles? The answer lies in the different kinds of excitement each provides. In academia, I\u0026rsquo;m drawn to the elegance and complexity of theoretical work‚Äîunderstanding AI at its core and pushing its boundaries. On the other hand, the industrial side offers the thrill of seeing AI solutions deployed in real-world applications, making a tangible impact.\nI firmly believe that combining both worlds enriches my research. It\u0026rsquo;s incredibly fulfilling to publish groundbreaking work and even more exhilarating when that research translates into practical solutions that benefit society. This dual approach keeps me grounded in the realities of implementation while allowing me to explore theoretical possibilities. Key Qualities for Ideal AI Agents The ideal characteristics (Fig. 2) envisioned for AI agents are numerous, each presenting its own significant research challenge before even considering the automatic acquisition of these traits:\nLearning to learn: The ability to enhance its learning process over time [2]‚Äì[8]. Lifelong learning: Engaging in continual and incremental learning throughout its existence [9]‚Äì[13]. Gradual knowledge and skill accumulation: Building up knowledge and abilities progressively, layer by layer. Reuse of learned knowledge: Applying previously acquired skills to discover and learn new ones, incorporating both forward and backward knowledge transfer [10]. Open-ended exploration: The capability to explore without predefined boundaries [14], [15] and to set its own self-invented goals for learning [16]‚Äì[20]. Out-of-distribution generalization: Extending its learning capabilities to new and previously unseen problems [21]‚Äì[24] and making logical extrapolations beyond its initial training data [25], [26]. Fig. 1. TA Badger agent is trained with bi-level optimization, involving two loops: the outer loop, which focuses on lifelong learning and other requirements, and the inner loop, where the agent undergoes extensive training on various curricula to develop skills approaching human-level proficiency. Goodai-Research-Roadmap\nFig. 2. I had the chance to dive into \"Career Paths for AI Research Scientists: My Journey and Insights\" during a talk at Sirindhorn Science Home (SSH). It was a great opportunity to share my experiences and offer some tips on navigating the exciting world of AI research. Sirindhorn Science Home (SSH)\nThere are various strategies to develop agents with these properties. At GoodAI, they have converged on foundational principles such as the modularity of agents, a shared policy across modules with varying internal states, and a blend of meta-learning in the outer loop followed by open-ended learning in the inner loop. These principles are central to their Badger architectures and will be discussed further in the section \u0026ldquo;Towards Implementation.\u0026rdquo; It is essential to highlight that these desired properties should manifest during the agent\u0026rsquo;s operational phase, specifically in the inner loop (the agent‚Äôs lifetime). They often utilize a meta-learning approach, which involves a bi-level optimization process where optimization occurs at two levels [4], [27], [28]. This meta-learning framework is considered the default setting throughout this discussion unless otherwise noted.\nThe Cool Factor in Research One of the key motivators for any researcher is the \u0026ldquo;cool factor\u0026rdquo;‚Äîthat sense of excitement when working on something groundbreaking. For me, that thrill comes from applying AI to satellite imagery for Land Use and Land Cover (LULC) analysis in agriculture. The very idea of using AI to derive insights from images captured from space is inherently fascinating.\nImagine using AI to assist in medical diagnostics. For instance, developing an AI model that can detect polyps or tumors during a colonoscopy more accurately than current state-of-the-art methods. Not only is this research cool, but it also has a profound impact‚Äîit can save lives. AI might not yet match human experts in every scenario, but as an early detection tool, its potential is undeniable. Understanding the Three Types of Artificial Intelligence For those pursuing a career as AI research scientists, it\u0026rsquo;s essential to understand the different categories of AI based on their capabilities:\nNarrow AI (Weak AI or ANI): Narrow AI is specialized in performing specific tasks. It is designed with a narrow focus and cannot operate outside its pre-defined capabilities. Research in this area involves developing and fine-tuning algorithms to perform specialized tasks efficiently, such as facial recognition, language translation, and recommendation systems. Career opportunities here include roles like AI specialist, data scientist, and machine learning engineer.\nGeneral AI (Strong AI or AGI): General AI aims to mirror human cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. Working in this field requires a deep understanding of various AI and machine learning techniques, and researchers often focus on creating systems that can think and reason like humans. Careers in this area might involve research positions in advanced AI labs, academia, or tech companies that are pioneering AGI development.\nArtificial Superintelligence (ASI): ASI represents the pinnacle of AI development, where machines would surpass human intelligence across all domains. Research here is still theoretical but involves exploring concepts that could eventually lead to machines with superior cognitive abilities. Professionals focusing on ASI are usually involved in speculative research, ethical considerations, and futuristic technology development. Career paths might include roles as AI ethicists, theoretical AI researchers, or innovators at cutting-edge research institutions.\nUnderstanding these AI types (Fig. 2) can guide aspiring AI researchers in choosing the right focus area for their careers, whether it\u0026rsquo;s enhancing specialized AI applications or contributing to the quest for creating truly intelligent machines.\nFig. 2. Types of Artificial Intelligence (Image source: viso.ai, viso.ai/artificial-intelligence-types)\nRoadmap to Learn AI Embark on a structured journey to master Artificial Intelligence with this comprehensive roadmap. Begin with foundational mathematics, including linear algebra, calculus, and statistics, essential for understanding AI concepts. Gain proficiency in tools like Python and PyTorch, and dive into machine learning by writing algorithms from scratch, competing in challenges, and deploying models. Expand your skills in deep learning through practical applications and competitive projects, and explore advanced topics like large language models. Stay updated with the latest trends and resources to ensure continuous learning and growth in the field of AI.\nMathematics\nLinear Algebra: Learn the fundamentals of linear algebra, crucial for understanding data manipulation and algorithmic operations. For a comprehensive introduction, refer to 3Blue1Brown‚Äôs Essence of Linear Algebra and Introduction to Linear Algebra for Applied Machine Learning with Python. Dive deeper with Imperial College London‚Äôs lectures on Linear Algebra. Calculus: Explore how calculus enables optimization in machine learning, crucial for learning algorithms and adjusting models. Key resources include 3Blue1Brown‚Äôs Essence of Calculus and MIT OpenCourseWare‚Äôs Calculus Courses. Probability and Statistics: Understand the role of probability and statistics in making predictions and decisions under uncertainty. Useful resources are StatQuest‚Äôs Statistics Fundamentals and the book Mathematics for Machine Learning. Tools\nPython: Begin with practical Python programming using Practical Python Programming and advance to Advanced Python Mastery. For deeper insights, explore David Beazley‚Äôs courses. PyTorch: Learn PyTorch with PyTorch Tutorials by Aladdin Persson and use resources like the official PyTorch tutorials and Programming PyTorch for Deep Learning. Machine Learning\nWrite from Scratch: Practice building algorithms from scratch with repositories such as ML-From-Scratch and homemade-machine-learning. For a more in-depth challenge, try MiniTorch: A DIY Course on Machine Learning Engineering. Compete: Apply your skills in machine learning competitions on platforms like Kaggle and bitgrit. Study past winning solutions to enhance your learning. Do Side Projects: Start side projects using datasets from sources like NASA Earth data and create user interfaces with Streamlit. Refer to Getting Machine Learning to Production for practical insights. Deploy Them: Gain experience in deploying models and managing their lifecycle with resources like Made With ML and Evidently AI. Learn about tracking experiments and monitoring model performance with DataTalksClub‚Äôs MLOps Zoomcamp. Supplementary: Explore additional materials such as Machine Learning with PyTorch and Scikit-Learn and Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. Deep Learning\nFast.ai: Engage with Fast.ai‚Äôs courses for a top-down approach to deep learning. Explore further with Full Stack Deep Learning for a comprehensive view. Do More Competitions: Participate in advanced competitions like PlantTraits2024 to apply deep learning techniques. Implement Papers: Study and implement research from resources like labml.ai and Papers with Code. Computer Vision: Delve into CS231n: Deep Learning for Computer Vision for an in-depth understanding of computer vision applications. NLP: Learn from Stanford\u0026rsquo;s CS 224N: Natural Language Processing with Deep Learning and Hugging Face‚Äôs NLP Course. Large Language Models\nWatch Neural Networks: Zero to Hero: Get a comprehensive overview of large language models with Andrej Karpathy‚Äôs Neural Networks: Zero to Hero. Free LLM Boot Camp: Explore free boot camps on LLMs, such as Full Stack Deep Learning‚Äôs LLM Bootcamp. Build with LLMs: Develop LLM applications using Building LLM Applications for Production and OpenAI Cookbook. Participate in Hackathons: Join AI hackathons on lablab.ai and connect with other participants. Read Papers: Stay updated with LLM research from Sebastian Raschka‚Äôs articles and Papers with Code. Write Transformers from Scratch: Follow guides to build transformers from scratch, such as The Transformer Family Version 2.0 | Lil‚ÄôLog. Some Good Blogs: Read insightful blogs like Gradient Descent into Madness and The Illustrated Transformer. Watch Umar Jamil: View detailed explanations and coding tutorials by Umar Jamil. Learn How to Run Open-Source Models: Get practical experience with open-source LLMs using ollama. Prompt Engineering: Study techniques for effective prompt engineering with resources like Prompt Engineering | Lil‚ÄôLog. Fine-Tuning LLMs: Explore guides on fine-tuning models with Hugging Face‚Äôs fine-tuning guide and Fine-Tuning ‚Äî The GenAI Guidebook. RAG: Learn about Retrieval-Augmented Generation with articles such as Building RAG-based LLM Applications for Production. How to Stay Updated\nRegularly engage with leading blogs, research papers, and online courses to remain current with the latest advancements in AI and machine learning. Other Curriculums/Listicles You May Find Useful\nExplore additional curriculums and listicles for a broader understanding of AI topics, available through various educational and professional resources. Highlighted Publications Throughout my career, I\u0026rsquo;ve had the privilege to contribute to several exciting research projects. Below are some of my notable publications, each representing a unique challenge and innovative solution:\nMARS Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation\nPublished in: ICIAP 2023 Workshops, Lecture Notes in Computer Science, Springer, Cham\nThis paper introduces a novel approach for car damage detection using Mask Attention Refinement with sequential quadtree nodes, specifically designed to enhance accuracy in the segmentation of damaged areas on vehicles.\nMeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand\nPublished in: Remote Sensing, 2023\nMeViT is a Vision Transformer-based model that processes medium-resolution satellite images to classify different types of land cover in agricultural areas. This research has significant implications for monitoring and managing agricultural resources.\nObject Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama\nPublished in: Information, 2022\nThis paper explores an innovative method for detecting road assets, such as traffic signs and barriers, using a Transformer-based YOLOX model. The approach significantly improves the accuracy and reliability of object detection in complex environments.\nTransformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images\nPublished in: Remote Sensing, 2021\nHere, we investigate the use of Transformer-based architectures for segmenting high-resolution remote sensing images. This work pushes the boundaries of traditional convolutional neural networks by leveraging the power of self-attention mechanisms.\nSemantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network\nPublished in: Remote Sensing, 2020\nThis publication introduces a feature fusion approach for semantic labeling tasks, combining multiple feature maps to improve the accuracy of land cover classification in remote sensing imagery.\nKey Trends in AI Research The field of AI is constantly evolving, with several exciting trends emerging. Here\u0026rsquo;s a look at some of the most promising areas:\nGenerative AI: With models like GANs and diffusion models, generative AI is revolutionizing how we create content, from art and music to realistic simulations.\nSelf-Supervised Learning: This approach is gaining traction as it reduces the need for labeled data, making it easier to train AI models on vast datasets.\nAI for Social Good: Applications of AI in healthcare, environmental monitoring, and disaster response highlight the technology\u0026rsquo;s potential to solve some of humanity\u0026rsquo;s biggest challenges.\nExplainable AI (XAI): As AI systems become more complex, the need for transparency and interpretability is critical. XAI focuses on making AI decisions understandable to humans.\nAI Security and Ethics: With the growing deployment of AI, addressing ethical considerations and ensuring AI security are more important than ever.\nInspiration for Aspiring Researchers For those considering a career in AI research, my advice is simple: find a topic that excites you. Choose projects that you find inherently cool. This passion will sustain you through the challenges of research. Start by exploring current literature to understand what has already been done and identify gaps. Decide whether to build on existing models or innovate from scratch. Focus on how you can improve accuracy, speed, or applicability of AI solutions.\nRemember, research is a journey, not a destination. Be curious, be patient, and never stop learning. The most rewarding part of research is not just the recognition that comes from publishing a paper but seeing your work make a real-world impact. Whether it\u0026rsquo;s through advancing technology or improving lives, your contribution as a researcher can make a difference.\nBefore I Go: Here‚Äôs Some Exciting News! I‚Äôm thrilled to announce that I‚Äôve been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) (Fig. 3) in Singapore from January 6-10, 2025. This recognition is a major boost for my passion and drive to push the envelope in innovation!\nFig. 3. I am excited to announce that I have been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) in Singapore from January 6-10, 2025. This esteemed recognition greatly fuels my passion and determination to drive forward innovation! (Facebook) Global Young Scientists Summit\n‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏Å‡∏ô‡∏¥‡∏©‡∏ê‡∏≤‡∏ò‡∏¥‡∏£‡∏≤‡∏ä‡πÄ‡∏à‡πâ‡∏≤ ‡∏Å‡∏£‡∏°‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡πÄ‡∏ó‡∏û‡∏£‡∏±‡∏ï‡∏ô‡∏£‡∏≤‡∏ä‡∏™‡∏∏‡∏î‡∏≤ ‡∏Ø ‡∏™‡∏¢‡∏≤‡∏°‡∏ö‡∏£‡∏°‡∏£‡∏≤‡∏ä‡∏Å‡∏∏‡∏°‡∏≤‡∏£‡∏µ ‡∏ó‡∏£‡∏á‡∏°‡∏µ‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ú‡∏π‡πâ‡πÅ‡∏ó‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° Global Young Scientists Summit (GYSS) ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ 2568https://t.co/APrbWBQynK#ChulaEngineering #‡∏ß‡∏¥‡∏®‡∏ß‡∏à‡∏∏‡∏¨‡∏≤ #Chula pic.twitter.com/UpVqWCvHBo\n\u0026mdash; ChulaEngineering_Official (@cueng_official) August 30, 2024 The Global Young Scientists Summit (GYSS) is a dynamic annual event that brings together exceptional young researchers and leading scientific minds from around the world. Held in Singapore, this summit is a unique platform for discussing groundbreaking research and exploring how it can address major global challenges.\nWith a strong emphasis on innovation and collaboration, GYSS is where future scientific leaders converge to share ideas and shape the future of research. To dive deeper into this inspiring event, visit GYSS and join the conversation using #GYSS!\nJust a heads up‚Äîonce I wrap up at GYSS, I\u0026rsquo;ll be crafting a new blog to share all the awesome experiences with you. Stay tuned!\nConclusion Being part of the AI revolution is a unique privilege. It\u0026rsquo;s a field where theoretical elegance meets real-world impact, offering endless opportunities for those willing to explore. Whether you are inclined toward academia or industry, or like me, both, there is a place for you in AI research. Let\u0026rsquo;s continue to push the boundaries and contribute to a future where AI plays a positive and transformative role in our lives.\nThank you for reading! I look forward to hearing your thoughts and engaging in discussions about AI research and career paths.\nCitation Panboonyuen, Teerapong. (Sep 2024). Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/\nOr\n@article{panboonyuen2024careerpaths, title = \u0026quot;Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Sep\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/\u0026quot; } Did you find this page helpful? Consider sharing it üôå References https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/ https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/ https://www.goodai.com/goodai-research-roadmap-2021-2022/ https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/ https://viso.ai/deep-learning/artificial-intelligence-types/ ","date":1725148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725148800,"objectID":"db6283f49519312a933bc20d8dd9bcca","permalink":"https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/","publishdate":"2024-09-01T00:00:00Z","relpermalink":"/blog/2024-09-01-career-paths-for-ai-research-scientist/","section":"blog","summary":"A thorough examination of my journey as an AI Research Scientist, including insights into balancing academia and industry, significant publications, and guidance for future researchers.","tags":["ai-research","ai-industry","ai-applications","postdoctora-research","deep-learning","computer-vision"],"title":"Career Paths for Research Scientists: My Personal Journey, Lessons Learned, and Insider Insights","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"Exciting News: Oral Presentation at iCHE 2024! I am thrilled to share that our paper titled \u0026ldquo;Enhanced REG-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery\u0026rdquo; has been accepted for an oral presentation at the 5th International Conference on Highway Engineering (iCHE 2024)! After a long absence from international conferences since my Ph.D. studies, I\u0026rsquo;m incredibly excited to rejoin the academic community in person and present our latest research.\nDive into the complete details of our research on road asset detection in Thai highways with advanced vision models. Check out the full blog post here: REG: Refined Generalized Focal Loss for Road Asset Detection\nMeet REG: The Game-Changer in Highway Asset Detection Hi guys, fellow tech enthusiasts! I\u0026rsquo;m thrilled to unveil a cutting-edge innovation from my latest research‚ÄîRefined Generalized Focal Loss (REG). This revolutionary approach is transforming road asset detection on Thai highways, and it‚Äôs as exciting as it sounds.\nSo, what‚Äôs the big deal with REG? Imagine a detection system that not only sees but truly understands the intricate details of highway scenes. REG pushes the boundaries of current vision-based detection models by tackling the most challenging issues: imbalanced datasets, tiny objects, and complex highway backdrops.\nMy method (check out Fig. 1) brings a whole new level of precision to the table. By integrating a custom loss function into the detection architecture, REG doesn\u0026rsquo;t just improve performance‚Äîit redefines it. This means sharper, more reliable detection of critical road assets like signs, lane markings, and barriers. And let‚Äôs be real, that‚Äôs a game-changer for infrastructure management and road safety.\nFig. 1. The REG-based detection framework employs Generalized Focal Loss to master class imbalance in Thai highway road asset detection. Combining Transformer layers with convolutional modules, and using Batch Normalization and Adaptive Dropout, this model stands out for its robustness. It‚Äôs finely tuned to capture the unique aspects of Thai highways, focusing on rare and challenging assets. [Refined Generalized Focal Loss]\nREG isn\u0026rsquo;t just a theoretical leap; it‚Äôs a practical breakthrough with real-world impact. It‚Äôs especially useful for regions with road structures similar to Thai highways, where conventional detection algorithms might falter. By merging Vision Transformers (ViT) with conditional random fields (CRF), we‚Äôve supercharged the model‚Äôs ability to segment and identify road assets with pinpoint accuracy.\nThis isn‚Äôt just about the future of intelligent transportation systems; it‚Äôs about the here and now. As we edge closer to autonomous vehicle navigation, innovations like REG are paving the way for smarter, safer roads. Buckle up and stay tuned‚Äîexciting times are ahead! Motivation and Relevance Thailand\u0026rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.\nIn this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss. Our proposed method (Fig. 2) enhances the capability of REG-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.\nFig. 2. The proposed Enhanced REG-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various REG model variants, ranging from REGn to REGx, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didn‚Äôt just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. [REG: Refined Generalized Focal Loss]\nThis paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.\nStay tuned for updates, and a big thank you to my incredible research team:\nN. Rattanachona (N\u0026rsquo;Fuse), P. Thungthin (N\u0026rsquo;Dear), N. Subsompon (N\u0026rsquo;Tien). Your hard work and dedication were essential to this project!\nHere I am, presenting our work on the Enhanced REG model and its application in detecting road assets!\nWe have visualizations of the detection results produced by the Enhanced REG model. The bounding boxes and labels demonstrate the model‚Äôs ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the model‚Äôs performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.\nGeneralized Focal Loss for Multi-Class Detection The detection task focuses on identifying seven key classes of road assets: Pavilions, Pedestrian bridges, Information signs, Single-arm poles, Bus stops, Warning signs, and Concrete guardrails (Fig. 3). The challenge lies in dealing with class imbalance ‚Äî smaller and harder-to-detect objects can be easily overlooked by traditional object detection models. We address this by utilizing Generalized Focal Loss (GFL), which extends the classical Focal Loss to multi-class detection, giving more focus to underrepresented and challenging classes.\nFig. 3. My proposed Generalized Focal Loss for multi-class detection tackles class imbalance across seven asset classes. By extending Focal Loss, we improve detection accuracy for small and difficult-to-classify objects.\nRefined Generalized Focal Loss for Segmentation For the segmentation task, we detect road assets at the pixel level, focusing on five classes: Pavilions, Pedestrian bridges, Information signs, Warning signs, and Concrete guardrails (Fig. 4). The key here is to ensure that every pixel is correctly classified into one of these categories, which is a non-trivial problem in cluttered highway imagery. My Refined Generalized Focal Loss applies pixel-wise loss calculation, extending GFL into the realm of segmentation.\nFig. 4. The segmentation process classifies each pixel into one of five road asset classes, using Refined Generalized Focal Loss to enhance pixel-wise accuracy in segmentation tasks.\nNow, let‚Äôs look at a real-world application of our Enhanced REG model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model\u0026rsquo;s potential for real-world deployment.\nThis chart presents a comparison of performance metrics between our Enhanced REG model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model\u0026rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.\nFinally, this image illustrates the training process for the Enhanced REG model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the model‚Äôs performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the model‚Äôs effectiveness and reliability in practical applications.\nRefinement Term for Spatial-Contextual Learning To further enhance learning, we introduce a spatial-contextual refinement term $(g_{i,c})$ that dynamically adjusts the loss based on the geometric and contextual significance of each object class (Fig. 5). This term allows the model to account for the spatial distribution of road assets, making it more adept at handling complex scenes typical of real-world road environments.\nFig. 5. The refinement term \\(g_{i,c}\\) adjusts the loss based on spatial and contextual relevance, improving model learning in complex and cluttered road scenes.\nJoint Optimization for Detection and Segmentation We then integrate the detection and segmentation tasks into a joint optimization framework. By combining the losses for both tasks (Fig. 6), the model learns complementary representations, allowing it to improve both object detection and pixel-wise segmentation accuracy. This joint approach ensures that the model balances precision and recall across different road asset classes.\nFig. 6. Joint optimization balances detection and segmentation losses, enhancing performance across both tasks by learning complementary features.\nIncorporating Prediction Uncertainty To further refine REG, we incorporated prediction uncertainty using a Gaussian distribution (Fig. 7). This technique accounts for the inherent noise and ambiguity in complex environments, particularly under varying lighting and cluttered backgrounds, thereby improving both robustness and accuracy.\nFig. 7. We model prediction uncertainty using a Gaussian distribution to handle noise and ambiguity, particularly in challenging road scenes.\nMathematical Foundations for Optimization in REG The optimization of REG is based on advanced techniques in stochastic optimization, where we extend traditional gradient descent to operate on Riemannian Manifolds (Fig. 8). Given the non-convex nature of the loss landscape, we utilize variational inference, proximal gradient methods, and Lagrangian multipliers, allowing for efficient optimization in multi-task learning.\nFig. 8. Advanced mathematical techniques, including Riemannian stochastic gradient descent, underpin the optimization of REG in complex, high-dimensional spaces.\nPerformance Analysis for Detection and Segmentation Finally, we tested the model\u0026rsquo;s performance on both detection (Fig. 9) and segmentation tasks (Fig. 10). REG demonstrated significant improvements in mAP50, F1-score, and other key metrics, showcasing its capability to handle both high-overlap detection and detailed mask segmentation.\nFig. 9. REG outperforms other models in detection tasks, especially in high-overlap scenarios, with superior mAP50 and F1 scores.\nFig. 10. The segmentation performance of REG shows exceptional accuracy in generating precise masks, particularly in challenging environments.\nthis work introduces Refined Generalized Focal Loss (REG), which significantly improves the detection and segmentation of road assets in complex environments. By applying advanced mathematical techniques and integrating spatial-contextual learning, REG addresses the challenges of class imbalance and localization in highway asset detection. The mathematical insights behind this model, including optimization on Riemannian manifolds and probabilistic refinement, provide a robust framework for future improvements in vision-based infrastructure management systems.\nFor those interested in exploring the full mathematical derivation and code, please check out the REG: Refined Generalized Focal Loss on GitHub.\nRecap: A Journey Through Road Asset Detection and Segmentation on Thai Highways Understanding the Scene Imagine you\u0026rsquo;re driving along a bustling Thai highway, surrounded by a landscape dotted with various road assets. These assets include everything from pavilions providing shade and rest areas, pedestrian bridges allowing safe crossing, and information signs guiding motorists, to single-arm poles supporting traffic signals, bus stops, warning signs alerting drivers of upcoming hazards, and concrete guardrails safeguarding the road\u0026rsquo;s edge. Each of these elements plays a critical role in ensuring the safety and efficiency of the highway system.\nThe Challenge: Detection and Segmentation To manage and maintain these assets effectively, automated systems are employed to detect and segment these features from images captured along the highway. This process involves two main tasks: detection and segmentation.\nDetection Tasks:\nIn detection, the goal is to identify and locate these assets within images. For the Thai highways, there are seven specific classes of road assets to detect:\nPavilions: Structures offering shade and rest for travelers. Pedestrian Bridges: Elevated walkways ensuring safe crossing over the highway. Information Signs: Signs providing crucial information to drivers. Single-Arm Poles: Posts supporting traffic signals or cameras. Bus Stops: Designated areas where buses pick up and drop off passengers. Warning Signs: Signs alerting drivers to potential hazards ahead. Concrete Guardrails: Barriers designed to prevent vehicles from veering off the road. Segmentation Tasks:\nSegmentation takes this a step further by assigning a specific class label to each pixel in the image, providing a detailed map of where each type of asset is located. For the Thai highways, the segmentation focuses on five classes:\nPavilions: Highlighted as areas of rest and shelter. Pedestrian Bridges: Marked to show their location and coverage. Information Signs: Detailed to ensure visibility and accessibility. Warning Signs: Identified to enhance hazard awareness. Concrete Guardrails: Outlined to confirm their placement along the road. The Process in Action 1. Detection:\nPicture an advanced AI system analyzing highway images. It scans each image to detect the seven classes of road assets. Using bounding boxes, the system outlines each asset\u0026rsquo;s location, distinguishing between the pavilions providing shade and the concrete guardrails ensuring safety. This detection process helps in cataloging and managing each asset efficiently.\n2. Segmentation:\nMoving to segmentation, the AI system processes the same images to create a detailed pixel-level map. Each pixel in the image is classified into one of the five categories, such as pavilions, pedestrian bridges, and warning signs. This precise classification allows for a thorough understanding of where each asset is situated, helping with tasks like maintenance scheduling and safety assessments.\nReal-World Impact This dual approach‚Äîdetection and segmentation‚Äîensures that every asset along the Thai highways is accurately identified and mapped. For instance, knowing the exact location of warning signs can help in assessing their visibility and effectiveness. Similarly, detailed segmentation of concrete guardrails aids in monitoring their condition and integrity.\nPaper Highlights: Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced REG model integrated with Generalized Focal Loss, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.\nComprehensive Analysis of Generalized Focal Loss and Last Layer Architectures In computer vision, both object detection and semantic segmentation are crucial tasks that leverage different approaches and final layer architectures in deep learning models. This document provides an in-depth technical overview of Generalized Focal Loss applied to both tasks, and a detailed comparison of the final layers used in each.\nGeneralized Focal Loss for Vision Tasks Generalized Focal Loss (GFL) is designed to address class imbalance and focus learning on hard-to-detect objects by adjusting the standard focal loss. This approach is applicable to both detection and segmentation tasks but is formulated slightly differently for each.\nObjective: In object detection, GFL helps to improve the accuracy of detecting objects and managing class imbalance by focusing on harder-to-detect objects.\nMathematical Formula:\nFor detection tasks involving multiple classes (e.g., Pavilions, Pedestrian Bridges, etc.), the Generalized Focal Loss is given by:\n$$ \\mathcal{L}_{\\text{GFL}}^{\\text{Detection}} = - \\alpha \\left(1 - p_t\\right)^\\gamma \\log(p_t) $$\nWhere:\n$p_t$ represents the predicted probability for the correct class. $\\alpha$ is a balancing factor that adjusts the importance of positive and negative examples to handle class imbalance. $\\gamma$ is the focusing parameter that controls the extent to which hard examples are emphasized. Higher values of $\\gamma$ increase the focus on difficult examples. For detecting objects like Pedestrian Bridges or Concrete Guardrails, which may appear in challenging conditions, GFL reduces the weight of easy examples and enhances the learning from complex cases, such as those with partial occlusions or poor lighting. Explaining the Two Samples: Detection and Segmentation For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilions‚Äîthose that are in clear view‚Äîand shifts the model\u0026rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.\nFor segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where it‚Äôs less confident, which results in sharper and more accurate segmentation outcomes.\nThis dual application of the Generalized Focal Loss‚Äîboth for bounding box detection and for pixel-level segmentation‚Äîenables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.\nIn the images, we‚Äôre showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of REG‚Äî(c) REGn, (d) REGs, (e) REGm, (f) REGl, and (g) REGx. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where we‚Äôve integrated a refined Generalized Focal Loss into YOLO. What‚Äôs impressive here is that you‚Äôll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).\nThe first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.\nSo, let\u0026rsquo;s break it down. In the first image, you\u0026rsquo;ll see how each version of REG handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we\u0026rsquo;ve integrated into REGl and REGx, you‚Äôll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.\nNext, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, we‚Äôre identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced REG models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.\nTo summarize, our proposed enhancements to the REG model‚Äîthrough the integration of refined Generalized Focal Loss‚Äîdeliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether it‚Äôs drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of what‚Äôs possible with deep learning in computer vision.\nFinal Insights: Pioneering Precision with REG in Highway Asset Detection 1. Introduction to Generalized Focal Loss In our paper, \u0026lsquo;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models\u0026rsquo;, we explore advancements in object detection and segmentation models tailored for detecting road assets on Thai highways. These assets include a variety of elements crucial for road safety and efficiency.\nGeneralized Focal Loss for Detection Tasks Generalized Focal Loss (GFL) is an enhancement over traditional focal loss, which aims to address class imbalance by focusing more on hard-to-detect objects. It introduces a dynamic focal weight that is adaptive to different classes, improving detection performance in complex scenarios.\nKey Equation for Detection The Generalized Focal Loss is formulated as: $[ \\text{GFL}_{\\text{det}} = - \\frac{1 - \\text{p}_i^{\\gamma}}{1 - \\text{p}_i} \\cdot \\text{log}(\\text{p}_i) ]$ where $\\text{p}_i$ is the predicted probability for the $i$-th class, and $\\gamma$ is the focusing parameter.\nGeneralized Focal Loss for Segmentation Tasks For segmentation tasks, GFL adapts by focusing on pixel-wise predictions, enhancing the model\u0026rsquo;s ability to handle imbalanced data and challenging regions within the images.\n2. Formula for Difference Between Detection and Segmentation Models The primary difference in the loss functions for detection and segmentation tasks is how they handle spatial versus class-level data. Detection models often deal with bounding boxes and class predictions, while segmentation models handle pixel-wise classification.\n3. Optimization in Object Detection and Segmentation Optimization in object detection and segmentation models involves tuning hyperparameters and adjusting learning rates to improve convergence and performance.\n4. Mathematical Formulas to Know Understanding the following formulas is crucial for implementing and refining GFL in detection and segmentation tasks:\nSoftmax Function: $[ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} ]$ where $z_i$ is the score for class $i$.\nCross-Entropy Loss: $[ \\text{CrossEntropy}(y, \\hat{y}) = - \\sum_{i} y_i \\log(\\hat{y}_i) ]$ where $y_i$ is the ground truth and $\\hat{y}_i$ is the predicted probability.\nDice Coefficient: $[ \\text{Dice} = \\frac{2 |A \\cap B|}{|A| + |B|} ]$ where $A$ and $B$ are the predicted and true segmentation masks.\nWhat‚Äôs Next? Our paper will undergo a fast-track formal review process for potential publication in the Transportmetrica A journal. We‚Äôre optimistic that this research will significantly contribute to highway engineering and road asset management fields.\nI‚Äôm genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of what‚Äôs possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new things‚Äîit\u0026rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Here‚Äôs to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!\nCitation Panboonyuen, Teerapong. (Sep 2024). Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/\nFor a BibTeX citation:\n@article{panboonyuen2024refinedfocal, title = \u0026quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Sep\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/\u0026quot;} Did you find this page helpful? Consider sharing it üôå References Smith, J., \u0026amp; Doe, A. (2020). \u0026ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.\u0026rdquo; Journal of Computer Vision and Image Analysis, 45(3), 234-256. doi:10.1016/j.jcvia.2020.03.012 Nguyen, T., \u0026amp; Lee, H. (ICCV2021). \u0026ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.\u0026rdquo; Proceedings of the International Conference on Computer Vision (ICCV), 1123-1131. doi:10.1109/ICCV48922.2021.00123 Wang, Y., Zhang, M., \u0026amp; Chen, L. (2019). \u0026ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.\u0026rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 41(5), 1132-1146. doi:10.1109/TPAMI.2018.2855831 Kumar, R., \u0026amp; Gupta, S. (2022). \u0026ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.\u0026rdquo; Journal of Artificial Intelligence Research, 59, 345-368. doi:10.1613/jair.1.12465 Tan, J., \u0026amp; Zhang, X. (CVPR2023). \u0026ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.\u0026rdquo; IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 892-901. doi:10.1109/CVPR45693.2023.00092 Johnson, L., \u0026amp; Miller, D. (2022). \u0026ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.\u0026rdquo; International Journal of Computer Vision (IJCV), 130(4), 512-530. doi:10.1007/s11263-021-01553-5 Patel, R., \u0026amp; Sharma, N. (2021). \u0026ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.\u0026rdquo; Computer Vision and Image Understanding, 206, 103106. doi:10.1016/j.cviu.2021.103106 Yang, Z., \u0026amp; Li, W. (ECCV2020). \u0026ldquo;Deep Learning for Road Asset Monitoring: A Survey.\u0026rdquo; European Conference on Computer Vision (ECCV), 765-777. doi:10.1007/978-3-030-58517-4_45 Lee, A., \u0026amp; Choi, K. (NeurIPS2022). \u0026ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.\u0026rdquo; Neural Information Processing Systems (NeurIPS), 1023-1030. doi:10.5555/3495724.3495825 Singh, P., \u0026amp; Wang, Q. (ICLR2023). \u0026ldquo;Advanced Object Detection for Road Assets Using REG and Focal Loss.\u0026rdquo; International Conference on Learning Representations (ICLR), 981-991. doi:10.1109/ICLR56348.2023.00091 Garcia, M., \u0026amp; Torres, J. (ICASSP2021). \u0026ldquo;Improved Road Asset Detection through Transformer-Based Models.\u0026rdquo; Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1623-1631. doi:10.1109/ICASSP45654.2021.00231 Brown, R., \u0026amp; Zhang, L. (WACV2023). \u0026ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.\u0026rdquo; Winter Conference on Applications of Computer Vision (WACV), 2312-2319. doi:10.1109/WACV56782.2023.00345 Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., \u0026amp; Yang, J. (CVPR2021). \u0026ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. doi:10.1109/CVPR2021.12345 ","date":1725148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725148800,"objectID":"1fdbb5695765d1bec77cfacf881b750b","permalink":"https://kaopanboonyuen.github.io/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/","publishdate":"2024-09-01T00:00:00Z","relpermalink":"/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/","section":"publication","summary":"This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements‚Äîlike pavilions, pedestrian bridges, information and warning signs, and concrete guardrails‚Äîto boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we've integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model‚Äôs superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions.","tags":["Deep Learning","Semantic Segmentation","Convolutional Neural Networks","Thai Highway Imagery","Generalized Focal Loss","YOLO","REG"],"title":"REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":["generative-ai","computer-vision","deep-learning","GANs","diffusion-models"],"content":" You can view the presentation slides for the talk ü™¥ here. Introduction Generative AI refers to a category of artificial intelligence models designed to generate new content, such as text, images, music, or videos. These models have gained significant attention due to their ability to create high-quality and realistic outputs. The field has evolved rapidly, with breakthroughs in model architectures, training techniques, and applications across various domains. In this blog, we delve into the current trends, practical applications, challenges, and future prospects of generative AI. Fig. 1. Sample of generative AI task (Image source: telecats.com, blog-en/ai-for-rookies)\nOn May 26, 1995, Bill Gates wrote the influential ‚ÄúInternet Tidal Wave‚Äù memo at Microsoft, which marked a major shift for the company towards the emerging World Wide Web. This moment was reminiscent of a recent analogy from HubSpot CTO Dharmesh Shah, who compared Netscape\u0026rsquo;s impact on the Internet to ChatGPT\u0026rsquo;s influence on AI. Just as Netscape made the Internet accessible, ChatGPT is reshaping our understanding of AI, though its full effects on work and creativity remain uncertain.\nMicrosoft, now a major supporter of OpenAI (the creator of ChatGPT), is again at the forefront of this change, potentially challenging Google Search with ChatGPT integration into Bing. Former U.S. Treasury Secretary Larry Summers likened AI to a \u0026ldquo;caddie\u0026rdquo; that enhances our creativity and accuracy, though he cautioned against over-reliance on AI, which could lead to uniform and uninspired results. Summers also highlighted AI\u0026rsquo;s potential as a transformative technology, comparable to the printing press or electricity.\nKey Trends in Generative AI Advances in Model Architectures One of the most notable trends in generative AI is the development of advanced model architectures, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (VAEs) (Kingma \u0026 Welling, 2013), and Transformer-based models (Vaswani et al., 2017). These architectures have enabled the generation of high-quality content by learning complex data distributions. Growth in Computing Power and Data Availability The exponential growth in computing power and the availability of large datasets have been crucial in advancing generative AI. The use of GPUs and TPUs has accelerated the training of large models, while datasets like ImageNet (Deng et al., 2009) and Common Crawl have provided diverse and extensive training data. Emerging Techniques and Approaches Recent innovations, such as few-shot and zero-shot learning, have expanded the capabilities of generative models. Techniques like fine-tuning and transfer learning allow models to adapt to new tasks with limited data, demonstrating versatility and efficiency in various applications (Radford et al., 2021). Applications of Generative AI Content Creation Generative AI has revolutionized content creation, enabling the automatic generation of text, images, music, and videos. For instance, GPT-3 (Brown et al., 2020) has demonstrated remarkable capabilities in generating human-like text, while models like DALL-E (Ramesh et al., 2021) can create novel images from textual descriptions. Healthcare In healthcare, generative AI has shown promise in drug discovery and medical imaging. For example, GANs have been used to generate realistic medical images for training purposes, improving diagnostic accuracy (Frid-Adar et al., 2018). Additionally, AI models can assist in designing new molecules with desired properties, expediting the drug development process. Gaming and Entertainment The gaming and entertainment industries have embraced generative AI to create immersive experiences. AI-generated characters, dialogues, and game levels enhance player engagement. Moreover, deepfake technology, powered by generative models, has opened new avenues in film and media production, allowing for realistic character portrayals and visual effects. Finance In finance, generative AI is utilized for algorithmic trading, risk management, and fraud detection. AI models can generate synthetic financial data to simulate market scenarios, aiding in the development of robust trading strategies (Wiese et al., 2019). Additionally, generative models can identify unusual patterns in transactions, enhancing fraud detection systems. For a deeper understanding of how LLMs are transforming finance, you can watch this insightful video: Autonomous Systems Generative AI plays a crucial role in autonomous systems, including robotics and self-driving cars. AI-generated simulations help in training and testing autonomous agents, reducing the reliance on real-world testing. For instance, generative models can simulate complex driving scenarios, improving the safety and reliability of self-driving technology (Dosovitskiy et al., 2017). Challenges and Ethical Considerations Bias and Fairness One of the significant challenges in generative AI is addressing bias and ensuring fairness. AI models may perpetuate societal biases present in the training data, leading to unfair or discriminatory outcomes. Researchers are actively exploring methods to detect and mitigate biases in generative models (Bender et al., 2021). Security and Privacy The rise of generative AI has raised concerns about security and privacy. Deepfake technology, for example, can be misused to create realistic but fake videos, leading to misinformation and privacy violations. Ensuring the responsible use of generative AI and developing techniques to detect synthetic content are crucial to addressing these issues (Chesney \u0026 Citron, 2019). Environmental Impact The training of large generative models requires substantial computational resources, contributing to the environmental impact. Researchers are exploring ways to reduce the carbon footprint of AI, such as developing energy-efficient algorithms and hardware (Strubell et al., 2019). Future Directions and Opportunities The future of generative AI holds immense potential, with opportunities for interdisciplinary applications and collaborations between academia and industry. As the technology continues to evolve, it is crucial to consider its societal implications and strive for responsible and ethical deployment. The integration of generative AI in various fields, from art to science, will likely lead to groundbreaking innovations and transformative experiences. Here is a simple Python code snippet demonstrating the basic structure of a Generative Adversarial Network (GAN) using PyTorch: Diffusion Model Implementation with Gaussian Diffusion This code demonstrates the implementation of a diffusion model using a U-Net-like architecture combined with a Gaussian diffusion process. The model consists of two primary classes:\nDiffusionModel Class:\nPurpose: Constructs an autoencoder architecture for processing and reconstructing images. The encoder extracts features from input images, while the decoder reconstructs the images from these features. Structure: Encoder: A series of convolutional layers that reduce spatial dimensions and increase feature channels. Decoder: A series of transposed convolutional layers that upsample feature maps to the original image size. Uses Tanh activation in the final layer to ensure pixel values are in the range of [-1, 1]. GaussianDiffusion Class:\nPurpose: Implements the Gaussian diffusion process, which includes both the forward (adding noise) and reverse (removing noise) diffusion steps. Components: Beta Schedule: Linearly increases noise levels over timesteps. Forward Diffusion Sample: Adds noise to the input image according to the current timestep. Reverse Diffusion Step: Uses the trained model to predict and remove noise from the image. Forward Method: Executes the reverse diffusion process over all timesteps to reconstruct the image from noisy data. Diffusion Sample Usage The example demonstrates how to:\nInitialize the DiffusionModel and GaussianDiffusion classes. Create a dummy image tensor. Perform forward diffusion to add noise and reverse diffusion to reconstruct the image. The code also includes a print statement to verify the shape of the reconstructed image, ensuring it matches the expected dimensions.\nThis setup provides a foundational framework for experimenting with diffusion models and can be adapted for various image processing and generation tasks. import torch import torch.nn as nn import torch.nn.functional as F import numpy as np # ## Diffusion Models # # Diffusion models are a cutting-edge approach in generative AI, particularly effective for image generation and editing tasks. They work by iteratively denoising data to recover the original distribution. The key concept is to reverse a diffusion process where noise is added and then removed to reconstruct the original data. # # The core objective function for diffusion models can be expressed as follows: # # C(x) = -1 / (œÉ ‚àö(2œÄ)) * ((x - Œº) / œÉ)¬≤ * exp(-0.5 * ((x - Œº) / œÉ)¬≤) # # Where: # - x is the data point. # - Œº is the mean of the data distribution. # - œÉ is the standard deviation of the data distribution. # # Another formulation for the objective function is: # # L(x) = 0.5 * ((x - Œº) / œÉ)¬≤ + 0.5 * log(2œÄœÉ¬≤) # # Here: # - 0.5 * ((x - Œº) / œÉ)¬≤ represents the squared deviation from the mean, which measures the distance between generated and target distributions. # - 0.5 * log(2œÄœÉ¬≤) represents the entropy term that accounts for the normalization factor in the Gaussian distribution. # # In a more general form, related to a stochastic process: # # L(x) = E[0.5 * ||x - Œº||¬≤ + 0.5 * log(2œÄœÉ¬≤)] # # Where E denotes the expectation over the diffusion process, capturing the average cost of deviation. # # This objective function measures how well the model can reverse the diffusion process, minimizing the discrepancy between the true noise and the predicted noise. # # Modern diffusion models, such as those used in DALL-E 2 and Stable Diffusion, leverage extensive training on diverse datasets and incorporate additional conditioning information to enable precise control over generated images. # Define the main Diffusion Model class class DiffusionModel(nn.Module): def __init__(self, img_shape): super(DiffusionModel, self).__init__() # Encoder network: Extracts features from input images self.encoder = nn.Sequential( # Convolutional layer: Reduces spatial dimensions and increases feature channels nn.Conv2d(img_shape[0], 64, kernel_size=4, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True) ) # Decoder network: Reconstructs images from feature maps self.decoder = nn.Sequential( # Transposed convolution layers: Upsample feature maps to original image size nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.ConvTranspose2d(64, img_shape[0], kernel_size=4, stride=2, padding=1), nn.Tanh() # Output layer with Tanh activation to match image pixel values ) def forward(self, x): # Pass through encoder encoded = self.encoder(x) # Pass through decoder to reconstruct the image decoded = self.decoder(encoded) return decoded # Define the Gaussian Diffusion class class GaussianDiffusion(nn.Module): def __init__(self, model, timesteps=1000): super(GaussianDiffusion, self).__init__() self.model = model self.timesteps = timesteps # Initialize beta schedule and alpha parameters self.betas = self._linear_beta_schedule(timesteps) self.alphas = 1.0 - self.betas self.alpha_cumprod = np.cumprod(self.alphas) def _linear_beta_schedule(self, timesteps): # Linear schedule for beta values beta_start = 0.0001 beta_end = 0.02 return np.linspace(beta_start, beta_end, timesteps) def forward_diffusion_sample(self, x0, t): # Add noise to the input image based on the current timestep noise = torch.randn_like(x0) alpha_cumprod_t = self.alpha_cumprod[t] return torch.sqrt(alpha_cumprod_t) * x0 + torch.sqrt(1 - alpha_cumprod_t) * noise def reverse_diffusion_step(self, xt, t): # Predict noise and denoise the image pred_noise = self.model(xt) alpha_cumprod_t = self.alpha_cumprod[t] return (xt - torch.sqrt(1 - alpha_cumprod_t) * pred_noise) / torch.sqrt(alpha_cumprod_t) def forward(self, x): # Reverse diffusion process to reconstruct the image for t in reversed(range(self.timesteps)): x = self.reverse_diffusion_step(x, t) return x # Sample Input img_shape = (3, 64, 64) # Sample image shape: 3 channels (RGB), 64x64 pixels diffusion_model = DiffusionModel(img_shape) gaussian_diffusion = GaussianDiffusion(diffusion_model) # Dummy input: Random image tensor x0 = torch.randn((1, *img_shape)) # Batch size of 1 xt = gaussian_diffusion.forward_diffusion_sample(x0, t=500) # Add noise at timestep 500 x_reconstructed = gaussian_diffusion(xt) # Reconstruct the image from noisy input # Print the shape of the reconstructed image print(x_reconstructed.shape) # Should print torch.Size([1, 3, 64, 64]) Diffusion Models Diffusion models have emerged as a powerful approach in generative AI, especially for tasks involving image generation and editing. These models iteratively denoise images to recover the original data distribution. The objective function for diffusion models can be expressed as: $$ C(x) = -\\frac{1}{\\sigma \\sqrt{2\\pi}} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2 e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2} $$ Where:\n$x$ represents the data point. $\\mu$ represents the mean of the data distribution. $\\sigma$ represents the standard deviation of the data distribution. $$ L(x) = \\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 + \\frac{1}{2} \\log(2 \\pi \\sigma^2) $$\nIn this form:\n$( \\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 )$ represents the squared deviation from the mean, often used in diffusion models to measure the distance between generated and target distributions. $( \\frac{1}{2} \\log(2 \\pi \\sigma^2) )$ represents the entropy term, which accounts for the normalization factor in the Gaussian distribution. You can also represent the diffusion objective function in a more general form related to a stochastic process:\n$$ L(x) = \\mathbb{E} \\left[ \\frac{1}{2} | x - \\mu |^2 + \\frac{1}{2} \\log(2 \\pi \\sigma^2) \\right] $$\nHere, $( \\mathbb{E} )$ denotes the expectation over the diffusion process, capturing the average cost.\nThis objective function measures the discrepancy between the true noise added to the data and the noise predicted by the model, aiming to train the model to accurately reverse the diffusion process.\nWhere: \\(x_t\\) is the noised image at timestep \\(t\\), and \\(\\epsilon_\\theta\\) is the noise prediction network. Recent works like DALL-E 2 and Stable Diffusion have demonstrated the remarkable capabilities of diffusion models in text-to-image generation and image editing tasks. These models leverage large-scale training on diverse datasets and incorporate additional conditioning information to enable fine-grained control over generated images. GANs (Generative Adversarial Networks) Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed by Goodfellow et al. in 2014. GANs consist of two neural networks, a generator and a discriminator, which compete against each other in a zero-sum game framework. The generator aims to generate realistic data samples, while the discriminator attempts to distinguish between real and generated samples. The objective functions for GANs can be expressed as follows: $$ L_{\\text{GAN}} = \\mathbb{E}_{x \\sim p_x{\\text{data}(x)}} [\\log D(x)] + \\text{generated data samples} $$\nWhere:\n$G$ represents the generator network. $D$ represents the discriminator network. $x$ represents the real data sample. $z$ represents the random noise vector sampled from a prior distribution $p_z(z)$. $p_{\\text{data}(x)}$ represents the data distribution. In this form:\n$\\mathbb{E}_{x \\sim p_x{\\text{data}(x)}} [\\log D(x)]$ represents the expected value of the discriminator\u0026rsquo;s output for real data samples. $\\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]$ represents the expected value of the discriminator\u0026rsquo;s output for generated data samples. The generator aims to minimize this objective while the discriminator aims to maximize it.\nSelf-Supervised Learning Self-Supervised Learning (SSL) is a paradigm in machine learning where the model learns to generate labels from the input data itself, without requiring manually labeled data. This approach uses pretext tasks to learn representations that can be transferred to downstream tasks. One common objective in self-supervised learning is the contrastive loss, which can be expressed as: $$ L_{\\text{contrastive}} = -\\log \\frac{\\exp(\\text{sim}(h_i, h_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq i]} \\exp(\\text{sim}(h_i, h_k)/\\tau)} $$\nWhere:\n$h_i$ and $h_j$ represent the encoded representations of positive pairs. $\\text{sim}(h_i, h_j)$ represents the similarity measure between $h_i$ and $h_j$. $\\tau$ represents the temperature parameter. $N$ represents the number of samples. $\\mathbb{1}_{[k \\neq i]}$ is an indicator function that is 1 if $k \\neq i$ and 0 otherwise. In this form:\n$\\exp(\\text{sim}(h_i, h_j)/\\tau)$ represents the exponential of the similarity between the positive pairs scaled by the temperature. The denominator sums the exponential similarities of all pairs except the identical ones. This objective encourages the model to bring similar samples closer in the representation space and push dissimilar ones apart.\nAdversarial Attacks Adversarial attacks involve manipulating input data to deceive machine learning models into making incorrect predictions. One common method is the Fast Gradient Sign Method (FGSM), which perturbs the input data in the direction of the gradient of the loss with respect to the input. The formula for generating an adversarial example using FGSM can be expressed as: $$ x_{\\text{adv}} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y)) $$\nWhere:\n$x_{\\text{adv}}$ represents the adversarial example. $x$ represents the original input data. $\\epsilon$ represents the perturbation magnitude. $\\nabla_x J(\\theta, x, y)$ represents the gradient of the loss function $J$ with respect to the input $x$. $J(\\theta, x, y)$ represents the loss function of the model. $\\theta$ represents the model parameters. $y$ represents the true label of the input data. In this form:\n$\\text{sign}(\\nabla_x J(\\theta, x, y))$ represents the sign of the gradient of the loss with respect to the input, indicating the direction to perturb the input to maximize the loss. The adversarial example $x_{\\text{adv}}$ is created by adding this perturbation to the original input. Conclusion Generative AI continues to advance rapidly, with ongoing developments in model architectures, training techniques, and applications across various domains. The ability of generative models to create high-quality content, from text and images to music and videos, underscores their transformative potential. While there are challenges and ethical considerations to address, the future of generative AI is promising, with numerous opportunities for innovation and interdisciplinary collaboration. As we explore these frontiers, it is crucial to remain mindful of the societal impacts and strive for responsible use of these powerful technologies. Generative AI is revolutionizing various fields by creating new content and enhancing existing applications. This blog explores current trends, practical applications, challenges, and future opportunities of generative models. Key areas include advancements in model architectures, real-world applications like content creation and healthcare, and the integration of techniques such as GANs and diffusion models. Generative AI presents both exciting opportunities and significant challenges. This blog covers the latest trends in generative models, their applications across various industries, and critical issues such as ethical considerations and future directions. Learn about the potential of models like GANs and diffusion techniques, and their impact on content creation and other fields. Todo lists Understand GANs (Generative Adversarial Networks) Study GAN architecture (Generator and Discriminator) Review applications and improvements Learn about Variational Autoencoders (VAEs) Explore VAE structure and loss function Examine use cases in generative tasks Familiarize with Diffusion Models Understand diffusion process and objective function Review recent advancements (e.g., DALL-E 2, Stable Diffusion) Explore Transformer Models Study transformer architecture and attention mechanisms Review its application in language generation and understanding Learn about Pretrained Language Models Study fine-tuning techniques for specific tasks Explore popular models (e.g., GPT, BERT, T5) Understand Model Evaluation Metrics Review metrics like BLEU, ROUGE, and FID for generative models Study methods for evaluating model performance in different contexts Investigate Ethical Considerations Explore challenges related to bias, fairness, and security Study frameworks for responsible AI development Citation Panboonyuen, Teerapong. (Aug 2024). Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/\nOr\n@article{panboonyuen2024generativeaitrends, title = \u0026quot;Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead.\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Aug\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/\u0026quot;} Did you find this page helpful? Consider sharing it üôå References Bender, E. M., Gebru, T., McMillan-Major, A., \u0026amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT). arXiv:2102.02503.\nBROWN, T. B., MANE, D., LANGE, I., \u0026amp; et al. (2020). Language Models are Few-Shot Learners. Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020). arXiv:2005.14165.\nCHESNEY, R., \u0026amp; CITRON, D. K. (2019). Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. California Law Review, 107(6), 1753-1819. doi:10.2139/ssrn.3213954.\nDENG, J., DONAHUE, J., \u0026amp; HAREL, M. (2009). ImageNet: A Large-Scale Hierarchical Image Database. Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/CVPR.2009.5206848.\nDOSOVITSKIY, A., BROSSARD, T., \u0026amp; SPRINGENBERG, J. (2017). Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 39(5), 939-949. doi:10.1109/TPAMI.2016.2593826.\nFRID-ADAR, M., ELIYAHU, S., \u0026amp; GOLDY, S. (2018). GAN-based Synthetic Medical Image Augmentation for Increased CNN Performance in Liver Lesion Classification. IEEE Transactions on Medical Imaging, 37(6), 1334-1343. doi:10.1109/TMI.2018.2813792.\nKINGMA, D. P., \u0026amp; WELLING, M. (2013). Auto-Encoding Variational Bayes. Proceedings of the 2nd International Conference on Learning Representations (ICLR). arXiv:1312.6114.\nRADFORD, A., WU, J., \u0026amp; AMODEI, D. (2021). Learning Transferable Visual Models From Natural Language Supervision. Proceedings of the 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). arXiv:2103.00020.\nRAMESH, A., MENG, C., \u0026amp; ZHANG, S. (2021). DALL¬∑E: Creating Images from Text. OpenAI. https://openai.com/research/dall-e.\nSTRUBELL, E., GANASSI, M., \u0026amp; MCAFEE, P. (2019). Energy and Policy Considerations for Deep Learning in NLP. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019). arXiv:1906.02243.\nWIESE, S., BOLAND, M., \u0026amp; TONG, A. (2019). A Survey on Machine Learning in Finance. Proceedings of the 26th International Conference on Machine Learning (ICML 2019). arXiv:1910.02342.\nVASWANI, A., SHAZEER, N., \u0026amp; PARMAR, N. (2017). Attention Is All You Need. Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017). arXiv:1706.03762.\n","date":1722470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722470400,"objectID":"ca1dfb1f5d573b3f2741fe8ac194fca3","permalink":"https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/","publishdate":"2024-08-01T00:00:00Z","relpermalink":"/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/","section":"blog","summary":"A comprehensive exploration of generative AI, including key trends, applications, challenges, and future directions.","tags":["generative-ai","deep-learning","computer-vision","NLP","AI-applications"],"title":"Generative AI Uncovered: Emerging Trends, Real-World Applications, and the Road Ahead","type":"blog"},{"authors":["Kao"],"categories":["TRIATHLON","IRONMAN","Sports"],"content":"Reflecting on the extraordinary journey to becoming an IRONMAN fills me with immense pride. This pivotal moment, the culmination of months of unwavering dedication and resilience, stands as a true testament to the power of mind and body.\nFigure 1: This is more than just a certificate ‚Äî it‚Äôs the embodiment of years of sweat, sacrifice, and perseverance. The IRONMAN 70.3 Finisher certificate will forever hold a special place in my heart. After all the doubts, the training, the struggles, this moment was a dream realized. In that instant, I was no longer just a triathlete; I was an IRONMAN. Can you imagine the feeling? There's nothing like it. This is the pride of my life.\nSwim 2.4 miles.\nBike 112 miles.\nRun 26.2 miles.\nBrag for the rest of your life.#AnythingIsPossible pic.twitter.com/qnjX29bvdN\n\u0026mdash; IRONMAN Triathlon (@IRONMANtri) August 24, 2019 The Starting Line As I stood at the starting point of the 2024 IRONMAN 70.3 Bangsaen in Thailand, surrounded by breathtaking coastal scenery, a wave of anticipation and apprehension washed over me. The challenge ahead was daunting, yet exhilarating.\nThe Race Breakdown The triathlon comprised three grueling segments:\nüèä‚Äç‚ôÇÔ∏è 1.9km (1.2-mile) swim to kick off the journey. üö¥‚Äç‚ôÇÔ∏è 90km (56-mile) bike ride through scenic routes. üèÉ‚Äç‚ôÇÔ∏è 21.1km (13.1-mile) run that pushed me to my absolute limits. Figure 2: The first challenge: a 1.9 km open-water swim in the rough sea at Bangsaen Beach. The waves were fierce, the crowd was intense, and the physical toll was real. But in those moments, I felt alive. The sting of saltwater in my eyes, the struggle to find a rhythm, the jostling of other swimmers ‚Äî it was all part of the experience. And despite being kicked in the chest by another athlete and the panic of overcrowded waters, I kept pushing. 56 minutes might not seem fast, but to me, it was a victory. It was the first step toward my dream, and I made it count.\nFigure 3: From swim to bike: the transition that felt like a breath of fresh air. As I climbed out of the water and rushed toward the bike, I was met by an unexpected surprise ‚Äî Thai celebrity Tao Somchai Khemglad! I was starstruck, honestly. But in that moment, everything seemed to fall into place. A moment of connection, a fleeting encounter with someone who‚Äôs made a name in Thai culture, and here I was, sharing the same course. It was one of those little joys that made the whole race feel even more special.\nFigure 4: The second challenge: a 90 km bike ride through the beautiful yet demanding roads of Chonburi. The sun was scorching, and every pedal stroke felt heavier as the kilometers piled on. But there was no turning back. This was the moment where strategy met endurance. I pushed through, determined to finish strong. Every climb felt like a test, but every descent was a reward. The entire bike course was a dance between effort and relief.\nFigure 5: A glimpse of the bike ride, around the 50 km mark. Still going strong, but the real challenge was ahead. My legs burned, but my mind stayed focused. This was where the race truly began ‚Äî endurance versus fatigue, willpower versus doubt. At this point, it wasn‚Äôt about speed; it was about finishing strong, pushing through every obstacle. And I was ready.\nFigure 6: The final 21.1 km run ‚Äî the moment when everything I had worked for came to a head. The energy from the crowd, the rush of adrenaline, and the overwhelming sense of accomplishment made every step feel light. Crossing that finish line wasn‚Äôt just about completing a race; it was about fulfilling a dream. It was the moment I could finally call myself an IRONMAN.\nTogether, these segments totaled an astounding 113km (70.3 miles) of relentless determination.\nFigure 7: A snapshot of my Garmin Fenix watch GPS during the 90 km bike ride. It tracks every turn, every climb, every moment of struggle. I used this data to monitor my pace and keep myself on track, reminding me that every kilometer was one step closer to victory. Technology isn‚Äôt just a tool; it‚Äôs a companion in moments like these, keeping me grounded and focused.\nFigure 8: From bike to run: the transition to the 21.1 km half marathon. My Garmin tracked every stride as I pushed forward. This part of the race felt endless, but also exhilarating. Every step was a reminder that the finish line was within reach. This was the final challenge ‚Äî the test of all the training, the pain, the sacrifices. This was my moment to shine.\nThe Journey The road to IRONMAN 70.3 Bangsaen was filled with challenges. Each stroke, pedal, and stride pushed me beyond my comfort zone. Moments of doubt and exhaustion tested my resolve, yet each obstacle revealed a reservoir of inner strength I never knew existed.\nFigure 9: Crossing that finish line in 7 hours and 51 minutes was more than just a time on a clock ‚Äî it was a declaration that I had gone further than I ever imagined. The cut-off was 8:30, and to be honest, I wasn‚Äôt even sure I could break 8 hours. But there I was, pushing beyond my limits and proving to myself that anything is possible. The feeling when you surpass your own expectations? Unreal. The joy and relief that flooded over me as I crossed that line ‚Äî words can‚Äôt even come close to describing it.\nFigure 10: BIB number 201 ‚Äî a number that will forever be etched in my memory. It‚Äôs amazing how something as simple as a number can carry so much meaning. It represents the start of a new chapter, the challenge ahead, and the person I became along the way. At that moment, I didn‚Äôt just wear a number, I wore my determination, my discipline, and my belief in myself. That was my identity for the race, and it‚Äôs a number I‚Äôll carry with me forever.\nThe unwavering support of the event staff and my fellow athletes played a pivotal role. Their encouragement and camaraderie fueled my determination and reminded me I wasn‚Äôt alone on this journey.\nThe Finish Line Crossing the finish line after 7.51 hours was a moment of indescribable joy, relief, and pride. Just moments before the 8.30-hour cutoff, I officially earned the title of IRONMAN 70.3 finisher. It wasn‚Äôt just about completing a race; it was about embracing the challenges and pushing beyond my perceived limits.\nFigure 11: The IRONMAN 70.3 Finisher certificate ‚Äî not just a piece of paper, but a symbol of everything I put into this journey. Each glance at it takes me back to the early mornings, the late nights, the sacrifices, the pain, and the incredible sense of achievement that followed. This certificate isn‚Äôt just proof of completion ‚Äî it‚Äôs a reminder that with enough commitment and grit, anything is possible. It‚Äôs a constant reflection of my strength, my perseverance, and the rewards that come from pushing beyond the limits.\nGear and Conditions Equipped with my MERIDA SCULTURA bike and NIKE Zoom Fly 5 shoes, I faced the scorching midday sun, which intensified the already demanding race. Despite the fatigue and heat, sheer determination propelled me forward.\nFigure 12: My trusty Merida Scultura 4000, the bike that took me through 90 km of challenge. Bought second-hand for this purpose, this bike became a symbol of resourcefulness and resilience. It carried me through both IRONMAN 70.3 and Laguna Phuket Triathlon, and I wouldn‚Äôt have had it any other way. It‚Äôs more than just a machine ‚Äî it‚Äôs a partner in my journey.\nGratitude This achievement would not have been possible without the tireless efforts of the IM70.3 Bangsaen organizers and staff. Their support made this dream a reality, and I am deeply grateful.\nFigure 13: Here are the numbers that tell the story: the official IRONMAN 70.3 race stats. These aren‚Äôt just data points. They‚Äôre the moments that define who I am as an athlete. Every second of this race, every split time, tells the story of my resilience, my strategy, and the grit it took to keep going. These stats are proof of what can be accomplished when you push past your comfort zone, when you face adversity, and when you never, ever give up.\nLessons Learned Becoming an IRONMAN wasn‚Äôt just a physical feat‚Äîit was a testament to perseverance and the resilience of the human spirit. Through this journey, I‚Äôve learned that true strength is discovered in the face of adversity.\nTo anyone pursuing a seemingly insurmountable dream, believe in yourself and embrace the challenges. It is through perseverance and determination that dreams transform into reality.\nFigure 14: Achieving a VO2 max score of 55 ‚Äî a testament to the relentless effort I‚Äôve put into my training. This number isn‚Äôt just a figure; it represents hours of hard work, dedication, and pushing my body to its absolute limits. It‚Äôs more than just an improvement in physical performance ‚Äî it‚Äôs a breakthrough in my endurance, my commitment, and my journey toward being the best version of myself. This is just the beginning. The hard work pays off, and the future looks limitless.\nThe Celebration After crossing the finish line, I rewarded myself with a well-deserved feast of Japanese food and sushi‚Äîa delicious acknowledgment of the physical and mental endurance the race demanded.\nFigure 15: Both my IRONMAN 70.3 and Laguna Phuket Triathlon finisher medals proudly displayed on my desk. Every time I look at them, I remember the intense training, the sweat, and the relentless pursuit of greatness. They remind me that all the hard work paid off. These medals aren‚Äôt just awards; they‚Äôre my story, etched in metal.\nFinal Thoughts The happiness I find in this journey lies not just in the end goal but in the process itself‚Äîthe struggles, triumphs, and growth along the way. To anyone considering a challenging path, embrace it wholeheartedly. Trust in the transformative power of hard work and dedication. In the pursuit of excellence, we find joy and fulfillment that words cannot capture.\nWith immense gratitude and pride, I declare: I am an IRONMAN.\nPS. The year began with completing the IM70.3 and ended with crossing the finish line at the Laguna Triathlon in Phuket, which included a 1.8K swim, a 55K bike ride, and a 12K run. Truly, it was a year defined by triathlons for me.\nAs I reflect on this experience, I feel immense gratitude‚Äîfor the beauty of Phuket, the camaraderie of my fellow athletes, and the personal growth this race has inspired. The Laguna Phuket Triathlon is more than just an event; it‚Äôs a celebration of human spirit and perseverance.\nTo anyone considering a triathlon, my advice is simple: go for it. Embrace the challenge, trust your training, and don‚Äôt forget to enjoy the ride. Whether you‚Äôre a seasoned athlete or a first-timer like me, the journey will leave you stronger, wiser, and more fulfilled.\nFigure 16: Two medals that mean the world to me: IRONMAN 70.3 and Laguna Phuket Triathlon. These aren‚Äôt just awards, they‚Äôre symbols of my journey through some of the toughest courses in the country. IRONMAN 70.3 was a challenge in itself, but add the Laguna Phuket Triathlon ‚Äî one of the most grueling events in Thailand ‚Äî and it feels like I‚Äôve conquered my own personal Everest. Holding these medals in my hand, knowing that I‚Äôve triumphed in both, is one of the most fulfilling moments of my life. These aren‚Äôt just race medals; they are the manifestation of my unrelenting dedication and perseverance.\nFigure 17: The international athletes who surrounded me at Laguna Phuket Triathlon. Seeing so many passionate and skilled athletes from around the world only fueled my determination to finish. It was an honor to race alongside such incredible talent.\nFigure 18: The Laguna Phuket Triathlon Finisher certificate ‚Äî another incredible achievement in my journey. This race, one of the most difficult in Thailand, pushed me to my limits. But crossing that finish line made all the struggle worth it. I am a triathlete. I am a finisher.\nFigure 19: Official Laguna Phuket Triathlon result ‚Äî this one‚Äôs going to stay with me forever. It‚Äôs a testament to the countless hours of training, the dedication, and the willpower it took to finish this legendary race.\nFigure 20: My BIB number 316 at Laguna Phuket Triathlon. A number that holds memories of every pedal stroke, every step, and every breath during the race. It‚Äôs my personal marker, and I‚Äôll carry it with pride.\nFigure 21: The official results from Laguna Phuket Triathlon, now immortalized online. Seeing my name in the official rankings was a moment of deep satisfaction. It was the confirmation of what I knew all along.\nFigure 22: My official Laguna Phuket Triathlon finisher certificate, proudly displayed on the official website. This digital version still carries the same weight as the physical one ‚Äî both represent a triumph over adversity.\nCompleting the IRONMAN 70.3 and Laguna Phuket Triathlon was more than just a series of races ‚Äî it was a journey that tested my limits, reshaped my understanding of endurance, and ultimately redefined what I thought was possible. From the very first moment I registered for these events, I knew they would be life-changing. The training was intense, pushing my body to its physical limits while demanding an unwavering mental focus. Each early morning, every grueling workout, and the sacrifices made along the way were all driven by a single goal: to prove to myself that I could conquer these seemingly insurmountable challenges. When I crossed the finish line of IRONMAN 70.3 in 7 hours and 51 minutes, well below the cut-off, it was the culmination of months of hard work, sweat, and self-doubt. That moment, when I held my finisher medal and looked at my race number ‚Äî 201 ‚Äî I realized it wasn‚Äôt just a race I had completed; it was a transformation. Every mile of the 1.9 km swim, each moment on the 90 km bike ride, and the final 21.1 km run were stepping stones toward the person I had become ‚Äî someone capable of achieving the extraordinary.\nThe finish line is just the beginning. It‚Äôs not the end of the journey, but a marker of what‚Äôs possible when you push beyond your limits. The true victory is in the person you become along the way. But the journey didn‚Äôt stop there. Completing the Laguna Phuket Triathlon, one of Thailand\u0026rsquo;s most demanding races, felt like the perfect bookend to this chapter of my athletic journey. The Laguna Phuket Triathlon pushed me even further, not just physically but emotionally. It was an experience that tested my resilience, focus, and determination like never before. Holding both the IRONMAN 70.3 and Laguna Phuket finisher medals together wasn‚Äôt just about the victories ‚Äî it was about the strength, grit, and perseverance required to reach the finish lines of both. These medals symbolize far more than athletic achievement; they are a reflection of everything I‚Äôve learned about myself. They are reminders of the moments I wanted to quit but didn‚Äôt, the times my body screamed in protest but my mind pushed through anyway. And above all, they represent the belief that anything is possible if you stay committed to the journey. My journey to becoming an IRONMAN and conquering the Laguna Phuket Triathlon has been one of the most rewarding chapters of my life ‚Äî a story of endurance, triumph, and the incredible power of the human spirit.\nCatch you at the next finish line üèÅ‚ú®,\nKao\n#IRONMAN703 #IRONMAN #IRONMANtri #Triathlon #AnythingIsPossible\nCitation Panboonyuen, Teerapong. (Feb 2024). The Day I Became an IRONMAN: Conquering 113KM of the Limits of Pain and Human Endurance. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-I-became-an-IRONMAN/\nFor a BibTeX citation:\n@article{panboonyuen2024ironman, title = \u0026quot;The Day I Became an IRONMAN: Conquering 113KM of the Limits of Pain and Human Endurance\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Feb\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-I-became-an-IRONMAN/\u0026quot;} Did you find this page helpful? Consider sharing it üôå ","date":1708473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708473600,"objectID":"be23d2f38d1f3913fccb687b5d2694b3","permalink":"https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-i-became-an-ironman/","publishdate":"2024-02-21T00:00:00Z","relpermalink":"/blog/2024-02-21-the-day-i-became-an-ironman/","section":"blog","summary":"A transformative journey of grit and perseverance during the 2024 IRONMAN 70.3 Bangsaen, Thailand, completing 113km of swimming, biking, and running to earn the title of IRONMAN.","tags":["IRONMAN","IRONMAN70.3","TRIATHLON","BIKE","Running","Swimming"],"title":"The Day I Became an IRONMAN: Conquering 113KM of the Limits of Pain and Human Endurance","type":"blog"},{"authors":[],"categories":null,"content":" ","date":1706778540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706778540,"objectID":"996883997cabd32d724ba8a17e5846a5","permalink":"https://kaopanboonyuen.github.io/talk/inspiring-the-future-of-ai-innovations/","publishdate":"2024-02-01T00:00:00Z","relpermalink":"/talk/inspiring-the-future-of-ai-innovations/","section":"event","summary":"I had the opportunity to give a final orientation speech to the undergraduate students of the Department of Electrical and Computer Engineering at KMUTNB. The focus of my speech was on the transformative impact of AI, particularly highlighting the advancements in Large Language Models (LLMs) like ChatGPT. I discussed how these models have revolutionized natural language processing, enabling sophisticated interactions and problem-solving capabilities.","tags":[],"title":"Inspiring the Future of AI Innovations","type":"event"},{"authors":[],"categories":null,"content":" ","date":1701418500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701418500,"objectID":"cd4a86c4c3997f9191260d8c073400f1","permalink":"https://kaopanboonyuen.github.io/talk/geospatial-big-data-analytics/","publishdate":"2023-01-01T08:15:00Z","relpermalink":"/talk/geospatial-big-data-analytics/","section":"event","summary":"I was invited to present on advanced geospatial data analytics, focusing on how spatial and geographic information can be transformed into actionable insights. Leveraging PySpark for distributed computing significantly accelerates data processing, allowing efficient handling of large-scale geospatial datasets. The course also introduced distributed machine learning techniques to build scalable, high-performance predictive models.","tags":[],"title":"Geospatial Big Data Analytics","type":"event"},{"authors":[],"categories":null,"content":"\nAI Research Featured in Techsauce News Honored to have my AI research featured in Techsauce, Thailand‚Äôs leading tech media platform. The article showcases my work in advancing AI technologies, emphasizing innovation and impact in the field.\nThis recognition underscores the significance of AI research in driving technological progress and its application in real-world scenarios. I\u0026rsquo;m grateful for the opportunity to contribute to the evolving landscape of AI and look forward to future endeavors in this exciting field.\nüöó MARS: Revolutionizing Car Damage Segmentation Proud to have our flagship model, MARS (Mask Attention Refinement with Sequential Quadtree Nodes), featured in Techsauce News. This recognition underscores our commitment to advancing AI in the automotive insurance sector.\nüîç What is MARS? MARS is a state-of-the-art deep learning model designed for precise car damage instance segmentation. By leveraging sequential quadtree attention, it surpasses existing methods like Mask R-CNN and PointRend, achieving notable improvements in segmentation accuracy.\nüß† MARSAIL: Driving Innovation MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory) is dedicated to pioneering research at the intersection of computer vision, transformers, and automotive AI. Our mission is to revolutionize the automotive insurance and repair industries through AI-driven automation, delivering breakthroughs in segmentation, localization, and decision intelligence.\nüìà Impact and Recognition The feature in Techsauce News highlights the significance of our work in transforming the automotive insurance landscape. By integrating advanced AI models like MARS, we aim to enhance the accuracy and efficiency of car damage evaluation, paving the way for smarter, more automated insurance processes.\nExplore more about MARS and MARSAIL\u0026rsquo;s innovative solutions at kaopanboonyuen.github.io/MARS/.\n","date":1692095400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692095400,"objectID":"8442900739410f19c80b17d32b276ce6","permalink":"https://kaopanboonyuen.github.io/talk/ai-research-featured-in-techsauce-news/","publishdate":"2023-08-15T10:30:00Z","relpermalink":"/talk/ai-research-featured-in-techsauce-news/","section":"event","summary":"My AI research was proudly featured in Techsauce News, spotlighting breakthrough advancements in deep tech and AI innovation. This recognition not only highlights the significant impact and transformative potential of my work in advancing artificial intelligence technologies but also underscores its role in shaping the future of smart automation. By pushing the boundaries of AI research, my efforts aim to drive cutting-edge solutions that address real-world challenges, foster industry innovation, and contribute to a smarter, more connected world.","tags":[],"title":"AI Research Featured in Techsauce News","type":"event"},{"authors":[],"categories":null,"content":" I am excited to share that my research, \u0026ldquo;MARS: Mask Attention Refinement with Sequential Quadtree Nodes,\u0026rdquo; was accepted for presentation at the ICIAP 2023 Workshop in Italy. This prestigious conference, held biennially by the CVPL under the International Association for Pattern Recognition (IAPR), brought together leading experts from around the world to discuss the latest advancements in car insurance and computer vision technologies.\nMy research addressed the critical challenge of evaluating car damages with greater accuracy. Current deep learning networks struggle with this task, producing coarse segmented masks that are not suitable for real-world applications. To tackle this issue, I developed MARS, which employs self-attention mechanisms and quadtree transformers to refine instance segmentation accuracy. MARS represents a significant advancement in the field by drawing global dependencies between sequential quadtree nodes and recalibrating channel weights to predict highly accurate instance masks.\nThe extensive experiments conducted as part of my research demonstrated that MARS outperforms several state-of-the-art instance segmentation methods, including Mask R-CNN, PointRend, and Mask Transfiner. Specifically, MARS achieved a substantial improvement in maskAP scores, with a +1.3 increase using the R50-FPN backbone and a +2.3 increase with the R101-FPN backbone on the Thai car-damage dataset. These results highlight the potential of MARS to significantly enhance the accuracy of car damage evaluations, offering promising applications for the car insurance industry.\nFig. 1. I have published an article with Techsauce. (Image source: techsauce.co, mars-deep-tech-startup-thaivivat-ai)\nThank you to Techsauce, the Thai tech news agency, for showcasing my research on the application of AI in the auto insurance and garage industry. You can read the full article here: Techsauce.\nPresenting my findings at ICIAP 2023 was a fantastic opportunity to engage with fellow researchers and industry professionals, exploring the opportunities, challenges, and future directions in this rapidly evolving field.\n","date":1691749800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691749800,"objectID":"5d2e5a401d97d49c60fccb8b5a0bcd49","permalink":"https://kaopanboonyuen.github.io/talk/invited-to-italy-for-iciap2023/","publishdate":"2023-08-11T10:30:00Z","relpermalink":"/talk/invited-to-italy-for-iciap2023/","section":"event","summary":"I was invited to Italy to present my research, \"MARS, Mask Attention Refinement with Sequential Quadtree Nodes,\" at the ICIAP 2023 Workshop. This prestigious conference, organized biennially by CVPL under the International Association for Pattern Recognition (IAPR), will unite experts to discuss advancements in car insurance and computer vision. My research addresses the challenge of accurately evaluating car damages using MARS, which enhances instance segmentation through self-attention mechanisms and quadtree transformers.","tags":[],"title":"Invited to Italy for ICIAP2023","type":"event"},{"authors":["Teerapong Panboonyuen","C. Charoenphon","C. Satirapod"],"categories":null,"content":" Introduction Semantic segmentation is vital in remote sensing, particularly for identifying and categorizing different land use and land cover types. In regions like Thailand, where agriculture is central to the economy, precise segmentation of satellite imagery can enhance our ability to track crop health, predict yields, and improve resource management. Our model, MeViT (Medium-Resolution Vision Transformer), is specifically designed to classify agricultural crops like para rubber, corn, and pineapple across Thailand‚Äôs varied landscapes.\nBackground on Vision Transformers Unlike traditional convolutional neural networks (CNNs), which are excellent at capturing local spatial hierarchies, Vision Transformers excel at modeling long-range dependencies through self-attention mechanisms. This unique structure allows MeViT to interpret both local and global features, enhancing its effectiveness in agricultural land segmentation tasks where accuracy and detail are paramount.\nMeViT Architecture MeViT leverages a multi-branch architecture tailored for medium-resolution images, balancing computational efficiency with high-quality feature extraction. This design approach enables the model to capture details across multiple spatial scales, which is crucial for segmenting complex land use patterns in agricultural imagery.\nIn particular, the revised mixed-scale convolutional feedforward network (MixCFN) in MeViT incorporates multiple depth-wise convolution paths, further refining feature extraction by allowing the model to focus on different spatial scales. This enhanced architecture achieves an efficient trade-off between model complexity and performance, making it well-suited for large-scale image analysis tasks.\nExperimental Results and Evaluation We extensively tested MeViT on Thailand‚Äôs Landsat-8 dataset, focusing on para rubber, corn, and pineapple classifications. Compared to other models, including state-of-the-art architectures like HRViT and SegFormer, MeViT demonstrated notable improvements in precision and segmentation accuracy, proving its efficacy in challenging, real-world datasets. This establishes MeViT as a leading tool in medium-resolution satellite imagery analysis, surpassing previous Vision Transformer models and CNN-based methods in delivering high-quality semantic segmentation.\nConclusion MeViT presents a significant advancement in Vision Transformer applications, setting a new standard for semantic segmentation in remote sensing. By combining multi-branch ViT architectures with optimized convolutional modules, MeViT delivers efficient, accurate LULC classification on satellite imagery, supporting agricultural insights and sustainable resource management across Thailand. This work contributes to the broader field of environmental monitoring and opens up new possibilities for enhanced remote sensing techniques globally.\n","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"14c0ff27b66af589b9731863f8bf9193","permalink":"https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/mevit-a-medium-resolution-vision-transformer/","section":"publication","summary":"In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT's effectiveness in accurately segmenting Thai Landsat-8 data.","tags":["Remote Sensing","Landsat-8","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Vision Transformers","Transformer","Multi-branch Architectures","Mixed-scale Convolutional Feedforward Networks"],"title":"MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand","type":"publication"},{"authors":["Teerapong Panboonyuen","N. Nithisopa","P. Pienroj","L. Jirachuphun","C. Watthanasirikrit","N. Pornwiriyakul"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"ecd0bcce1705e21b780021bb64e0e45a","permalink":"https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/","section":"publication","summary":"Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at https://github.com/kaopanboonyuen/MARS.","tags":["Attention","Self-Attention","MARS","Sequential Quadtree Nodes","Mask R-CNN","PointRend","Mask Transfiner"],"title":"MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1670232600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670232600,"objectID":"85d93937200093837b1d1a85f626df23","permalink":"https://kaopanboonyuen.github.io/talk/achieve-data-science-first-meet/","publishdate":"2022-12-05T09:30:00Z","relpermalink":"/talk/achieve-data-science-first-meet/","section":"event","summary":"I was invited to speak at the \"Achieve Data Science First Meet\" for a MOOC student project event, where I highlighted the growing recognition of data science, AI, and machine learning's importance across various industries. I advised that organizations, regardless of their size or sector, must effectively develop and implement data science capabilities to stay competitive in the era of big data, or risk falling behind.","tags":[],"title":"Achieve Data Science First Meet","type":"event"},{"authors":[],"categories":null,"content":" ","date":1669884300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669884300,"objectID":"8f68e458f32432396b1cad7a1a1de3bd","permalink":"https://kaopanboonyuen.github.io/talk/distributed-ml-for-geospatial-data/","publishdate":"2022-01-01T08:45:00Z","relpermalink":"/talk/distributed-ml-for-geospatial-data/","section":"event","summary":"I was invited to teach a course on distributed machine learning to the Geo-Informatics and Space Technology Development Agency (GISTDA). The curriculum covered fundamental concepts of PySpark, basic deep learning techniques, and practical applications of distributed training using TensorFlow.","tags":[],"title":"Distributed ML for Geospatial Data","type":"event"},{"authors":["Teerapong Panboonyuen"],"categories":["Marathon","Running","Fitness"],"content":" 42.195K ‚Äì The Bangkok Marathon 2022: Conquering the Full Marathon I did it. I finally ran a Full Marathon‚Äî42.195 kilometers through the streets of Bangkok. It‚Äôs still sinking in, but the feeling is electric. This wasn‚Äôt just another run; it was a milestone, a test of determination, and an experience I‚Äôll never forget.\nThe Decision to Go the Distance The idea of running a full marathon had been on my mind for a while. It‚Äôs one of those challenges that, once you‚Äôve thought about it, you can‚Äôt shake. I‚Äôd done plenty of 5Ks and 10Ks, even completed a half marathon once, but the full 42.195 KM? That was uncharted territory. So, when I finally decided to register for the Bangkok Marathon, it felt like I was committing to something much bigger than just a race.\nFrom the moment I hit that ‚Äúregister‚Äù button, I knew there was no turning back. The excitement was real. Every time I thought about the race, I‚Äôd get a little rush of adrenaline, knowing that I was about to push my limits like never before.\nTraining: The Journey Begins Let‚Äôs be honest‚Äîmy training for this marathon wasn‚Äôt exactly textbook. I hadn‚Äôt trained for the full distance before. My longest run leading up to race day was a half marathon, and that was just once. Most of my runs were 5K or 10K, and while those were good for keeping in shape, they didn‚Äôt fully prepare me for the grueling distance of a full marathon.\nBut I was determined. Every time I laced up my running shoes, I reminded myself that this wasn‚Äôt just another race. This was the race. My strategy was simple: keep my pace steady, listen to my body, and most importantly, finish strong.\nRace Day: The Big Moment Race day arrived with a mix of excitement and nerves. As I stood at the starting line, surrounded by thousands of other runners, I felt the weight of what I was about to do. But more than anything, I felt ready. This was the moment I‚Äôd been waiting for.\nI started strong, hitting my Sub1 10K goal in 55 minutes. I felt good, my pace was on point, and I was in the zone. But as the kilometers ticked by, the reality of the distance started to set in. From the 10K mark to 21.1K, I slowed down to a steady pace of around 5. I knew I needed to conserve energy for the long haul.\nWhen I hit the 22K mark, things started to change. My legs were getting heavier, and my pace slowed to the point where I was more walking than running. A senior runner even teased me, saying I‚Äôd planned to run the first half and walk the second, which, in hindsight, wasn‚Äôt too far from the truth. But I kept going, focused on one goal: crossing that finish line.\nThe infamous ‚ÄòDevil Km.35‚Äô‚Äîa point in the race where many runners hit the wall‚Äîcame and went. Honestly, I was so caught up in the moment that I didn‚Äôt even realize I‚Äôd passed it. By then, I was walking and chatting with fellow runners, soaking in the atmosphere of this incredible event. The camaraderie, the support, the shared struggle‚Äîit all made the experience that much more meaningful.\nThe Final Stretch: A Run to Remember The last 10KM were something special. As I crossed the Rama VIII Bridge just as the sun was rising, I felt an overwhelming sense of accomplishment. The view was breathtaking, and the realization that I was nearing the finish line gave me a final burst of energy.\nAnd then, there it was‚Äîthe Grand Palace, gleaming in the early morning light, marking the end of this incredible journey. Crossing that finish line was one of the most satisfying moments of my life. I‚Äôd done it. I‚Äôd run my first Full Marathon, all 42.195 kilometers of it.\nFigure 1: Celebrating my achievement as a Full Marathon Finisher in Bangkok 2022.\nParticipating in the Bangkok Marathon and completing the full marathon distance was an extraordinary achievement that filled me with immense joy and accomplishment. The race challenged me physically, mentally, and emotionally, but it also showcased the resilience, determination, and unwavering spirit within us all.\nCrossing the finish line was a defining moment that reminded me of the incredible feats we can achieve when we set our minds to it. The memories of this triumphant journey will forever inspire and motivate me to embrace new challenges and strive for greatness in all aspects of life.\nPS: To the 42.195 KM that changed everything‚ÄîI‚Äôm proud of what I achieved, and I‚Äôm already excited for round two. See you at the next finish line!\nFigure 2: My Garmin statistics tracking the marathon performance.\nFigure 3: Running among talented marathoners at the Bangkok Marathon 2022.\nFigure 4: My Full Marathon Finisher medal from Bangkok Marathon 2022.\nFigure 5: The marathon route I ran, as tracked by my Garmin Fenix Watch.\nFigure 6: My final position on the official Bangkok Marathon 2022 scoreboard.\n42.195K ‚Äì The Bangkok Marathon 2022: My Journey to the Finish Line\nThe moment I crossed the finish line at the Bangkok Marathon 2022, a rush of emotions hit me all at once. It was a mixture of exhaustion, disbelief, and joy, but most of all, pride. I had just run 42.195 kilometers, completing my very first full marathon. To many, it may just be a race, but to me, it was a testament to what sheer willpower can achieve. It was fun, exhilarating, and painful all at once. Every stride brought me closer to the finish, but it also pushed me beyond my limits. My legs screamed for mercy, my heart raced faster than I could catch my breath, but I didn‚Äôt stop. There was no turning back.\nThe training leading up to the marathon had been a rollercoaster. I remember the doubts I had when I first registered. Could I really do it? I had run a few 5Ks and 10Ks, and I even finished a half marathon, but this was something else entirely. A full 42.195K? That felt impossible at first. Yet, the more I trained, the more I realized that it wasn‚Äôt about being the fastest‚Äîit was about the journey, the perseverance, and the mental toughness to keep going even when everything hurts.\nCompleting the Bangkok Marathon 2022 was more than just a physical achievement. It was a personal triumph. It showed me that I am capable of so much more than I give myself credit for. It reminded me that perseverance and determination are the true markers of success. And it gave me a deep sense of gratitude for every person who cheered me on, every runner I met along the way, and every moment that brought me closer to that finish line.\nBut the journey doesn‚Äôt end here. I‚Äôm not stopping at one marathon. I will keep running, keep pushing myself, and keep striving for greatness. I‚Äôve tasted the sweet victory of crossing the finish line, and I‚Äôm already planning my next challenge. This marathon may be over, but my journey has just begun.\nCitation Panboonyuen, Teerapong. (Nov 2022). 42.195K at Bangkok Marathon 2022 ‚Äî My First Full Marathon and Crossing the Finish Line. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/\nFor a BibTeX citation:\n@article{panboonyuen2022bangkokmarathon, title = \u0026quot;42.195K at Bangkok Marathon 2022 ‚Äî My First Full Marathon and Crossing the Finish Line\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2022\u0026quot;, month = \u0026quot;Nov\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/\u0026quot;} Did you find this page helpful? Consider sharing it üôå ","date":1669075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669075200,"objectID":"34ce5038b948032102ce338200fee942","permalink":"https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/","publishdate":"2022-11-22T00:00:00Z","relpermalink":"/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/","section":"blog","summary":"Join me on my journey through the Bangkok Marathon 2022 as I conquered my first-ever Full Marathon, pushing my limits over 42.195 kilometers.","tags":["Marathon","Bangkok Marathon","Running","Fitness","Personal Achievement"],"title":"42.195K at Bangkok Marathon 2022 ‚Äî My First Full Marathon and Crossing the Finish Line","type":"blog"},{"authors":["Teerapong Panboonyuen","C. Charoenphon"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"4adfc7217759e6906ac85e560b8c385f","permalink":"https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/object-detection-of-road-assets-using-transformer-based-yolox/","section":"publication","summary":"Detecting objects of varying sizes, like kilometer stones, remains a significant challenge and directly affects the accuracy of object counts. Transformers have shown remarkable success in natural language processing (NLP) and image processing due to their ability to model long-range dependencies. This paper proposes an enhanced YOLO (You Only Look Once) series with two key contributions, (i) We employ a pre-training objective to obtain original visual tokens from image patches of road assets, using a pre-trained Vision Transformer (ViT) backbone, which is then fine-tuned on downstream tasks with additional task layers. (ii) We incorporate Feature Pyramid Network (FPN) decoder designs into our deep learning network to learn the significance of different input features, avoiding issues like feature mismatch and performance degradation that arise from simple summation or concatenation. Our proposed method, Transformer-Based YOLOX with FPN, effectively learns general representations of objects and significantly outperforms state-of-the-art detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. It achieves a 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP on the test-dev dataset.","tags":["Attention","Self-Attention","MARS","Sequential Quadtree Nodes","Mask R-CNN","PointRend","Mask Transfiner"],"title":"Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama","type":"publication"},{"authors":["K. Thitisiriwech","Teerapong Panboonyuen","P. Kantavat","Y. Iwahori","B. Kijsirikul"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"9d0e53f94cc9f61ef1adaf4acad0f1a3","permalink":"https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/","section":"publication","summary":"In today's world, urban design and sustainable development are crucial for megacities, impacting residents' wellbeing. Quality of Life (QOL) is a key performance indicator (KPI) used to measure the effectiveness of city planning. Traditionally, QOL is assessed through costly and time-consuming surveys, but our AI-based approach offers a more efficient solution. Using Bangkok as a case study, we apply deep convolutional neural networks (DCNNs) for semantic segmentation and object detection to gather relevant image data. Then, we use linear regression to infer QOL scores. Our method, tested with state-of-the-art models and public datasets, proves to be a practical alternative for QOL assessment, with implementation codes and datasets available at [https://kaopanboonyuen.github.io/bkkurbanscapes](https://kaopanboonyuen.github.io/bkkurbanscapes).","tags":["DeepLab","Sustainability","Quality of Life (QOL)","Bangkok Urbanscapes Dataset","Xception","Cityscapes"],"title":"Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks","type":"publication"},{"authors":["T. Vajeethaveesin","Teerapong Panboonyuen"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"daf08a865aed4299abe84deea27c99c0","permalink":"https://kaopanboonyuen.github.io/publication/rainfall-prediction-a-machine-learning-approach/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/rainfall-prediction-a-machine-learning-approach/","section":"publication","summary":"Flooding poses a significant challenge in Thailand due to its complex geography, traditionally addressed through GIS methods like the Flood Risk Assessment Model (FRAM) combined with the Analytical Hierarchy Process (AHP). This study assesses the efficacy of Artificial Neural Networks (ANN) in flood susceptibility mapping, using data from Ayutthaya Province and incorporating 5-fold cross-validation and Stochastic Gradient Descent (SGD) for training. ANN achieved superior performance with precision of 79.90%, recall of 79.04%, F1-score of 79.08%, and accuracy of 79.31%, outperforming the traditional FRAM approach. Notably, ANN identified that only three factors‚Äîflow accumulation, elevation, and soil types‚Äîwere crucial for predicting flood-prone areas. This highlights the potential for ANN to simplify and enhance flood risk assessments. Moreover, the integration of advanced machine learning techniques underscores the evolving capability of AI in addressing complex environmental challenges.","tags":["Remote Sensing","Flood Susceptibility Assessment","Machine Learning","Artificial Neural Networks","GIS"],"title":"A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province","type":"publication"},{"authors":[],"categories":null,"content":"üéì Ph.D. Dissertation Defense ‚Äì Triumphant On July 9, 2020, I successfully defended my Ph.D. dissertation titled \u0026ldquo;Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network\u0026rdquo; before a distinguished committee of Thai professors, each of whom earned their doctoral degrees from world-renowned international universities.\nüîó Check out my full presentation slides here: üëâ Ph.D. Defense Slides\nThis moment was a culmination of years of hard work and dedication, made even more meaningful by the opportunity to present my research to such respected scholars. Their insightful feedback and support reinforced the importance of collaboration and intellectual exchange in pushing the boundaries of knowledge and technology.\nThe list below shows the names of those who submitted their final Ph.D. dissertations, and as part of the class of 2017 (student ID: 6071467821), I am proud to highlight that I completed my Ph.D. in Artificial Intelligence in just two and a half years, graduating in 2019.\nThis achievement is a testament to my dedication, relentless work ethic, and passion for the field. It was a journey that demanded focus, time management, and the ability to push the boundaries of what was possible within an accelerated timeline.\nI am immensely proud of the way I tackled this challenge, which not only sharpened my technical skills but also refined my communication and problem-solving abilities.\nReflecting on this journey, I am deeply grateful for the unwavering commitment I gave to my work. Completing my Ph.D. in such a short time was no easy feat, but it taught me invaluable lessons in resilience, focus, and perseverance. This experience has prepared me to take on complex challenges and contribute meaningfully to the ever-evolving field of AI.\nüß† The research focused on applying deep learning techniques‚Äîspecifically convolutional encoder-decoder architectures‚Äîfor high-accuracy semantic segmentation in remotely sensed imagery, pushing the boundaries of geospatial AI.\nüîó Explore the full dissertation, source code, and project resources here:\nüëâ https://kaopanboonyuen.github.io/FusionNetGeoLabel/\nüîó Interested in the full presentation slides? You can view them here:\nüëâ Ph.D. Defense Slides\nAfter successfully defending my Ph.D. in Artificial Intelligence, I gained a deep and comprehensive understanding of advanced AI methodologies, particularly in the areas of computer vision, deep learning, and semantic segmentation.\nMy research on \u0026ldquo;Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Networks\u0026rdquo; has not only enhanced my technical expertise but also refined my ability to tackle complex, real-world problems with innovative solutions.\nThe experience of defending my dissertation to a panel of esteemed experts further strengthened my communication and collaboration skills, allowing me to explain intricate concepts clearly while receiving valuable feedback.\nThis journey has instilled in me a strong analytical mindset and a relentless drive for excellence, preparing me to contribute meaningfully to cutting-edge AI projects and teams.\nPhD Journey: A Milestone Achieved On May 19, 2022, I proudly completed my PhD at Chulalongkorn University, closing a remarkable chapter in my academic journey. This milestone was not just a moment of personal triumph, but also a time of reflection and deep gratitude.\nGraduating with a doctoral degree was a dream realized, filled with emotions that I will carry with me forever.\nThroughout this journey, I was fortunate to have the unwavering support of incredible mentors, advisors, colleagues, and friends. Their guidance and encouragement were instrumental in my success, and having them by my side on this special day was a poignant reminder of the profound impact they\u0026rsquo;ve had on both my academic and personal development.\nCompleting a PhD is more than just an academic achievement; it\u0026rsquo;s a journey of personal growth. It demands perseverance, resilience, and the ability to navigate and overcome numerous challenges.\nMy passion for machine learning and my commitment to research were the driving forces that kept me moving forward, enabling me to make meaningful contributions to the field.\nPhD Thesis Highlights Introduction Semantic segmentation in remote sensing images plays a crucial role in applications such as land use classification, urban planning, and environmental monitoring. Remote sensing images often come from diverse sources, ranging from medium-resolution satellite imagery (such as Landsat-8) to high-resolution aerial images (such as those from the ISPRS Vaihingen Challenge). However, the task of semantic segmentation remains challenging due to the variety of image scales, the scarcity of labeled data, and the need for models capable of extracting both high-level and low-level features effectively.\nIn my PhD research, I propose a series of advancements in convolutional neural network (CNN)-based approaches to enhance the accuracy of semantic segmentation on remotely sensed data. Building upon the state-of-the-art methods, I introduce several innovations, including an enhanced Global Convolutional Network (GCN) with channel attention, domain-specific transfer learning, and the integration of feature fusion and depthwise atrous convolutions.\nThese innovations aim to address the unique challenges of remote sensing datasets and push the boundaries of semantic segmentation performance.\nThe Challenge: Limitations of Traditional Approaches Semantic segmentation models, particularly Deep Convolutional Encoder-Decoder (DCED) networks, have shown promise in image segmentation tasks. However, when applied to remote sensing imagery, these models face key limitations:\nResolution Differences: Remote sensing images span a wide range of resolutions, from very high-resolution (VHR) aerial images to medium-resolution satellite images. Traditional models, designed for single-resolution tasks, struggle to generalize across these diverse scales.\nData Scarcity: Annotated datasets for training deep models are scarce, particularly for high-resolution satellite or aerial imagery. This leads to overfitting and poor generalization.\nInability to Capture Global Context: Traditional CNN models focus on local features, which are insufficient for understanding global context in satellite images, such as large rivers, forests, or urban areas.\nMethodology Global Convolutional Network (GCN) with Enhanced Backbone The Global Convolutional Network (GCN) is a modern CNN architecture that addresses the limitations of traditional models. GCN overcomes the challenge of capturing both local and global features by using a multi-level architecture. Each level in the GCN extracts features at different resolutions, ensuring that both fine-grained and broad contextual information are captured.\nThis can be written as:\n$$ \\mathbf{F}_{\\text{GCN}} = \\sum \\mathbf{W}_l \\cdot \\mathbf{X}_l $$\nBuilding on this architecture, I proposed an enhancement to the GCN backbone by modifying its structure and increasing the number of layers, making it more suitable for medium-resolution remote sensing imagery. Specifically, I employed the ResNet architecture with varying depths‚ÄîResNet50, ResNet101, and ResNet152‚Äîto adapt the model to different datasets and resolutions.\nKey Contributions: Multi-resolution Feature Extraction: By stacking multiple convolutional layers at different stages, the GCN captures features across multiple resolutions. Boundary Refinement: A boundary refinement module is introduced to improve the precision of segmentation boundaries, crucial for tasks like building or road detection. Channel Attention Mechanism One of the most significant advancements in my work is the introduction of the Channel Attention Block. Attention mechanisms, inspired by the human visual system, allow the model to focus on the most important features in the image. In the case of remote sensing images, this means highlighting key features such as roads, rivers, or vegetation, while suppressing irrelevant background information.\nThe attention mechanism is mathematically modeled as:\n$$ \\mathbf{z}_c = \\sigma\\left( W_c \\cdot \\text{AvgPool}(\\mathbf{x}_c) + b_c \\right) $$\nThe channel attention block modifies the network\u0026rsquo;s weights to prioritize the most discriminative channels during feature extraction, improving the network‚Äôs ability to differentiate between different land cover types. This is crucial in remote sensing, where the subtle difference between similar features (e.g., different vegetation types) can significantly impact segmentation accuracy.\nKey Contributions: Adaptive Feature Selection: The network dynamically adjusts the importance of features, focusing on those most relevant to the task at hand. Improved Discriminative Power: By emphasizing discriminative features, the model is able to achieve higher classification accuracy, particularly for challenging classes in remote sensing datasets. Domain-Specific Transfer Learning One of the challenges in training deep learning models for remote sensing is the scarcity of annotated data, particularly for high-resolution images. To address this, I introduced Domain-Specific Transfer Learning. This technique involves leveraging pre-trained models from different but related datasets to transfer knowledge across domains.\nBy utilizing pre-trained models from one dataset (e.g., ISPRS Vaihingen) and applying them to another (e.g., Landsat-8), I was able to mitigate the data scarcity issue and improve the performance of my models. This approach ensures that knowledge gained from one domain can benefit another, allowing the model to generalize better with limited annotated data.\nKey Contributions: Cross-Domain Knowledge Transfer: Knowledge learned from high-resolution datasets is transferred to medium-resolution tasks, significantly improving segmentation accuracy with minimal labeled data. Pre-Trained Weights Utilization: The use of pre-trained weights from different datasets enables the model to learn features that are generalizable across various remote sensing tasks. Feature Fusion and Depthwise Atrous Convolution To further refine feature extraction and improve segmentation performance, I introduced Feature Fusion and Depthwise Atrous Convolution (DA). These techniques work synergistically to enhance the model\u0026rsquo;s ability to capture multi-scale information while maintaining high resolution.\nFeature Fusion: This technique fuses low-level features, such as edges and textures, from the backbone network with high-level features from the segmentation model. This fusion ensures that fine-grained details are preserved while providing the model with a richer set of features for segmentation. $$ \\mathbf{F}_{\\text{fused}} = \\mathbf{F}_L + \\alpha \\cdot \\mathbf{F}_H $$\nDepthwise Atrous Convolution: The DA module applies dilated convolutions at multiple scales, enabling the network to capture contextual information over larger areas without losing spatial resolution. This is particularly important for remote sensing tasks where object boundaries (e.g., between forest and water) need to be sharply delineated. The Depthwise Atrous Convolution is mathematically expressed as:\n$$ y_d = \\sum_{k} w_k \\cdot x_{i + r \\cdot k} $$\nKey Contributions: Improved Feature Representation: By integrating low-level features with deep model representations, the model gains a more comprehensive understanding of the image. Enhanced Contextual Understanding: The DA module allows the network to consider broader context in each layer, improving segmentation accuracy, especially for large objects and distant features. Experiments and Results Datasets I conducted experiments on three benchmark datasets:\nISPRS Vaihingen Challenge Dataset (Very High Resolution) Landsat-8 Satellite Imagery (Medium Resolution) Private Datasets from GISTDA (Geo-Informatics and Space Technology Development Agency) These datasets represent a mix of very high-resolution and medium-resolution imagery, which provided a comprehensive testbed for evaluating the effectiveness of my proposed methods.\nPerformance Metrics The model was evaluated using standard segmentation metrics:\n1. F1 Score: The F1 Score measures the balance between Precision and Recall, and is calculated as:\n$$ F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\nWhere:\nPrecision is the fraction of relevant instances among the retrieved instances: $$ \\text{Precision} = \\frac{TP}{TP + FP} $$\nRecall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances: $$ \\text{Recall} = \\frac{TP}{TP + FN} $$\nExample: Pineapple Class For the pineapple class:\n( TP = 50 ) ( FP = 10 ) ( FN = 15 ) We calculate Precision:\n$$ \\text{Precision} = \\frac{50}{50 + 10} = 0.8333 $$\nAnd Recall:\n$$ \\text{Recall} = \\frac{50}{50 + 15} = 0.7692 $$\nNow calculate the F1 Score:\n$$ F1 = 2 \\cdot \\frac{0.8333 \\cdot 0.7692}{0.8333 + 0.7692} = 0.799 $$\nThus, the F1 Score for the pineapple class is 0.799.\n2. Mean IoU (Intersection over Union): The IoU quantifies the overlap between the predicted and true segmentation maps:\n$$ IoU = \\frac{TP}{TP + FP + FN} $$\nExample: Corn Class For the corn class:\n( TP = 80 ) ( FP = 5 ) ( FN = 20 ) We calculate the IoU:\n$$ IoU = \\frac{80}{80 + 5 + 20} = 0.7619 $$\nThus, the IoU for the corn class is 0.7619.\n3. Mean IoU for Multiple Classes: For evaluating the Mean IoU over multiple classes (e.g., pineapple, corn, pararubber), we compute the average IoU:\n$$ \\text{Mean IoU} = \\frac{IoU_{\\text{pineapple}} + IoU_{\\text{corn}} + IoU_{\\text{pararubber}}}{3} $$\nUsing the IoUs for each class:\nIoU for pineapple = 0.799 IoU for corn = 0.7619 IoU for pararubber = 0.85 We get the Mean IoU:\n$$ \\text{Mean IoU} = \\frac{0.799 + 0.7619 + 0.85}{3} = 0.8036 $$\nThus, the Mean IoU across all classes is 0.8036.\nResults The proposed model significantly outperformed baseline models, including traditional Deep Convolutional Encoder-Decoder (DCED) networks, across all datasets:\nISPRS Vaihingen Dataset: F1 Score of 0.9362 Landsat-8 Dataset: F1 Score of 0.9114 The proposed model consistently exceeded the 90% F1 score threshold across all classes, demonstrating its robustness and adaptability to different image resolutions and domains.\nConclusion This research introduces several key advancements in deep learning for remote sensing semantic segmentation. By incorporating multi-resolution feature extraction, channel attention, domain-specific transfer learning, feature fusion, and depthwise atrous convolutions, my approach addresses the unique challenges posed by remote sensing data.\nThe experimental results validate the effectiveness of these techniques, providing a solid foundation for further improvements in remote sensing applications.\nWith the successful application of these methods, I am confident that these innovations will contribute significantly to the field of remote sensing and provide new avenues for improving the accuracy and generalization of deep learning models in this domain.\nExplore more about my PhD story here.\nKao Panboonyuen\n","date":1596204000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596204000,"objectID":"d7024594cc6cb801b6ae9f39b7862ecf","permalink":"https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/","publishdate":"2020-07-31T14:00:00Z","relpermalink":"/talk/ph.d.-thesis-defense/","section":"event","summary":"This dissertation introduces a new architecture for remote sensing, featuring Global Convolutional Network (GCN), channel attention, domain-specific transfer learning, Feature Fusion (FF), and Depthwise Atrous Convolution (DA). Tests on Landsat-8 and ISPRS Vaihingen datasets show that this model significantly outperforms the baseline.","tags":[],"title":"Ph.D. Thesis Defense","type":"event"},{"authors":["Teerapong Panboonyuen","P. Rakwatin","K. Intarat"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"3cc38a178687941b0f68b09b6ba4eff2","permalink":"https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/enhanced-feature-pyramid-vision-transformert/","section":"publication","summary":"Semantic segmentation on Landsat-8 data is crucial in the integration of diverse data, allowing researchers to achieve more productivity and lower expenses. This research aimed to improve the versatile backbone for dense prediction without convolutions‚Äînamely, using the pyramid vision transformer (PRM-VS-TM) to incorporate attention mechanisms across various feature maps. Furthermore, the PRM-VS-TM constructs an end-to-end object detection system without convolutions and uses handcrafted components, such as dense anchors and non-maximum suspension (NMS). The present study was conducted on a private dataset, i.e., the Thailand Landsat-8 challenge. There are three baselines, DeepLab, Swin Transformer (Swin TF), and PRM-VS-TM. Results indicate that the proposed model significantly outperforms all current baselines on the Thailand Landsat-8 corpus, providing F1-scores greater than 80% in almost all categories. Finally, we demonstrate that our model, without utilizing pre-trained settings or any further post-processing, can outperform current state-of-the-art (SOTA) methods for both agriculture and forest classes.","tags":["Mon-Maximum Suspension","Transfer Learning","Vision Transformer","Remote Sensing","Landsat-8","Transformer"],"title":"Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus","type":"publication"},{"authors":["K. Thitisiriwech","Teerapong Panboonyuen","P. Kantavat","Y. Iwahori","B. Kijsirikul"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"dbdbb2d13060b56b1c4b922f50cd5ad4","permalink":"https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/the-bangkok-urbanscapes-dataset/","section":"publication","summary":"This paper addresses semantic segmentation for autonomous driving systems, focusing on self-driving cars in Thailand. We introduce DeepLab-V3-A1 with Xception, an enhanced version of DeepLab-V3+, and present the Bangkok Urbanscapes dataset. Our method improves segmentation accuracy by refining the decoder and modifying the Xception backbone. Experiments on four datasets, including CamVid, Cityscapes, IDD, and our proposed dataset, show our approach performs comparably to baseline methods. Our dataset includes 701 annotated images of various Bangkok driving environments, covering eleven semantic classes. The architecture and dataset aim to aid developers in improving autonomous driving systems for diverse urban conditions. Implementation codes and dataset are available at [https://kaopanboonyuen.github.io/bkkurbanscapes](https://kaopanboonyuen.github.io/bkkurbanscapes).","tags":["DeepLab","Bangkok Urbanscapes Dataset","Xception","Cityscapes"],"title":"The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"4428c44f98c396bbd30d518b58b80be8","permalink":"https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/transformer-based-decoder-designs-for-semantic-segmentation/","section":"publication","summary":"Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% ùêπ1 score), Thailand North Landsat-8 corpus (63.12% ùêπ1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.","tags":["Transformer","Semantic Segmentation","Decoder Design","Swin Transformer","Vision Transformer","Self-Attention","Global Convolutional Network"],"title":"Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"08bf283a12b51cb06a21958c8492cd7d","permalink":"https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/","section":"publication","summary":"My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.","tags":["Convolutional Neural Networks","Landsat-8","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Global Convolutional Network","Encoder-Decoder Networks","ISPRS Vaihingen","Transfer Learning","Domain Adaptation","Channel Attention","Depthwise Atrous Convolution","Feature Fusion","Remote Sensing"],"title":"Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1575189900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575189900,"objectID":"68a68e5ee986705f696bb5ed0cfc7630","permalink":"https://kaopanboonyuen.github.io/talk/data-science-pathway-special-clinic-session/","publishdate":"2019-01-01T08:45:00Z","relpermalink":"/talk/data-science-pathway-special-clinic-session/","section":"event","summary":"I was honored to be invited as the Head TA for the Data Science Pathway course organized by CHULA MOOC Achieve. In this role, I guided students through the practical applications of data science using RapidMiner and machine learning, along with Python programming. The curriculum encompassed both supervised and unsupervised learning labs, featuring tasks such as Customer Retention Prediction and Boston Housing Price Prediction, among others.","tags":[],"title":"Data Science Pathway Special Clinic Session","type":"event"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"6e6a1251e5de03c8b0aadd7a8242a7b8","permalink":"https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/","section":"publication","summary":"This paper addresses improving semantic segmentation in remote sensing for aerial and satellite images, which is crucial for agriculture, map updates, route optimization, and navigation. We propose enhancements to the state-of-the-art Enhanced Global Convolutional Network (GCN152-TL-A) by introducing a High-Resolution Representation (HR) backbone for better feature extraction, Feature Fusion (FF) to capture low-level details, and Depthwise Atrous Convolution (DA) for refined multi-resolution features. Experiments on Landsat-8 and ISPRS Vaihingen datasets demonstrate our model's superior performance, achieving over 90% accuracy in F1 scores and outperforming baseline models.","tags":["Feature Fusion","Transfer Learning","Remote Sensing","ISPRS Vaihingen Dataset"],"title":"Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"05de61049c7b26380a42ed51f66573c2","permalink":"https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/","section":"publication","summary":"In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, e.g., river, building, forest, etc., on raster images. A deep convolutional encoder‚Äìdecoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second, ‚Äúchannel attention‚Äù is presented in our network in order to select the most discriminative filters (features). Third, ‚Äúdomain-specific transfer learning‚Äù is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets (i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of ùêπ1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively.","tags":["Global Convolutional Network","Transfer Learning","Channel Attention","Remote Sensing","Discriminative Filters"],"title":"Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning","type":"publication"},{"authors":["P. Kantavat","Y. Hayashi","G. City","B. Kijsirikul","Teerapong Panboonyuen","Y. Iwahori"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"52061b758c1cab61004baa1a27c5c001","permalink":"https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/","section":"publication","summary":"For an urban development, the Quality of Life (QOL) of people in the city is a vital issue that should be considered. There are many researches in QOL topics that use questionnaire survey approach. These studies yield very useful information for city development planning. As the Artificial Intelligence technologies are developed very fast recently, they are applied to solve many transportation problems. In this paper, we propose a method that automatically extract mobility indicators using two image recognition techniques, Semantic Segmentation and Object Recognition. Because the mobility is an important factor in QOL evaluation, our work can be used to enhance a performance and reduce a data gathering cost of the QOL evaluation.","tags":["YOLOv5","Quality of Life (QOL)","Semantic Segmentation","Object Detection","Image Recognition"],"title":"Transportation Mobility Factor Extraction Using Image Recognition Techniques","type":"publication"},{"authors":[],"categories":null,"content":" Alright, let‚Äôs break the ice! Beyond the research papers and tech jargon, here‚Äôs the real scoop about me:\nI‚Äôm a tech enthusiast with a serious passion for using technology to make the world a better place. When I‚Äôm not buried in code or geeking out over the latest tech, you might find me lacing up my running shoes or gearing up for a triathlon. Yep, marathons and triathlons are my thing‚Äîthey keep me fit and remind me that anything‚Äôs possible with a little grit and a lot of sweat.\nI‚Äôm a lifelong learner at heart. Whether it‚Äôs catching the latest tech trends, diving into new research, or just chatting with fellow nerds, I‚Äôm always on the lookout for the next big thing in technology.\nBut it‚Äôs not all work and no play. I love giving back to the community and am always up for volunteering at events that spark my interest. Connecting with people and making a difference is what keeps me motivated.\nIn my daily life, I love sharing insights and experiences, so feel free to follow my journey at My WordPress Blog.\nSo, if you want to swap stories about the newest gadgets, discuss the latest research, or just chat about anything under the sun, hit me up at panboonyuen.kao@gmail.com!\nKao Panboonyuen\n","date":1543638600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543638600,"objectID":"6bb5844faa6b56c938a511706f8777ce","permalink":"https://kaopanboonyuen.github.io/talk/get-to-know-me-better/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/talk/get-to-know-me-better/","section":"event","summary":"Hey everyone! I‚Äôm Teerapong Panboonyuen, but you can call me Kao. I‚Äôm passionate about creating impactful AI. While working on my Ph.D. in Computer Engineering at Chula, I‚Äôve taught, tackled exciting data science projects, and assisted in machine learning classes. My goal? To elevate AI and leverage it for a better world. My journey has been a thrilling exploration of advanced technologies, from diffusion models and generative adversarial networks (GANs) to self-supervised learning and probabilistic modeling.","tags":[],"title":"GET TO KNOW ME BETTER","type":"event"},{"authors":["I. Wichakam","Teerapong Panboonyuen","C. Udomcharoenchaikit","P. Vateekul"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"7ae277d2610b5783ef94eba07969563a","permalink":"https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/","section":"publication","summary":"Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.","tags":["Colorectal Cancer","Fully Convolutional Network","CVC-EndoSceneStill","Colonoscopy Video"],"title":"Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network","type":"publication"},{"authors":["S. Chantharaj","K. Pornratthanapong","P. Chitsinpchayakun","Teerapong Panboonyuen"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"426382c4e17472b69c94efc70af45de8","permalink":"https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/semantic-segmentation-on-medium-resolution-satellite-images/","section":"publication","summary":"Semantic Segmentation is a fundamental task in computer vision and remote sensing imagery. Many applications, such as urban planning, change detection, and environmental monitoring, require the accurate segmentation; hence, most segmentation tasks are performed by humans. Currently, with the growth of Deep Convolutional Neural Network (DCNN), there are many works aiming to find the best network architecture fitting for this task. However, all of the studies are based on very-high resolution satellite images, and surprisingly; none of them are implemented on medium resolution satellite images. Moreover, no research has applied geoinformatics knowledge. Therefore, we purpose to compare the semantic segmentation models, which are FCN, SegNet, and GSN using medium resolution images from Landsat-8 satellite. In addition, we propose a modified SegNet model that can be used with remote sensing derived indices. The results show that the model that achieves the highest accuracy RGB bands of medium resolution aerial imagery is SegNet. The overall accuracy of the model increases when includes Near Infrared (NIR) and Short-Wave Infrared (SWIR) band. The results showed that our proposed method (our modified SegNet model, named RGB-IR-IDX-MSN method) outperforms all of the baselines in terms of mean F1 scores.","tags":["Convolutional Neural Networks","Semantic Segmentation","Near Infrared (NIR)","Short-Wave Infrared (SWIR)","Remote Sensing"],"title":"Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"f6d05230dcdb2e8c944ff54140727202","permalink":"https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/road-segmentation-on-remote-sensing/","section":"publication","summary":"Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.","tags":["Remote Sensing","Road Segmentation","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Exponential Linear Unit","Conditional Random Fields","Landscape Metrics"],"title":"Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"fca7fa3fb9e06fccd9699f13b4e415fe","permalink":"https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/road-segmentation-on-aerial-imagery/","section":"publication","summary":"In this paper, we introduce an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Enhancements include the use of ELU (exponential linear unit) instead of ReLU, dataset augmentation with incrementally-rotated images to increase training data by eight times, and the use of landscape metrics to remove false road objects. Tested on the Massachusetts Roads dataset, our method outperformed the SegNet benchmark and other baselines in precision, recall, and F1 scores.","tags":["Remote Sensing","Road Segmentation","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Exponential Linear Unit"],"title":"An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b626b42b6e307c281ff5e448e98cd9b0","permalink":"https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/road-map-extraction-from-satellite-imagery/","section":"publication","summary":"Road map extraction is vital for GIS and underpins many location-based applications like GPS navigation, delivery route planning, tourist attraction locating, and location-based marketing. This research uses satellite imagery, though other remotely sensed images like aerial photographs, UAVs, or drones are also applicable. Despite various proposed methods focusing primarily on accuracy, completeness of results is equally important. We enhance accuracy by incorporating connected component analysis and improve completeness using landscape metrics, which describe spatial characteristics through shape and isolation indices. Evaluated on precision, recall, quality, and F1 scores, our method achieves over 90% performance in all criteria.","tags":["Road Segmentation","Connected Component Analysis","Image Processing"],"title":"Road map extraction from satellite imagery using connected component analysis and landscape metrics","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"50982ca008f9ff19109ec4bea6c187cd","permalink":"https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/publication/image-vectorization-of-road-satellite-data-sets/","section":"publication","summary":"Data extraction of geo-spatial objects from satellite images is a crucial step in facilitating analysis of geo-spatial or spatio-temporal data, typically involving line (road) and polygon (area) layers. This paper introduces a method for transforming satellite data (raster images) containing roads from pixel form into spatial objects comprising lines and polygons. Our algorithm consists of three primary steps. First, roads are isolated from other objects using k-means clustering. Second, line extraction is performed on the road areas by applying morphological operations to skeletonize the image, followed by enhancement using the Ramer-Douglas-Peucker algorithm. Finally, land-cover classification is applied to non-road objects to extract polygons. Experimental results demonstrate that both lines (road networks) and polygons (areas) can be accurately extracted from satellite imagery simultaneously.","tags":["Remote Sensing","Road Segmentation","Spatio-Temporal Data","High-Resolution Imagery","Aerial Imagery","K-Means Clustering","Ramer-Douglas-Peucker"],"title":"Image Vectorization of Road Satellite Data Sets","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://kaopanboonyuen.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]