[{"authors":null,"categories":null,"content":"My research focuses on Pattern Recognition‚Äîdeveloping cutting-edge algorithms with Optimization Theory and Statistical Learning to push AI\u0026rsquo;s limits. I work with advanced models like GANs and Diffusion Models, use Self-Supervised Learning, and explore Adversarial Attacks and Large Language Models (LLMs) to revolutionize AI capabilities.\nI am currently a Senior AI Research Scientist at MARS (Motor AI Recognition Solution) and a Postdoctoral Fellow at Chulalongkorn University. I earned my Ph.D. in Computer Engineering from Chulalongkorn University, where I specialized in AI.\nMy passion lies in advancing AI Technologies to unlock human potential. I am particularly interested in Remote Sensing, where AI unveils transformative insights and redefines how we perceive and interact with our environment.\nYou can find summaries of my academic, industry, and teaching experience in my CV, and explore more about my personal life on my blog. Additionally, check out some of my music on SoundCloud.\nCall me Teerapong Panboonyuen, or just Kao (‡πÄ‡∏Å‡πâ‡∏≤) in Thai: ‡∏ò‡∏µ‡∏£‡∏û‡∏á‡∏®‡πå ‡∏õ‡∏≤‡∏ô‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô.\nDownload my CV.\n","date":1722470400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1722470400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/teerapong-panboonyuen/","section":"authors","summary":"My research focuses on Pattern Recognition‚Äîdeveloping cutting-edge algorithms with Optimization Theory and Statistical Learning to push AI\u0026rsquo;s limits. I work with advanced models like GANs and Diffusion Models, use Self-Supervised Learning, and explore Adversarial Attacks and Large Language Models (LLMs) to revolutionize AI capabilities.","tags":null,"title":"Teerapong Panboonyuen","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents What you will learn Program overview Courses in this program Meet your instructor FAQs What you will learn Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program Basic Python Build a foundation in Python. Visualization Learn how to visualize data with Plotly. Statistics Introduction to statistics for data science. Meet your instructor Teerapong Panboonyuen FAQs Are there prerequisites? There are no prerequisites for the first course.\nHow often do the courses run? Continuously, at your own pace.\nBegin the course ","date":1706054400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1706054400,"objectID":"9175b58bdbbc0b3316bae12a6c70af53","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/","publishdate":"2024-01-24T00:00:00Z","relpermalink":"/courses/learn-data-science/","section":"courses","summary":"An sample of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"Data Science for Beginners","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n1-2 hours per week, for 8 weeks\nLearn Quiz What is the difference between lists and tuples? Lists\nLists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world'] Tuples\nTuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world') Is Python case-sensitive? Yes\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"e9ca400ea73091d69f8c17a54f939765","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/python/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Basic Python","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n1-2 hours per week, for 8 weeks\nLearn Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nWrite Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show() ","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"c8051497f5da2d7333291e1d5361c7b4","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/visualization/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\nThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$. Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"90a9a7e769286f383e6e8f7355492613","permalink":"https://kaopanboonyuen.github.io/courses/learn-data-science/stats/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/courses/learn-data-science/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" ","date":1733058000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733058000,"objectID":"996883997cabd32d724ba8a17e5846a5","permalink":"https://kaopanboonyuen.github.io/talk/inspiring-the-future-of-ai-innovations-and-mastering-llm/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/talk/inspiring-the-future-of-ai-innovations-and-mastering-llm/","section":"event","summary":"I had the opportunity to give a final orientation speech to the undergraduate students of the Department of Electrical and Computer Engineering at KMUTNB. The focus of my speech was on the transformative impact of AI, particularly highlighting the advancements in Large Language Models (LLMs) like ChatGPT. I discussed how these models have revolutionized natural language processing, enabling sophisticated interactions and problem-solving capabilities. Emphasizing the importance of mastering AI tools, I encouraged students to develop strong prompting skills to effectively control and harness the potential of AI technologies. The future of AI holds immense possibilities, and by staying adept at these emerging trends, students can significantly contribute to the field and drive innovation.","tags":[],"title":"Inspiring the Future of AI Innovations and Mastering LLM","type":"event"},{"authors":["Teerapong Panboonyuen"],"categories":["generative-ai","computer-vision","deep-learning","GANs","diffusion-models"],"content":" You can view the presentation slides for the talk üòÉ here. Table of Contents Introduction Key Trends in Generative AI Advances in Model Architectures Growth in Computing Power and Data Availability Emerging Techniques and Approaches Applications of Generative AI Content Creation Healthcare Gaming and Entertainment Finance Autonomous Systems Challenges and Ethical Considerations Bias and Fairness Security and Privacy Environmental Impact Future Directions and Opportunities Diffusion Models Conclusion Todo lists Citation References Introduction Generative AI refers to a category of artificial intelligence models designed to generate new content, such as text, images, music, or videos. These models have gained significant attention due to their ability to create high-quality and realistic outputs. The field has evolved rapidly, with breakthroughs in model architectures, training techniques, and applications across various domains. In this blog, we delve into the current trends, practical applications, challenges, and future prospects of generative AI. Fig. 1. Sample of generative AI task (Image source: telecats.com, blog-en/ai-for-rookies)\nOn May 26, 1995, Bill Gates wrote the influential ‚ÄúInternet Tidal Wave‚Äù memo at Microsoft, which marked a major shift for the company towards the emerging World Wide Web. This moment was reminiscent of a recent analogy from HubSpot CTO Dharmesh Shah, who compared Netscape\u0026rsquo;s impact on the Internet to ChatGPT\u0026rsquo;s influence on AI. Just as Netscape made the Internet accessible, ChatGPT is reshaping our understanding of AI, though its full effects on work and creativity remain uncertain.\nMicrosoft, now a major supporter of OpenAI (the creator of ChatGPT), is again at the forefront of this change, potentially challenging Google Search with ChatGPT integration into Bing. Former U.S. Treasury Secretary Larry Summers likened AI to a \u0026ldquo;caddie\u0026rdquo; that enhances our creativity and accuracy, though he cautioned against over-reliance on AI, which could lead to uniform and uninspired results. Summers also highlighted AI\u0026rsquo;s potential as a transformative technology, comparable to the printing press or electricity.\nKey Trends in Generative AI Advances in Model Architectures One of the most notable trends in generative AI is the development of advanced model architectures, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (VAEs) (Kingma \u0026 Welling, 2013), and Transformer-based models (Vaswani et al., 2017). These architectures have enabled the generation of high-quality content by learning complex data distributions. Growth in Computing Power and Data Availability The exponential growth in computing power and the availability of large datasets have been crucial in advancing generative AI. The use of GPUs and TPUs has accelerated the training of large models, while datasets like ImageNet (Deng et al., 2009) and Common Crawl have provided diverse and extensive training data. Emerging Techniques and Approaches Recent innovations, such as few-shot and zero-shot learning, have expanded the capabilities of generative models. Techniques like fine-tuning and transfer learning allow models to adapt to new tasks with limited data, demonstrating versatility and efficiency in various applications (Radford et al., 2021). Applications of Generative AI Content Creation Generative AI has revolutionized content creation, enabling the automatic generation of text, images, music, and videos. For instance, GPT-3 (Brown et al., 2020) has demonstrated remarkable capabilities in generating human-like text, while models like DALL-E (Ramesh et al., 2021) can create novel images from textual descriptions. Healthcare In healthcare, generative AI has shown promise in drug discovery and medical imaging. For example, GANs have been used to generate realistic medical images for training purposes, improving diagnostic accuracy (Frid-Adar et al., 2018). Additionally, AI models can assist in designing new molecules with desired properties, expediting the drug development process. Gaming and Entertainment The gaming and entertainment industries have embraced generative AI to create immersive experiences. AI-generated characters, dialogues, and game levels enhance player engagement. Moreover, deepfake technology, powered by generative models, has opened new avenues in film and media production, allowing for realistic character portrayals and visual effects. Finance In finance, generative AI is utilized for algorithmic trading, risk management, and fraud detection. AI models can generate synthetic financial data to simulate market scenarios, aiding in the development of robust trading strategies (Wiese et al., 2019). Additionally, generative models can identify unusual patterns in transactions, enhancing fraud detection systems. For a deeper understanding of how LLMs are transforming finance, you can watch this insightful video: Autonomous Systems Generative AI plays a crucial role in autonomous systems, including robotics and self-driving cars. AI-generated simulations help in training and testing autonomous agents, reducing the reliance on real-world testing. For instance, generative models can simulate complex driving scenarios, improving the safety and reliability of self-driving technology (Dosovitskiy et al., 2017). Challenges and Ethical Considerations Bias and Fairness One of the significant challenges in generative AI is addressing bias and ensuring fairness. AI models may perpetuate societal biases present in the training data, leading to unfair or discriminatory outcomes. Researchers are actively exploring methods to detect and mitigate biases in generative models (Bender et al., 2021). Security and Privacy The rise of generative AI has raised concerns about security and privacy. Deepfake technology, for example, can be misused to create realistic but fake videos, leading to misinformation and privacy violations. Ensuring the responsible use of generative AI and developing techniques to detect synthetic content are crucial to addressing these issues (Chesney \u0026 Citron, 2019). Environmental Impact The training of large generative models requires substantial computational resources, contributing to the environmental impact. Researchers are exploring ways to reduce the carbon footprint of AI, such as developing energy-efficient algorithms and hardware (Strubell et al., 2019). Future Directions and Opportunities The future of generative AI holds immense potential, with opportunities for interdisciplinary applications and collaborations between academia and industry. As the technology continues to evolve, it is crucial to consider its societal implications and strive for responsible and ethical deployment. The integration of generative AI in various fields, from art to science, will likely lead to groundbreaking innovations and transformative experiences. Here is a simple Python code snippet demonstrating the basic structure of a Generative Adversarial Network (GAN) using PyTorch: import torch import torch.nn as nn import numpy as np class Generator(nn.Module): def __init__(self, latent_dim, img_shape): super(Generator, self).__init__() self.model = nn.Sequential( nn.Linear(latent_dim, 128), nn.LeakyReLU(0.2, inplace=True), nn.Linear(128, 256), nn.BatchNorm1d(256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, int(np.prod(img_shape))), nn.Tanh() ) self.img_shape = img_shape def forward(self, z): img = self.model(z) img = img.view(img.size(0), *self.img_shape) return img class Discriminator(nn.Module): def __init__(self, img_shape): super(Discriminator, self).__init__() self.model = nn.Sequential( nn.Linear(int(np.prod(img_shape)), 256), nn.LeakyReLU(0.2, inplace=True), nn.Linear(256, 128), nn.LeakyReLU(0.2, inplace=True), nn.Linear(128, 1), nn.Sigmoid() ) def forward(self, img): img_flat = img.view(img.size(0), -1) validity = self.model(img_flat) return validity Diffusion Models Diffusion models have emerged as a powerful approach in generative AI, especially for tasks involving image generation and editing. These models iteratively denoise images to recover the original data distribution. The objective function for diffusion models can be expressed as: $$ C(x) = -\\frac{1}{\\sigma \\sqrt{2\\pi}} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2 e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2} $$ Where:\n$\\theta$ represents the model parameters. $t$ is a time step uniformly sampled from the interval [0, 1]. $\\lambda(t)$ is a time-dependent weighting function. $x_0$ is the original data point (e.g., an image). $\\epsilon$ is Gaussian noise sampled from a normal distribution. $\\epsilon_\\theta(x_t, t)$ is the predicted noise at time step $t$ by the model. $$ L(x) = \\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 + \\frac{1}{2} \\log(2 \\pi \\sigma^2) $$\nIn this form:\n$( \\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 )$ represents the squared deviation from the mean, often used in diffusion models to measure the distance between generated and target distributions. $( \\frac{1}{2} \\log(2 \\pi \\sigma^2) )$ represents the entropy term, which accounts for the normalization factor in the Gaussian distribution. You can also represent the diffusion objective function in a more general form related to a stochastic process:\n$$ L(x) = \\mathbb{E} \\left[ \\frac{1}{2} | x - \\mu |^2 + \\frac{1}{2} \\log(2 \\pi \\sigma^2) \\right] $$\nHere, $( \\mathbb{E} )$ denotes the expectation over the diffusion process, capturing the average cost.\nThis objective function measures the discrepancy between the true noise added to the data and the noise predicted by the model, aiming to train the model to accurately reverse the diffusion process.\nwhere \\(x_t\\) is the noised image at timestep \\(t\\), and \\(\\epsilon_\\theta\\) is the noise prediction network. Recent works like DALL-E 2 and Stable Diffusion have demonstrated the remarkable capabilities of diffusion models in text-to-image generation and image editing tasks. These models leverage large-scale training on diverse datasets and incorporate additional conditioning information to enable fine-grained control over generated images. Conclusion Generative AI continues to advance rapidly, with ongoing developments in model architectures, training techniques, and applications across various domains. The ability of generative models to create high-quality content, from text and images to music and videos, underscores their transformative potential. While there are challenges and ethical considerations to address, the future of generative AI is promising, with numerous opportunities for innovation and interdisciplinary collaboration. As we explore these frontiers, it is crucial to remain mindful of the societal impacts and strive for responsible use of these powerful technologies. Generative AI is revolutionizing various fields by creating new content and enhancing existing applications. This blog explores current trends, practical applications, challenges, and future opportunities of generative models. Key areas include advancements in model architectures, real-world applications like content creation and healthcare, and the integration of techniques such as GANs and diffusion models. Generative AI presents both exciting opportunities and significant challenges. This blog covers the latest trends in generative models, their applications across various industries, and critical issues such as ethical considerations and future directions. Learn about the potential of models like GANs and diffusion techniques, and their impact on content creation and other fields. Todo lists Understand GANs (Generative Adversarial Networks) Study GAN architecture (Generator and Discriminator) Review applications and improvements Learn about Variational Autoencoders (VAEs) Explore VAE structure and loss function Examine use cases in generative tasks Familiarize with Diffusion Models Understand diffusion process and objective function Review recent advancements (e.g., DALL-E 2, Stable Diffusion) Explore Transformer Models Study transformer architecture and attention mechanisms Review its application in language generation and understanding Learn about Pretrained Language Models Study fine-tuning techniques for specific tasks Explore popular models (e.g., GPT, BERT, T5) Understand Model Evaluation Metrics Review metrics like BLEU, ROUGE, and FID for generative models Study methods for evaluating model performance in different contexts Investigate Ethical Considerations Explore challenges related to bias, fairness, and security Study frameworks for responsible AI development Citation Panboonyuen, Teerapong. (Aug 2024). Generative AI: Current Trends and Practical Applications. Blog post on Kao Panboonyuen. https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/\nOr\n@article{panboonyuen2024generativeaitrends, title = \u0026quot;Generative AI: Current Trends and Practical Applications.\u0026quot;, author = \u0026quot;Panboonyuen, Teerapong\u0026quot;, journal = \u0026quot;kaopanboonyuen.github.io/\u0026quot;, year = \u0026quot;2024\u0026quot;, month = \u0026quot;Aug\u0026quot;, url = \u0026quot;https://kaopanboonyuen.github.io/generative-ai-current-trends/\u0026quot; } Did you find this page helpful? Consider sharing it üôå References Bender, E. M., Gebru, T., McMillan-Major, A., \u0026amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT). arXiv:2102.02503.\nBROWN, T. B., MANE, D., LANGE, I., \u0026amp; et al. (2020). Language Models are Few-Shot Learners. Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020). arXiv:2005.14165.\nCHESNEY, R., \u0026amp; CITRON, D. K. (2019). Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. California Law Review, 107(6), 1753-1819. doi:10.2139/ssrn.3213954.\nDENG, J., DONAHUE, J., \u0026amp; HAREL, M. (2009). ImageNet: A Large-Scale Hierarchical Image Database. Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/CVPR.2009.5206848.\nDOSOVITSKIY, A., BROSSARD, T., \u0026amp; SPRINGENBERG, J. (2017). Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 39(5), 939-949. doi:10.1109/TPAMI.2016.2593826.\nFRID-ADAR, M., ELIYAHU, S., \u0026amp; GOLDY, S. (2018). GAN-based Synthetic Medical Image Augmentation for Increased CNN Performance in Liver Lesion Classification. IEEE Transactions on Medical Imaging, 37(6), 1334-1343. doi:10.1109/TMI.2018.2813792.\nKINGMA, D. P., \u0026amp; WELLING, M. (2013). Auto-Encoding Variational Bayes. Proceedings of the 2nd International Conference on Learning Representations (ICLR). arXiv:1312.6114.\nRADFORD, A., WU, J., \u0026amp; AMODEI, D. (2021). Learning Transferable Visual Models From Natural Language Supervision. Proceedings of the 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). arXiv:2103.00020.\nRAMESH, A., MENG, C., \u0026amp; ZHANG, S. (2021). DALL¬∑E: Creating Images from Text. OpenAI. https://openai.com/research/dall-e.\nSTRUBELL, E., GANASSI, M., \u0026amp; MCAFEE, P. (2019). Energy and Policy Considerations for Deep Learning in NLP. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019). arXiv:1906.02243.\nWIESE, S., BOLAND, M., \u0026amp; TONG, A. (2019). A Survey on Machine Learning in Finance. Proceedings of the 26th International Conference on Machine Learning (ICML 2019). arXiv:1910.02342.\nVASWANI, A., SHAZEER, N., \u0026amp; PARMAR, N. (2017). Attention Is All You Need. Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017). arXiv:1706.03762.\n","date":1722470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722470400,"objectID":"ca1dfb1f5d573b3f2741fe8ac194fca3","permalink":"https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/","publishdate":"2024-08-01T00:00:00Z","relpermalink":"/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/","section":"blog","summary":"A comprehensive exploration of generative AI, including key trends, applications, challenges, and future directions.","tags":["generative-ai","deep-learning","computer-vision","NLP","AI-applications"],"title":"Generative AI: Current Trends and Practical Applications","type":"blog"},{"authors":["Teerapong Panboonyuen"],"categories":["remote-sensing","LULC","LLM"],"content":"Table of Contents Introduction Understanding Large Language Models (LLMs) LLMs in LULC Classification Methodology Case Study: LULC Classification on Sentinel-2 Imagery Conclusion Introduction Remote sensing has revolutionized the way we observe and understand the Earth‚Äôs surface. With the advent of satellites like Sentinel, Landsat-8, and THEOS, we have access to a plethora of high-resolution imagery that can be used for various applications, including Land Use/Land Cover (LULC) classification and image classification. However, analyzing and interpreting this vast amount of data is a complex task. Enter Large Language Models (LLMs), which have shown promise in various domains, including natural language processing, computer vision, and remote sensing. In this blog, we will explore how LLMs can be applied to remote sensing, particularly in the domains of LULC and image classification. We will delve into the methodologies, algorithms, and techniques that can be utilized to harness the power of LLMs for these applications. Understanding Large Language Models (LLMs) Large Language Models, such as GPT-4, are deep learning models that have been trained on vast amounts of text data. They are capable of understanding and generating human-like text, making them highly versatile for various applications. In the context of remote sensing, LLMs can be used to analyze and interpret imagery data, aiding in tasks like LULC classification and image classification. LLMs in LULC Classification LULC classification involves categorizing different regions of an image into land use and land cover classes, such as forests, urban areas, water bodies, and agricultural land. Traditional methods for LULC classification include supervised and unsupervised learning techniques. LLMs, however, can enhance these methods by providing contextual understanding and improved feature extraction. Methodology The process of using LLMs for LULC classification can be summarized as follows: Data Preprocessing:\nCollect satellite imagery from sources like Sentinel, Landsat-8, and THEOS. Perform image correction and normalization to ensure consistency in the data. Feature Extraction:\nUse convolutional neural networks (CNNs) to extract features from the satellite images. Integrate LLMs to enhance feature extraction by incorporating contextual information from related text data (e.g., environmental reports, land use documentation). Model Training:\nTrain a classification model using the extracted features. The model can be a hybrid of CNNs and LLMs, where the CNN handles the spatial features and the LLM provides contextual understanding. Classification and Validation:\nApply the trained model to classify the satellite images into LULC categories. Validate the model using ground truth data and performance metrics like accuracy, precision, recall, and F1-score. Case Study: LULC Classification on Sentinel-2 Imagery To illustrate the application of LLMs in LULC classification, let‚Äôs consider a case study using Sentinel-2 imagery. Sentinel-2 provides high-resolution optical imagery, which is ideal for detailed LULC classification. Data Collection and Preprocessing We collected Sentinel-2 imagery for a region with diverse land cover types. The images were preprocessed to correct for atmospheric effects and normalize the reflectance values. Feature Extraction A pre-trained CNN, such as ResNet-50, was used to extract spatial features from the images. Simultaneously, a large corpus of environmental text data was fed into an LLM to extract contextual features. Model Training and Classification The extracted features were combined and fed into a hybrid classification model. The model was trained using labeled ground truth data. The results showed a significant improvement in classification accuracy compared to traditional methods. Conclusion The integration of LLMs in remote sensing, particularly for LULC and image classification, holds immense potential. By combining the spatial feature extraction capabilities of CNNs with the contextual understanding of LLMs, we can achieve more accurate and meaningful classifications. As remote sensing technology continues to evolve, the role of advanced AI models like LLMs will become increasingly crucial in unlocking new insights from satellite imagery. In this blog, we explored the methodologies and techniques for leveraging LLMs in remote sensing applications. The case study on Sentinel-2 imagery demonstrated the practical benefits of this approach. As we move forward, further research and development in this field will undoubtedly lead to more innovative and effective solutions for remote sensing challenges. ","date":1722124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722124800,"objectID":"81f92504ef13a550691c26befb1a616d","permalink":"https://kaopanboonyuen.github.io/blog/2024-07-29-leveraging-large-language-models-in-remote-sensing/","publishdate":"2024-07-28T00:00:00Z","relpermalink":"/blog/2024-07-29-leveraging-large-language-models-in-remote-sensing/","section":"blog","summary":"Exploring how LLMs can enhance remote sensing applications like LULC and image classification using Sentinel, Landsat-8, and THEOS imagery.","tags":["remote-sensing","LULC","image-classification","LLM","Sentinel","Landsat","THEOS"],"title":"Leveraging Large Language Models (LLMs) in Remote Sensing for Land Use/Land Cover (LULC) and Image Classification","type":"blog"},{"authors":[],"categories":null,"content":" ","date":1701435600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701435600,"objectID":"cd4a86c4c3997f9191260d8c073400f1","permalink":"https://kaopanboonyuen.github.io/talk/geospatial-big-data-analytics/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/talk/geospatial-big-data-analytics/","section":"event","summary":"Geospatial Data Analytics involves analyzing spatial and geographical data to gain insights and make informed decisions. Using PySpark, this process is accelerated through distributed computing, enabling the handling of large datasets efficiently. Distributed Machine Learning models further enhance the analysis by providing scalable and robust predictions. Visualization tools like Looker Studio present the analyzed data in an interactive and comprehensible format, facilitating better decision-making and strategic planning. This combination of technologies allows for comprehensive geospatial data analysis, uncovering patterns and trends that drive actionable insights.","tags":[],"title":"Geospatial Big Data Analytics","type":"event"},{"authors":["Teerapong Panboonyuen","C. Charoenphon","C. Satirapod"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"14c0ff27b66af589b9731863f8bf9193","permalink":"https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/mevit-a-medium-resolution-vision-transformer/","section":"publication","summary":"In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT's effectiveness in accurately segmenting Thai Landsat-8 data.","tags":["Remote Sensing","Landsat-8","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Vision Transformers","Transformer","Multi-branch Architectures","Mixed-scale Convolutional Feedforward Networks"],"title":"MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand","type":"publication"},{"authors":["Teerapong Panboonyuen","N. Nithisopa","P. Pienroj","L. Jirachuphun","C. Watthanasirikrit","N. Pornwiriyakul"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"ecd0bcce1705e21b780021bb64e0e45a","permalink":"https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/","section":"publication","summary":"Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at https://github.com/kaopanboonyuen/MARS.","tags":["Attention","Self-Attention","MARS","Sequential Quadtree Nodes","Mask R-CNN","PointRend","Mask Transfiner"],"title":"MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1669899600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669899600,"objectID":"8f68e458f32432396b1cad7a1a1de3bd","permalink":"https://kaopanboonyuen.github.io/talk/distributed-machine-learning-techniques-for-geospatial-data/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/talk/distributed-machine-learning-techniques-for-geospatial-data/","section":"event","summary":"I was invited to teach a course on distributed machine learning to the Geo-Informatics and Space Technology Development Agency (GISTDA). The curriculum covered fundamental concepts of PySpark, basic deep learning techniques, and practical applications of distributed training using TensorFlow. I also emphasized methods for leveraging Multi-GPU setups and implementing distributed training strategies, particularly in the context of geospatial data analytics, equipping participants with the skills needed to handle large-scale machine learning tasks efficiently.","tags":[],"title":"Distributed Machine Learning Techniques for Geospatial Data","type":"event"},{"authors":["Teerapong Panboonyuen","S. Thongbai","W. Wongweeranimit","P. Santitamnont","K. Suphan","C. Charoenphon"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"4adfc7217759e6906ac85e560b8c385f","permalink":"https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/publication/object-detection-of-road-assets-using-transformer-based-yolox/","section":"publication","summary":"Detecting objects of varying sizes, like kilometer stones, remains a significant challenge and directly affects the accuracy of object counts. Transformers have shown remarkable success in natural language processing (NLP) and image processing due to their ability to model long-range dependencies. This paper proposes an enhanced YOLO (You Only Look Once) series with two key contributions, (i) We employ a pre-training objective to obtain original visual tokens from image patches of road assets, using a pre-trained Vision Transformer (ViT) backbone, which is then fine-tuned on downstream tasks with additional task layers. (ii) We incorporate Feature Pyramid Network (FPN) decoder designs into our deep learning network to learn the significance of different input features, avoiding issues like feature mismatch and performance degradation that arise from simple summation or concatenation. Our proposed method, Transformer-Based YOLOX with FPN, effectively learns general representations of objects and significantly outperforms state-of-the-art detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. It achieves a 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP on the test-dev dataset.","tags":["Attention","Self-Attention","MARS","Sequential Quadtree Nodes","Mask R-CNN","PointRend","Mask Transfiner"],"title":"Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama","type":"publication"},{"authors":["K. Thitisiriwech","Teerapong Panboonyuen","P. Kantavat","Y. Iwahori","B. Kijsirikul"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"9d0e53f94cc9f61ef1adaf4acad0f1a3","permalink":"https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/","section":"publication","summary":"In the modern era, urban design and sustainable development are vital topics for megacities, as they are important for the wellbeing of its residents. One of the effective key performance indices (KPIs) measuring the city plan‚Äôs efficiency in quantity and quality factors is Quality of Life (QOL), an index that policymakers can use as a critical KPI to measure the quality of urbanscape design. In the traditional approach, the researchers conduct the questionnaire survey and then analyze the gathered data to acquire the QOL index. The conventional process is costly and time-consuming, but the result of the evaluation area is limited. Moreover, it is difficult to embed in an application or system; we proposed artificial intelligence (AI) approaches to solve the limitation of the traditional method in Bangkok as a case study. There are two steps for our proposed method. First, in the knowledge extraction step, we apply deep convolutional neural networks (DCNNs), including semantic segmentation and object detection, to extract helpful information images. Second, we use a linear regression model for inferring the QOL score. We conducted various state-of-the-art (SOTA) models and public datasets to evaluate the performance of our method. The experiment results show that our novel approach is practical and can be considered for use as an alternative QOL acquisition method. We also gain some understanding of drivers‚Äô insights from the experiment result. Implementation codes and dataset are available at [https://kaopanboonyuen.github.io/bkkurbanscapes](https://kaopanboonyuen.github.io/bkkurbanscapes).","tags":["DeepLab","Sustainability","Quality of Life (QOL)","Bangkok Urbanscapes Dataset","Xception","Cityscapes"],"title":"Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks","type":"publication"},{"authors":[],"categories":null,"content":" ","date":1606827600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606827600,"objectID":"85d93937200093837b1d1a85f626df23","permalink":"https://kaopanboonyuen.github.io/talk/achieve-data-science-first-meet/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/talk/achieve-data-science-first-meet/","section":"event","summary":"I was invited to speak at the \"Achieve Data Science First Meet\" for a MOOC student project event, where I highlighted the growing recognition of data science, AI, and machine learning's importance across various industries. I advised that organizations, regardless of their size or sector, must effectively develop and implement data science capabilities to stay competitive in the era of big data, or risk falling behind.","tags":[],"title":"Achieve Data Science First Meet","type":"event"},{"authors":[],"categories":null,"content":" ","date":1596200400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596200400,"objectID":"d7024594cc6cb801b6ae9f39b7862ecf","permalink":"https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/","publishdate":"2020-07-31T00:00:00Z","relpermalink":"/talk/ph.d.-thesis-defense/","section":"event","summary":"My thesis defense at the Faculty of Engineering, Chulalongkorn University. This dissertation introduces a new architecture for remote sensing, featuring Global Convolutional Network (GCN), channel attention, domain-specific transfer learning, Feature Fusion (FF), and Depthwise Atrous Convolution (DA). Tests on Landsat-8 and ISPRS Vaihingen datasets show that this model significantly outperforms the baseline.","tags":[],"title":"Ph.D. Thesis Defense","type":"event"},{"authors":["Teerapong Panboonyuen","P. Rakwatin","K. Intarat"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"3cc38a178687941b0f68b09b6ba4eff2","permalink":"https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/enhanced-feature-pyramid-vision-transformert/","section":"publication","summary":"Semantic segmentation on Landsat-8 data is crucial in the integration of diverse data, allowing researchers to achieve more productivity and lower expenses. This research aimed to improve the versatile backbone for dense prediction without convolutions‚Äînamely, using the pyramid vision transformer (PRM-VS-TM) to incorporate attention mechanisms across various feature maps. Furthermore, the PRM-VS-TM constructs an end-to-end object detection system without convolutions and uses handcrafted components, such as dense anchors and non-maximum suspension (NMS). The present study was conducted on a private dataset, i.e., the Thailand Landsat-8 challenge. There are three baselines, DeepLab, Swin Transformer (Swin TF), and PRM-VS-TM. Results indicate that the proposed model significantly outperforms all current baselines on the Thailand Landsat-8 corpus, providing F1-scores greater than 80% in almost all categories. Finally, we demonstrate that our model, without utilizing pre-trained settings or any further post-processing, can outperform current state-of-the-art (SOTA) methods for both agriculture and forest classes.","tags":["Mon-Maximum Suspension","Transfer Learning","Vision Transformer","Remote Sensing","Landsat-8","Transformer"],"title":"Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus","type":"publication"},{"authors":["K. Thitisiriwech","Teerapong Panboonyuen","P. Kantavat","Y. Iwahori","B. Kijsirikul"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"dbdbb2d13060b56b1c4b922f50cd5ad4","permalink":"https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/the-bangkok-urbanscapes-dataset/","section":"publication","summary":"This paper addresses semantic segmentation for autonomous driving systems, focusing on self-driving cars in Thailand. We introduce DeepLab-V3-A1 with Xception, an enhanced version of DeepLab-V3+, and present the Bangkok Urbanscapes dataset. Our method improves segmentation accuracy by refining the decoder and modifying the Xception backbone. Experiments on four datasets, including CamVid, Cityscapes, IDD, and our proposed dataset, show our approach performs comparably to baseline methods. Our dataset includes 701 annotated images of various Bangkok driving environments, covering eleven semantic classes. The architecture and dataset aim to aid developers in improving autonomous driving systems for diverse urban conditions. Implementation codes and dataset are available at [https://kaopanboonyuen.github.io/bkkurbanscapes](https://kaopanboonyuen.github.io/bkkurbanscapes).","tags":["DeepLab","Bangkok Urbanscapes Dataset","Xception","Cityscapes"],"title":"The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"4428c44f98c396bbd30d518b58b80be8","permalink":"https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/transformer-based-decoder-designs-for-semantic-segmentation/","section":"publication","summary":"Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% ùêπ1 score), Thailand North Landsat-8 corpus (63.12% ùêπ1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.","tags":["Transformer","Semantic Segmentation","Decoder Design","Swin Transformer","Vision Transformer","Self-Attention","Global Convolutional Network"],"title":"Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images","type":"publication"},{"authors":["Teerapong Panboonyuen"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"08bf283a12b51cb06a21958c8492cd7d","permalink":"https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/","section":"publication","summary":"My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.","tags":["Convolutional Neural Networks","Landsat-8","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Global Convolutional Network","Encoder-Decoder Networks","ISPRS Vaihingen"],"title":"Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"6e6a1251e5de03c8b0aadd7a8242a7b8","permalink":"https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/","section":"publication","summary":"This paper addresses improving semantic segmentation in remote sensing for aerial and satellite images, which is crucial for agriculture, map updates, route optimization, and navigation. We propose enhancements to the state-of-the-art Enhanced Global Convolutional Network (GCN152-TL-A) by introducing a High-Resolution Representation (HR) backbone for better feature extraction, Feature Fusion (FF) to capture low-level details, and Depthwise Atrous Convolution (DA) for refined multi-resolution features. Experiments on Landsat-8 and ISPRS Vaihingen datasets demonstrate our model's superior performance, achieving over 90% accuracy in F1 scores and outperforming baseline models.","tags":["Feature Fusion","Transfer Learning","Remote Sensing","ISPRS Vaihingen Dataset"],"title":"Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"05de61049c7b26380a42ed51f66573c2","permalink":"https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/","section":"publication","summary":"In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, e.g., river, building, forest, etc., on raster images. A deep convolutional encoder‚Äìdecoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second, ‚Äúchannel attention‚Äù is presented in our network in order to select the most discriminative filters (features). Third, ‚Äúdomain-specific transfer learning‚Äù is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets (i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of ùêπ1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively.","tags":["Global Convolutional Network","Transfer Learning","Channel Attention","Remote Sensing","Discriminative Filters"],"title":"Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning","type":"publication"},{"authors":["P. Kantavat","Y. Hayashi","G. City","B. Kijsirikul","Teerapong Panboonyuen","Y. Iwahori"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"52061b758c1cab61004baa1a27c5c001","permalink":"https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/","section":"publication","summary":"For an urban development, the Quality of Life (QOL) of people in the city is a vital issue that should be considered. There are many researches in QOL topics that use questionnaire survey approach. These studies yield very useful information for city development planning. As the Artificial Intelligence technologies are developed very fast recently, they are applied to solve many transportation problems. In this paper, we propose a method that automatically extract mobility indicators using two image recognition techniques, Semantic Segmentation and Object Recognition. Because the mobility is an important factor in QOL evaluation, our work can be used to enhance a performance and reduce a data gathering cost of the QOL evaluation.","tags":["YOLOv5","Quality of Life (QOL)","Semantic Segmentation","Object Detection","Image Recognition"],"title":"Transportation Mobility Factor Extraction Using Image Recognition Techniques","type":"publication"},{"authors":["I. Wichakam","Teerapong Panboonyuen","C. Udomcharoenchaikit","P. Vateekul"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"7ae277d2610b5783ef94eba07969563a","permalink":"https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/","section":"publication","summary":"Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.","tags":["Colorectal Cancer","Fully Convolutional Network","CVC-EndoSceneStill","Colonoscopy Video"],"title":"Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network","type":"publication"},{"authors":["S. Chantharaj","K. Pornratthanapong","P. Chitsinpchayakun","Teerapong Panboonyuen"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"426382c4e17472b69c94efc70af45de8","permalink":"https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/semantic-segmentation-on-medium-resolution-satellite-images/","section":"publication","summary":"Semantic Segmentation is a fundamental task in computer vision and remote sensing imagery. Many applications, such as urban planning, change detection, and environmental monitoring, require the accurate segmentation; hence, most segmentation tasks are performed by humans. Currently, with the growth of Deep Convolutional Neural Network (DCNN), there are many works aiming to find the best network architecture fitting for this task. However, all of the studies are based on very-high resolution satellite images, and surprisingly; none of them are implemented on medium resolution satellite images. Moreover, no research has applied geoinformatics knowledge. Therefore, we purpose to compare the semantic segmentation models, which are FCN, SegNet, and GSN using medium resolution images from Landsat-8 satellite. In addition, we propose a modified SegNet model that can be used with remote sensing derived indices. The results show that the model that achieves the highest accuracy RGB bands of medium resolution aerial imagery is SegNet. The overall accuracy of the model increases when includes Near Infrared (NIR) and Short-Wave Infrared (SWIR) band. The results showed that our proposed method (our modified SegNet model, named RGB-IR-IDX-MSN method) outperforms all of the baselines in terms of mean F1 scores.","tags":["Convolutional Neural Networks","Semantic Segmentation","Near Infrared (NIR)","Short-Wave Infrared (SWIR)","Remote Sensing"],"title":"Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"f6d05230dcdb2e8c944ff54140727202","permalink":"https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/road-segmentation-on-remote-sensing/","section":"publication","summary":"Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.","tags":["Remote Sensing","Road Segmentation","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Exponential Linear Unit","Conditional Random Fields","Landscape Metrics"],"title":"Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"fca7fa3fb9e06fccd9699f13b4e415fe","permalink":"https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/road-segmentation-on-aerial-imagery/","section":"publication","summary":"In this paper, we introduce an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Enhancements include the use of ELU (exponential linear unit) instead of ReLU, dataset augmentation with incrementally-rotated images to increase training data by eight times, and the use of landscape metrics to remove false road objects. Tested on the Massachusetts Roads dataset, our method outperformed the SegNet benchmark and other baselines in precision, recall, and F1 scores.","tags":["Remote Sensing","Road Segmentation","Deep Learning","Semantic Segmentation","High-Resolution Imagery","Aerial Imagery","Convolutional Neural Networks","Encoder-Decoder Networks","Exponential Linear Unit"],"title":"An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","S. Lawawirojwong"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b626b42b6e307c281ff5e448e98cd9b0","permalink":"https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/road-map-extraction-from-satellite-imagery/","section":"publication","summary":"Road map extraction is vital for GIS and underpins many location-based applications like GPS navigation, delivery route planning, tourist attraction locating, and location-based marketing. This research uses satellite imagery, though other remotely sensed images like aerial photographs, UAVs, or drones are also applicable. Despite various proposed methods focusing primarily on accuracy, completeness of results is equally important. We enhance accuracy by incorporating connected component analysis and improve completeness using landscape metrics, which describe spatial characteristics through shape and isolation indices. Evaluated on precision, recall, quality, and F1 scores, our method achieves over 90% performance in all criteria.","tags":["Road Segmentation","Connected Component Analysis","Image Processing"],"title":"Road map extraction from satellite imagery using connected component analysis and landscape metrics","type":"publication"},{"authors":["Teerapong Panboonyuen","P. Vateekul","P. Srestasathiern","S. Lawawirojwong"],"categories":null,"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"50982ca008f9ff19109ec4bea6c187cd","permalink":"https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/publication/image-vectorization-of-road-satellite-data-sets/","section":"publication","summary":"Data extraction of geo-spatial objects from satellite images is a crucial step in facilitating analysis of geo-spatial or spatio-temporal data, typically involving line (road) and polygon (area) layers. This paper introduces a method for transforming satellite data (raster images) containing roads from pixel form into spatial objects comprising lines and polygons. Our algorithm consists of three primary steps. First, roads are isolated from other objects using k-means clustering. Second, line extraction is performed on the road areas by applying morphological operations to skeletonize the image, followed by enhancement using the Ramer-Douglas-Peucker algorithm. Finally, land-cover classification is applied to non-road objects to extract polygons. Experimental results demonstrate that both lines (road networks) and polygons (areas) can be accurately extracted from satellite imagery simultaneously.","tags":["Remote Sensing","Road Segmentation","Spatio-Temporal Data","High-Resolution Imagery","Aerial Imagery","K-Means Clustering","Ramer-Douglas-Peucker"],"title":"Image Vectorization of Road Satellite Data Sets","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://kaopanboonyuen.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]