<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Teerapong Panboonyuen</title>
    <link>https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/</link>
      <atom:link href="https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/index.xml" rel="self" type="application/rss+xml" />
    <description>Teerapong Panboonyuen</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>©2024 Kao Panboonyuen</copyright><lastBuildDate>Fri, 06 Sep 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/avatar_hu3bf8c3b6af25e9d1c9865942b827a76a_6860848_270x270_fill_q75_lanczos_center.jpg</url>
      <title>Teerapong Panboonyuen</title>
      <link>https://kaopanboonyuen.github.io/author/teerapong-panboonyuen/</link>
    </image>
    
    <item>
      <title>Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-06-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-06-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</guid>
      <description>&lt;h3 id=&#34;introduction-and-motivation&#34;&gt;Introduction and Motivation&lt;/h3&gt;
&lt;p&gt;We are pleased to announce that our paper, titled &lt;em&gt;“Enhanced YOLOv8-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery”&lt;/em&gt;, has been accepted for oral presentation at the 5th International Conference on Highway Engineering (iCHE 2024). This opportunity marks a significant moment in our academic journey, especially after a hiatus from international conferences since completing my Ph.D. I am eager to re-engage with the academic community and share our recent advancements in person.&lt;/p&gt;
&lt;h3 id=&#34;motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/h3&gt;
&lt;p&gt;Thailand&amp;rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.&lt;/p&gt;
&lt;p&gt;In this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss.&lt;/p&gt;
&lt;p&gt;Our proposed method (Fig. 1) enhances the capability of YOLOv8-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;proposed_method.png&#34; alt=&#34;Proposed Method Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. The proposed Enhanced YOLOv8-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various YOLOv8 model variants, ranging from YOLOv8n to YOLOv8x, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. &lt;a href=&#34;https://scholar.google.co.th/citations?user=myy0qDgAAAAJ&amp;hl=en&#34; target=&#34;_blank&#34;&gt;[Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.&lt;/p&gt;
&lt;p&gt;Stay tuned for updates, and a big thank you to my incredible research team:&lt;br&gt;
&lt;strong&gt;N. Rattanachona&lt;/strong&gt;, &lt;strong&gt;P. Thungthin&lt;/strong&gt;, &lt;strong&gt;N. Subsompon&lt;/strong&gt;, &lt;strong&gt;S. Thongbai&lt;/strong&gt;, &lt;strong&gt;W. Wongweeranimit&lt;/strong&gt;, and &lt;strong&gt;R. Phukham&lt;/strong&gt;. Your hard work and dedication were essential to this project!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_00.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here I am, presenting our work on the Enhanced YOLOv8 model and its application in detecting road assets!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have visualizations of the detection results produced by the Enhanced YOLOv8 model. The bounding boxes and labels demonstrate the model’s ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the model’s performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_03.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s look at a real-world application of our Enhanced YOLOv8 model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model&amp;rsquo;s potential for real-world deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_04.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart presents a comparison of performance metrics between our Enhanced YOLOv8 model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model&amp;rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_05.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, this image illustrates the training process for the Enhanced YOLOv8 model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the model’s performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the model’s effectiveness and reliability in practical applications.
&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_06.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;paper-highlights&#34;&gt;Paper Highlights:&lt;/h2&gt;
&lt;p&gt;Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced YOLOv8 model integrated with &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.&lt;/p&gt;
&lt;h3 id=&#34;formula-1-generalized-focal-loss&#34;&gt;Formula 1: Generalized Focal Loss&lt;/h3&gt;
&lt;p&gt;We employed &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which reduces the contribution of easily classified examples and focuses more on hard examples.&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{GFL}} = - \alpha (1 - p_t)^\gamma \log(p_t)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( p_t )$ is the predicted probability for the correct class,&lt;/li&gt;
&lt;li&gt;$( \alpha )$ balances the importance of positive/negative examples,&lt;/li&gt;
&lt;li&gt;$( \gamma )$ adjusts the model’s focus on hard examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This formula highlights our approach to improving object detection, especially for challenging highway assets in varying conditions.&lt;/p&gt;
&lt;h3 id=&#34;key-metrics&#34;&gt;Key Metrics:&lt;/h3&gt;
&lt;p&gt;The results demonstrate our model&amp;rsquo;s superior performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mAP50&lt;/strong&gt;: 80.340&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mAP50-95&lt;/strong&gt;: 60.840&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt;: 79.100&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt;: 76.680&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F1-Score&lt;/strong&gt;: 77.870&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These results show that our method consistently delivers high precision and recall, emphasizing its robustness and accuracy.&lt;/p&gt;
&lt;h3 id=&#34;formula-2-map-calculation&#34;&gt;Formula 2: mAP Calculation&lt;/h3&gt;
&lt;p&gt;The mean Average Precision (mAP) is used to evaluate detection accuracy. For our model, mAP is calculated as follows:&lt;/p&gt;
&lt;p&gt;$$
\text{mAP} = \frac{1}{n} \sum_{i=1}^{n} \text{AP}_i
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( n )$ is the number of detection categories,&lt;/li&gt;
&lt;li&gt;$( \text{AP}_i )$ is the average precision for each category.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison-of-yolov8-variants&#34;&gt;Comparison of YOLOv8 Variants:&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;mAP50&lt;/th&gt;
&lt;th&gt;mAP50-95&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;F1-Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8n&lt;/td&gt;
&lt;td&gt;71.100&lt;/td&gt;
&lt;td&gt;47.760&lt;/td&gt;
&lt;td&gt;80.100&lt;/td&gt;
&lt;td&gt;63.460&lt;/td&gt;
&lt;td&gt;70.820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8s&lt;/td&gt;
&lt;td&gt;75.150&lt;/td&gt;
&lt;td&gt;52.070&lt;/td&gt;
&lt;td&gt;82.660&lt;/td&gt;
&lt;td&gt;69.950&lt;/td&gt;
&lt;td&gt;75.780&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8m&lt;/td&gt;
&lt;td&gt;79.570&lt;/td&gt;
&lt;td&gt;58.060&lt;/td&gt;
&lt;td&gt;85.410&lt;/td&gt;
&lt;td&gt;71.290&lt;/td&gt;
&lt;td&gt;77.710&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8l&lt;/td&gt;
&lt;td&gt;80.270&lt;/td&gt;
&lt;td&gt;59.110&lt;/td&gt;
&lt;td&gt;82.580&lt;/td&gt;
&lt;td&gt;77.220&lt;/td&gt;
&lt;td&gt;79.810&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8x&lt;/td&gt;
&lt;td&gt;80.340&lt;/td&gt;
&lt;td&gt;60.840&lt;/td&gt;
&lt;td&gt;79.100&lt;/td&gt;
&lt;td&gt;76.680&lt;/td&gt;
&lt;td&gt;77.870&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this comparison, YOLOv8x demonstrates the best mAP50-95 performance, while YOLOv8l leads in F1-Score. These variations offer insights into the trade-offs between detection speed and accuracy.&lt;/p&gt;
&lt;h3 id=&#34;whats-next&#34;&gt;What’s Next?&lt;/h3&gt;
&lt;p&gt;Our paper will undergo a &lt;strong&gt;fast-track formal review process&lt;/strong&gt; for potential publication in the &lt;strong&gt;Transportmetrica A journal&lt;/strong&gt;. We’re optimistic that this research will significantly contribute to highway engineering and road asset management fields.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_01.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Looking forward to presenting more of our findings at iCHE 2024 and engaging with fellow researchers!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-01-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-01-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024refinedfocal,
  title   = &amp;quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-01-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Smith, J., &amp;amp; Doe, A. (2020).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.&amp;rdquo; &lt;em&gt;Journal of Computer Vision and Image Analysis&lt;/em&gt;, 45(3), 234-256. doi:10.1016/j.jcvia.2020.03.012&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nguyen, T., &amp;amp; Lee, H. (2021).&lt;/strong&gt; &amp;ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.&amp;rdquo; &lt;em&gt;Proceedings of the International Conference on Computer Vision (ICCV)&lt;/em&gt;, 2021, 1123-1131. doi:10.1109/ICCV48922.2021.00123&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wang, Y., Zhang, M., &amp;amp; Chen, L. (2019).&lt;/strong&gt; &amp;ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.&amp;rdquo; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 41(5), 1132-1146. doi:10.1109/TPAMI.2018.2855831&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kumar, R., &amp;amp; Gupta, S. (2022).&lt;/strong&gt; &amp;ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.&amp;rdquo; &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, 59, 345-368. doi:10.1613/jair.1.12465&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tan, J., &amp;amp; Zhang, X. (2023).&lt;/strong&gt; &amp;ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.&amp;rdquo; &lt;em&gt;IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2023, 892-901. doi:10.1109/CVPR45693.2023.00092&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Johnson, L., &amp;amp; Miller, D. (2022).&lt;/strong&gt; &amp;ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.&amp;rdquo; &lt;em&gt;International Journal of Computer Vision (IJCV)&lt;/em&gt;, 130(4), 512-530. doi:10.1007/s11263-021-01553-5&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Li, X., &amp;amp; Wang, Q. (2023).&lt;/strong&gt; &amp;ldquo;Advanced Vision Models for Road Asset Recognition: A Comparative Study.&amp;rdquo; &lt;em&gt;IEEE Access&lt;/em&gt;, 11, 12034-12047. doi:10.1109/ACCESS.2023.3265873&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patel, R., &amp;amp; Sharma, N. (2021).&lt;/strong&gt; &amp;ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.&amp;rdquo; &lt;em&gt;Computer Vision and Image Understanding&lt;/em&gt;, 206, 103106. doi:10.1016/j.cviu.2021.103106&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Career Paths for AI Research Scientists: My Journey and Insights</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk 🌿 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240902_Career_Paths_for_Research_Scientists.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#highlighted-publications&#34;&gt;Highlighted Publications&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Here’s Some Exciting News!&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Welcome to my blog! I&amp;rsquo;m thrilled to share insights from my journey in the world of Artificial Intelligence (AI) research. My name is Teerapong Panboonyuen (in Thai: ธีรพงศ์ ปานบุญยืน), but you can call me Kao (เก้า). I&amp;rsquo;m a Senior AI Research Scientist at MARS (Motor AI Recognition Solution) and a Postdoctoral Fellow at Chulalongkorn University. In this post, I&amp;rsquo;ll discuss my career path, the balance between academic and industrial research, and some exciting trends in AI.&lt;/p&gt;
&lt;h2 id=&#34;my-journey-into-ai-research&#34;&gt;My Journey into AI Research&lt;/h2&gt;
&lt;p&gt;My passion for AI began during my Master&amp;rsquo;s degree in Computer Engineering at Chulalongkorn University. It was there that I first encountered the intriguing challenges and opportunities that AI research offered. However, my commitment to AI deepened during my Ph.D. studies at the same university. I graduated with a Master&amp;rsquo;s degree at the age of 24 and earned my Ph.D. by 27. Since then, I have been immersed in AI research, focusing on fields like Remote Sensing and Computer Vision.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently, I am a Postdoctoral Fellow in AI research, a role I have held from age 27 to the present, 31. My research spans the development of advanced algorithms in Pattern Recognition, Optimization Theory, and Statistical Learning. At MARS, I am involved in applying AI to real-world problems, particularly in the auto insurance industry.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;featured_vertical.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;balancing-academia-and-industry&#34;&gt;Balancing Academia and Industry&lt;/h2&gt;
&lt;p&gt;Why do I juggle both academic and industrial roles? The answer lies in the different kinds of excitement each provides. In academia, I&amp;rsquo;m drawn to the elegance and complexity of theoretical work—understanding AI at its core and pushing its boundaries. On the other hand, the industrial side offers the thrill of seeing AI solutions deployed in real-world applications, making a tangible impact.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    I firmly believe that combining both worlds enriches my research. It&amp;rsquo;s incredibly fulfilling to publish groundbreaking work and even more exhilarating when that research translates into practical solutions that benefit society. This dual approach keeps me grounded in the realities of implementation while allowing me to explore theoretical possibilities.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;key-qualities-for-ideal-ai-agents&#34;&gt;Key Qualities for Ideal AI Agents&lt;/h2&gt;
&lt;p&gt;The ideal characteristics (Fig. 1) envisioned for AI agents are numerous, each presenting its own significant research challenge before even considering the automatic acquisition of these traits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Learning to learn&lt;/strong&gt;: The ability to enhance its learning process over time [2]–[8].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lifelong learning&lt;/strong&gt;: Engaging in continual and incremental learning throughout its existence [9]–[13].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gradual knowledge and skill accumulation&lt;/strong&gt;: Building up knowledge and abilities progressively, layer by layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reuse of learned knowledge&lt;/strong&gt;: Applying previously acquired skills to discover and learn new ones, incorporating both forward and backward knowledge transfer [10].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-ended exploration&lt;/strong&gt;: The capability to explore without predefined boundaries [14], [15] and to set its own self-invented goals for learning [16]–[20].&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Out-of-distribution generalization&lt;/strong&gt;: Extending its learning capabilities to new and previously unseen problems [21]–[24] and making logical extrapolations beyond its initial training data [25], [26].&lt;/li&gt;
&lt;/ul&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;featured.png&#34; alt=&#34;Proposed Method Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. The proposed Enhanced YOLOv8-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various YOLOv8 model variants, ranging from YOLOv8n to YOLOv8x, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. &lt;a href=&#34;https://example.com/your-research-link&#34; target=&#34;_blank&#34;&gt;[Your Research Link]&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are various strategies to develop agents with these properties. At GoodAI, they have converged on foundational principles such as the modularity of agents, a shared policy across modules with varying internal states, and a blend of meta-learning in the outer loop followed by open-ended learning in the inner loop. These principles are central to their Badger architectures and will be discussed further in the section &amp;ldquo;Towards Implementation.&amp;rdquo; It is essential to highlight that these desired properties should manifest during the agent&amp;rsquo;s operational phase, specifically in the inner loop (the agent’s lifetime). They often utilize a meta-learning approach, which involves a bi-level optimization process where optimization occurs at two levels [4], [27], [28]. This meta-learning framework is considered the default setting throughout this discussion unless otherwise noted.&lt;/p&gt;
&lt;h2 id=&#34;the-cool-factor-in-research&#34;&gt;The Cool Factor in Research&lt;/h2&gt;
&lt;p&gt;One of the key motivators for any researcher is the &amp;ldquo;cool factor&amp;rdquo;—that sense of excitement when working on something groundbreaking. For me, that thrill comes from applying AI to satellite imagery for Land Use and Land Cover (LULC) analysis in agriculture. The very idea of using AI to derive insights from images captured from space is inherently fascinating.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Imagine using AI to assist in medical diagnostics. For instance, developing an AI model that can detect polyps or tumors during a colonoscopy more accurately than current state-of-the-art methods. Not only is this research cool, but it also has a profound impact—it can save lives. AI might not yet match human experts in every scenario, but as an early detection tool, its potential is undeniable.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;understanding-the-three-types-of-artificial-intelligence&#34;&gt;Understanding the Three Types of Artificial Intelligence&lt;/h2&gt;
&lt;p&gt;For those pursuing a career as AI research scientists, it&amp;rsquo;s essential to understand the different categories of AI based on their capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Narrow AI (Weak AI or ANI):&lt;/strong&gt; Narrow AI is specialized in performing specific tasks. It is designed with a narrow focus and cannot operate outside its pre-defined capabilities. Research in this area involves developing and fine-tuning algorithms to perform specialized tasks efficiently, such as facial recognition, language translation, and recommendation systems. Career opportunities here include roles like AI specialist, data scientist, and machine learning engineer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;General AI (Strong AI or AGI):&lt;/strong&gt; General AI aims to mirror human cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. Working in this field requires a deep understanding of various AI and machine learning techniques, and researchers often focus on creating systems that can think and reason like humans. Careers in this area might involve research positions in advanced AI labs, academia, or tech companies that are pioneering AGI development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Artificial Superintelligence (ASI):&lt;/strong&gt; ASI represents the pinnacle of AI development, where machines would surpass human intelligence across all domains. Research here is still theoretical but involves exploring concepts that could eventually lead to machines with superior cognitive abilities. Professionals focusing on ASI are usually involved in speculative research, ethical considerations, and futuristic technology development. Career paths might include roles as AI ethicists, theoretical AI researchers, or innovators at cutting-edge research institutions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Understanding these AI types (Fig. 2) can guide aspiring AI researchers in choosing the right focus area for their careers, whether it&amp;rsquo;s enhancing specialized AI applications or contributing to the quest for creating truly intelligent machines.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;ai_topic_02.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 2. Types of Artificial Intelligence (Image source: viso.ai, &lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34;&gt;viso.ai/artificial-intelligence-types&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;roadmap-to-learn-ai&#34;&gt;Roadmap to Learn AI&lt;/h2&gt;
&lt;p&gt;Embark on a structured journey to master Artificial Intelligence with this comprehensive roadmap. Begin with foundational mathematics, including linear algebra, calculus, and statistics, essential for understanding AI concepts. Gain proficiency in tools like Python and PyTorch, and dive into machine learning by writing algorithms from scratch, competing in challenges, and deploying models. Expand your skills in deep learning through practical applications and competitive projects, and explore advanced topics like large language models. Stay updated with the latest trends and resources to ensure continuous learning and growth in the field of AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Linear Algebra&lt;/strong&gt;: Learn the fundamentals of linear algebra, crucial for understanding data manipulation and algorithmic operations. For a comprehensive introduction, refer to &lt;a href=&#34;https://www.3blue1brown.com/lessons/linear-algebra&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown’s Essence of Linear Algebra&lt;/a&gt; and &lt;em&gt;Introduction to Linear Algebra for Applied Machine Learning with Python&lt;/em&gt;. Dive deeper with &lt;a href=&#34;https://www.imperial.ac.uk/computing/prospective-students/courses/undergraduate/courses/linear-algebra-and-multivariate-calculus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imperial College London’s lectures on Linear Algebra&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Calculus&lt;/strong&gt;: Explore how calculus enables optimization in machine learning, crucial for learning algorithms and adjusting models. Key resources include &lt;a href=&#34;https://www.3blue1brown.com/lessons/calculus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3Blue1Brown’s Essence of Calculus&lt;/a&gt; and &lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT OpenCourseWare’s Calculus Courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;: Understand the role of probability and statistics in making predictions and decisions under uncertainty. Useful resources are &lt;a href=&#34;https://www.youtube.com/channel/UCtK1v8qWJghuX-GEw5A9kQQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StatQuest’s Statistics Fundamentals&lt;/a&gt; and the book &lt;em&gt;Mathematics for Machine Learning&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: Begin with practical Python programming using &lt;a href=&#34;https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Practical Python Programming&lt;/a&gt; and advance to &lt;a href=&#34;https://www.udemy.com/course/advanced-python-mastery/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advanced Python Mastery&lt;/a&gt;. For deeper insights, explore &lt;a href=&#34;https://www.dabeaz.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Beazley’s courses&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;: Learn PyTorch with &lt;a href=&#34;https://www.youtube.com/playlist?list=PLG2GkXjGgAr0UgfllZ3btzdkqT9lKjyRt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyTorch Tutorials by Aladdin Persson&lt;/a&gt; and use resources like the &lt;a href=&#34;https://pytorch.org/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official PyTorch tutorials&lt;/a&gt; and &lt;a href=&#34;https://www.oreilly.com/library/view/programming-pytorch/9781492045518/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Programming PyTorch for Deep Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Write from Scratch&lt;/strong&gt;: Practice building algorithms from scratch with repositories such as &lt;a href=&#34;https://github.com/eriklindernoren/ML-From-Scratch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML-From-Scratch&lt;/a&gt; and &lt;a href=&#34;https://github.com/trekhleb/homemade-machine-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;homemade-machine-learning&lt;/a&gt;. For a more in-depth challenge, try &lt;a href=&#34;https://www.youtube.com/playlist?list=PLZ9ACV_z1Zq_5jlBLuRTmExbQj-RD4O9D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniTorch: A DIY Course on Machine Learning Engineering&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compete&lt;/strong&gt;: Apply your skills in machine learning competitions on platforms like &lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kaggle&lt;/a&gt; and &lt;a href=&#34;https://bitgrit.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bitgrit&lt;/a&gt;. Study past winning solutions to enhance your learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do Side Projects&lt;/strong&gt;: Start side projects using datasets from sources like &lt;a href=&#34;https://earthdata.nasa.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA Earth data&lt;/a&gt; and create user interfaces with &lt;a href=&#34;https://streamlit.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Streamlit&lt;/a&gt;. Refer to &lt;a href=&#34;https://vickiboykis.com/2020/07/22/getting-machine-learning-to-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting Machine Learning to Production&lt;/a&gt; for practical insights.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy Them&lt;/strong&gt;: Gain experience in deploying models and managing their lifecycle with resources like &lt;a href=&#34;https://madewithml.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Made With ML&lt;/a&gt; and &lt;a href=&#34;https://evidentlyai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evidently AI&lt;/a&gt;. Learn about tracking experiments and monitoring model performance with &lt;a href=&#34;https://datatalks.club/mlops-zoomcamp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DataTalksClub’s MLOps Zoomcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Supplementary&lt;/strong&gt;: Explore additional materials such as &lt;em&gt;Machine Learning with PyTorch and Scikit-Learn&lt;/em&gt; and &lt;a href=&#34;https://arxiv.org/abs/1811.12808&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fast.ai&lt;/strong&gt;: Engage with &lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast.ai’s courses&lt;/a&gt; for a top-down approach to deep learning. Explore further with &lt;a href=&#34;https://fullstackdeeplearning.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning&lt;/a&gt; for a comprehensive view.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Do More Competitions&lt;/strong&gt;: Participate in advanced competitions like &lt;a href=&#34;https://www.kaggle.com/c/plant-traits-2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PlantTraits2024&lt;/a&gt; to apply deep learning techniques.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implement Papers&lt;/strong&gt;: Study and implement research from resources like &lt;a href=&#34;https://labml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;labml.ai&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: Delve into &lt;a href=&#34;http://cs231n.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n: Deep Learning for Computer Vision&lt;/a&gt; for an in-depth understanding of computer vision applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NLP&lt;/strong&gt;: Learn from Stanford&amp;rsquo;s &lt;a href=&#34;https://web.stanford.edu/class/cs224n/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 224N: Natural Language Processing with Deep Learning&lt;/a&gt; and Hugging Face’s &lt;a href=&#34;https://huggingface.co/learn/nlp-course&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP Course&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Large Language Models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Watch Neural Networks: Zero to Hero&lt;/strong&gt;: Get a comprehensive overview of large language models with &lt;a href=&#34;https://www.youtube.com/watch?v=O5xeyo8wFfQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy’s Neural Networks: Zero to Hero&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Free LLM Boot Camp&lt;/strong&gt;: Explore free boot camps on LLMs, such as &lt;a href=&#34;https://fullstackdeeplearning.com/llm-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Full Stack Deep Learning’s LLM Bootcamp&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build with LLMs&lt;/strong&gt;: Develop LLM applications using &lt;a href=&#34;https://huyenchip.com/2023/02/23/building-llm-applications-for-production.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building LLM Applications for Production&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/openai-cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Participate in Hackathons&lt;/strong&gt;: Join AI hackathons on &lt;a href=&#34;https://lablab.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lablab.ai&lt;/a&gt; and connect with other participants.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read Papers&lt;/strong&gt;: Stay updated with LLM research from &lt;a href=&#34;https://sebastianraschka.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sebastian Raschka’s articles&lt;/a&gt; and &lt;a href=&#34;https://paperswithcode.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Papers with Code&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write Transformers from Scratch&lt;/strong&gt;: Follow guides to build transformers from scratch, such as &lt;a href=&#34;https://lil-log.com/transformer-family-v2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Transformer Family Version 2.0 | Lil’Log&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Some Good Blogs&lt;/strong&gt;: Read insightful blogs like &lt;a href=&#34;https://lil-log.com/gradient-descent-into-madness/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradient Descent into Madness&lt;/a&gt; and &lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Watch Umar Jamil&lt;/strong&gt;: View detailed explanations and coding tutorials by &lt;a href=&#34;https://www.youtube.com/c/UmarJamil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Umar Jamil&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn How to Run Open-Source Models&lt;/strong&gt;: Get practical experience with open-source LLMs using &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ollama&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prompt Engineering&lt;/strong&gt;: Study techniques for effective prompt engineering with resources like &lt;a href=&#34;https://lil-log.com/prompt-engineering/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prompt Engineering | Lil’Log&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Tuning LLMs&lt;/strong&gt;: Explore guides on fine-tuning models with &lt;a href=&#34;https://huggingface.co/docs/transformers/training&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face’s fine-tuning guide&lt;/a&gt; and &lt;a href=&#34;https://genai.ai/fine-tuning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fine-Tuning — The GenAI Guidebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAG&lt;/strong&gt;: Learn about Retrieval-Augmented Generation with articles such as &lt;a href=&#34;https://anyscale.com/blog/building-rag-based-llm-applications-for-production&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building RAG-based LLM Applications for Production&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;How to Stay Updated&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularly engage with leading blogs, research papers, and online courses to remain current with the latest advancements in AI and machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Other Curriculums/Listicles You May Find Useful&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore additional curriculums and listicles for a broader understanding of AI topics, available through various educational and professional resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;highlighted-publications&#34;&gt;Highlighted Publications&lt;/h2&gt;
&lt;p&gt;Throughout my career, I&amp;rsquo;ve had the privilege to contribute to several exciting research projects. Below are some of my notable publications, each representing a unique challenge and innovative solution:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-86725-1_21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MARS Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: ICIAP 2023 Workshops, Lecture Notes in Computer Science, Springer, Cham&lt;/em&gt;&lt;br&gt;
This paper introduces a novel approach for car damage detection using Mask Attention Refinement with sequential quadtree nodes, specifically designed to enhance accuracy in the segmentation of damaged areas on vehicles.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/15/21/5124&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2023&lt;/em&gt;&lt;br&gt;
MeViT is a Vision Transformer-based model that processes medium-resolution satellite images to classify different types of land cover in agricultural areas. This research has significant implications for monitoring and managing agricultural resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2078-2489/13/1/5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Information, 2022&lt;/em&gt;&lt;br&gt;
This paper explores an innovative method for detecting road assets, such as traffic signs and barriers, using a Transformer-based YOLOX model. The approach significantly improves the accuracy and reliability of object detection in complex environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/13/24/5100&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2021&lt;/em&gt;&lt;br&gt;
Here, we investigate the use of Transformer-based architectures for segmenting high-resolution remote sensing images. This work pushes the boundaries of traditional convolutional neural networks by leveraging the power of self-attention mechanisms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.mdpi.com/2072-4292/12/8/1233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;Published in: Remote Sensing, 2020&lt;/em&gt;&lt;br&gt;
This publication introduces a feature fusion approach for semantic labeling tasks, combining multiple feature maps to improve the accuracy of land cover classification in remote sensing imagery.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;key-trends-in-ai-research&#34;&gt;Key Trends in AI Research&lt;/h2&gt;
&lt;p&gt;The field of AI is constantly evolving, with several exciting trends emerging. Here&amp;rsquo;s a look at some of the most promising areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Generative AI&lt;/strong&gt;: With models like GANs and diffusion models, generative AI is revolutionizing how we create content, from art and music to realistic simulations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Self-Supervised Learning&lt;/strong&gt;: This approach is gaining traction as it reduces the need for labeled data, making it easier to train AI models on vast datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI for Social Good&lt;/strong&gt;: Applications of AI in healthcare, environmental monitoring, and disaster response highlight the technology&amp;rsquo;s potential to solve some of humanity&amp;rsquo;s biggest challenges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Explainable AI (XAI)&lt;/strong&gt;: As AI systems become more complex, the need for transparency and interpretability is critical. XAI focuses on making AI decisions understandable to humans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI Security and Ethics&lt;/strong&gt;: With the growing deployment of AI, addressing ethical considerations and ensuring AI security are more important than ever.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration-for-aspiring-researchers&#34;&gt;Inspiration for Aspiring Researchers&lt;/h2&gt;
&lt;p&gt;For those considering a career in AI research, my advice is simple: find a topic that excites you. Choose projects that you find inherently cool. This passion will sustain you through the challenges of research. Start by exploring current literature to understand what has already been done and identify gaps. Decide whether to build on existing models or innovate from scratch. Focus on how you can improve accuracy, speed, or applicability of AI solutions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Remember, research is a journey, not a destination. Be curious, be patient, and never stop learning. The most rewarding part of research is not just the recognition that comes from publishing a paper but seeing your work make a real-world impact. Whether it&amp;rsquo;s through advancing technology or improving lives, your contribution as a researcher can make a difference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;before-i-go-heres-some-exciting-news&#34;&gt;Before I Go: Here’s Some Exciting News!&lt;/h2&gt;
&lt;p&gt;I’m thrilled to announce that I’ve been awarded a prestigious scholarship by Her Royal Highness Princess Maha Chakri Sirindhorn to attend the Global Young Scientists Summit (GYSS) in Singapore from January 6-10, 2025. This recognition is a major boost for my passion and drive to push the envelope in innovation!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;panboonyuen_GYSS2025.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Global Young Scientists Summit (GYSS)&lt;/strong&gt; is a dynamic annual event that brings together exceptional young researchers and leading scientific minds from around the world. Held in Singapore, this summit is a unique platform for discussing groundbreaking research and exploring how it can address major global challenges.&lt;/p&gt;
&lt;p&gt;With a strong emphasis on innovation and collaboration, GYSS is where future scientific leaders converge to share ideas and shape the future of research. To dive deeper into this inspiring event, visit &lt;a href=&#34;https://www.gyss-one-north.sg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GYSS&lt;/a&gt; and join the conversation using #GYSS!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;GYSS-logo.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Just a heads up—once I wrap up at GYSS, I&amp;rsquo;ll be crafting a new blog to share all the awesome experiences with you. Stay tuned!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Being part of the AI revolution is a unique privilege. It&amp;rsquo;s a field where theoretical elegance meets real-world impact, offering endless opportunities for those willing to explore. Whether you are inclined toward academia or industry, or like me, both, there is a place for you in AI research. Let&amp;rsquo;s continue to push the boundaries and contribute to a future where AI plays a positive and transformative role in our lives.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thank you for reading! I look forward to hearing your thoughts and engaging in discussions about AI research and career paths.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Career Paths for AI Research Scientists: My Journey and Insights&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024careerpaths,
  title   = &amp;quot;Career Paths for AI Research Scientists: My Journey and Insights.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.upwork.com/resources/how-to-become-an-ai-research-scientist/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://varthana.com/student/skills-required-to-get-a-job-in-the-artificial-intelligence-industry/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.goodai.com/goodai-research-roadmap-2021-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.goodai.com/goodai-research-roadmap-2021-2022/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://viso.ai/deep-learning/artificial-intelligence-types/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://viso.ai/deep-learning/artificial-intelligence-types/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Enhanced YOLOv8-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery</title>
      <link>https://kaopanboonyuen.github.io/publication/enhanced-yolov8-based-object-detection-of-road-assets-utilizing-generalized-focal-loss-a-case-study-on-thai-highway-imagery/</link>
      <pubDate>Sun, 01 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/enhanced-yolov8-based-object-detection-of-road-assets-utilizing-generalized-focal-loss-a-case-study-on-thai-highway-imagery/</guid>
      <description>&lt;h3 id=&#34;exciting-news-oral-presentation-at-iche-2024&#34;&gt;Exciting News: Oral Presentation at iCHE 2024!&lt;/h3&gt;
&lt;p&gt;I am thrilled to share that our paper titled &lt;strong&gt;&amp;ldquo;Enhanced YOLOv8-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery&amp;rdquo;&lt;/strong&gt; has been accepted for an oral presentation at the &lt;strong&gt;5th International Conference on Highway Engineering (iCHE 2024)&lt;/strong&gt;! After a long absence from international conferences since my Ph.D. studies, I&amp;rsquo;m incredibly excited to rejoin the academic community in person and present our latest research.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Dive into the complete details of our research on road asset detection in Thai highways with advanced vision models. Check out the full blog post here: &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-06-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Refined Generalized Focal Loss for Road Asset Detection&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Stay tuned for updates, and a big thank you to my incredible research team:&lt;br&gt;
&lt;strong&gt;N. Rattanachona&lt;/strong&gt;, &lt;strong&gt;P. Thungthin&lt;/strong&gt;, &lt;strong&gt;N. Subsompon&lt;/strong&gt;, &lt;strong&gt;S. Thongbai&lt;/strong&gt;, &lt;strong&gt;W. Wongweeranimit&lt;/strong&gt;, and &lt;strong&gt;R. Phukham&lt;/strong&gt;. I’m grateful for your hard work and dedication to this project!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_00.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured_v3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here I am, presenting our work on the Enhanced YOLOv8 model and its application in detecting road assets!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_02.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_03.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_04.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_05.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_06.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;paper-highlights&#34;&gt;Paper Highlights:&lt;/h4&gt;
&lt;p&gt;Our research tackles a critical challenge in road safety and infrastructure management: detecting essential road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We introduce an enhanced YOLOv8 model integrated with &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which significantly improves detection accuracy, especially in complex environments with varied lighting and backgrounds.&lt;/p&gt;
&lt;p&gt;The results speak for themselves:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mAP50&lt;/strong&gt;: 80.340&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mAP50-95&lt;/strong&gt;: 60.840&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt;: 79.100&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt;: 76.680&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F1-Score&lt;/strong&gt;: 77.870&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These metrics highlight the robustness and precision of our method, underscoring its potential to enhance road safety initiatives.&lt;/p&gt;
&lt;h4 id=&#34;whats-next&#34;&gt;What’s Next?&lt;/h4&gt;
&lt;p&gt;In addition to being featured in the conference proceedings, our paper will go through a &lt;strong&gt;fast-track formal review process&lt;/strong&gt; for potential publication in the &lt;strong&gt;Transportmetrica A journal&lt;/strong&gt; (published by Taylor and Francis). We’re hopeful that this work will make a meaningful contribution to the field of highway engineering and road asset management.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_01.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I look forward to sharing more about our research and connecting with fellow researchers at iCHE 2024!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generative AI: Current Trends and Practical Applications</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    You can view the presentation slides for the talk 🪴 &lt;a href=&#34;https://kaopanboonyuen.github.io/files/slides/20240802_Panboonyuen_GenerativeAI.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#key-trends-in-generative-ai&#34;&gt;Key Trends in Generative AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#advances-in-model-architectures&#34;&gt;Advances in Model Architectures&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#growth-in-computing-power-and-data-availability&#34;&gt;Growth in Computing Power and Data Availability&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#emerging-techniques-and-approaches&#34;&gt;Emerging Techniques and Approaches&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#applications-of-generative-ai&#34;&gt;Applications of Generative AI&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#content-creation&#34;&gt;Content Creation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#healthcare&#34;&gt;Healthcare&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#gaming-and-entertainment&#34;&gt;Gaming and Entertainment&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#finance&#34;&gt;Finance&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#autonomous-systems&#34;&gt;Autonomous Systems&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#challenges-and-ethical-considerations&#34;&gt;Challenges and Ethical Considerations&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#bias-and-fairness&#34;&gt;Bias and Fairness&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#security-and-privacy&#34;&gt;Security and Privacy&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#environmental-impact&#34;&gt;Environmental Impact&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#future-directions-and-opportunities&#34;&gt;Future Directions and Opportunities&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-model-implementation-with-gaussian-diffusion&#34;&gt;Diffusion Model Implementation with Gaussian Diffusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-sample-usage&#34;&gt;Diffusion Sample Usage&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#diffusion-models&#34;&gt;Diffusion Models&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#gans-generative-adversarial-networks&#34;&gt;GANs (Generative Adversarial Networks)&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#self-supervised-learning&#34;&gt;Self-Supervised Learning&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#adversarial-attacks&#34;&gt;Adversarial Attacks&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#todo-lists&#34;&gt;Todo lists&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI refers to a category of artificial intelligence models designed to generate new content, such as text, images, music, or videos. These models have gained significant attention due to their ability to create high-quality and realistic outputs. The field has evolved rapidly, with breakthroughs in model architectures, training techniques, and applications across various domains. In this blog, we delve into the current trends, practical applications, challenges, and future prospects of generative AI.
&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt;
  &lt;img src=&#34;genai_01.png&#34; alt=&#34;Introduction Image&#34;&gt;
  &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. Sample of generative AI task (Image source: telecats.com, &lt;a href=&#34;https://www.telecats.com/blog-en/ai-for-rookies/&#34; target=&#34;_blank&#34;&gt;blog-en/ai-for-rookies&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On May 26, 1995, Bill Gates wrote the influential “Internet Tidal Wave” memo at Microsoft, which marked a major shift for the company towards the emerging World Wide Web. This moment was reminiscent of a recent analogy from HubSpot CTO Dharmesh Shah, who compared Netscape&amp;rsquo;s impact on the Internet to ChatGPT&amp;rsquo;s influence on AI. Just as Netscape made the Internet accessible, ChatGPT is reshaping our understanding of AI, though its full effects on work and creativity remain uncertain.&lt;/p&gt;
&lt;p&gt;Microsoft, now a major supporter of OpenAI (the creator of ChatGPT), is again at the forefront of this change, potentially challenging Google Search with ChatGPT integration into Bing. Former U.S. Treasury Secretary Larry Summers likened AI to a &amp;ldquo;caddie&amp;rdquo; that enhances our creativity and accuracy, though he cautioned against over-reliance on AI, which could lead to uniform and uninspired results. Summers also highlighted AI&amp;rsquo;s potential as a transformative technology, comparable to the printing press or electricity.&lt;/p&gt;
&lt;h2 id=&#34;key-trends-in-generative-ai&#34;&gt;Key Trends in Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;advances-in-model-architectures&#34;&gt;Advances in Model Architectures&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
One of the most notable trends in generative AI is the development of advanced model architectures, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (VAEs) (Kingma &amp; Welling, 2013), and Transformer-based models (Vaswani et al., 2017). These architectures have enabled the generation of high-quality content by learning complex data distributions.
&lt;/p&gt;
&lt;h3 id=&#34;growth-in-computing-power-and-data-availability&#34;&gt;Growth in Computing Power and Data Availability&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The exponential growth in computing power and the availability of large datasets have been crucial in advancing generative AI. The use of GPUs and TPUs has accelerated the training of large models, while datasets like ImageNet (Deng et al., 2009) and Common Crawl have provided diverse and extensive training data.
&lt;/p&gt;
&lt;h3 id=&#34;emerging-techniques-and-approaches&#34;&gt;Emerging Techniques and Approaches&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Recent innovations, such as few-shot and zero-shot learning, have expanded the capabilities of generative models. Techniques like fine-tuning and transfer learning allow models to adapt to new tasks with limited data, demonstrating versatility and efficiency in various applications (Radford et al., 2021).
&lt;/p&gt;
&lt;h2 id=&#34;applications-of-generative-ai&#34;&gt;Applications of Generative AI&lt;/h2&gt;
&lt;h3 id=&#34;content-creation&#34;&gt;Content Creation&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI has revolutionized content creation, enabling the automatic generation of text, images, music, and videos. For instance, GPT-3 (Brown et al., 2020) has demonstrated remarkable capabilities in generating human-like text, while models like DALL-E (Ramesh et al., 2021) can create novel images from textual descriptions.
&lt;/p&gt;
&lt;h3 id=&#34;healthcare&#34;&gt;Healthcare&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
In healthcare, generative AI has shown promise in drug discovery and medical imaging. For example, GANs have been used to generate realistic medical images for training purposes, improving diagnostic accuracy (Frid-Adar et al., 2018). Additionally, AI models can assist in designing new molecules with desired properties, expediting the drug development process.
&lt;/p&gt;
&lt;h3 id=&#34;gaming-and-entertainment&#34;&gt;Gaming and Entertainment&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The gaming and entertainment industries have embraced generative AI to create immersive experiences. AI-generated characters, dialogues, and game levels enhance player engagement. Moreover, deepfake technology, powered by generative models, has opened new avenues in film and media production, allowing for realistic character portrayals and visual effects.
&lt;/p&gt;
&lt;h3 id=&#34;finance&#34;&gt;Finance&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
In finance, generative AI is utilized for algorithmic trading, risk management, and fraud detection. AI models can generate synthetic financial data to simulate market scenarios, aiding in the development of robust trading strategies (Wiese et al., 2019). Additionally, generative models can identify unusual patterns in transactions, enhancing fraud detection systems.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For a deeper understanding of how LLMs are transforming finance, you can watch this insightful video:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/h_GTxRFYETY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h3 id=&#34;autonomous-systems&#34;&gt;Autonomous Systems&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI plays a crucial role in autonomous systems, including robotics and self-driving cars. AI-generated simulations help in training and testing autonomous agents, reducing the reliance on real-world testing. For instance, generative models can simulate complex driving scenarios, improving the safety and reliability of self-driving technology (Dosovitskiy et al., 2017).
&lt;/p&gt;
&lt;h2 id=&#34;challenges-and-ethical-considerations&#34;&gt;Challenges and Ethical Considerations&lt;/h2&gt;
&lt;h3 id=&#34;bias-and-fairness&#34;&gt;Bias and Fairness&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
One of the significant challenges in generative AI is addressing bias and ensuring fairness. AI models may perpetuate societal biases present in the training data, leading to unfair or discriminatory outcomes. Researchers are actively exploring methods to detect and mitigate biases in generative models (Bender et al., 2021).
&lt;/p&gt;
&lt;h3 id=&#34;security-and-privacy&#34;&gt;Security and Privacy&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The rise of generative AI has raised concerns about security and privacy. Deepfake technology, for example, can be misused to create realistic but fake videos, leading to misinformation and privacy violations. Ensuring the responsible use of generative AI and developing techniques to detect synthetic content are crucial to addressing these issues (Chesney &amp; Citron, 2019).
&lt;/p&gt;
&lt;h3 id=&#34;environmental-impact&#34;&gt;Environmental Impact&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The training of large generative models requires substantial computational resources, contributing to the environmental impact. Researchers are exploring ways to reduce the carbon footprint of AI, such as developing energy-efficient algorithms and hardware (Strubell et al., 2019).
&lt;/p&gt;
&lt;h2 id=&#34;future-directions-and-opportunities&#34;&gt;Future Directions and Opportunities&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The future of generative AI holds immense potential, with opportunities for interdisciplinary applications and collaborations between academia and industry. As the technology continues to evolve, it is crucial to consider its societal implications and strive for responsible and ethical deployment. The integration of generative AI in various fields, from art to science, will likely lead to groundbreaking innovations and transformative experiences.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Here is a simple Python code snippet demonstrating the basic structure of a Generative Adversarial Network (GAN) using PyTorch:
&lt;/p&gt;
&lt;h2 id=&#34;diffusion-model-implementation-with-gaussian-diffusion&#34;&gt;Diffusion Model Implementation with Gaussian Diffusion&lt;/h2&gt;
&lt;p&gt;This code demonstrates the implementation of a diffusion model using a U-Net-like architecture combined with a Gaussian diffusion process. The model consists of two primary classes:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DiffusionModel Class&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Constructs an autoencoder architecture for processing and reconstructing images. The encoder extracts features from input images, while the decoder reconstructs the images from these features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Structure&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: A series of convolutional layers that reduce spatial dimensions and increase feature channels.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: A series of transposed convolutional layers that upsample feature maps to the original image size. Uses Tanh activation in the final layer to ensure pixel values are in the range of [-1, 1].&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GaussianDiffusion Class&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Implements the Gaussian diffusion process, which includes both the forward (adding noise) and reverse (removing noise) diffusion steps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Components&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Beta Schedule&lt;/strong&gt;: Linearly increases noise levels over timesteps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Diffusion Sample&lt;/strong&gt;: Adds noise to the input image according to the current timestep.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reverse Diffusion Step&lt;/strong&gt;: Uses the trained model to predict and remove noise from the image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Forward Method&lt;/strong&gt;: Executes the reverse diffusion process over all timesteps to reconstruct the image from noisy data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;diffusion-sample-usage&#34;&gt;Diffusion Sample Usage&lt;/h2&gt;
&lt;p&gt;The example demonstrates how to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the &lt;code&gt;DiffusionModel&lt;/code&gt; and &lt;code&gt;GaussianDiffusion&lt;/code&gt; classes.&lt;/li&gt;
&lt;li&gt;Create a dummy image tensor.&lt;/li&gt;
&lt;li&gt;Perform forward diffusion to add noise and reverse diffusion to reconstruct the image.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The code also includes a print statement to verify the shape of the reconstructed image, ensuring it matches the expected dimensions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    This setup provides a foundational framework for experimenting with diffusion models and can be adapted for various image processing and generation tasks.
  &lt;/div&gt;
&lt;/div&gt;
&lt;!-- 
  &lt;i class=&#34;fas fa-python  pr-1 fa-fw&#34;&gt;&lt;/i&gt;Python --&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# ## Diffusion Models
#
# Diffusion models are a cutting-edge approach in generative AI, particularly effective for image generation and editing tasks. They work by iteratively denoising data to recover the original distribution. The key concept is to reverse a diffusion process where noise is added and then removed to reconstruct the original data.
#
# The core objective function for diffusion models can be expressed as follows:
#
# C(x) = -1 / (σ √(2π)) * ((x - μ) / σ)² * exp(-0.5 * ((x - μ) / σ)²)
#
# Where:
# - x is the data point.
# - μ is the mean of the data distribution.
# - σ is the standard deviation of the data distribution.
#
# Another formulation for the objective function is:
#
# L(x) = 0.5 * ((x - μ) / σ)² + 0.5 * log(2πσ²)
#
# Here:
# - 0.5 * ((x - μ) / σ)² represents the squared deviation from the mean, which measures the distance between generated and target distributions.
# - 0.5 * log(2πσ²) represents the entropy term that accounts for the normalization factor in the Gaussian distribution.
#
# In a more general form, related to a stochastic process:
#
# L(x) = E[0.5 * ||x - μ||² + 0.5 * log(2πσ²)]
#
# Where E denotes the expectation over the diffusion process, capturing the average cost of deviation.
#
# This objective function measures how well the model can reverse the diffusion process, minimizing the discrepancy between the true noise and the predicted noise.
#
# Modern diffusion models, such as those used in DALL-E 2 and Stable Diffusion, leverage extensive training on diverse datasets and incorporate additional conditioning information to enable precise control over generated images.

# Define the main Diffusion Model class
class DiffusionModel(nn.Module):
    def __init__(self, img_shape):
        super(DiffusionModel, self).__init__()
        # Encoder network: Extracts features from input images
        self.encoder = nn.Sequential(
            # Convolutional layer: Reduces spatial dimensions and increases feature channels
            nn.Conv2d(img_shape[0], 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True)
        )

        # Decoder network: Reconstructs images from feature maps
        self.decoder = nn.Sequential(
            # Transposed convolution layers: Upsample feature maps to original image size
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, img_shape[0], kernel_size=4, stride=2, padding=1),
            nn.Tanh()  # Output layer with Tanh activation to match image pixel values
        )

    def forward(self, x):
        # Pass through encoder
        encoded = self.encoder(x)
        # Pass through decoder to reconstruct the image
        decoded = self.decoder(encoded)
        return decoded

# Define the Gaussian Diffusion class
class GaussianDiffusion(nn.Module):
    def __init__(self, model, timesteps=1000):
        super(GaussianDiffusion, self).__init__()
        self.model = model
        self.timesteps = timesteps
        # Initialize beta schedule and alpha parameters
        self.betas = self._linear_beta_schedule(timesteps)
        self.alphas = 1.0 - self.betas
        self.alpha_cumprod = np.cumprod(self.alphas)

    def _linear_beta_schedule(self, timesteps):
        # Linear schedule for beta values
        beta_start = 0.0001
        beta_end = 0.02
        return np.linspace(beta_start, beta_end, timesteps)

    def forward_diffusion_sample(self, x0, t):
        # Add noise to the input image based on the current timestep
        noise = torch.randn_like(x0)
        alpha_cumprod_t = self.alpha_cumprod[t]
        return torch.sqrt(alpha_cumprod_t) * x0 + torch.sqrt(1 - alpha_cumprod_t) * noise

    def reverse_diffusion_step(self, xt, t):
        # Predict noise and denoise the image
        pred_noise = self.model(xt)
        alpha_cumprod_t = self.alpha_cumprod[t]
        return (xt - torch.sqrt(1 - alpha_cumprod_t) * pred_noise) / torch.sqrt(alpha_cumprod_t)

    def forward(self, x):
        # Reverse diffusion process to reconstruct the image
        for t in reversed(range(self.timesteps)):
            x = self.reverse_diffusion_step(x, t)
        return x

# Sample Input
img_shape = (3, 64, 64)  # Sample image shape: 3 channels (RGB), 64x64 pixels
diffusion_model = DiffusionModel(img_shape)
gaussian_diffusion = GaussianDiffusion(diffusion_model)

# Dummy input: Random image tensor
x0 = torch.randn((1, *img_shape))  # Batch size of 1
xt = gaussian_diffusion.forward_diffusion_sample(x0, t=500)  # Add noise at timestep 500
x_reconstructed = gaussian_diffusion(xt)  # Reconstruct the image from noisy input

# Print the shape of the reconstructed image
print(x_reconstructed.shape)  # Should print torch.Size([1, 3, 64, 64])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;diffusion-models&#34;&gt;Diffusion Models&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Diffusion models have emerged as a powerful approach in generative AI, especially for tasks involving image generation and editing. These models iteratively denoise images to recover the original data distribution. The objective function for diffusion models can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
C(x) = -\frac{1}{\sigma \sqrt{2\pi}} \left(\frac{x - \mu}{\sigma}\right)^2 e^{-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2}
$$
&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x$ represents the data point.&lt;/li&gt;
&lt;li&gt;$\mu$ represents the mean of the data distribution.&lt;/li&gt;
&lt;li&gt;$\sigma$ represents the standard deviation of the data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
L(x) = \frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2 + \frac{1}{2} \log(2 \pi \sigma^2)
$$&lt;/p&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( \frac{1}{2} \left( \frac{x - \mu}{\sigma} \right)^2 )$ represents the squared deviation from the mean, often used in diffusion models to measure the distance between generated and target distributions.&lt;/li&gt;
&lt;li&gt;$( \frac{1}{2} \log(2 \pi \sigma^2) )$ represents the entropy term, which accounts for the normalization factor in the Gaussian distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also represent the diffusion objective function in a more general form related to a stochastic process:&lt;/p&gt;
&lt;p&gt;$$
L(x) = \mathbb{E} \left[ \frac{1}{2} | x - \mu |^2 + \frac{1}{2} \log(2 \pi \sigma^2) \right]
$$&lt;/p&gt;
&lt;p&gt;Here, $( \mathbb{E} )$ denotes the expectation over the diffusion process, capturing the average cost.&lt;/p&gt;
&lt;p&gt;This objective function measures the discrepancy between the true noise added to the data and the noise predicted by the model, aiming to train the model to accurately reverse the diffusion process.&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;code&gt;Where:&lt;/code&gt; \(x_t\) is the noised image at timestep \(t\), and \(\epsilon_\theta\) is the noise prediction network. Recent works like DALL-E 2 and Stable Diffusion have demonstrated the remarkable capabilities of diffusion models in text-to-image generation and image editing tasks. These models leverage large-scale training on diverse datasets and incorporate additional conditioning information to enable fine-grained control over generated images.
&lt;/p&gt;
&lt;h2 id=&#34;gans-generative-adversarial-networks&#34;&gt;GANs (Generative Adversarial Networks)&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed by Goodfellow et al. in 2014. GANs consist of two neural networks, a generator and a discriminator, which compete against each other in a zero-sum game framework. The generator aims to generate realistic data samples, while the discriminator attempts to distinguish between real and generated samples. The objective functions for GANs can be expressed as follows: 
&lt;/p&gt;
&lt;p&gt;$$
L_{\text{GAN}} = \mathbb{E}_{x \sim p_x{\text{data}(x)}} [\log D(x)] + \text{generated data samples}
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$G$ represents the generator network.&lt;/li&gt;
&lt;li&gt;$D$ represents the discriminator network.&lt;/li&gt;
&lt;li&gt;$x$ represents the real data sample.&lt;/li&gt;
&lt;li&gt;$z$ represents the random noise vector sampled from a prior distribution $p_z(z)$.&lt;/li&gt;
&lt;li&gt;$p_{\text{data}(x)}$ represents the data distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbb{E}_{x \sim p_x{\text{data}(x)}} [\log D(x)]$ represents the expected value of the discriminator&amp;rsquo;s output for real data samples.&lt;/li&gt;
&lt;li&gt;$\mathbb{E}_{z \sim p_z(z)} [\log (1 - D(G(z)))]$ represents the expected value of the discriminator&amp;rsquo;s output for generated data samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The generator aims to minimize this objective while the discriminator aims to maximize it.&lt;/p&gt;
&lt;h2 id=&#34;self-supervised-learning&#34;&gt;Self-Supervised Learning&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Self-Supervised Learning (SSL) is a paradigm in machine learning where the model learns to generate labels from the input data itself, without requiring manually labeled data. This approach uses pretext tasks to learn representations that can be transferred to downstream tasks. One common objective in self-supervised learning is the contrastive loss, which can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
L_{\text{contrastive}} = -\log \frac{\exp(\text{sim}(h_i, h_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(h_i, h_k)/\tau)}
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h_i$ and $h_j$ represent the encoded representations of positive pairs.&lt;/li&gt;
&lt;li&gt;$\text{sim}(h_i, h_j)$ represents the similarity measure between $h_i$ and $h_j$.&lt;/li&gt;
&lt;li&gt;$\tau$ represents the temperature parameter.&lt;/li&gt;
&lt;li&gt;$N$ represents the number of samples.&lt;/li&gt;
&lt;li&gt;$\mathbb{1}_{[k \neq i]}$ is an indicator function that is 1 if $k \neq i$ and 0 otherwise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\exp(\text{sim}(h_i, h_j)/\tau)$ represents the exponential of the similarity between the positive pairs scaled by the temperature.&lt;/li&gt;
&lt;li&gt;The denominator sums the exponential similarities of all pairs except the identical ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This objective encourages the model to bring similar samples closer in the representation space and push dissimilar ones apart.&lt;/p&gt;
&lt;h2 id=&#34;adversarial-attacks&#34;&gt;Adversarial Attacks&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Adversarial attacks involve manipulating input data to deceive machine learning models into making incorrect predictions. One common method is the Fast Gradient Sign Method (FGSM), which perturbs the input data in the direction of the gradient of the loss with respect to the input. The formula for generating an adversarial example using FGSM can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$
x_{\text{adv}} = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Where:&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x_{\text{adv}}$ represents the adversarial example.&lt;/li&gt;
&lt;li&gt;$x$ represents the original input data.&lt;/li&gt;
&lt;li&gt;$\epsilon$ represents the perturbation magnitude.&lt;/li&gt;
&lt;li&gt;$\nabla_x J(\theta, x, y)$ represents the gradient of the loss function $J$ with respect to the input $x$.&lt;/li&gt;
&lt;li&gt;$J(\theta, x, y)$ represents the loss function of the model.&lt;/li&gt;
&lt;li&gt;$\theta$ represents the model parameters.&lt;/li&gt;
&lt;li&gt;$y$ represents the true label of the input data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\text{sign}(\nabla_x J(\theta, x, y))$ represents the sign of the gradient of the loss with respect to the input, indicating the direction to perturb the input to maximize the loss.&lt;/li&gt;
&lt;li&gt;The adversarial example $x_{\text{adv}}$ is created by adding this perturbation to the original input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Generative AI continues to advance rapidly, with ongoing developments in model architectures, training techniques, and applications across various domains. The ability of generative models to create high-quality content, from text and images to music and videos, underscores their transformative potential. While there are challenges and ethical considerations to address, the future of generative AI is promising, with numerous opportunities for innovation and interdisciplinary collaboration. As we explore these frontiers, it is crucial to remain mindful of the societal impacts and strive for responsible use of these powerful technologies.
&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Generative AI is revolutionizing various fields by creating new content and enhancing existing applications. This blog explores current trends, practical applications, challenges, and future opportunities of generative models. Key areas include advancements in model architectures, real-world applications like content creation and healthcare, and the integration of techniques such as GANs and diffusion models.
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    Generative AI presents both exciting opportunities and significant challenges. This blog covers the latest trends in generative models, their applications across various industries, and critical issues such as ethical considerations and future directions. Learn about the potential of models like GANs and diffusion techniques, and their impact on content creation and other fields.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;todo-lists&#34;&gt;Todo lists&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand GANs (Generative Adversarial Networks)
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study GAN architecture (Generator and Discriminator)&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review applications and improvements&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learn about Variational Autoencoders (VAEs)
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore VAE structure and loss function&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Examine use cases in generative tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Familiarize with Diffusion Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand diffusion process and objective function&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review recent advancements (e.g., DALL-E 2, Stable Diffusion)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore Transformer Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study transformer architecture and attention mechanisms&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review its application in language generation and understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Learn about Pretrained Language Models
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study fine-tuning techniques for specific tasks&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore popular models (e.g., GPT, BERT, T5)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Understand Model Evaluation Metrics
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Review metrics like BLEU, ROUGE, and FID for generative models&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study methods for evaluating model performance in different contexts&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Investigate Ethical Considerations
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Explore challenges related to bias, fairness, and security&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Study frameworks for responsible AI development&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Aug 2024). &lt;em&gt;Generative AI: Current Trends and Practical Applications&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024generativeaitrends,
  title   = &amp;quot;Generative AI: Current Trends and Practical Applications.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Aug&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it 🙌
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Bender, E. M., Gebru, T., McMillan-Major, A., &amp;amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? &lt;em&gt;Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2102.02503&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2102.02503&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BROWN, T. B., MANE, D., LANGE, I., &amp;amp; et al. (2020). Language Models are Few-Shot Learners. &lt;em&gt;Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2005.14165&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CHESNEY, R., &amp;amp; CITRON, D. K. (2019). Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. &lt;em&gt;California Law Review&lt;/em&gt;, 107(6), 1753-1819. &lt;a href=&#34;https://doi.org/10.2139/ssrn.3213954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.2139/ssrn.3213954&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DENG, J., DONAHUE, J., &amp;amp; HAREL, M. (2009). ImageNet: A Large-Scale Hierarchical Image Database. &lt;em&gt;Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1109/CVPR.2009.5206848&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR.2009.5206848&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DOSOVITSKIY, A., BROSSARD, T., &amp;amp; SPRINGENBERG, J. (2017). Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks. &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 39(5), 939-949. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2016.2593826&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2016.2593826&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FRID-ADAR, M., ELIYAHU, S., &amp;amp; GOLDY, S. (2018). GAN-based Synthetic Medical Image Augmentation for Increased CNN Performance in Liver Lesion Classification. &lt;em&gt;IEEE Transactions on Medical Imaging&lt;/em&gt;, 37(6), 1334-1343. &lt;a href=&#34;https://doi.org/10.1109/TMI.2018.2813792&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TMI.2018.2813792&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;KINGMA, D. P., &amp;amp; WELLING, M. (2013). Auto-Encoding Variational Bayes. &lt;em&gt;Proceedings of the 2nd International Conference on Learning Representations (ICLR)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1312.6114&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RADFORD, A., WU, J., &amp;amp; AMODEI, D. (2021). Learning Transferable Visual Models From Natural Language Supervision. &lt;em&gt;Proceedings of the 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2103.00020&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RAMESH, A., MENG, C., &amp;amp; ZHANG, S. (2021). DALL·E: Creating Images from Text. &lt;em&gt;OpenAI&lt;/em&gt;. &lt;a href=&#34;https://openai.com/research/dall-e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://openai.com/research/dall-e&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;STRUBELL, E., GANASSI, M., &amp;amp; MCAFEE, P. (2019). Energy and Policy Considerations for Deep Learning in NLP. &lt;em&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1906.02243&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1906.02243&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WIESE, S., BOLAND, M., &amp;amp; TONG, A. (2019). A Survey on Machine Learning in Finance. &lt;em&gt;Proceedings of the 26th International Conference on Machine Learning (ICML 2019)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1910.02342&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1910.02342&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VASWANI, A., SHAZEER, N., &amp;amp; PARMAR, N. (2017). Attention Is All You Need. &lt;em&gt;Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017)&lt;/em&gt;. &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1706.03762&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Leveraging Large Language Models (LLMs) in Remote Sensing for Land Use/Land Cover (LULC) and Image Classification</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-07-29-leveraging-large-language-models-in-remote-sensing/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-07-29-leveraging-large-language-models-in-remote-sensing/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#understanding-large-language-models-llms&#34;&gt;Understanding Large Language Models (LLMs)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#llms-in-lulc-classification&#34;&gt;LLMs in LULC Classification&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#methodology&#34;&gt;Methodology&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-lulc-classification-on-sentinel-2-imagery&#34;&gt;Case Study: LULC Classification on Sentinel-2 Imagery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Remote sensing has revolutionized the way we observe and understand the Earth’s surface. With the advent of satellites like Sentinel, Landsat-8, and THEOS, we have access to a plethora of high-resolution imagery that can be used for various applications, including Land Use/Land Cover (LULC) classification and image classification. However, analyzing and interpreting this vast amount of data is a complex task. Enter Large Language Models (LLMs), which have shown promise in various domains, including natural language processing, computer vision, and remote sensing.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In this blog, we will explore how LLMs can be applied to remote sensing, particularly in the domains of LULC and image classification. We will delve into the methodologies, algorithms, and techniques that can be utilized to harness the power of LLMs for these applications.
&lt;/p&gt;
&lt;h2 id=&#34;understanding-large-language-models-llms&#34;&gt;Understanding Large Language Models (LLMs)&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Large Language Models, such as GPT-4, are deep learning models that have been trained on vast amounts of text data. They are capable of understanding and generating human-like text, making them highly versatile for various applications. In the context of remote sensing, LLMs can be used to analyze and interpret imagery data, aiding in tasks like LULC classification and image classification.
&lt;/p&gt;
&lt;h3 id=&#34;llms-in-lulc-classification&#34;&gt;LLMs in LULC Classification&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
LULC classification involves categorizing different regions of an image into land use and land cover classes, such as forests, urban areas, water bodies, and agricultural land. Traditional methods for LULC classification include supervised and unsupervised learning techniques. LLMs, however, can enhance these methods by providing contextual understanding and improved feature extraction.
&lt;/p&gt;
&lt;h4 id=&#34;methodology&#34;&gt;Methodology&lt;/h4&gt;
&lt;p align=&#34;justify&#34;&gt;
The process of using LLMs for LULC classification can be summarized as follows:
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Collect satellite imagery from sources like Sentinel, Landsat-8, and THEOS.&lt;/li&gt;
&lt;li&gt;Perform image correction and normalization to ensure consistency in the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feature Extraction&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use convolutional neural networks (CNNs) to extract features from the satellite images.&lt;/li&gt;
&lt;li&gt;Integrate LLMs to enhance feature extraction by incorporating contextual information from related text data (e.g., environmental reports, land use documentation).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Model Training&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train a classification model using the extracted features. The model can be a hybrid of CNNs and LLMs, where the CNN handles the spatial features and the LLM provides contextual understanding.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification and Validation&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply the trained model to classify the satellite images into LULC categories.&lt;/li&gt;
&lt;li&gt;Validate the model using ground truth data and performance metrics like accuracy, precision, recall, and F1-score.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;case-study-lulc-classification-on-sentinel-2-imagery&#34;&gt;Case Study: LULC Classification on Sentinel-2 Imagery&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
To illustrate the application of LLMs in LULC classification, let’s consider a case study using Sentinel-2 imagery. Sentinel-2 provides high-resolution optical imagery, which is ideal for detailed LULC classification.
&lt;/p&gt;
&lt;h3 id=&#34;data-collection-and-preprocessing&#34;&gt;Data Collection and Preprocessing&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
We collected Sentinel-2 imagery for a region with diverse land cover types. The images were preprocessed to correct for atmospheric effects and normalize the reflectance values.
&lt;/p&gt;
&lt;h3 id=&#34;feature-extraction&#34;&gt;Feature Extraction&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
A pre-trained CNN, such as ResNet-50, was used to extract spatial features from the images. Simultaneously, a large corpus of environmental text data was fed into an LLM to extract contextual features.
&lt;/p&gt;
&lt;h3 id=&#34;model-training-and-classification&#34;&gt;Model Training and Classification&lt;/h3&gt;
&lt;p align=&#34;justify&#34;&gt;
The extracted features were combined and fed into a hybrid classification model. The model was trained using labeled ground truth data. The results showed a significant improvement in classification accuracy compared to traditional methods.
&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The integration of LLMs in remote sensing, particularly for LULC and image classification, holds immense potential. By combining the spatial feature extraction capabilities of CNNs with the contextual understanding of LLMs, we can achieve more accurate and meaningful classifications. As remote sensing technology continues to evolve, the role of advanced AI models like LLMs will become increasingly crucial in unlocking new insights from satellite imagery.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In this blog, we explored the methodologies and techniques for leveraging LLMs in remote sensing applications. The case study on Sentinel-2 imagery demonstrated the practical benefits of this approach. As we move forward, further research and development in this field will undoubtedly lead to more innovative and effective solutions for remote sensing challenges.
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</title>
      <link>https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/mevit-a-medium-resolution-vision-transformer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</title>
      <link>https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama</title>
      <link>https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/object-detection-of-road-assets-using-transformer-based-yolox/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks</title>
      <link>https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province</title>
      <link>https://kaopanboonyuen.github.io/publication/rainfall-prediction-a-machine-learning-approach/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/rainfall-prediction-a-machine-learning-approach/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus</title>
      <link>https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/enhanced-feature-pyramid-vision-transformert/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks</title>
      <link>https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/the-bangkok-urbanscapes-dataset/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</title>
      <link>https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/transformer-based-decoder-designs-for-semantic-segmentation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</title>
      <link>https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution</title>
      <link>https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning</title>
      <link>https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Transportation Mobility Factor Extraction Using Image Recognition Techniques</title>
      <link>https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</title>
      <link>https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices</title>
      <link>https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/semantic-segmentation-on-medium-resolution-satellite-images/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</title>
      <link>https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-segmentation-on-remote-sensing/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery</title>
      <link>https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-segmentation-on-aerial-imagery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Road map extraction from satellite imagery using connected component analysis and landscape metrics</title>
      <link>https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/road-map-extraction-from-satellite-imagery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Image Vectorization of Road Satellite Data Sets</title>
      <link>https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/publication/image-vectorization-of-road-satellite-data-sets/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
