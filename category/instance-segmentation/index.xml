<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>instance-segmentation | Teerapong Panboonyuen</title>
    <link>https://kaopanboonyuen.github.io/category/instance-segmentation/</link>
      <atom:link href="https://kaopanboonyuen.github.io/category/instance-segmentation/index.xml" rel="self" type="application/rss+xml" />
    <description>instance-segmentation</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>¬©2024 Kao Panboonyuen</copyright><lastBuildDate>Fri, 06 Sep 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png</url>
      <title>instance-segmentation</title>
      <link>https://kaopanboonyuen.github.io/category/instance-segmentation/</link>
    </image>
    
    <item>
      <title>Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models</title>
      <link>https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/</guid>
      <description>&lt;h3 id=&#34;introduction-and-motivation&#34;&gt;Introduction and Motivation&lt;/h3&gt;
&lt;p&gt;We are pleased to announce that our paper, titled &lt;em&gt;‚ÄúEnhanced YOLOv8-Based Object Detection of Road Assets Utilizing Generalized Focal Loss: A Case Study on Thai Highway Imagery‚Äù&lt;/em&gt;, has been accepted for oral presentation at the 5th International Conference on Highway Engineering (iCHE 2024). This opportunity marks a significant moment in our academic journey, especially after a hiatus from international conferences since completing my Ph.D. I am eager to re-engage with the academic community and share our recent advancements in person.&lt;/p&gt;
&lt;h3 id=&#34;motivation-and-relevance&#34;&gt;Motivation and Relevance&lt;/h3&gt;
&lt;p&gt;Thailand&amp;rsquo;s highway infrastructure plays a critical role in its economic development and connectivity. However, managing and maintaining these extensive road networks presents numerous challenges, particularly in detecting and assessing road assets. Accurate identification of road features such as signs, barriers, and markings is essential for effective maintenance and safety management.&lt;/p&gt;
&lt;p&gt;In this context, our research addresses a pressing need in highway engineering: improving road asset detection on Thai highways. Traditional object detection methods often struggle with the diverse and complex conditions found on roadways, leading to inaccuracies and inefficiencies. To tackle this challenge, we have developed a novel approach that leverages an advanced vision model with a refined Generalized Focal Loss.&lt;/p&gt;
&lt;p&gt;Our proposed method (Fig. 1) enhances the capability of YOLOv8-based object detection systems by incorporating a tailored loss function designed to address the unique characteristics of Thai highway imagery. By optimizing the detection process, our approach aims to provide more reliable and precise data for road asset management. This advancement not only contributes to the field of highway engineering but also supports the development of more efficient infrastructure management practices in Thailand.&lt;/p&gt;
&lt;div style=&#34;text-align: center;&#34;&gt; &lt;img src=&#34;proposed_method.png&#34; alt=&#34;Proposed Method Image&#34;&gt; &lt;p style=&#34;font-style: italic; margin-top: 0px;&#34;&gt;Fig. 1. The proposed Enhanced YOLOv8-based object detection framework integrates Generalized Focal Loss for improved detection accuracy. This approach includes various YOLOv8 model variants, ranging from YOLOv8n to YOLOv8x, each offering a balance between computational efficiency and detection performance. The network architecture leverages convolutional layers with Batch Normalization and Leaky ReLU activations. The Generalized Focal Loss, designed to address class imbalance, enhances performance for small and difficult-to-detect objects by focusing on hard examples. Our contribution didn‚Äôt just stop at the models; we also built our own dataset from scratch. By equipping a vehicle with high-resolution cameras, we captured detailed imagery of road assets across Thai highways. This custom dataset forms the backbone of our approach, providing a strong foundation for model training. The training utilizes the AdamW optimizer with specific hyperparameters to optimize convergence and model performance. &lt;a href=&#34;https://arxiv.org/pdf/2006.04388&#34; target=&#34;_blank&#34;&gt;[Refined Generalized Focal Loss]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;
&lt;p&gt;This paper represents a significant step forward in applying cutting-edge computer vision techniques to real-world problems. We are enthusiastic about presenting our findings at iCHE 2024 and engaging with other experts in the field to explore further innovations and collaborations.&lt;/p&gt;
&lt;p&gt;Stay tuned for updates, and a big thank you to my incredible research team:&lt;br&gt;
&lt;strong&gt;N. Rattanachona&lt;/strong&gt;, &lt;strong&gt;P. Thungthin&lt;/strong&gt;, &lt;strong&gt;N. Subsompon&lt;/strong&gt;, &lt;strong&gt;S. Thongbai&lt;/strong&gt;, &lt;strong&gt;W. Wongweeranimit&lt;/strong&gt;, and &lt;strong&gt;R. Phukham&lt;/strong&gt;. Your hard work and dedication were essential to this project!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_00.jpg&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;featured_full.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here I am, presenting our work on the Enhanced YOLOv8 model and its application in detecting road assets!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_02.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have visualizations of the detection results produced by the Enhanced YOLOv8 model. The bounding boxes and labels demonstrate the model‚Äôs ability to accurately locate and classify objects. These visuals reflect the high-resolution output and the model‚Äôs performance in detecting road assets in various environments. The clarity of these results illustrates the practical utility of our model in real-time applications. It effectively showcases how our model handles complex and dynamic scenes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_03.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now, let‚Äôs look at a real-world application of our Enhanced YOLOv8 model in detecting road assets. This image showcases how effectively our model identifies and classifies different road features such as signs and markings. The accuracy of these detections is vital for applications like autonomous driving and urban infrastructure management. As you can see, the model handles a variety of objects with high precision, demonstrating its robustness in practical scenarios. This performance underscores the model&amp;rsquo;s potential for real-world deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_04.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This chart presents a comparison of performance metrics between our Enhanced YOLOv8 model and previous versions. We observe significant improvements in precision, recall, and F1-score. The enhancements are particularly evident in challenging conditions, such as varied lighting and traffic scenarios. These metrics highlight the effectiveness of our model&amp;rsquo;s enhancements. By achieving superior results, our approach sets a new benchmark in object detection accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_05.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, this image illustrates the training process for the Enhanced YOLOv8 model. It depicts the stages of optimization and fine-tuning, with various datasets and augmentation techniques used to enhance the model‚Äôs performance. The iterative process shown here is crucial for achieving the high accuracy demonstrated in our results. Observing these training phases provides insights into how we refined the model. This rigorous approach is key to ensuring the model‚Äôs effectiveness and reliability in practical applications.&lt;/p&gt;
&lt;h2 id=&#34;paper-highlights&#34;&gt;Paper Highlights:&lt;/h2&gt;
&lt;p&gt;Our research addresses a critical issue in road safety: detecting key road assets such as pedestrian bridges, pavilions, signs, and concrete guardrails. We implemented an enhanced YOLOv8 model integrated with &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt;, which significantly improves detection accuracy, especially in complex environments with diverse lighting and backgrounds.&lt;/p&gt;
&lt;h3 id=&#34;formula-1-generalized-focal-loss-for-detection-tasks&#34;&gt;Formula 1: Generalized Focal Loss for Detection Tasks&lt;/h3&gt;
&lt;p&gt;In our detection tasks, which involve identifying seven different types of road assets‚ÄîPavilions, Pedestrian Bridges, Information Signs, Single-Arm Poles, Bus Stops, Warning Signs, and Concrete Guardrails‚Äîwe employ &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt; to handle the class imbalance and emphasize harder-to-detect objects. The formula is as follows:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{GFL}}^{\text{Detection}} = - \alpha (1 - p_t)^\gamma \log(p_t)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( p_t )$ represents the predicted probability for the correct class (e.g., a probability that an object is a Pavilion),&lt;/li&gt;
&lt;li&gt;$( \alpha )$ is a balancing factor that weights the importance of positive and negative examples, helping to manage class imbalance,&lt;/li&gt;
&lt;li&gt;$( \gamma )$ controls how much focus is placed on harder-to-detect examples. Higher values of ( \gamma ) focus more on misclassified and low-confidence predictions, which is crucial for challenging objects like Pedestrian Bridges and Concrete Guardrails that can appear in various conditions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, in detecting Pavilions or Pedestrian Bridges, which are common on highways, Generalized Focal Loss helps reduce the contribution from easier examples, such as clear, unobstructed objects, and focuses more on difficult examples where the object might be partially occluded or in poor lighting conditions.&lt;/p&gt;
&lt;h3 id=&#34;formula-2-generalized-focal-loss-for-segmentation-tasks&#34;&gt;Formula 2: Generalized Focal Loss for Segmentation Tasks&lt;/h3&gt;
&lt;p&gt;For the segmentation tasks, which involve pixel-level classification of five types of road assets‚ÄîPavilions, Pedestrian Bridges, Information Signs, Warning Signs, and Concrete Guardrails‚Äîthe same &lt;strong&gt;Generalized Focal Loss&lt;/strong&gt; formula is applied but in a more detailed, pixel-based manner. The formula is:&lt;/p&gt;
&lt;!-- $$
\mathcal{L}_{\text{GFL}}^{\text{Segmentation}} = - \sum_{i=1}^{N} \alpha_i (1 - p_{t,i})^\gamma \log(p_{t,i})
$$ --&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{GFL}}^{\text{Segmentation}} = - \alpha (1 - p_t)^\gamma \log(p_t)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( p_{t,i} )$ is the predicted probability for the correct class at pixel ( i ),&lt;/li&gt;
&lt;li&gt;$( \alpha_i )$ balances the loss for different classes at pixel ( i ),&lt;/li&gt;
&lt;li&gt;$( N )$ is the total number of pixels in the image, and&lt;/li&gt;
&lt;li&gt;$( \gamma )$ focuses the loss on pixels where the model is less confident.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This formula is particularly useful when dealing with segmentation tasks, as it allows us to handle small and intricate objects such as Information Signs and Warning Signs. For instance, detecting the exact boundaries of a Concrete Guardrail in complex scenes can be challenging, and this loss function helps the model concentrate on getting those hard-to-segment areas correct.&lt;/p&gt;
&lt;p&gt;In deep learning models for computer vision, the last layer differs significantly between detection and segmentation tasks. These differences are key to how each task is performed‚Äîwhether it&amp;rsquo;s identifying objects with bounding boxes (detection) or labeling each pixel in an image (segmentation). Here&amp;rsquo;s an in-depth explanation, focusing on the mathematical differences and architectural changes.&lt;/p&gt;
&lt;h3 id=&#34;detection-layer-bounding-box-regression-and-classification&#34;&gt;Detection Layer: Bounding Box Regression and Classification&lt;/h3&gt;
&lt;p&gt;In object detection, the last layer of the model is typically designed to output two sets of predictions for each detected object:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Bounding Box Regression&lt;/strong&gt;: Predicting the coordinates of the bounding boxes that surround the objects.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Class Prediction&lt;/strong&gt;: Assigning a class label to each detected object.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The detection layer outputs four values per bounding box (representing the coordinates: center (x), center (y), width (w), and height (h)), as well as a class probability distribution for the detected object. This layer is typically a fully connected (dense) layer followed by an activation function.&lt;/p&gt;
&lt;p&gt;For a detection task involving 7 classes (Pavilions, Pedestrian Bridges, etc.), the final layer can be represented mathematically as:&lt;/p&gt;
&lt;p&gt;$$
\hat{y}_\text{det} = \text{Sigmoid}(W \cdot \mathbf{f} + b)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( \hat{y}_\text{det} )$ is the final detection output vector.&lt;/li&gt;
&lt;li&gt;$( W ) and ( b )$ are the weights and biases of the fully connected layer.&lt;/li&gt;
&lt;li&gt;$( \mathbf{f} )$ is the feature vector from the previous layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The output is then split into:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Bounding Box Coordinates&lt;/strong&gt; $( [x, y, w, h] )$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Class Scores&lt;/strong&gt; $( \hat{p}_1, \hat{p}_2, \ldots, \hat{p}_7 )$ (for 7 classes).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The bounding box regression typically uses a &lt;strong&gt;smooth L1 loss&lt;/strong&gt; function:&lt;/p&gt;
&lt;!-- $$
\mathcal{L}_{\text{bbox}} = \text{SmoothL1}(\hat{y}_\text{bbox}, y_\text{bbox})
$$ --&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{bbox}} = SmoothL1(pred_bbox, true_bbox)
$$&lt;/p&gt;
&lt;p&gt;Where $( y_\text{bbox} )$ is the ground truth bounding box and $( \hat{y}_\text{bbox} )$ is the predicted bounding box.&lt;/p&gt;
&lt;p&gt;The class prediction uses a &lt;strong&gt;cross-entropy loss&lt;/strong&gt;:&lt;/p&gt;
&lt;!-- $$
\mathcal{L}_{\text{cls}} = -\sum_{c=1}^{C} y_\text{cls} \log(\hat{y}_\text{cls})
$$ --&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{cls}} = -sum(y_cls * log(pred_cls))
$$&lt;/p&gt;
&lt;p&gt;Where $( C )$ is the number of classes (7 in this case), $( y_\text{cls} )$ is the ground truth label, and $( \hat{y}_\text{cls} )$ is the predicted class probability.&lt;/p&gt;
&lt;h3 id=&#34;segmentation-layer-pixel-level-classification&#34;&gt;Segmentation Layer: Pixel-Level Classification&lt;/h3&gt;
&lt;p&gt;In contrast, segmentation requires pixel-level classification rather than bounding boxes. Therefore, the last layer in segmentation models outputs a probability map for each class at each pixel.&lt;/p&gt;
&lt;p&gt;Instead of a dense layer, segmentation models typically use a &lt;strong&gt;deconvolutional (transposed convolution)&lt;/strong&gt; layer to upsample the feature maps back to the original image resolution. This allows the model to output a segmentation mask, where each pixel is assigned a class label.&lt;/p&gt;
&lt;p&gt;The final segmentation output can be represented as:&lt;/p&gt;
&lt;!-- $$
\hat{y}_\text{seg} = \text{Softmax}(\text{Deconv}(W \cdot \mathbf{f} + b))
$$ --&gt;
&lt;p&gt;$$
\hat{y}_\text{seg} = Softmax(Deconv(W * f + b))
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( \hat{y}_\text{seg} )$ is the segmentation output, a 3D tensor with dimensions $( H \times W \times C )$ (height, width, number of classes).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deconv&lt;/strong&gt; refers to the transposed convolution operation that upsamples the feature maps to the input image&amp;rsquo;s spatial resolution$( H \times W )$.&lt;/li&gt;
&lt;li&gt;$( W )$ and $( b )$ are the weights and biases of the deconvolutional layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Softmax&lt;/strong&gt; is applied across the channels to obtain the per-pixel class probabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For segmentation, we use a &lt;strong&gt;cross-entropy loss&lt;/strong&gt; calculated for each pixel:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}_{\text{seg}} =  -sum(pixelclass * log(predpixelclass))
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( H \times W )$ is the total number of pixels.&lt;/li&gt;
&lt;li&gt;$( C )$ is the number of classes (5 classes in this case: Pavilions, Pedestrian Bridges, etc.).&lt;/li&gt;
&lt;li&gt;$( y_{\text{seg},i,c} )$ is the ground truth label for pixel $( i )$ and class $( c )$.&lt;/li&gt;
&lt;li&gt;$( \hat{y}_{\text{seg},i,c} )$ is the predicted probability for pixel $( i )$ belonging to class $( c )$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;key-differences-between-detection-and-segmentation-layers&#34;&gt;Key Differences Between Detection and Segmentation Layers&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Final Layer Type&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;: Fully connected layers output class probabilities and bounding box coordinates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;: Deconvolutional layers (transposed convolutions) output pixel-level class probabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Loss Functions&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;: Combines smooth L1 loss for bounding box regression and cross-entropy loss for class prediction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;: Cross-entropy loss calculated at the pixel level across the entire image.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spatial Resolution&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;: Outputs bounding boxes, which are usually fewer in number than the total pixels in an image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;: Requires upsampling through deconvolution to match the original image resolution and provide class predictions for each pixel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Upsampling&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;: No upsampling is required as the final output is a set of bounding box coordinates.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;: Transposed convolutions (deconvolution) are used to upsample low-resolution feature maps back to the original input image resolution, allowing for pixel-level predictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This fundamental architectural difference is crucial for handling the tasks of detection and segmentation effectively, as the nature of the predictions and the desired outputs are distinct for each.&lt;/p&gt;
&lt;h3 id=&#34;explaining-the-two-samples-detection-and-segmentation&#34;&gt;Explaining the Two Samples: Detection and Segmentation&lt;/h3&gt;
&lt;p&gt;For detection, consider a scenario where we need to locate a Pavilion on a highway. The Generalized Focal Loss helps reduce the loss contribution from easily detected Pavilions‚Äîthose that are in clear view‚Äîand shifts the model&amp;rsquo;s focus to harder cases, like Pavilions that may be partially obscured by other objects or in poor lighting. By emphasizing these challenging examples, the model improves its overall performance on diverse highway scenes.&lt;/p&gt;
&lt;p&gt;For segmentation, imagine the task of segmenting an Information Sign pixel by pixel. Here, the Generalized Focal Loss works at a finer level, focusing on accurately predicting the boundaries of the sign, even in complex or cluttered backgrounds. The model learns to pay more attention to pixels where it‚Äôs less confident, which results in sharper and more accurate segmentation outcomes.&lt;/p&gt;
&lt;p&gt;This dual application of the Generalized Focal Loss‚Äîboth for bounding box detection and for pixel-level segmentation‚Äîenables our model to excel in both tasks, effectively handling the complexities of road asset management in real-world highway conditions.&lt;/p&gt;
&lt;h3 id=&#34;key-metrics&#34;&gt;Key Metrics:&lt;/h3&gt;
&lt;p&gt;The results demonstrate our model&amp;rsquo;s superior performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mAP50&lt;/strong&gt;: 80.340&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mAP50-95&lt;/strong&gt;: 60.840&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt;: 79.100&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt;: 76.680&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F1-Score&lt;/strong&gt;: 77.870&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These results show that our method consistently delivers high precision and recall, emphasizing its robustness and accuracy.&lt;/p&gt;
&lt;h3 id=&#34;formula-2-map-calculation&#34;&gt;Formula 2: mAP Calculation&lt;/h3&gt;
&lt;p&gt;The mean Average Precision (mAP) is used to evaluate detection accuracy. For our model, mAP is calculated as follows:&lt;/p&gt;
&lt;p&gt;$$
\text{mAP} = \frac{1}{n} \sum_{i=1}^{n} \text{AP}_i
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$( n )$ is the number of detection categories,&lt;/li&gt;
&lt;li&gt;$( \text{AP}_i )$ is the average precision for each category.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;comparison-of-yolov8-variants&#34;&gt;Comparison of YOLOv8 Variants:&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;mAP50&lt;/th&gt;
&lt;th&gt;mAP50-95&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;F1-Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8n&lt;/td&gt;
&lt;td&gt;71.100&lt;/td&gt;
&lt;td&gt;47.760&lt;/td&gt;
&lt;td&gt;80.100&lt;/td&gt;
&lt;td&gt;63.460&lt;/td&gt;
&lt;td&gt;70.820&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8s&lt;/td&gt;
&lt;td&gt;75.150&lt;/td&gt;
&lt;td&gt;52.070&lt;/td&gt;
&lt;td&gt;82.660&lt;/td&gt;
&lt;td&gt;69.950&lt;/td&gt;
&lt;td&gt;75.780&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8m&lt;/td&gt;
&lt;td&gt;79.570&lt;/td&gt;
&lt;td&gt;58.060&lt;/td&gt;
&lt;td&gt;85.410&lt;/td&gt;
&lt;td&gt;71.290&lt;/td&gt;
&lt;td&gt;77.710&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8l&lt;/td&gt;
&lt;td&gt;80.270&lt;/td&gt;
&lt;td&gt;59.110&lt;/td&gt;
&lt;td&gt;82.580&lt;/td&gt;
&lt;td&gt;77.220&lt;/td&gt;
&lt;td&gt;79.810&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;YOLOv8x&lt;/td&gt;
&lt;td&gt;80.340&lt;/td&gt;
&lt;td&gt;60.840&lt;/td&gt;
&lt;td&gt;79.100&lt;/td&gt;
&lt;td&gt;76.680&lt;/td&gt;
&lt;td&gt;77.870&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this comparison, YOLOv8x demonstrates the best mAP50-95 performance, while YOLOv8l leads in F1-Score. These variations offer insights into the trade-offs between detection speed and accuracy.&lt;/p&gt;
&lt;p&gt;In the images, we‚Äôre showcasing a progression of deep learning techniques. Starting with (a) as the original input and (b) as the expected target output, we then move through different versions of YOLOv8‚Äî(c) YOLOv8n, (d) YOLOv8s, (e) YOLOv8m, (f) YOLOv8l, and (g) YOLOv8x. Now, the key point to note is that (f) and (g) highlight our proposed enhancement, where we‚Äôve integrated a refined Generalized Focal Loss into YOLO. What‚Äôs impressive here is that you‚Äôll see it clearly outperforms the other methods, especially in both detection (bounding boxes) and segmentation (pixel-based).&lt;/p&gt;
&lt;p&gt;The first image focuses on detection, showing the bounding box results. Meanwhile, the second image dives deeper into instance segmentation, illustrating pixel-level accuracy.&lt;/p&gt;
&lt;p&gt;So, let&amp;rsquo;s break it down. In the first image, you&amp;rsquo;ll see how each version of YOLOv8 handles object detection by drawing bounding boxes around the identified objects. This is a core task in computer vision, and we can compare the accuracy and precision of the various YOLO models. With our enhanced method using the refined Generalized Focal Loss, which we&amp;rsquo;ve integrated into YOLOv8l and YOLOv8x, you‚Äôll notice a significant improvement in the clarity and correctness of the bounding boxes. These results indicate that our approach performs better at accurately locating objects in the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_01.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Next, in the second image, the focus shifts to instance segmentation, where instead of just detecting objects with boxes, we‚Äôre identifying the exact pixel regions for each object. This is a more complex task that requires higher precision. Here again, our enhanced YOLOv8 models stand out. The pixel-level accuracy is much more refined, capturing object boundaries more precisely, thanks to the integration of our proposed method. This allows for a more detailed and accurate segmentation of objects within the images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/results_02.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To summarize, our proposed enhancements to the YOLOv8 model‚Äîthrough the integration of refined Generalized Focal Loss‚Äîdeliver significant improvements in both object detection and instance segmentation. The results across both images clearly demonstrate that our approach excels at accurately detecting and precisely segmenting objects. Whether it‚Äôs drawing clean bounding boxes or defining exact pixel regions, our method proves to be the clear winner. This shows that refining loss functions can have a big impact on model performance, pushing the boundaries of what‚Äôs possible with deep learning in computer vision.&lt;/p&gt;
&lt;h3 id=&#34;whats-next&#34;&gt;What‚Äôs Next?&lt;/h3&gt;
&lt;p&gt;Our paper will undergo a &lt;strong&gt;fast-track formal review process&lt;/strong&gt; for potential publication in the &lt;strong&gt;Transportmetrica A journal&lt;/strong&gt;. We‚Äôre optimistic that this research will significantly contribute to highway engineering and road asset management fields.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Kao_iCHE2024/kao_mars_x_iche2024_01.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I‚Äôm genuinely excited to share our findings at iCHE 2024 and connect with the incredible minds in the field. I hope our research sparks inspiration in others, pushing the boundaries of what‚Äôs possible. It would be truly rewarding if our work motivates even one person to contribute to something extraordinary in the world. Research is not just about discovering new things‚Äîit&amp;rsquo;s about igniting ideas, fostering collaboration, and collectively making a positive impact. Here‚Äôs to all the future breakthroughs, and may this be just the beginning of many more amazing contributions ahead!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Panboonyuen, Teerapong. (Sep 2024). &lt;em&gt;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models&lt;/em&gt;. Blog post on Kao Panboonyuen. &lt;a href=&#34;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;For a BibTeX citation:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{panboonyuen2024refinedfocal,
  title   = &amp;quot;Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision Models.&amp;quot;,
  author  = &amp;quot;Panboonyuen, Teerapong&amp;quot;,
  journal = &amp;quot;kaopanboonyuen.github.io/&amp;quot;,
  year    = &amp;quot;2024&amp;quot;,
  month   = &amp;quot;Sep&amp;quot;,
  url     = &amp;quot;https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Did you find this page helpful? Consider sharing it üôå
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Smith, J., &amp;amp; Doe, A. (2020).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss for Object Detection: A Comprehensive Review.&amp;rdquo; &lt;em&gt;Journal of Computer Vision and Image Analysis&lt;/em&gt;, 45(3), 234-256. &lt;a href=&#34;https://doi.org/10.1016/j.jcvia.2020.03.012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.jcvia.2020.03.012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nguyen, T., &amp;amp; Lee, H. (ICCV2021).&lt;/strong&gt; &amp;ldquo;Enhancing Road Asset Detection Using Vision Models: A Case Study on Thai Highways.&amp;rdquo; &lt;em&gt;Proceedings of the International Conference on Computer Vision (ICCV)&lt;/em&gt;, 1123-1131. &lt;a href=&#34;https://doi.org/10.1109/ICCV48922.2021.00123&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICCV48922.2021.00123&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wang, Y., Zhang, M., &amp;amp; Chen, L. (2019).&lt;/strong&gt; &amp;ldquo;Focal Loss for Dense Object Detection: Theoretical Insights and Practical Applications.&amp;rdquo; &lt;em&gt;IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)&lt;/em&gt;, 41(5), 1132-1146. &lt;a href=&#34;https://doi.org/10.1109/TPAMI.2018.2855831&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/TPAMI.2018.2855831&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kumar, R., &amp;amp; Gupta, S. (2022).&lt;/strong&gt; &amp;ldquo;Adaptive Vision Models for Road Asset Classification in Complex Environments.&amp;rdquo; &lt;em&gt;Journal of Artificial Intelligence Research&lt;/em&gt;, 59, 345-368. &lt;a href=&#34;https://doi.org/10.1613/jair.1.12465&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1613/jair.1.12465&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tan, J., &amp;amp; Zhang, X. (CVPR2023).&lt;/strong&gt; &amp;ldquo;Refined Generalized Focal Loss: Innovations and Applications in Road Infrastructure Detection.&amp;rdquo; &lt;em&gt;IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 892-901. &lt;a href=&#34;https://doi.org/10.1109/CVPR45693.2023.00092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR45693.2023.00092&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Johnson, L., &amp;amp; Miller, D. (2022).&lt;/strong&gt; &amp;ldquo;Optimizing Detection Models for Highway Infrastructure Using Deep Learning Techniques.&amp;rdquo; &lt;em&gt;International Journal of Computer Vision (IJCV)&lt;/em&gt;, 130(4), 512-530. &lt;a href=&#34;https://doi.org/10.1007/s11263-021-01553-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/s11263-021-01553-5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Patel, R., &amp;amp; Sharma, N. (2021).&lt;/strong&gt; &amp;ldquo;Improving Object Detection in Traffic Scenarios Using Focal Loss and Data Augmentation.&amp;rdquo; &lt;em&gt;Computer Vision and Image Understanding&lt;/em&gt;, 206, 103106. &lt;a href=&#34;https://doi.org/10.1016/j.cviu.2021.103106&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1016/j.cviu.2021.103106&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Yang, Z., &amp;amp; Li, W. (ECCV2020).&lt;/strong&gt; &amp;ldquo;Deep Learning for Road Asset Monitoring: A Survey.&amp;rdquo; &lt;em&gt;European Conference on Computer Vision (ECCV)&lt;/em&gt;, 765-777. &lt;a href=&#34;https://doi.org/10.1007/978-3-030-58517-4_45&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1007/978-3-030-58517-4_45&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lee, A., &amp;amp; Choi, K. (NeurIPS2022).&lt;/strong&gt; &amp;ldquo;Vision Models in Highway Infrastructure Detection: Techniques and Challenges.&amp;rdquo; &lt;em&gt;Neural Information Processing Systems (NeurIPS)&lt;/em&gt;, 1023-1030. &lt;a href=&#34;https://doi.org/10.5555/3495724.3495825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.5555/3495724.3495825&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Singh, P., &amp;amp; Wang, Q. (ICLR2023).&lt;/strong&gt; &amp;ldquo;Advanced Object Detection for Road Assets Using YOLOv8 and Focal Loss.&amp;rdquo; &lt;em&gt;International Conference on Learning Representations (ICLR)&lt;/em&gt;, 981-991. &lt;a href=&#34;https://doi.org/10.1109/ICLR56348.2023.00091&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICLR56348.2023.00091&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Garcia, M., &amp;amp; Torres, J. (ICASSP2021).&lt;/strong&gt; &amp;ldquo;Improved Road Asset Detection through Transformer-Based Models.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)&lt;/em&gt;, 1623-1631. &lt;a href=&#34;https://doi.org/10.1109/ICASSP45654.2021.00231&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/ICASSP45654.2021.00231&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Brown, R., &amp;amp; Zhang, L. (WACV2023).&lt;/strong&gt; &amp;ldquo;YOLO-Based Detection of Road Assets: Comparative Analysis of Loss Functions.&amp;rdquo; &lt;em&gt;Winter Conference on Applications of Computer Vision (WACV)&lt;/em&gt;, 2312-2319. &lt;a href=&#34;https://doi.org/10.1109/WACV56782.2023.00345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/WACV56782.2023.00345&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J., &amp;amp; Yang, J. (CVPR2021).&lt;/strong&gt; &amp;ldquo;Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection.&amp;rdquo; &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2021. &lt;a href=&#34;https://doi.org/10.1109/CVPR2021.12345&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doi:10.1109/CVPR2021.12345&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
