<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.2.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Teerapong Panboonyuen" />

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="https://kaopanboonyuen.github.io/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    
    
    
      
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.08f2e04360a1c87f5ad39547c02bf219.css" />

  



  

  

  




  
  
  
    <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
  

  
    <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Teerapong Panboonyuen" />
  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://kaopanboonyuen.github.io/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Teerapong Panboonyuen" />
  <meta property="og:url" content="https://kaopanboonyuen.github.io/" />
  <meta property="og:title" content="Teerapong Panboonyuen" />
  <meta property="og:description" content="" /><meta property="og:image" content="https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2025-10-15T00:00:00&#43;00:00" />
    
  

  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "https://kaopanboonyuen.github.io/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "https://kaopanboonyuen.github.io/"
}
</script>


  

  

  





  <title>Teerapong Panboonyuen</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper   "  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.4be02a3b391999348b0c7478778a0e4b.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#awards" data-target="#awards"><span>Awards</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#press" data-target="#press"><span>Press</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured" data-target="#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects" data-target="#projects"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#tags" data-target="#tags"><span>Topics</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks" data-target="#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#communities" data-target="#communities"><span>Communities</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/blog/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
          
          <li class="nav-item d-none d-lg-inline-flex">
            <a class="nav-link" href="https://x.com/kaopanboonyuen" data-toggle="tooltip" data-placement="bottom" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
              <i class="fab fa-twitter" aria-hidden="true"></i>
            </a>
          </li>
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    











  
<span class="js-widget-page d-none"></span>





  
  
  
  




  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="about" class="home-section wg-about  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    

      




  










<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">

      
      
      <img class="avatar avatar-circle" src="/author/teerapong-panboonyuen/avatar_hu00be49bf8d306be2acce952c8fa7a11f_1239445_270x270_fill_lanczos_center_3.png" alt="Teerapong Panboonyuen">
      

      <div class="portrait-title">
        <h2>Teerapong Panboonyuen</h2>
        

        
        <h3>
          <a href="https://kaopanboonyuen.github.io/" target="_blank" rel="noopener">
          <span>Senior AI Research Scientist, PostDoc Fellow</span>
          </a>
        </h3>
        
        <h3>
          
          <span>MARSAIL, Chula</span>
          
        </h3>
        
      </div>

      <ul class="network-icon" aria-hidden="true">
        
        
        
        
          
        
        
        
        
        
        <li>
          <a href="mailto:teerapong.panboonyuen@gmail.com"  aria-label="envelope">
            <i class="fas fa-envelope big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://x.com/kaopanboonyuen" target="_blank" rel="noopener" aria-label="twitter">
            <i class="fab fa-twitter big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://scholar.google.co.th/citations?user=myy0qDgAAAAJ&amp;hl=en" target="_blank" rel="noopener" aria-label="graduation-cap">
            <i class="fas fa-graduation-cap big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://www.youtube.com/@kaopanboonyuen" target="_blank" rel="noopener" aria-label="youtube">
            <i class="fab fa-youtube big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener" aria-label="github">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://www.linkedin.com/in/teerapong-panboonyuen" target="_blank" rel="noopener" aria-label="linkedin">
            <i class="fab fa-linkedin big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://kaopanboonyuen.github.io/files/panboonyuen_cv.pdf" target="_blank" rel="noopener" aria-label="download">
            <i class="fas fa-download big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-12 col-lg-8">

    
    

    <div class="article-style">
      <p>My research focuses on <strong>Learning Representations</strong>—developing cutting-edge algorithms with optimization theory to push AI&rsquo;s limits. I work with advanced models like GANs and Diffusion Models, leverage Self-Supervised Learning, explore how Adversarial Attacks on Large Language Models (LLMs) could reshape the future of AI.</p>
<!-- My research focuses on **Learning Representations**, developing advanced algorithms based on optimization theory to push the boundaries of AI. I work with models like GANs and Diffusion Models, leverage Self-Supervised Learning, and explore Adversarial Attacks on Large Language Models (LLMs) to redefine AI's potential. -->
<p>I am currently a <strong>Senior Research Scientist</strong> at <a href="https://kaopanboonyuen.github.io/MARS/" target="_blank" rel="noopener"><strong>MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory)</strong></a> and a <a href="https://kaopanboonyuen.github.io/files/scholarship/C2F-PD_May-2025_FY-4-2568.pdf" style="color:#FF69B4;" target="_blank"><strong>C2F High-Potential Postdoc</strong></a> at <a href="https://www.chula.ac.th/en/" target="_blank" rel="noopener"><strong>Chulalongkorn University</strong></a>. I received my <a href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener"><strong>Ph.D. in Computer Engineering</strong></a> from <a href="https://www.chula.ac.th/en/" style="color:#FF69B4;" target="_blank"><strong>Chulalongkorn University</strong></a>, where I specialized in AI. On top of that, I’m the founder of <a href="https://pbylab.github.io/" style="color:#FF69B4; font-weight:600;" target="_blank"><strong>PBY Laboratory</strong></a>.</p>
<!-- My passion is focused on **Cognitive Intelligence** to unlock human potential. I am keenly interested in **Remote Sensing**, where LLMs reveals transformative insights and redefines how we perceive and interact with our environment. -->
<p>Passionate about <strong>Cognitive Intelligence</strong> and unlocking human potential, I’m also deeply immersed in <strong>Geospatial Intelligence</strong>, where LLMs uncover groundbreaking insights that reshape how we understand and interact with our world.</p>
<p>Detailed summaries of my academic, industry, and teaching experience can be found in my <a href="https://kaopanboonyuen.github.io/files/panboonyuen_cv.pdf" target="_blank" rel="noopener"><strong>CV</strong></a> or <a href="https://kaopanboonyuen.github.io/files/IEEE/IEEE_Biography_Panboonyuen.pdf" target="_blank" rel="noopener"><strong>IEEE Biography</strong></a>, and get a glimpse into my personal life on my <a href="https://kaopanboonyuen.github.io/blog/" target="_blank" rel="noopener"><strong>blog</strong></a> and <a href="https://kaopanboonyuen.tumblr.com/" target="_blank" rel="noopener"><strong>tumblr</strong></a>. By the way, feel free to vibe to my <a href="https://soundcloud.com/kaopanboonyuen" target="_blank" rel="noopener"><strong>music on SoundCloud</strong></a>.</p>
<p>Thai name: <a href="https://pbylab.github.io/members/teerapong-panboonyuen/" target="_blank" rel="noopener"><strong>ธีรพงศ์ ปานบุญยืน</strong></a>, aka <a href="https://kaopanboonyuen.wordpress.com/" target="_blank" rel="noopener"><strong>Kao Panboonyuen</strong></a>, or just Kao (เก้า).</p>
<p>
  <i class="fas fa-download  pr-1 fa-fw"></i> <a href="/files/panboonyuen_cv.pdf" target="_blank">Download my CV</a>. 
  <i class="fas fa-user  pr-1 fa-fw"></i> <a href="https://pbylab.github.io/members/teerapong-panboonyuen/" target="_blank">Know Me in a Minute</a>. 
  <i class="fas fa-image  pr-1 fa-fw"></i> <a href="/talk/get-to-know-me-better/" target="_blank">Profile Picture</a>.</p>
 <!-- 
  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/files/panboonyuen_cv_th.pdf" target="_blank">Download my Thai CV (Restricted to Thai government use only)</a>.   -->
<!-- 
  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/files/panboonyuen_cv_Thai.pdf" target="_blank">Download my Thai CV</a>. -->
<!-- 

  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/uploads/panboonyuen_cv_Thai.pdf" target="_blank">Download my Thai CV</a>. -->

    </div>

    <div class="row">

      
      <div class="col-md-5">
        <div class="section-subheading">Interests</div>
        <ul class="ul-interests mb-0">
          
          <li>Applied Earth Observations</li>
          
          <li>Geoscience and Remote Sensing</li>
          
          <li>Computer Vision</li>
          
          <li>Semantic Distillation</li>
          
          <li>Human-AI Interaction</li>
          
          <li>Learning Representations</li>
          
        </ul>
      </div>
      

      
      <div class="col-md-7">
        <div class="section-subheading">Education</div>
        <ul class="ul-edu fa-ul mb-0">
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PostDoc Fellow in AI, 2026</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD in Computer Engineering, 2020</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">MEng in Computer Engineering, 2017</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">BEng in Computer Engineering, 2015</p>
              <p class="institution">KMUTNB (Top 1% in University Mathematics)</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Pre-Engineering School (PET21), 2012</p>
              <p class="institution">KMUTNB (Senior High School, 10th - 12th Grade)</p>
            </div>
          </li>
          
        </ul>
      </div>
      

    </div>
  </div>
</div>


    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="awards" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Selected Awards</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/panboonyuen_72nd_anniversary_of_HM_King_Bhumibol_scholarship_v2.jpg" target="_blank" rel="noopener">H.M. the King Bhumibhol Adulyadej’s 72nd Birthday Anniversary Scholarship</a> (Master) (<a href="https://kaopanboonyuen.github.io/files/scholarship/panboonyuen_72nd_anniversary_of_HM_King_Bhumibol_scholarship_v3.jpg" target="_blank" rel="noopener">Recipients</a>)</li>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/panboonyuen_Recipient_100years_2_2560_v2.jpg" target="_blank" rel="noopener">The 100th Anniversary Chulalongkorn University Fund for Doctoral Scholarship</a> (Ph.D.) (<a href="https://kaopanboonyuen.github.io/files/scholarship/Recipient_100years_2_2560.pdf" target="_blank" rel="noopener">Recipients</a>)</li>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/Recipient_90yearsfund_2_2563.pdf" target="_blank" rel="noopener">The 90th Anniversary of Chulalongkorn University Scholarship</a> (Ph.D.)</li>
<li><a href="https://kaopanboonyuen.github.io/files/postdoc/Recipient_Posdoc_6_65.pdf" target="_blank" rel="noopener">Postdoctoral Grant, Ratchadapisek Research Fund (RRF) (Chulalongkorn University)</a> (Postdoc, 2021-2025) (<a href="https://kaopanboonyuen.github.io/files/postdoc/Recipient_Posdoc_6_65.pdf" target="_blank" rel="noopener">Recipients</a>)</li>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/C2F-PD_May-2025_FY-4-2568.pdf" target="_blank" rel="noopener">Postdoctoral Research Grant, Second Century Fund (C2F) (Chulalongkorn University)</a> (Postdoc, 2025-2026) (<a href="https://kaopanboonyuen.github.io/files/scholarship/C2F-PD_May-2025_FY-4-2568.pdf" target="_blank" rel="noopener">Recipients</a>) (<a href="https://kaopanboonyuen.github.io/files/postdoc/panboonyuen_C2F_PostDoc_Experience_Certificate_2025_2026.pdf" target="_blank" rel="noopener">C2F Certificate</a>)</li>
<li>Top 1% Score in University <a href="https://en.wikipedia.org/wiki/Differential_calculus" target="_blank" rel="noopener">Differential Calculus</a> and <a href="https://en.wikipedia.org/wiki/Engineering_mathematics" target="_blank" rel="noopener">Engineering Mathematics</a></li>
<li>2017 <a href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">Best Student Paper Award</a> in International Conference on Computing and Information Technology (<a href="https://link.springer.com/conference/ic2it" target="_blank" rel="noopener">IC2IT</a>)</li>
<li>2019 <a href="https://kaopanboonyuen.github.io/QOL-TransportAI/" target="_blank" rel="noopener">Best Young Researcher Paper Award</a> in First International Conference on Smart Technology &amp; Urban Development (<a href="https://ieeexplore.ieee.org/document/9018796" target="_blank" rel="noopener">STUD</a>)</li>
<li>2020 <a href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener">Ph.D. Defense Pass</a>. Proudly completed my doctoral journey, advancing the frontier of geospatial AI with <a href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">FusionNetGeoLabel</a> (<a href="https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">Ph.D. Defense Slides</a>)</li>
<li>2022 <a href="https://kaopanboonyuen.github.io/blog/2022-11-22-bangkok-marathon-2022conquering-the-full-marathon/" target="_blank" rel="noopener">Bangkok Marathon 42.195K Finisher</a> with successfully completed a full marathon run (42.195 kilometers) (<a href="https://www.bkkmarathon.com" target="_blank" rel="noopener">Bangkok Marathon</a>)</li>
<li>2023 <a href="https://kaopanboonyuen.github.io/files/panboonyuen_Techsauce2023.jpg" target="_blank" rel="noopener">AI Research Featured in Techsauce News</a>. Grateful to be spotlighted for pushing the frontier of AI innovation. (<a href="https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai" target="_blank" rel="noopener">Techsauce</a>).</li>
<li>2024 <a href="https://kaopanboonyuen.github.io/blog/2024-02-21-the-day-i-became-an-ironman/" target="_blank" rel="noopener">IRONMAN 70.3 Finisher</a> with successfully completed a challenging triathlon consisting of a 1.9K swim, 90K bike ride, and 21.1K run (<a href="https://www.ironman.com/races" target="_blank" rel="noopener">IM70.3</a>)</li>
<li>2024 <a href="https://kaopanboonyuen.github.io/files/Laguna_Phuket_Triathlon/Panboonyuen_RaceCertificate_LAGUNA_PHUKHET_TRI_2024.png" target="_blank" rel="noopener">Laguna Phuket Triathlon Finisher</a> with successfully completed a challenging triathlon consisting of a 1.8K swim, 55K bike ride, and 12K run (<a href="https://www.lagunaphukettri.com/lpt-individual/" target="_blank" rel="noopener">LPT</a>)</li>
<li>2024 <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42" target="_blank" rel="noopener">Distinguished Reviewer for the Bronze Level</a> of IEEE Transactions on Medical Imaging (<a href="https://kaopanboonyuen.github.io/files/certificate/IEEE_Transactions_on_Medical_Imaging_Distinguished_Reviewer_Certificate_2024.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>2025 <a href="https://kaopanboonyuen.github.io/blog/2025-01-20-chom-bueng-marathon-2025/" target="_blank" rel="noopener">Chombueng Marathon 42.195K Finisher</a> with successfully completed a full marathon run (42.195 kilometers) (<a href="https://www.runningconnect.com/event/CBM2025" target="_blank" rel="noopener">Chombueng Marathon</a>)</li>
<li>2025 <a href="https://kaopanboonyuen.github.io/blog/2025-10-13-my25th-blood-donation/" target="_blank" rel="noopener">My 25th Blood Donation</a> marks a milestone of compassion, generosity, and giving back to society, helping fellow humans (<a href="https://kaopanboonyuen.github.io/blog/2025-10-13-my25th-blood-donation/my_25th_blood_donation/KAO_25th_Blood_Donation_00010.jpg" target="_blank" rel="noopener">Thai Red Cross Society</a>).</li>
<li>2025 <a href="https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/" target="_blank" rel="noopener">Oral Presentation – Selected Among Top 12.5% Abstracts</a> at the 14th Critical Care Conference, organized by the Thai Society of Critical Care Medicine (<a href="https://www.tsccm2025.com/home.php" target="_blank" rel="noopener">TSCCM</a>)</li>
<li>2025 <a href="https://kaopanboonyuen.github.io/blog/2025-01-11-where-science-meets-inspiration/" target="_blank" rel="noopener">Global Young Scientists Summit (GYSS) Scholarship</a> from Her Royal Highness Princess Maha Chakri Sirindhorn (<a href="https://kaopanboonyuen.github.io/files/GYSS/panboonyuen_GYSS2025_announcement_EN.pdf" target="_blank" rel="noopener">GYSS</a>)</li>
</ul>
<p>Reviewer for International Journals/Conferences:</p>
<ul>
<li>Invited Reviewer of <a href="https://kaopanboonyuen.github.io/files/certificate/2025/Certificate_PR_Recognised.pdf" target="_blank" rel="noopener">Pattern Recognition</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://kaopanboonyuen.github.io/files/certificate/2025/Certificate_NEUCOM_Recognised.pdf" target="_blank" rel="noopener">Neurocomputing</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://dl.acm.org/journal/tkdd" target="_blank" rel="noopener">Transactions on Knowledge Discovery from Data</a> (ACM)</li>
<li>Invited Reviewer of <a href="https://kaopanboonyuen.github.io/files/certificate/2025/Certificate_YCVIU_Recognised.pdf" target="_blank" rel="noopener">Computer Vision and Image Understanding</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/computers-and-geosciences" target="_blank" rel="noopener">Computers and Geosciences</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://kaopanboonyuen.github.io/files/certificate/2025/Certificate_NN_Recognised.pdf" target="_blank" rel="noopener">Neural Networks</a> (Elsevier) (<a href="https://kaopanboonyuen.github.io/files/certificate/2025/Certificate_NN_Recognised.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://www.mdpi.com/journal/remotesensing" target="_blank" rel="noopener">Remote Sensing</a> (MDPI)</li>
<li>Invited Reviewer of <a href="https://kaopanboonyuen.github.io/files/certificate/2025/Reviewer_Certificate_03_September_2025_AR.pdf" target="_blank" rel="noopener">Artificial Intelligence Review</a> (Nature Portfolio)</li>
<li>Invited Reviewer of <a href="https://www.nature.com/srep/" target="_blank" rel="noopener">Scientific Reports</a> (Nature Portfolio) (<a href="https://kaopanboonyuen.github.io/files/certificate/2025/Reviewer_Certificate_14_August_2025_SR.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/toc/tgis20/current" target="_blank" rel="noopener">GIScience &amp; Remote Sensing</a> (Taylor &amp; Francis)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/journals/tejr20" target="_blank" rel="noopener">European Journal of Remote Sensing</a> (Taylor &amp; Francis)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/journals/tres20" target="_blank" rel="noopener">International Journal of Remote Sensing</a> (Taylor &amp; Francis)</li>
<li>Invited Reviewer of <a href="https://cis.ieee.org/publications/ieee-transactions-on-artificial-intelligence" target="_blank" rel="noopener">IEEE Transactions on Artificial Intelligence</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://kaopanboonyuen.github.io/files/certificate/2025/IEEE_Transactions_Reviewer_Certificate_BigData.pdf" target="_blank" rel="noopener">IEEE Transactions on Big Data</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank" rel="noopener">IEEE Transactions on Image Processing</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://www.embs.org/tmi/" target="_blank" rel="noopener">IEEE Transactions on Medical Imaging</a> (IEEE) (<a href="https://kaopanboonyuen.github.io/files/certificate/IEEE_Transactions_on_Medical_Imaging_Distinguished_Reviewer_Certificate_2024.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36" target="_blank" rel="noopener">IEEE Transactions on Geoscience and Remote Sensing</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://kaopanboonyuen.github.io/files/certificate/2025/IEEE_Transactions_Reviewer_Certificate_PAMI.pdf" target="_blank" rel="noopener">Pattern Analysis and Machine Intelligence (PAMI)</a> (IEEE)</li>
<li>Recognized as an <a href="https://kaopanboonyuen.github.io/files/certificate/2025/panboonyuen_IOP_Trusted_Reviewer.pdf" target="_blank" rel="noopener">IOP Trusted Reviewer</a> (IOP Publishing)</li>
<li>More reviews can be found under my WoS ID <a href="https://www.webofscience.com/wos/author/rid/AAO-4985-2020" target="_blank" rel="noopener">AAO-4985-2020</a>.</li>
<li>More certificates of reviewers can be found at <a href="https://github.com/kaopanboonyuen/kaopanboonyuen.github.io/tree/main/files/certificate" target="_blank" rel="noopener">my GitHub Repository</a>.</li>
</ul>
<p>Additional Certifications in Research Ethics:</p>
<ul>
<li>My certificate in GCP (Good Clinical Practice) – <em>Ethics in Human Research</em> is available in <a href="https://kaopanboonyuen.github.io/files/certificate/GCP/panboonyuen_GCP_certificate_2027.pdf" target="_blank" rel="noopener">my GCP Certificate (English)</a> and <a href="https://kaopanboonyuen.github.io/files/certificate/GCP/panboonyuen_GCP_certificate_2027_Thai.pdf" target="_blank" rel="noopener">my GCP Certificate (Thai)</a>.</li>
</ul>


  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="press" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Selected Press</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li><a href="https://theleaderasia.com/?p=35538" target="_blank" rel="noopener">The Leader Asia</a>: <em>Dr. Teerapong and his team introduced their advanced AI for car damage detection at ICIAP 2023 in Udine, setting new accuracy standards with their innovative MARS model.</em></li>
<li><a href="https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai" target="_blank" rel="noopener">Techsauce</a>: <em>Highlighted their AI technology for automatic car damage assessment, earning recognition for excellence at ICIAP 2023 in Italy.</em></li>
<li><a href="https://today.line.me/th/v2/article/RBJPMlY" target="_blank" rel="noopener">LINE TODAY</a>: <em>Showcased the MARS model at ICIAP 2023, noted for its high accuracy and setting new global standards in car damage detection.</em></li>
<li><a href="https://moneychat.co.th/mars-deep-tech-startup-gets-award-from-iciap-2023/" target="_blank" rel="noopener">Moneychat</a>: <em>Reported the award-winning innovation in AI for car damage estimation presented at ICIAP 2023.</em></li>
<li><a href="https://www.kaohoon.com/pr/632082" target="_blank" rel="noopener">Kaohoon</a>: <em>Celebrated the award-winning success of MARSAIL at ICIAP 2023.</em></li>
<li><a href="https://www.mitihoon.com/2023/10/09/411039/" target="_blank" rel="noopener">Mitistock</a>: <em>Introduced the MARS model, featuring advanced self-attention mechanisms for vehicle damage assessment in Thailand.</em></li>
<li><a href="https://www.thestorythailand.com/10/10/2023/113372/" target="_blank" rel="noopener">The Story Thailand</a>: <em>Presented cutting-edge AI techniques in car wound detection, achieving high accuracy and setting international benchmarks.</em></li>
<li><a href="https://www.mediaofthailand.com/2023/10/mars-deep-tech-startup-ai-iciap-2023.html" target="_blank" rel="noopener">Media of Thailand</a>: <em>Unveiled the MARS model at ICIAP 2023, recognized globally for its precision in car damage detection.</em></li>
<li><a href="https://thailandinsurancenews.com/featured/mars-deep-tech-startup-%e0%b9%82%e0%b8%8a%e0%b8%a7%e0%b9%8c%e0%b8%99%e0%b8%a7%e0%b8%b1%e0%b8%95%e0%b8%81%e0%b8%a3%e0%b8%a3%e0%b8%a1%e0%b8%aa%e0%b8%b3%e0%b8%a3%e0%b8%a7%e0%b8%88%e0%b8%84%e0%b8%a7/" target="_blank" rel="noopener">Thailand Insurance News</a>: <em>Featured Dr. Teerapong&rsquo;s MARS model at ICIAP 2023 for its groundbreaking accuracy in car damage detection.</em></li>
<li><a href="https://www.wealthplustoday.com/2023/10/09/mars-deep/" target="_blank" rel="noopener">WealthPlusToday</a>: <em>Dr. Teerapong’s MARSAIL wowed ICIAP 2023 in Italy, clinching an excellence award for next-gen car damage detection.</em></li>
<li><a href="https://www.car.chula.ac.th/display7.php?bib=2156287" target="_blank" rel="noopener">Chulalongkorn University</a>: <em>Published a study on semantic road segmentation using deep convolutional neural networks.</em></li>
<li><a href="https://www.eng.chula.ac.th/th/48902" target="_blank" rel="noopener">Chula Engineering News</a>: <em>Featured Dr. Teerapong&rsquo;s participation in the Global Young Scientists Summit (GYSS) 2025, highlighting academic leadership and global collaboration.</em></li>
<li><a href="https://careers.thaivivat.co.th/en/newsandevents/6808628ebfad6e8912fd5c57" target="_blank" rel="noopener">Thaivivat Insurance</a>: <em>Announced Dr. Teerapong’s research recognition at UAMC 2025, emphasizing advancements in AI for urban analytics and mobility challenges.</em></li>
</ul>


  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="featured" class="home-section wg-featured  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Featured Publications</h1>
            
          </div>
        
      
    

      






























  




<div class="col-12 col-lg-8">

  

  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Transactions on Geoscience and Remote Sensing (TGRS, Impact Factor 8.6)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/">
    <img src="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/featured_hu7fd8995c7b296eb410e755d372039a47_6513836_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/">KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image</a>
  </div>

  
  <a href="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/" class="summary-link">
    <div class="article-style">
      <p>Satellite image inpainting is a crucial task in remote sensing, where accurately restoring missing or occluded regions is essential for robust image analysis. In this paper, we propose KAO, a novel framework that utilizes Kernel-Adaptive Optimization within diffusion models for satellite image inpainting. KAO is specifically designed to address the challenges posed by very high-resolution (VHR) satellite datasets, such as DeepGlobe and the Massachusetts Roads Dataset. Unlike existing methods that rely on preconditioned models requiring extensive retraining or postconditioned models with significant computational overhead, KAO introduces a Latent Space Conditioning approach, optimizing a compact latent space to achieve efficient and accurate inpainting. Furthermore, we incorporate Explicit Propagation into the diffusion process, facilitating forward-backward fusion, which improves the stability and precision of the method. Experimental results demonstrate that KAO sets a new benchmark for VHR satellite image restoration, providing a scalable, high-performance solution that balances the efficiency of preconditioned models with the flexibility of postconditioned models.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11204656/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/KAO" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/KAO/" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/KAO/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11204656/" target="_blank" rel="noopener">
    
    IEEE TGRS Paper
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2511.02462" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>European Journal of Remote Sensing (Taylor &amp; Francis)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/">
    <img src="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/featured_hu8e44bcbfacc54c1e370fb6bc954d5801_3021347_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="GuidedBox: A segmentation-guided box teacher-student approach for weakly supervised road segmentation" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/">GuidedBox: A segmentation-guided box teacher-student approach for weakly supervised road segmentation</a>
  </div>

  
  <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/" class="summary-link">
    <div class="article-style">
      <p>Road segmentation in remote sensing is crucial for applications like urban planning, traffic monitoring, and autonomous driving. Labeling objects via pixel-wise segmentation is challenging compared to bounding boxes. Existing weakly supervised segmentation methods often rely on heuristic bounding box priors, but we propose that box-supervised techniques can yield better results. Introducing GuidedBox, an end-to-end framework for weakly supervised instance segmentation. GuidedBox uses a teacher model to generate high-quality pseudo-masks and employs a confidence scoring mechanism to filter out noisy masks. We also introduce a noise-aware pixel loss and affinity loss to optimize the student model with pseudo-masks. Our extensive experiments show that GuidedBox outperforms state-of-the-art methods like SOLOv2, CondInst, and Mask R-CNN on the Massachusetts Roads Dataset, achieving an AP50 score of 0.9231. It also shows strong performance on SpaceNet and DeepGlobe datasets, proving its versatility in remote sensing applications. Code has been made available at <a href="https://github.com/kaopanboonyuen/GuidedBox" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/GuidedBox</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/22797254.2025.2540963" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/GuidedBox/" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/GuidedBox/" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/GuidedBox/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://pbylab.github.io/" target="_blank" rel="noopener">
    
    PBY.LAB
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >K. Sermsri</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>EMNLP 2025 Workshop WiNLP</em> (9th Widening NLP Workshop, Suzhou, China)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/debiasing-llms-thai-political-stance/">
    <img src="/publication/debiasing-llms-thai-political-stance/featured_hufcf9af5e7b4331699dc7e6d2812e65bc_986705_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/debiasing-llms-thai-political-stance/">Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration</a>
  </div>

  
  <a href="/publication/debiasing-llms-thai-political-stance/" class="summary-link">
    <div class="article-style">
      <p>Political stance detection in low-resource and culturally complex settings poses a critical challenge for large language models (LLMs). In the Thai political landscape - marked by indirect language, polarized figures, and entangled sentiment and stance - LLMs often display systematic biases such as sentiment leakage and favoritism toward entities. These biases undermine fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic calibration framework that mitigates political bias without requiring fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and rationale-based supervision to disentangle sentiment from stance and reduce bias. We also release the first high-quality Thai political stance dataset, annotated with stance, sentiment, rationales, and bias markers across diverse entities and events. Experimental results show that ThaiFACTUAL significantly reduces spurious correlations, enhances zero-shot generalization, and improves fairness across multiple LLMs. This work highlights the importance of culturally grounded debiasing techniques for underrepresented languages.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2025.winlp-main.13/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/debiasing-llms-thai-political-stance/cite.bib">
  Cite
</a>





<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2025.winlp-main.13/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/poster/WiNLP-EMNLP2025-Panboonyuen-Poster.pdf" target="_blank" rel="noopener">
  Poster
</a>








  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2509.21946v1" target="_blank" rel="noopener">
    
    ArXiv Paper
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/people/teerapong-panboonyuen/" target="_blank" rel="noopener">
    
    My ACL Profile
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2025.winlp-main.13/" target="_blank" rel="noopener">
    
    ACL Official Publication
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>the 14th Critical Care Conference (King Chulalongkorn Memorial Hospital)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/">
    <img src="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/featured_hu7f6434b80a37c107b7c93aa015098a03_836055_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/">CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer</a>
  </div>

  
  <a href="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/" class="summary-link">
    <div class="article-style">
      <p>Integrating large language models into specialized domains like healthcare presents unique challenges, including domain adaptation and limited labeled data. We introduce CU-ICU, a method for customizing unsupervised instruction-finetuned language models for ICU datasets by leveraging the Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse fine-tuning approach that combines few-shot prompting with selective parameter updates, enabling efficient adaptation with minimal supervision. Our evaluation across critical ICU tasks—early sepsis detection, mortality prediction, and clinical note generation—demonstrates that CU-ICU consistently improves predictive accuracy and interpretability over standard fine-tuning methods. Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and a 20% enhancement in generating clinically relevant explanations while updating fewer than 1% of model parameters in its most efficient configuration. These results establish CU-ICU as a scalable, low-overhead solution for delivering accurate and interpretable clinical decision support in real-world ICU environments.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://arxiv.org/pdf/2507.13655" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/slides/Panboonyuen_CUICU_TSCCM2025_Slide.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="http://arxiv.org/abs/2507.13655" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access (Supported by the Ratchadapisek Somphot Fund for Postdoctoral Fellowship, 2024–2025)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/">
    <img src="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/featured_hua09e2e14fc39ba01fcd3d2647b5c4baf_4177266_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/">SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery</a>
  </div>

  
  <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/" class="summary-link">
    <div class="article-style">
      <p>Satellite image inpainting is a critical task in remote sensing, requiring accurate restoration of missing or occluded regions for reliable image analysis. In this paper, we present SatDiff, an advanced inpainting framework based on diffusion models, specifically designed to tackle the challenges posed by very high-resolution (VHR) satellite datasets such as DeepGlobe and the Massachusetts Roads Dataset. Building on insights from our previous work, SatInPaint, we enhance the approach to achieve even higher recall and overall performance. SatDiff introduces a novel Latent Space Conditioning technique that leverages a compact latent space for efficient and precise inpainting. Additionally, we integrate Explicit Propagation into the diffusion process, enabling forward-backward fusion for improved stability and accuracy. Inspired by encoder-decoder architectures like the Segment Anything Model (SAM), SatDiff is seamlessly adaptable to diverse satellite imagery scenarios. By balancing the efficiency of preconditioned models with the flexibility of postconditioned approaches, SatDiff establishes a new benchmark in VHR satellite datasets, offering a scalable and high-performance solution for satellite image restoration. The code for SatDiff is publicly available at <a href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/SatDiff</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/SatDiff" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access (Supported by C2F Postdoctoral Funding, 2025–2026)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/">
    <img src="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/featured_hu7067d4f970503b780b06d8d85d8da3e6_5323982_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="CHULA: Custom Heuristic Uncertainty-Guided Loss for Accurate Land Title Deed Segmentation" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/">CHULA: Custom Heuristic Uncertainty-Guided Loss for Accurate Land Title Deed Segmentation</a>
  </div>

  
  <a href="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/" class="summary-link">
    <div class="article-style">
      <p>Accurately segmenting land boundaries from Thai land title deeds is crucial for reliable land management and legal processes, but remains challenging due to low-quality scans, diverse layouts, and complex overlapping elements in documents. Existing methods often struggle with these difficulties, resulting in imprecise delineations that can cause disputes or inefficiencies. To address these issues, we propose CHULA, a novel Custom Heuristic Uncertainty-guided Loss tailored specifically for robust land title deed segmentation. CHULA uniquely combines domain-specific heuristic priors with uncertainty modeling in a unified loss function that effectively guides the model to focus on clearer regions while refining boundaries and suppressing noisy areas. Evaluated on a carefully curated Thai Land Title Deed Dataset, CHULA achieves an impressive 92.4% accuracy, significantly surpassing standard segmentation baselines. Our results highlight the promise of integrating uncertainty and heuristic knowledge to enhance segmentation accuracy in complex, real-world documents. The code is publicly available at <a href="https://github.com/kaopanboonyuen/CHULA" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/CHULA</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11146788" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/CHULA" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/CHULA/" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/CHULA/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://pbylab.github.io/" target="_blank" rel="noopener">
    
    PBY.LAB
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >C. Charoenphon</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >B. Zhang</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Journal of Spatial Science (Taylor &amp; Francis)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/">
    <img src="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/featured_hub6ad554aafaa30da1fa3a7863d98a90a_4805592_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Investigating the use of deep learning-derived weighted mean temperature for GPS-PWVs estimation" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/">Investigating the use of deep learning-derived weighted mean temperature for GPS-PWVs estimation</a>
  </div>

  
  <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/" class="summary-link">
    <div class="article-style">
      <p>GNSS data offers a reliable alternative for estimating Precipitable Water Vapor (PWV), but accurate GPS-PWV determination in tropical climates requires weighted mean temperature (Tm). With traditional measurement methods often unavailable in Thailand, and existing empirical models showing low accuracy, we propose a deep learning approach. Our Bidirectional Learning with Attention (BLA) model incorporates GRUs and an attention mechanism for Tm modeling. Trained on ERA5 data (2017-2021) and evaluated on 2022 data, BLA-Tm achieved 76% improvement over conventional models, reducing biases significantly. Validation with 280 GNSS stations confirmed BLA-Tm’s superior accuracy in GPS-PWV estimation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  Source Document
</a>



  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>arXiv:2506.10524 [cs.CV]</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/">
    <img src="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/featured_hub7a516f3a892706b22056f6c3c1feca2_1061609_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/">ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation</a>
  </div>

  
  <a href="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/" class="summary-link">
    <div class="article-style">
      <p>This paper introduces ALBERT, an instance segmentation model designed specifically for comprehensive car damage and part segmentation. Leveraging the power of Bidirectional Encoder Representations, ALBERT incorporates advanced localization mechanisms to accurately identify and differentiate between real and fake damages as well as segment individual car parts. The model is trained on a large-scale, richly annotated automotive dataset, categorizing damage into 26 types, identifying 7 fake damage variants, and segmenting 61 distinct car parts. Our approach demonstrates strong performance in both segmentation accuracy and damage classification, paving the way for intelligent automotive inspection and assessment applications. This work not only contributes a powerful tool for automated vehicle inspection but also lays the groundwork for future research in intelligent automotive diagnostics, safety evaluation, and insurance claim automation, with significant implications for both industry and research communities.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2506.10524" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2506.10524" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/" target="_blank" rel="noopener">
    
    MARSAIL
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/" target="_blank" rel="noopener">
    
    Blog
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>arXiv:2506.10528 [cs.CV]</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/">
    <img src="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/featured_hu59968ee85a170b1c99a7e8e97682de19_1142531_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/">SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance</a>
  </div>

  
  <a href="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/" class="summary-link">
    <div class="article-style">
      <p>We propose SLICK, a novel and efficient framework for high-precision car damage segmentation, designed for real-world deployment in automotive insurance and inspection workflows. SLICK introduces five synergistic components, selective part segmentation guided by structural priors, localization-aware attention to highlight fine-grained damage, instance-sensitive refinement for precise boundary separation, cross-channel calibration to amplify subtle cues like scratches and dents, and a knowledge fusion module that integrates synthetic crash data, part geometry, and annotated insurance datasets. Trained using a teacher–student distillation strategy with ALBERT as the teacher, SLICK retains high segmentation fidelity while achieving up to 7× faster inference. Extensive experiments on large-scale automotive datasets demonstrate SLICK’s superior accuracy, generalization, and runtime efficiency—making it ideal for real-time, high-stakes applications in insurance automation and vehicle inspection.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2506.10528" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2506.10528" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/" target="_blank" rel="noopener">
    
    MARSAIL
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/" target="_blank" rel="noopener">
    
    Blog
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In 17th International Conference on Knowledge and Smart Technology (KST2025)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/sea-vit-sea-surface-currents-forecasting/">
    <img src="/publication/sea-vit-sea-surface-currents-forecasting/featured_hu149d7f089d7ef724d8c51beb2497e6de_5716842_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/sea-vit-sea-surface-currents-forecasting/">SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling</a>
  </div>

  
  <a href="/publication/sea-vit-sea-surface-currents-forecasting/" class="summary-link">
    <div class="article-style">
      <p>Forecasting sea surface currents is essential for applications such as maritime navigation, environmental monitoring, and climate analysis, particularly in regions like the Gulf of Thailand and the Andaman Sea. This paper introduces SEA-ViT, an advanced deep learning model that integrates Vision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to capture spatio-temporal covariance for predicting sea surface currents (U, V) using high-frequency radar (HF) data. The name SEA-ViT is derived from Sea Surface Currents Forecasting using Vision Transformer, highlighting the model&rsquo;s emphasis on ocean dynamics and its use of the ViT architecture to enhance forecasting capabilities. SEA-ViT is designed to unravel complex dependencies by leveraging a rich dataset spanning over 30 years and incorporating ENSO indices (El Niño, La Niña, and neutral phases) to address the intricate relationship between geographic coordinates and climatic variations. This development enhances the predictive capabilities for sea surface currents, supporting the efforts of the Geo-Informatics and Space Technology Development Agency (GISTDA) in Thailand&rsquo;s maritime regions. The code and pretrained models are available at <a href="https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11003320" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sea-vit-sea-surface-currents-forecasting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/GISTDA-SeaViT" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/GISTDA-SeaViT/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2409.16313" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-56.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>5th International Conference on Highway Engineering</em> <strong>ICHE 2024</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/">
    <img src="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/featured_hud0a2bba3e417817482b2e4c65e1f0435_5323024_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/">REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models</a>
  </div>

  
  <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" class="summary-link">
    <div class="article-style">
      <p>This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements—like pavilions, pedestrian bridges, information and warning signs, and concrete guardrails—to boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we&rsquo;ve integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model’s superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11003314" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/REG" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/REG/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/REG/" target="_blank" rel="noopener">
  Poster
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/slides/20240906_Panboonyuen_AI_ThaiHighway.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
    
    Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.arxiv.org/abs/2409.09877" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-57.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing (Supported by Ratchadapisek Somphot Postdoctoral Fund, 2022–2024)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/mevit-a-medium-resolution-vision-transformer/">
    <img src="/publication/mevit-a-medium-resolution-vision-transformer/featured_hubb2d2a67c4b44c04dfd372ab97ed06f7_2102839_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mevit-a-medium-resolution-vision-transformer/">MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</a>
  </div>

  
  <a href="/publication/mevit-a-medium-resolution-vision-transformer/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT&rsquo;s effectiveness in accurately segmenting Thai Landsat-8 data.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/15/21/5124" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mevit-a-medium-resolution-vision-transformer/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MeVit" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MeViT/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/GYSS/panboonyuen_MeViT_Poster_toGYSS2025.pdf" target="_blank" rel="noopener">
  Poster
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=tgcKR97Ea8I" target="_blank" rel="noopener">
  Video
</a>




  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Nithisopa</span>, <span >P. Pienroj</span>, <span >L. Jirachuphun</span>, <span >C. Watthanasirikrit</span>, <span >N. Pornwiriyakul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Image Analysis and Processing</em> <strong>ICIAP 2023</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/">
    <img src="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/featured_hu2872fb430e8c008b7eed1ab9972290e6_1008939_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/">MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</a>
  </div>

  
  <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" class="summary-link">
    <div class="article-style">
      <p>Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at <a href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/MARS</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MARS" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/MARS/panboonyuen_MARS_ICIAP2023_Poster.pdf" target="_blank" rel="noopener">
  Poster
</a>








  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/MARS/panboonyuen_MARS_ICIAP2023_Poster.pdf" target="_blank" rel="noopener">
    
    ICIAP Poster
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/doi/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
    
    ACM
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2305.04743" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/" target="_blank" rel="noopener">
    
    MARSAIL
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MARS" target="_blank" rel="noopener">
    
    Website
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >T. Vajeethaveesin</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Trends in Sciences (Trends Sci. or TiS)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/rainfall-prediction-a-machine-learning-approach/">
    <img src="/publication/rainfall-prediction-a-machine-learning-approach/featured_hu1be311b1bf32db7800481f0b35db29c4_2683455_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/rainfall-prediction-a-machine-learning-approach/">A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province</a>
  </div>

  
  <a href="/publication/rainfall-prediction-a-machine-learning-approach/" class="summary-link">
    <div class="article-style">
      <p>Flooding poses a significant challenge in Thailand due to its complex geography, traditionally addressed through GIS methods like the Flood Risk Assessment Model (FRAM) combined with the Analytical Hierarchy Process (AHP). This study assesses the efficacy of Artificial Neural Networks (ANN) in flood susceptibility mapping, using data from Ayutthaya Province and incorporating 5-fold cross-validation and Stochastic Gradient Descent (SGD) for training. ANN achieved superior performance with precision of 79.90%, recall of 79.04%, F1-score of 79.08%, and accuracy of 79.31%, outperforming the traditional FRAM approach. Notably, ANN identified that only three factors—flow accumulation, elevation, and soil types—were crucial for predicting flood-prone areas. This highlights the potential for ANN to simplify and enhance flood risk assessments. Moreover, the integration of advanced machine learning techniques underscores the evolving capability of AI in addressing complex environmental challenges.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://tis.wu.ac.th/index.php/tis/article/view/2038" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rainfall-prediction-a-machine-learning-approach/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RainNet-ML" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RainNet-ML" target="_blank" rel="noopener">
  Project
</a>










  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Chulalongkorn University</em> <strong>Thesis Evaluation - Very Good Score (Outstanding Achievement)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/">
    <img src="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/featured_hub0e8538b64c26dd17fceedaa9984021c_2172024_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/">Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</a>
  </div>

  
  <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" class="summary-link">
    <div class="article-style">
      <p>My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/FusionNetGeoLabel" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener">
  Dataset
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/picture/phd_defense_day.jpg" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener">
    
    PhD Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    PhD Showcase
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >I. Wichakam</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Udomcharoenchaikit</span>, <span >P. Vateekul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Multimedia Modeling</em> <strong>MMM 2018</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/">
    <img src="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/featured_hue5389440b0bf3553b83698c7fc10eb5d_1115912_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/">Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</a>
  </div>

  
  <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" class="summary-link">
    <div class="article-style">
      <p>Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-73603-7_32" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/VisionDL" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/VisionDL/" target="_blank" rel="noopener">
  Project
</a>










  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/road-segmentation-on-remote-sensing/">
    <img src="/publication/road-segmentation-on-remote-sensing/featured_hu5832d1824d617f359dcd8e3656d08a5e_2953080_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/road-segmentation-on-remote-sensing/">Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</a>
  </div>

  
  <a href="/publication/road-segmentation-on-remote-sensing/" class="summary-link">
    <div class="article-style">
      <p>Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/9/7/680" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-remote-sensing/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
    
    IC2IT 2017 Best Paper Award
  </a>

  </div>
  

</div>
    
  

  
  
  

</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="projects" class="home-section wg-portfolio  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  justify-content-center">
      
        
          <div class="section-heading col-12 mb-3 text-center">
            <h1 class="mb-0">Publications</h1>
            
          </div>
        
      
    

      







<div class="col-12 ">

  <p>To find relevant content, try <a href="./publication/">searching publications</a>, filtering using the buttons below, or exploring <a href="#tags">popular topics</a>. A * denotes equal contribution.</p>


  

    

    
    
    
    
      
    

    <span class="d-none default-project-filter">*</span>

    
    
    <div class="project-toolbar">
      <div class="project-filters">
        <div class="btn-toolbar">
          <div class="btn-group flex-wrap">
            
              
              
              
                
              
              <a href="#" data-filter="*" class="btn btn-primary btn-lg active">All</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Deep-Learning" class="btn btn-primary btn-lg">Deep Learning</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Semantic-Segmentation" class="btn btn-primary btn-lg">Semantic Segmentation</a>
            
              
              
              
              <a href="#" data-filter=".js-id-High-Resolution-Imagery" class="btn btn-primary btn-lg">High-Resolution Imagery</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Remote-Sensing" class="btn btn-primary btn-lg">Remote Sensing</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Multi-branch-Architectures" class="btn btn-primary btn-lg">Multi-branch Architectures</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Mixed-scale-Convolutional-Feedforward-Networks" class="btn btn-primary btn-lg">Mixed-scale Convolutional Feedforward Networks</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Encoder-Decoder-Networks" class="btn btn-primary btn-lg">Encoder-Decoder Networks</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Transformer" class="btn btn-primary btn-lg">Transformer</a>
            
          </div>
        </div>
      </div>
    </div>
    
  

  <div class="isotope projects-container row js-layout-row ">

    
    
      
    

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-kernel-adaptive-optimization js-id-diffusion-models js-id-satellite-image-inpainting js-id-remote-sensing js-id-vhr-datasets js-id-deepglobe js-id-massachusetts-roads-dataset js-id-latent-space-conditioning js-id-explicit-propagation js-id-forward-backward-fusion js-id-image-restoration js-id-geospatial-ai js-id-computer-vision js-id-scalable-inpainting js-id-high-resolution-imagery">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/" >KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image</a>
      </div>

      
      <a href="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/"  class="summary-link">
        <div class="article-style">
          Satellite image inpainting is a crucial task in remote sensing, where accurately restoring missing or occluded regions is essential for robust image analysis. In this paper, we propose KAO, a novel framework that utilizes Kernel-Adaptive Optimization within diffusion models for satellite image inpainting. KAO is specifically designed to address the challenges posed by very high-resolution (VHR) satellite datasets, such as DeepGlobe and the Massachusetts Roads Dataset. Unlike existing methods that rely on preconditioned models requiring extensive retraining or postconditioned models with significant computational overhead, KAO introduces a Latent Space Conditioning approach, optimizing a compact latent space to achieve efficient and accurate inpainting. Furthermore, we incorporate Explicit Propagation into the diffusion process, facilitating forward-backward fusion, which improves the stability and precision of the method. Experimental results demonstrate that KAO sets a new benchmark for VHR satellite image restoration, providing a scalable, high-performance solution that balances the efficiency of preconditioned models with the flexibility of postconditioned models.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Transactions on Geoscience and Remote Sensing (TGRS, Impact Factor 8.6)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11204656/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/KAO" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/KAO/" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/KAO/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11204656/" target="_blank" rel="noopener">
    
    IEEE TGRS Paper
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2511.02462" target="_blank" rel="noopener">
    
    ArXiv
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/" >
        <img src="/publication/kao-kernel-adaptive-optimization-in-diffusion-for-satellite-image/compact_hu4cbfcba23b22ea264ec14477c4067d92_256236_300x0_resize_lanczos_3.png" alt="KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-weakly-supervised-learning js-id-road-segmentation js-id-remote-sensing js-id-teacher-student-framework js-id-pseudo-labeling js-id-instance-segmentation js-id-computer-vision js-id-geospatial-ai js-id-mask-generation js-id-noise-aware-learning">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/" >GuidedBox: A segmentation-guided box teacher-student approach for weakly supervised road segmentation</a>
      </div>

      
      <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/"  class="summary-link">
        <div class="article-style">
          Road segmentation in remote sensing is crucial for applications like urban planning, traffic monitoring, and autonomous driving. Labeling objects via pixel-wise segmentation is challenging compared to bounding boxes. Existing weakly supervised segmentation methods often rely on heuristic bounding box priors, but we propose that box-supervised techniques can yield better results. Introducing GuidedBox, an end-to-end framework for weakly supervised instance segmentation. GuidedBox uses a teacher model to generate high-quality pseudo-masks and employs a confidence scoring mechanism to filter out noisy masks. We also introduce a noise-aware pixel loss and affinity loss to optimize the student model with pseudo-masks. Our extensive experiments show that GuidedBox outperforms state-of-the-art methods like SOLOv2, CondInst, and Mask R-CNN on the Massachusetts Roads Dataset, achieving an AP50 score of 0.9231. It also shows strong performance on SpaceNet and DeepGlobe datasets, proving its versatility in remote sensing applications. Code has been made available at <a href="https://github.com/kaopanboonyuen/GuidedBox" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/GuidedBox</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>European Journal of Remote Sensing (Taylor &amp; Francis)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/22797254.2025.2540963" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/GuidedBox/" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/GuidedBox/" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/GuidedBox/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://pbylab.github.io/" target="_blank" rel="noopener">
    
    PBY.LAB
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/" >
        <img src="/publication/guidedbox-a-segmentation-guided-box-teacher-student-approach-for-weakly-supervised-road-segmentation/compact_hu9b8e82bb734e9d02b1fef6fdeacc1ca4_1093084_300x0_resize_lanczos_3.png" alt="GuidedBox: A segmentation-guided box teacher-student approach for weakly supervised road segmentation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-political-stance-detection js-id-debiasing-llms js-id-low-resource-languages js-id-thai-language js-id-cultural-bias js-id-counterfactual-data js-id-fairness-in-ai js-id-zero-shot-learning js-id-rationale-supervision js-id-language-model-evaluation js-id-computational-linguistics js-id-bias-mitigation js-id-sentiment-analysis js-id-model-agnostic-methods">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/debiasing-llms-thai-political-stance/" >Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration</a>
      </div>

      
      <a href="/publication/debiasing-llms-thai-political-stance/"  class="summary-link">
        <div class="article-style">
          Political stance detection in low-resource and culturally complex settings poses a critical challenge for large language models (LLMs). In the Thai political landscape - marked by indirect language, polarized figures, and entangled sentiment and stance - LLMs often display systematic biases such as sentiment leakage and favoritism toward entities. These biases undermine fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic calibration framework that mitigates political bias without requiring fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and rationale-based supervision to disentangle sentiment from stance and reduce bias. We also release the first high-quality Thai political stance dataset, annotated with stance, sentiment, rationales, and bias markers across diverse entities and events. Experimental results show that ThaiFACTUAL significantly reduces spurious correlations, enhances zero-shot generalization, and improves fairness across multiple LLMs. This work highlights the importance of culturally grounded debiasing techniques for underrepresented languages.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >K. Sermsri</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>EMNLP 2025 Workshop WiNLP</em> (9th Widening NLP Workshop, Suzhou, China)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2025.winlp-main.13/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/debiasing-llms-thai-political-stance/cite.bib">
  Cite
</a>





<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2025.winlp-main.13/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/poster/WiNLP-EMNLP2025-Panboonyuen-Poster.pdf" target="_blank" rel="noopener">
  Poster
</a>








  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2509.21946v1" target="_blank" rel="noopener">
    
    ArXiv Paper
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/people/teerapong-panboonyuen/" target="_blank" rel="noopener">
    
    My ACL Profile
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2025.winlp-main.13/" target="_blank" rel="noopener">
    
    ACL Official Publication
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/debiasing-llms-thai-political-stance/" >
        <img src="/publication/debiasing-llms-thai-political-stance/compact_huf2ba3c84b75a84f69b57e6356a76b8fe_540863_300x0_resize_lanczos_3.png" alt="Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-clinical-llms js-id-parameter-efficient-finetuning js-id-peft js-id-medical-ai js-id-prompt-learning js-id-flan-t5 js-id-healthcare-innovation js-id-sepsis-detection js-id-icu-mortality-prediction">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/" >CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer</a>
      </div>

      
      <a href="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/"  class="summary-link">
        <div class="article-style">
          Integrating large language models into specialized domains like healthcare presents unique challenges, including domain adaptation and limited labeled data. We introduce CU-ICU, a method for customizing unsupervised instruction-finetuned language models for ICU datasets by leveraging the Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse fine-tuning approach that combines few-shot prompting with selective parameter updates, enabling efficient adaptation with minimal supervision. Our evaluation across critical ICU tasks—early sepsis detection, mortality prediction, and clinical note generation—demonstrates that CU-ICU consistently improves predictive accuracy and interpretability over standard fine-tuning methods. Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and a 20% enhancement in generating clinically relevant explanations while updating fewer than 1% of model parameters in its most efficient configuration. These results establish CU-ICU as a scalable, low-overhead solution for delivering accurate and interpretable clinical decision support in real-world ICU environments.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>the 14th Critical Care Conference (King Chulalongkorn Memorial Hospital)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://arxiv.org/pdf/2507.13655" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/slides/Panboonyuen_CUICU_TSCCM2025_Slide.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/blog/2025-07-17-cuicu-customizing-unsupervised-instruction-finetuned-language-models/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="http://arxiv.org/abs/2507.13655" target="_blank" rel="noopener">
    
    ArXiv
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/" >
        <img src="/publication/cu-icu-customizing-unsupervised-instruction-finetuned-language-models-for-icu-datasets/compact_hu4362fb0089d46a9d21657368cdd0cb76_301850_300x0_resize_lanczos_3.png" alt="CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Satellite-Image-Inpainting js-id-Diffusion-Models js-id-Latent-Space-Conditioning js-id-VHR-Satellite-Datasets js-id-Image-Restoration">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/" >SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery</a>
      </div>

      
      <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/"  class="summary-link">
        <div class="article-style">
          Satellite image inpainting is a critical task in remote sensing, requiring accurate restoration of missing or occluded regions for reliable image analysis. In this paper, we present SatDiff, an advanced inpainting framework based on diffusion models, specifically designed to tackle the challenges posed by very high-resolution (VHR) satellite datasets such as DeepGlobe and the Massachusetts Roads Dataset. Building on insights from our previous work, SatInPaint, we enhance the approach to achieve even higher recall and overall performance. SatDiff introduces a novel Latent Space Conditioning technique that leverages a compact latent space for efficient and precise inpainting. Additionally, we integrate Explicit Propagation into the diffusion process, enabling forward-backward fusion for improved stability and accuracy. Inspired by encoder-decoder architectures like the Segment Anything Model (SAM), SatDiff is seamlessly adaptable to diverse satellite imagery scenarios. By balancing the efficiency of preconditioned models with the flexibility of postconditioned approaches, SatDiff establishes a new benchmark in VHR satellite datasets, offering a scalable and high-performance solution for satellite image restoration. The code for SatDiff is publicly available at <a href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/SatDiff</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access (Supported by the Ratchadapisek Somphot Fund for Postdoctoral Fellowship, 2024–2025)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/SatDiff" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
    
    ArXiv
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/" >
        <img src="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/compact_huaf5cbfabad837d74a6ff8a88a71377d4_1877760_300x0_resize_lanczos_3.png" alt="SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-document-segmentation js-id-uncertainty-modeling js-id-heuristic-loss js-id-land-title-deeds js-id-thai-documents js-id-geospatial-ai js-id-weak-supervision js-id-noisy-labels js-id-legal-document-analysis js-id-deep-learning js-id-custom-loss-function js-id-computer-vision js-id-boundary-refinement js-id-real-world-data js-id-image-segmentation">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/" >CHULA: Custom Heuristic Uncertainty-Guided Loss for Accurate Land Title Deed Segmentation</a>
      </div>

      
      <a href="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/"  class="summary-link">
        <div class="article-style">
          Accurately segmenting land boundaries from Thai land title deeds is crucial for reliable land management and legal processes, but remains challenging due to low-quality scans, diverse layouts, and complex overlapping elements in documents. Existing methods often struggle with these difficulties, resulting in imprecise delineations that can cause disputes or inefficiencies. To address these issues, we propose CHULA, a novel Custom Heuristic Uncertainty-guided Loss tailored specifically for robust land title deed segmentation. CHULA uniquely combines domain-specific heuristic priors with uncertainty modeling in a unified loss function that effectively guides the model to focus on clearer regions while refining boundaries and suppressing noisy areas. Evaluated on a carefully curated Thai Land Title Deed Dataset, CHULA achieves an impressive 92.4% accuracy, significantly surpassing standard segmentation baselines. Our results highlight the promise of integrating uncertainty and heuristic knowledge to enhance segmentation accuracy in complex, real-world documents. The code is publicly available at <a href="https://github.com/kaopanboonyuen/CHULA" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/CHULA</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access (Supported by C2F Postdoctoral Funding, 2025–2026)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11146788" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/CHULA" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/CHULA/" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/CHULA/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://pbylab.github.io/" target="_blank" rel="noopener">
    
    PBY.LAB
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/" >
        <img src="/publication/chula-custom-heuristic-uncertainty-guided-loss-for-accurate-land-title-deed-segmentation/compact_hu3e75a114b342bfe505a9b9ec030e2c3b_447198_300x0_resize_lanczos_3.png" alt="CHULA: Custom Heuristic Uncertainty-Guided Loss for Accurate Land Title Deed Segmentation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-gnss js-id-precipitable-water-vapor js-id-pwv-estimation js-id-weighted-mean-temperature js-id-deep-learning js-id-attention-mechanism js-id-gru js-id-tropical-climate js-id-atmospheric-science js-id-remote-sensing">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/" >Investigating the use of deep learning-derived weighted mean temperature for GPS-PWVs estimation</a>
      </div>

      
      <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/"  class="summary-link">
        <div class="article-style">
          GNSS data offers a reliable alternative for estimating Precipitable Water Vapor (PWV), but accurate GPS-PWV determination in tropical climates requires weighted mean temperature (Tm). With traditional measurement methods often unavailable in Thailand, and existing empirical models showing low accuracy, we propose a deep learning approach. Our Bidirectional Learning with Attention (BLA) model incorporates GRUs and an attention mechanism for Tm modeling. Trained on ERA5 data (2017-2021) and evaluated on 2022 data, BLA-Tm achieved 76% improvement over conventional models, reducing biases significantly. Validation with 280 GNSS stations confirmed BLA-Tm’s superior accuracy in GPS-PWV estimation.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >C. Charoenphon</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >B. Zhang</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Journal of Spatial Science (Taylor &amp; Francis)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  Project
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.tandfonline.com/doi/full/10.1080/14498596.2025.2506695" target="_blank" rel="noopener">
  Source Document
</a>



      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/" >
        <img src="/publication/investigating-the-use-of-deep-learning-derived-weighted-mean-temperature-for-gps-pwvs-estimation/compact_hu02d2c55b833f3b38a2d2cd3499a0b5f4_944504_300x0_resize_lanczos_3.png" alt="Investigating the use of deep learning-derived weighted mean temperature for GPS-PWVs estimation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Automotive-Damage-Detection js-id-Instance-Segmentation js-id-Vision-Transformers js-id-Bidirectional-Encoder-Representations js-id-Fake-Damage-Classification js-id-Car-Part-Segmentation js-id-Deep-Learning-for-Transportation js-id-AI-for-Auto-Insurance js-id-Car-Insurance-Automation js-id-Intelligent-Vehicle-Inspection js-id-ALBERT-Model js-id-Transformer-based-Segmentation js-id-Automotive-Visual-AI js-id-Insurance-Fraud-Detection">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/" >ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation</a>
      </div>

      
      <a href="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/"  class="summary-link">
        <div class="article-style">
          This paper introduces ALBERT, an instance segmentation model designed specifically for comprehensive car damage and part segmentation. Leveraging the power of Bidirectional Encoder Representations, ALBERT incorporates advanced localization mechanisms to accurately identify and differentiate between real and fake damages as well as segment individual car parts. The model is trained on a large-scale, richly annotated automotive dataset, categorizing damage into 26 types, identifying 7 fake damage variants, and segmenting 61 distinct car parts. Our approach demonstrates strong performance in both segmentation accuracy and damage classification, paving the way for intelligent automotive inspection and assessment applications. This work not only contributes a powerful tool for automated vehicle inspection but also lays the groundwork for future research in intelligent automotive diagnostics, safety evaluation, and insurance claim automation, with significant implications for both industry and research communities.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>arXiv:2506.10524 [cs.CV]</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2506.10524" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2506.10524" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/" target="_blank" rel="noopener">
    
    MARSAIL
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/" target="_blank" rel="noopener">
    
    Blog
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/" >
        <img src="/publication/albert-advanced-localization-and-bidirectional-encoder-representations-from-transformers-for-automotive-damage-evaluation/compact_hu5212dc8bbc1656c7916fd4385d06b28b_472449_300x0_resize_lanczos_3.png" alt="ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Car-Damage-Segmentation js-id-Automotive-Insurance-AI js-id-Instance-Calibration js-id-Structural-Priors-in-Vision js-id-Localization-Aware-Attention js-id-Real-Time-Vehicle-Inspection js-id-Auto-Insurance-Automation js-id-Knowledge-Distillation js-id-Teacher–Student-Learning js-id-Transformer-Based-Segmentation js-id-Efficient-Deep-Learning js-id-AI-Model-Compression js-id-Fast-Inference-Models js-id-Insurance-Fraud-Detection js-id-Synthetic-Crash-Data js-id-Deep-Learning-for-Automotive-Claims js-id-Lightweight-Vision-Models">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/" >SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance</a>
      </div>

      
      <a href="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/"  class="summary-link">
        <div class="article-style">
          We propose SLICK, a novel and efficient framework for high-precision car damage segmentation, designed for real-world deployment in automotive insurance and inspection workflows. SLICK introduces five synergistic components, selective part segmentation guided by structural priors, localization-aware attention to highlight fine-grained damage, instance-sensitive refinement for precise boundary separation, cross-channel calibration to amplify subtle cues like scratches and dents, and a knowledge fusion module that integrates synthetic crash data, part geometry, and annotated insurance datasets. Trained using a teacher–student distillation strategy with ALBERT as the teacher, SLICK retains high segmentation fidelity while achieving up to 7× faster inference. Extensive experiments on large-scale automotive datasets demonstrate SLICK’s superior accuracy, generalization, and runtime efficiency—making it ideal for real-time, high-stakes applications in insurance automation and vehicle inspection.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>arXiv:2506.10528 [cs.CV]</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2506.10528" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2506.10528" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/" target="_blank" rel="noopener">
    
    MARSAIL
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-02-albert-vs-slick-marsail-new-ai-fashion/" target="_blank" rel="noopener">
    
    Blog
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/" >
        <img src="/publication/slick-selective-localization-and-instance-calibration-for-knowledge-enhanced-car-damage-segmentation-in-automotive-insurance/compact_hud8724758882bae9466c4501f62e26490_494996_300x0_resize_lanczos_3.png" alt="SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Text-Recognition js-id-Conditional-Random-Fields js-id-Vision-Transformer js-id-Optical-Character-Recognition js-id-Deformable-Convolutions">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/dota-deformable-optimized-transformer-architecture/" >DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation</a>
      </div>

      
      <a href="/publication/dota-deformable-optimized-transformer-architecture/"  class="summary-link">
        <div class="article-style">
          In this paper, we present a novel end-to-end framework that integrates ResNet and Vision Transformer (ViT) backbones with cutting-edge techniques such as Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations work together to significantly improve feature representation and Optical Character Recognition (OCR) performance. By replacing the standard convolution layers in the third and fourth blocks with Deformable Convolutions, the framework adapts more flexibly to complex text layouts, while adaptive dropout helps prevent overfitting and enhance generalization. Moreover, incorporating CRFs refines the sequence modeling for more accurate text recognition. Extensive experiments on six benchmark datasets—IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80—demonstrate the framework’s exceptional performance. Our method represents a significant leap forward in OCR technology, addressing challenges in recognizing text with various distortions, fonts, and orientations. The framework has proven not only effective in controlled conditions but also adaptable to more complex, real-world scenarios. The code for this framework is available at <a href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/DOTA</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >N. Nithisopa</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In 17th International Conference on Knowledge and Smart Technology (KST2025)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11003289" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dota-deformable-optimized-transformer-architecture/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2505.04175" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-53.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/dota-deformable-optimized-transformer-architecture/" >
        <img src="/publication/dota-deformable-optimized-transformer-architecture/compact_hue28a23c2ef65ff884772a0df9b38ad0f_797010_300x0_resize_lanczos_3.png" alt="DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Formal-Verification js-id-Model-Checking js-id-Petri-Nets js-id-Computational-Modeling js-id-Analytical-Models">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/" >ENGRU: A Preliminary Investigation of AI-Augmented Formal Verification and Its Challenges</a>
      </div>

      
      <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/"  class="summary-link">
        <div class="article-style">
          State-space graphs and automata are essential for modeling and analyzing computational systems. Recurrent neural networks (RNNs) underpin language models by processing sequential data and capturing contextual dependencies. Both RNNs and state-space graphs evaluate discrete-time systems, but their equivalence, especially in sentence structure modeling, remains unresolved. This paper introduces ENGRU (Enhanced Gated Recurrent Units), a deep learning approach for formal verification. ENGRU combines model checking, Colored Petri Nets (CPNs), and sequential learning to analyze systems abstractly. CPNs undergo state-space enumeration to generate graphs and automata, which are transformed into sequential representations for ENGRU to learn and predict system behaviors. ENGRU effectively predicts goal states in discrete-time models, aiding early bug detection and predictive state-space exploration. Experimental results show high accuracy and efficiency in goal state predictions. ENGRU’s source code is available at <a href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/ENGRU</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >C. Dechsupa</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >W. Vatanawood</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/10993355" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/" >
        <img src="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/compact_hueaa165650bc07e7d97d5637af97f5ff5_733744_300x0_resize_lanczos_3.png" alt="ENGRU: A Preliminary Investigation of AI-Augmented Formal Verification and Its Challenges" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Deep-Learning js-id-Sea-Surface-Currents-Forecasting js-id-Vision-Transformer js-id-GRU js-id-Spatio-Temporal-Covariance-Modeling">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/sea-vit-sea-surface-currents-forecasting/" >SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling</a>
      </div>

      
      <a href="/publication/sea-vit-sea-surface-currents-forecasting/"  class="summary-link">
        <div class="article-style">
          Forecasting sea surface currents is essential for applications such as maritime navigation, environmental monitoring, and climate analysis, particularly in regions like the Gulf of Thailand and the Andaman Sea. This paper introduces SEA-ViT, an advanced deep learning model that integrates Vision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to capture spatio-temporal covariance for predicting sea surface currents (U, V) using high-frequency radar (HF) data. The name SEA-ViT is derived from Sea Surface Currents Forecasting using Vision Transformer, highlighting the model&rsquo;s emphasis on ocean dynamics and its use of the ViT architecture to enhance forecasting capabilities. SEA-ViT is designed to unravel complex dependencies by leveraging a rich dataset spanning over 30 years and incorporating ENSO indices (El Niño, La Niña, and neutral phases) to address the intricate relationship between geographic coordinates and climatic variations. This development enhances the predictive capabilities for sea surface currents, supporting the efforts of the Geo-Informatics and Space Technology Development Agency (GISTDA) in Thailand&rsquo;s maritime regions. The code and pretrained models are available at <a href="https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In 17th International Conference on Knowledge and Smart Technology (KST2025)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11003320" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sea-vit-sea-surface-currents-forecasting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/GISTDA-SeaViT" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/GISTDA-SeaViT/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2409.16313" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-56.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/sea-vit-sea-surface-currents-forecasting/" >
        <img src="/publication/sea-vit-sea-surface-currents-forecasting/compact_hu6dfea2a8864ffe6d781e9905895c9bc9_1453080_300x0_resize_lanczos_3.png" alt="SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Deep-Learning js-id-Semantic-Segmentation js-id-Convolutional-Neural-Networks js-id-Thai-Highway-Imagery js-id-Generalized-Focal-Loss js-id-YOLO js-id-REG">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" >REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models</a>
      </div>

      
      <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/"  class="summary-link">
        <div class="article-style">
          This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements—like pavilions, pedestrian bridges, information and warning signs, and concrete guardrails—to boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we&rsquo;ve integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model’s superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>5th International Conference on Highway Engineering</em> <strong>ICHE 2024</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/11003314" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/REG" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/REG/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/REG/" target="_blank" rel="noopener">
  Poster
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/slides/20240906_Panboonyuen_AI_ThaiHighway.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
    
    Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.arxiv.org/abs/2409.09877" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-57.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" >
        <img src="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/compact_hu36f1f18a22f4ac4ee5bd7950f6fd263e_843865_300x0_resize_lanczos_3.png" alt="REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Landsat-8 js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Vision-Transformers js-id-Transformer js-id-Multi-branch-Architectures js-id-Mixed-scale-Convolutional-Feedforward-Networks">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/mevit-a-medium-resolution-vision-transformer/" >MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</a>
      </div>

      
      <a href="/publication/mevit-a-medium-resolution-vision-transformer/"  class="summary-link">
        <div class="article-style">
          In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT&rsquo;s effectiveness in accurately segmenting Thai Landsat-8 data.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing (Supported by Ratchadapisek Somphot Postdoctoral Fund, 2022–2024)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/15/21/5124" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mevit-a-medium-resolution-vision-transformer/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MeVit" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MeViT/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/GYSS/panboonyuen_MeViT_Poster_toGYSS2025.pdf" target="_blank" rel="noopener">
  Poster
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=tgcKR97Ea8I" target="_blank" rel="noopener">
  Video
</a>




      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/mevit-a-medium-resolution-vision-transformer/" >
        <img src="/publication/mevit-a-medium-resolution-vision-transformer/compact_hud19889f4bffd1304aea8878ea4a99c88_1404370_300x0_resize_lanczos_3.png" alt="MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Attention js-id-Self-Attention js-id-MARS js-id-Sequential-Quadtree-Nodes js-id-Mask-R-CNN js-id-PointRend js-id-Mask-Transfiner">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" >MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</a>
      </div>

      
      <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/"  class="summary-link">
        <div class="article-style">
          Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at <a href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/MARS</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Nithisopa</span>, <span >P. Pienroj</span>, <span >L. Jirachuphun</span>, <span >C. Watthanasirikrit</span>, <span >N. Pornwiriyakul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Image Analysis and Processing</em> <strong>ICIAP 2023</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MARS" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/MARS/panboonyuen_MARS_ICIAP2023_Poster.pdf" target="_blank" rel="noopener">
  Poster
</a>








  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/MARS/panboonyuen_MARS_ICIAP2023_Poster.pdf" target="_blank" rel="noopener">
    
    ICIAP Poster
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/doi/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
    
    ACM
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2305.04743" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2025-07-01-marsail-the-smart-engine-behind-the-future-of-car-insurance/" target="_blank" rel="noopener">
    
    MARSAIL
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MARS" target="_blank" rel="noopener">
    
    Website
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" >
        <img src="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/compact_hu2872fb430e8c008b7eed1ab9972290e6_1008939_300x0_resize_lanczos_3.png" alt="MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Attention js-id-Self-Attention js-id-MARS js-id-Sequential-Quadtree-Nodes js-id-Mask-R-CNN js-id-PointRend js-id-Mask-Transfiner">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/" >Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama</a>
      </div>

      
      <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/"  class="summary-link">
        <div class="article-style">
          Detecting objects of varying sizes, like kilometer stones, remains a significant challenge and directly affects the accuracy of object counts. Transformers have shown remarkable success in natural language processing (NLP) and image processing due to their ability to model long-range dependencies. This paper proposes an enhanced YOLO (You Only Look Once) series with two key contributions, (i) We employ a pre-training objective to obtain original visual tokens from image patches of road assets, using a pre-trained Vision Transformer (ViT) backbone, which is then fine-tuned on downstream tasks with additional task layers. (ii) We incorporate Feature Pyramid Network (FPN) decoder designs into our deep learning network to learn the significance of different input features, avoiding issues like feature mismatch and performance degradation that arise from simple summation or concatenation. Our proposed method, Transformer-Based YOLOX with FPN, effectively learns general representations of objects and significantly outperforms state-of-the-art detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. It achieves a 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP on the test-dev dataset.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Information</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2078-2489/13/1/5" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/object-detection-of-road-assets-using-transformer-based-yolox/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/" >
        <img src="/publication/object-detection-of-road-assets-using-transformer-based-yolox/compact_hu3b317f59a79e72de1b05205333c2d7f9_616141_300x0_resize_lanczos_3.png" alt="Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-DeepLab js-id-Sustainability js-id-Quality-of-Life-(QOL) js-id-Bangkok-Urbanscapes-Dataset js-id-Xception js-id-Cityscapes">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/" >Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks</a>
      </div>

      
      <a href="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/"  class="summary-link">
        <div class="article-style">
          In today&rsquo;s world, urban design and sustainable development are crucial for megacities, impacting residents&rsquo; wellbeing. Quality of Life (QOL) is a key performance indicator (KPI) used to measure the effectiveness of city planning. Traditionally, QOL is assessed through costly and time-consuming surveys, but our AI-based approach offers a more efficient solution. Using Bangkok as a case study, we apply deep convolutional neural networks (DCNNs) for semantic segmentation and object detection to gather relevant image data. Then, we use linear regression to infer QOL scores. Our method, tested with state-of-the-art models and public datasets, proves to be a practical alternative for QOL assessment, with implementation codes and datasets available at <a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">https://kaopanboonyuen.github.io/bkkurbanscapes</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >K. Thitisiriwech</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Kantavat</span>, <span >Y. Iwahori</span>, <span >B. Kijsirikul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Sustainability</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2071-1050/15/3/2847" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/QOL-TransportAI" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/QOL-TransportAI" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/QOL-TransportAI" target="_blank" rel="noopener">
    
    GitHub Page
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/" >
        <img src="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/compact_hu415a1e80b87b33a556a682352986d296_897020_300x0_resize_lanczos_3.png" alt="Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Flood-Susceptibility-Assessment js-id-Machine-Learning js-id-Artificial-Neural-Networks js-id-GIS">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/rainfall-prediction-a-machine-learning-approach/" >A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province</a>
      </div>

      
      <a href="/publication/rainfall-prediction-a-machine-learning-approach/"  class="summary-link">
        <div class="article-style">
          Flooding poses a significant challenge in Thailand due to its complex geography, traditionally addressed through GIS methods like the Flood Risk Assessment Model (FRAM) combined with the Analytical Hierarchy Process (AHP). This study assesses the efficacy of Artificial Neural Networks (ANN) in flood susceptibility mapping, using data from Ayutthaya Province and incorporating 5-fold cross-validation and Stochastic Gradient Descent (SGD) for training. ANN achieved superior performance with precision of 79.90%, recall of 79.04%, F1-score of 79.08%, and accuracy of 79.31%, outperforming the traditional FRAM approach. Notably, ANN identified that only three factors—flow accumulation, elevation, and soil types—were crucial for predicting flood-prone areas. This highlights the potential for ANN to simplify and enhance flood risk assessments. Moreover, the integration of advanced machine learning techniques underscores the evolving capability of AI in addressing complex environmental challenges.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >T. Vajeethaveesin</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Trends in Sciences (Trends Sci. or TiS)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://tis.wu.ac.th/index.php/tis/article/view/2038" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rainfall-prediction-a-machine-learning-approach/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RainNet-ML" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RainNet-ML" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/rainfall-prediction-a-machine-learning-approach/" >
        <img src="/publication/rainfall-prediction-a-machine-learning-approach/compact_hub06b8241b47209da7c8f2d1d7857e458_1587300_300x0_resize_lanczos_3.png" alt="A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Mon-Maximum-Suspension js-id-Transfer-Learning js-id-Vision-Transformer js-id-Remote-Sensing js-id-Landsat-8 js-id-Transformer">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/enhanced-feature-pyramid-vision-transformert/" >Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus</a>
      </div>

      
      <a href="/publication/enhanced-feature-pyramid-vision-transformert/"  class="summary-link">
        <div class="article-style">
          Semantic segmentation on Landsat-8 data is crucial in the integration of diverse data, allowing researchers to achieve more productivity and lower expenses. This research aimed to improve the versatile backbone for dense prediction without convolutions—namely, using the pyramid vision transformer (PRM-VS-TM) to incorporate attention mechanisms across various feature maps. Furthermore, the PRM-VS-TM constructs an end-to-end object detection system without convolutions and uses handcrafted components, such as dense anchors and non-maximum suspension (NMS). The present study was conducted on a private dataset, i.e., the Thailand Landsat-8 challenge. There are three baselines, DeepLab, Swin Transformer (Swin TF), and PRM-VS-TM. Results indicate that the proposed model significantly outperforms all current baselines on the Thailand Landsat-8 corpus, providing F1-scores greater than 80% in almost all categories. Finally, we demonstrate that our model, without utilizing pre-trained settings or any further post-processing, can outperform current state-of-the-art (SOTA) methods for both agriculture and forest classes.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Rakwatin</span>, <span >K. Intarat</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Information</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2078-2489/13/5/259" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/enhanced-feature-pyramid-vision-transformert/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/GeoAI-Landslides" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/GeoAI-Landslides/" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/enhanced-feature-pyramid-vision-transformert/" >
        <img src="/publication/enhanced-feature-pyramid-vision-transformert/compact_hu6fc248b430e2b5795f1f28d8ca3f99ec_800538_300x0_resize_lanczos_3.png" alt="Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-DeepLab js-id-Bangkok-Urbanscapes-Dataset js-id-Xception js-id-Cityscapes">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/the-bangkok-urbanscapes-dataset/" >The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks</a>
      </div>

      
      <a href="/publication/the-bangkok-urbanscapes-dataset/"  class="summary-link">
        <div class="article-style">
          This paper addresses semantic segmentation for autonomous driving systems, focusing on self-driving cars in Thailand. We introduce DeepLab-V3-A1 with Xception, an enhanced version of DeepLab-V3+, and present the Bangkok Urbanscapes dataset. Our method improves segmentation accuracy by refining the decoder and modifying the Xception backbone. Experiments on four datasets, including CamVid, Cityscapes, IDD, and our proposed dataset, show our approach performs comparably to baseline methods. Our dataset includes 701 annotated images of various Bangkok driving environments, covering eleven semantic classes. The architecture and dataset aim to aid developers in improving autonomous driving systems for diverse urban conditions. Implementation codes and dataset are available at <a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">https://kaopanboonyuen.github.io/bkkurbanscapes</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >K. Thitisiriwech</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Kantavat</span>, <span >Y. Iwahori</span>, <span >B. Kijsirikul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779212" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/the-bangkok-urbanscapes-dataset/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/bkkurbanscapes" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">
    
    GitHub Page
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/the-bangkok-urbanscapes-dataset/" >
        <img src="/publication/the-bangkok-urbanscapes-dataset/compact_hu979c8f0d7e1c7eb9d595e6800592be70_1249132_300x0_resize_lanczos_3.png" alt="The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Transformer js-id-Semantic-Segmentation js-id-Decoder-Design js-id-Swin-Transformer js-id-Vision-Transformer js-id-Self-Attention js-id-Global-Convolutional-Network">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/" >Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</a>
      </div>

      
      <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/"  class="summary-link">
        <div class="article-style">
          Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% 𝐹1 score), Thailand North Landsat-8 corpus (63.12% 𝐹1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sesning (Supported by Ratchadapisek Somphot Postdoctoral Fund, 2021–2022)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/13/24/5100" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/transformer-based-decoder-designs-for-semantic-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
    
    Slides
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    Blog
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/" >
        <img src="/publication/transformer-based-decoder-designs-for-semantic-segmentation/compact_huff770b2ecf5c710cbddd5786ef8411d7_456055_300x0_resize_lanczos_3.png" alt="Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Convolutional-Neural-Networks js-id-Landsat-8 js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Global-Convolutional-Network js-id-Encoder-Decoder-Networks js-id-ISPRS-Vaihingen js-id-Transfer-Learning js-id-Domain-Adaptation js-id-Channel-Attention js-id-Depthwise-Atrous-Convolution js-id-Feature-Fusion js-id-Remote-Sensing">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" >Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</a>
      </div>

      
      <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/"  class="summary-link">
        <div class="article-style">
          My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Chulalongkorn University</em> <strong>Thesis Evaluation - Very Good Score (Outstanding Achievement)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/FusionNetGeoLabel" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener">
  Dataset
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/picture/phd_defense_day.jpg" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/talk/ph.d.-thesis-defense/" target="_blank" rel="noopener">
    
    PhD Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    PhD Showcase
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" >
        <img src="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/compact_hucc5c721db64a119cec0d8b135db7f95b_676763_300x0_resize_lanczos_3.png" alt="Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Feature-Fusion js-id-Transfer-Learning js-id-Remote-Sensing js-id-ISPRS-Vaihingen-Dataset">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/" >Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution</a>
      </div>

      
      <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/"  class="summary-link">
        <div class="article-style">
          This paper addresses improving semantic segmentation in remote sensing for aerial and satellite images, which is crucial for agriculture, map updates, route optimization, and navigation. We propose enhancements to the state-of-the-art Enhanced Global Convolutional Network (GCN152-TL-A) by introducing a High-Resolution Representation (HR) backbone for better feature extraction, Feature Fusion (FF) to capture low-level details, and Depthwise Atrous Convolution (DA) for refined multi-resolution features. Experiments on Landsat-8 and ISPRS Vaihingen datasets demonstrate our model&rsquo;s superior performance, achieving over 90% accuracy in F1 scores and outperforming baseline models.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/12/8/1233" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
    
    Slides
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    Blog
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/" >
        <img src="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/compact_hu8e1f0cf6e17ba7cf8fe2dcb2d4677106_777696_300x0_resize_lanczos_3.png" alt="Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Global-Convolutional-Network js-id-Transfer-Learning js-id-Channel-Attention js-id-Remote-Sensing js-id-Discriminative-Filters">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/" >Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning</a>
      </div>

      
      <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/"  class="summary-link">
        <div class="article-style">
          In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, e.g., river, building, forest, etc., on raster images. A deep convolutional encoder–decoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second, “channel attention” is presented in our network in order to select the most discriminative filters (features). Third, “domain-specific transfer learning” is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets (i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of 𝐹1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sesning</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/11/1/83" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/" >
        <img src="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/compact_hu37254e66b17731df190db781954cf922_1620348_300x0_resize_lanczos_3.png" alt="Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-YOLOv5 js-id-Quality-of-Life-(QOL) js-id-Semantic-Segmentation js-id-Object-Detection js-id-Image-Recognition">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/" >Transportation Mobility Factor Extraction Using Image Recognition Techniques</a>
      </div>

      
      <a href="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/"  class="summary-link">
        <div class="article-style">
          For an urban development, the Quality of Life (QOL) of people in the city is a vital issue that should be considered. There are many researches in QOL topics that use questionnaire survey approach. These studies yield very useful information for city development planning. As the Artificial Intelligence technologies are developed very fast recently, they are applied to solve many transportation problems. In this paper, we propose a method that automatically extract mobility indicators using two image recognition techniques, Semantic Segmentation and Object Recognition. Because the mobility is an important factor in QOL evaluation, our work can be used to enhance a performance and reduce a data gathering cost of the QOL evaluation.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >P. Kantavat</span>, <span >Y. Hayashi</span>, <span >G. City</span>, <span >B. Kijsirikul</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >Y. Iwahori</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>First International Conference on Smart Technology &amp; Urban Development</em> <strong>Best Paper; (STUD 2019)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/9018796" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/QOL-TransportAI" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/QOL-TransportAI/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/QOL-TransportAI/" target="_blank" rel="noopener">
    
    GitHub Page
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/QOL-TransportAI/" target="_blank" rel="noopener">
    
    Best Young Researcher Paper Award
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/" >
        <img src="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/compact_hu66f853e7e796cad48a347fa6c9fa4aae_566432_300x0_resize_lanczos_3.png" alt="Transportation Mobility Factor Extraction Using Image Recognition Techniques" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Colorectal-Cancer js-id-Fully-Convolutional-Network js-id-CVC-EndoSceneStill js-id-Colonoscopy-Video">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" >Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</a>
      </div>

      
      <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/"  class="summary-link">
        <div class="article-style">
          Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >I. Wichakam</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Udomcharoenchaikit</span>, <span >P. Vateekul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Multimedia Modeling</em> <strong>MMM 2018</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-73603-7_32" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/VisionDL" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/VisionDL/" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" >
        <img src="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/compact_hu3df151b013ef19e8db16ff9485f84888_309033_300x0_resize_lanczos_3.png" alt="Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Convolutional-Neural-Networks js-id-Semantic-Segmentation js-id-Near-Infrared-(NIR) js-id-Short-Wave-Infrared-(SWIR) js-id-Remote-Sensing">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/semantic-segmentation-on-medium-resolution-satellite-images/" >Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices</a>
      </div>

      
      <a href="/publication/semantic-segmentation-on-medium-resolution-satellite-images/"  class="summary-link">
        <div class="article-style">
          Semantic Segmentation is a fundamental task in computer vision and remote sensing imagery. Many applications, such as urban planning, change detection, and environmental monitoring, require the accurate segmentation; hence, most segmentation tasks are performed by humans. Currently, with the growth of Deep Convolutional Neural Network (DCNN), there are many works aiming to find the best network architecture fitting for this task. However, all of the studies are based on very-high resolution satellite images, and surprisingly; none of them are implemented on medium resolution satellite images. Moreover, no research has applied geoinformatics knowledge. Therefore, we purpose to compare the semantic segmentation models, which are FCN, SegNet, and GSN using medium resolution images from Landsat-8 satellite. In addition, we propose a modified SegNet model that can be used with remote sensing derived indices. The results show that the model that achieves the highest accuracy RGB bands of medium resolution aerial imagery is SegNet. The overall accuracy of the model increases when includes Near Infrared (NIR) and Short-Wave Infrared (SWIR) band. The results showed that our proposed method (our modified SegNet model, named RGB-IR-IDX-MSN method) outperforms all of the baselines in terms of mean F1 scores.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >S. Chantharaj</span>, <span >K. Pornratthanapong</span>, <span >P. Chitsinpchayakun</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In *15th International Joint Conference on Computer Science and Software Engineering * <strong>JCSSE 2018</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/8457378/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/semantic-segmentation-on-medium-resolution-satellite-images/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener">
  Dataset
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    Blog
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/semantic-segmentation-on-medium-resolution-satellite-images/" >
        <img src="/publication/semantic-segmentation-on-medium-resolution-satellite-images/compact_hua87129e6e693cceff941178b50e40d9b_837150_300x0_resize_lanczos_3.png" alt="Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Road-Segmentation js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Exponential-Linear-Unit js-id-Conditional-Random-Fields js-id-Landscape-Metrics">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-segmentation-on-remote-sensing/" >Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</a>
      </div>

      
      <a href="/publication/road-segmentation-on-remote-sensing/"  class="summary-link">
        <div class="article-style">
          Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/9/7/680" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-remote-sensing/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
    
    IC2IT 2017 Best Paper Award
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-segmentation-on-remote-sensing/" >
        <img src="/publication/road-segmentation-on-remote-sensing/compact_hu9f88805f555d8244de91ac83c5923989_1612368_300x0_resize_lanczos_3.png" alt="Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Road-Segmentation js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Exponential-Linear-Unit">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-segmentation-on-aerial-imagery/" >An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery</a>
      </div>

      
      <a href="/publication/road-segmentation-on-aerial-imagery/"  class="summary-link">
        <div class="article-style">
          In this paper, we introduce an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Enhancements include the use of ELU (exponential linear unit) instead of ReLU, dataset augmentation with incrementally-rotated images to increase training data by eight times, and the use of landscape metrics to remove false road objects. Tested on the Massachusetts Roads dataset, our method outperformed the SegNet benchmark and other baselines in precision, recall, and F1 scores.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Computing and Information Technology (IC2IT)</em> <strong>Best Student Paper Honourable Mention (top 0.26% of submissions)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-aerial-imagery/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
    
    IC2IT 2017 Best Paper Award
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-segmentation-on-aerial-imagery/" >
        <img src="/publication/road-segmentation-on-aerial-imagery/compact_hua498d26fbf435a648e4226cf29584fa2_591805_300x0_resize_lanczos_3.png" alt="An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Road-Segmentation js-id-Connected-Component-Analysis js-id-Image-Processing">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-map-extraction-from-satellite-imagery/" >Road map extraction from satellite imagery using connected component analysis and landscape metrics</a>
      </div>

      
      <a href="/publication/road-map-extraction-from-satellite-imagery/"  class="summary-link">
        <div class="article-style">
          Road map extraction is vital for GIS and underpins many location-based applications like GPS navigation, delivery route planning, tourist attraction locating, and location-based marketing. This research uses satellite imagery, though other remotely sensed images like aerial photographs, UAVs, or drones are also applicable. Despite various proposed methods focusing primarily on accuracy, completeness of results is equally important. We enhance accuracy by incorporating connected component analysis and improve completeness using landscape metrics, which describe spatial characteristics through shape and isolation indices. Evaluated on precision, recall, quality, and F1 scores, our method achieves over 90% performance in all criteria.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE International Conference on Big Data</em> <strong>Big Data 2017</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/8258330" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-map-extraction-from-satellite-imagery/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
    
    Slides
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
    
    Blog
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-map-extraction-from-satellite-imagery/" >
        <img src="/publication/road-map-extraction-from-satellite-imagery/compact_hu97a7f026cf9c527f7189f7bb72e86721_577453_300x0_resize_lanczos_3.png" alt="Road map extraction from satellite imagery using connected component analysis and landscape metrics" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Road-Segmentation js-id-Spatio-Temporal-Data js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-K-Means-Clustering js-id-Ramer-Douglas-Peucker">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/image-vectorization-of-road-satellite-data-sets/" >Image Vectorization of Road Satellite Data Sets</a>
      </div>

      
      <a href="/publication/image-vectorization-of-road-satellite-data-sets/"  class="summary-link">
        <div class="article-style">
          Data extraction of geo-spatial objects from satellite images is a crucial step in facilitating analysis of geo-spatial or spatio-temporal data, typically involving line (road) and polygon (area) layers. This paper introduces a method for transforming satellite data (raster images) containing roads from pixel form into spatial objects comprising lines and polygons. Our algorithm consists of three primary steps. First, roads are isolated from other objects using k-means clustering. Second, line extraction is performed on the road areas by applying morphological operations to skeletonize the image, followed by enhancement using the Ramer-Douglas-Peucker algorithm. Finally, land-cover classification is applied to non-road objects to extract polygons. Experimental results demonstrate that both lines (road networks) and polygons (areas) can be accurately extracted from satellite imagery simultaneously.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Journal of Remote Sensing and GIS Association of Thailand</em> <strong>RESGAT</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://learn.gistda.or.th/wp-content/uploads/2017/06/GISTDA-Research-Image-Understaning-2559-Image-Vectorization.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/image-vectorization-of-road-satellite-data-sets/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/RemoteSegTransformer" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/RemoteSegTransformer/" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/image-vectorization-of-road-satellite-data-sets/" >
        <img src="/publication/image-vectorization-of-road-satellite-data-sets/compact_hu5f735fde0cbbf0b088a30bae6537e8b5_2587210_300x0_resize_lanczos_3.png" alt="Image Vectorization of Road Satellite Data Sets" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    
  </div>
</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="tags" class="home-section wg-tag-cloud  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Popular Topics</h1>
            
          </div>
        
      
    

      








  








<div class="col-12 col-lg-8">
  

  

    
    
    
    
    
    

    <div class="tag-cloud">
      
        
        
        
        <a href="/tag/aerial-imagery/" style="font-size:1.1333333333333333rem">Aerial Imagery</a>
      
        
        
        
        <a href="/tag/ai-applications/" style="font-size:0.7rem">AI-applications</a>
      
        
        
        
        <a href="/tag/ai-research/" style="font-size:0.7rem">AI-research</a>
      
        
        
        
        <a href="/tag/computer-vision/" style="font-size:1.6403008339583356rem">computer-vision</a>
      
        
        
        
        <a href="/tag/convolutional-neural-networks/" style="font-size:1.3868170836458344rem">Convolutional Neural Networks</a>
      
        
        
        
        <a href="/tag/deep-learning/" style="font-size:1.9596525914303582rem">deep-learning</a>
      
        
        
        
        <a href="/tag/encoder-decoder-networks/" style="font-size:1.1333333333333333rem">Encoder-Decoder Networks</a>
      
        
        
        
        <a href="/tag/geospatial-ai/" style="font-size:0.9534837503125009rem">geospatial-ai</a>
      
        
        
        
        <a href="/tag/global-convolutional-network/" style="font-size:0.9534837503125009rem">Global Convolutional Network</a>
      
        
        
        
        <a href="/tag/high-resolution-imagery/" style="font-size:1.3868170836458344rem">High-Resolution Imagery</a>
      
        
        
        
        <a href="/tag/instance-segmentation/" style="font-size:0.9534837503125009rem">instance-segmentation</a>
      
        
        
        
        <a href="/tag/landsat-8/" style="font-size:0.9534837503125009rem">Landsat-8</a>
      
        
        
        
        <a href="/tag/remote-sensing/" style="font-size:1.8701905445278066rem">Remote Sensing</a>
      
        
        
        
        <a href="/tag/road-segmentation/" style="font-size:1.2728355077845237rem">road-segmentation</a>
      
        
        
        
        <a href="/tag/running/" style="font-size:1.1333333333333333rem">Running</a>
      
        
        
        
        <a href="/tag/self-attention/" style="font-size:0.9534837503125009rem">Self-Attention</a>
      
        
        
        
        <a href="/tag/semantic-segmentation/" style="font-size:1.5666666666666667rem">Semantic Segmentation</a>
      
        
        
        
        <a href="/tag/transfer-learning/" style="font-size:1.1333333333333333rem">Transfer Learning</a>
      
        
        
        
        <a href="/tag/transformer/" style="font-size:0.9534837503125009rem">Transformer</a>
      
        
        
        
        <a href="/tag/vision-transformer/" style="font-size:1.1333333333333333rem">Vision Transformer</a>
      
    </div>
  

</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="talks" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Featured Talks</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  

  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/get-to-know-me-better/" >GET TO KNOW ME BETTER</a>
      </div>

      
      <a href="/talk/get-to-know-me-better/"  class="summary-link">
        <div class="article-style">
          Let’s meet Kao, a Senior Research Scientist at MARSAIL and a C2F High-Potential Postdoc at Chulalongkorn University. Since earning his Ph.D. in 2020, he’s been pushing the limits of AI—using transformers to unlock new insights from satellite images and geospatial data. By combining deep learning with smart optimization, Kao tackles some of the toughest challenges in remote sensing. Beyond his research, he’s passionate about inspiring and mentoring the next generation of AI innovators.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2025 5:30 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>C2F Postdoc, Chulalongkorn University</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/get-to-know-me-better/" >
          <img src="/talk/get-to-know-me-better/featured_hu00be49bf8d306be2acce952c8fa7a11f_1239445_300x0_resize_lanczos_3.png" alt="GET TO KNOW ME BETTER" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/tsccm2025-the-14th-critical-care-conference/" >TSCCM2025 (The 14th Critical Care Conference)</a>
      </div>

      
      <a href="/talk/tsccm2025-the-14th-critical-care-conference/"  class="summary-link">
        <div class="article-style">
          I was honored to be selected—among only 8 individuals—for an oral presentation at the 14th Critical Care Conference. My talk focused on CU-ICU, a Thai-language instruction-tuned model designed to assist ICU practitioners. Built on the T5 architecture, the model was fine-tuned using efficient techniques such as LoRA, AdaLoRA, and IA3. CU-ICU integrates evidence-based clinical guidelines, including those from the Surviving Sepsis Campaign, to support clinical reasoning.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2025 1:30 PM
          </span>
          
          <span class="middot-divider"></span>
          <span>Bhumisiri Mangkhalanusorn Building, King Chulalongkorn Memorial Hospital</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/tsccm2025-the-14th-critical-care-conference/" >
          <img src="/talk/tsccm2025-the-14th-critical-care-conference/featured_hu9b88248f9daf977bc66b4e98d7c2defd_1340480_300x0_resize_lanczos_3.png" alt="TSCCM2025 (The 14th Critical Care Conference)" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/uamc2025-applied-mathematics-conference/" >UAMC2025 (Applied Mathematics Conference)</a>
      </div>

      
      <a href="/talk/uamc2025-applied-mathematics-conference/"  class="summary-link">
        <div class="article-style">
          I was invited to give a talk on the transformative impact of Vision Transformers (ViTs) in the car insurance industry. The session explored the mathematical foundations of ViTs, highlighting how the self-attention mechanism enables precise visual data analysis. I also examined the use of custom loss functions to enhance claim prediction accuracy and discussed the integration of Large Language Models (LLMs) to further advance AI capabilities.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2025 1:00 PM
          </span>
          
          <span class="middot-divider"></span>
          <span>King Mongkut&#39;s Institute of Technology Ladkrabang (KMITL)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/uamc2025-applied-mathematics-conference/" >
          <img src="/talk/uamc2025-applied-mathematics-conference/featured_hu1aa0874992620a4a5b125fdd892bf6ab_631062_300x0_resize_lanczos_3.png" alt="UAMC2025 (Applied Mathematics Conference)" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/nac2025-nstda-annual-conference-2025/" >NAC2025 (NSTDA Annual Conference 2025)</a>
      </div>

      
      <a href="/talk/nac2025-nstda-annual-conference-2025/"  class="summary-link">
        <div class="article-style">
          I was invited by NSTDA to teach a session focused on advanced artificial intelligence and computer vision, using the iconic &ldquo;Where’s Waldo&rdquo; problem as a practical case study. The session explored how AI can be trained to detect complex visual patterns—not just identifying a character in a busy scene, but demonstrating the power of modern deep learning techniques. I explored cutting-edge techniques like Vision Transformers to develop models with exceptional precision in image recognition, showcasing how these advanced architectures are redefining the limits of AI perception and understanding.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2025 9:30 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>National Science and Technology Development Agency (NSTDA)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/nac2025-nstda-annual-conference-2025/" >
          <img src="/talk/nac2025-nstda-annual-conference-2025/featured_hu886a67331a8a992673e34dee8a816b44_2380872_300x0_resize_lanczos_3.png" alt="NAC2025 (NSTDA Annual Conference 2025)" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/young-scientists-quickfire-pitch/" >Young Scientists Quickfire Pitch</a>
      </div>

      
      <a href="/talk/young-scientists-quickfire-pitch/"  class="summary-link">
        <div class="article-style">
          In this quick pitch, I’m thrilled to introduce MeViT—a medium-resolution Vision Transformer developed for high-precision semantic segmentation of Landsat satellite imagery.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2024 6:30 PM
          </span>
          
          <span class="middot-divider"></span>
          <span>National University of Singapore, Singapore</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
          <iframe width="300" src="https://www.youtube.com/embed/tgcKR97Ea8I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/exploring-careers-as-an-ai-research-scientist/" >Exploring Careers as an AI Research Scientist</a>
      </div>

      
      <a href="/talk/exploring-careers-as-an-ai-research-scientist/"  class="summary-link">
        <div class="article-style">
          I recently had the opportunity to speak with high school students about ‘Career Paths for AI Research Scientists.’ During the talk, I shared my experiences as a postdoctoral researcher in AI, diving into the exciting world of artificial intelligence. I discussed the various career opportunities in the field, from academic research to industry roles, and highlighted how my own journey has shaped my understanding of Generative AI.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2024 9:05 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>NSTDA, Pathum Thani, Thailand</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/exploring-careers-as-an-ai-research-scientist/" >
          <img src="/talk/exploring-careers-as-an-ai-research-scientist/featured_hu9f97ca678baa234138ccd8f90bbea5df_3761120_300x0_resize_lanczos_3.png" alt="Exploring Careers as an AI Research Scientist" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/inspiring-the-future-of-ai-innovations/" >Inspiring the Future of AI Innovations</a>
      </div>

      
      <a href="/talk/inspiring-the-future-of-ai-innovations/"  class="summary-link">
        <div class="article-style">
          I had the opportunity to give a final orientation speech to the undergraduate students of the Department of Electrical and Computer Engineering at KMUTNB. The focus of my speech was on the transformative impact of AI, particularly highlighting the advancements in Large Language Models (LLMs) like ChatGPT. I discussed how these models have revolutionized natural language processing, enabling sophisticated interactions and problem-solving capabilities.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2024 9:09 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>ECE KMUTNB, BKK</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/inspiring-the-future-of-ai-innovations/" >
          <img src="/talk/inspiring-the-future-of-ai-innovations/featured_hu5c5d44915c3b7c0a66ab40e234b02de5_430879_300x0_resize_lanczos_3.png" alt="Inspiring the Future of AI Innovations" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/geospatial-big-data-analytics/" >Geospatial Big Data Analytics</a>
      </div>

      
      <a href="/talk/geospatial-big-data-analytics/"  class="summary-link">
        <div class="article-style">
          I was invited to present on advanced geospatial data analytics, focusing on how spatial and geographic information can be transformed into actionable insights. Leveraging PySpark for distributed computing significantly accelerates data processing, allowing efficient handling of large-scale geospatial datasets. The course also introduced distributed machine learning techniques to build scalable, high-performance predictive models.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2023 8:15 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>Geo-Informatics and Space Technology Development Agency (GISTDA)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/geospatial-big-data-analytics/" >
          <img src="/talk/geospatial-big-data-analytics/featured_huadd97d1e3023033bd097289582bc5705_219718_300x0_resize_lanczos_3.png" alt="Geospatial Big Data Analytics" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/ai-research-featured-in-techsauce-news/" >AI Research Featured in Techsauce News</a>
      </div>

      
      <a href="/talk/ai-research-featured-in-techsauce-news/"  class="summary-link">
        <div class="article-style">
          My AI research was proudly featured in Techsauce News, spotlighting breakthrough advancements in deep tech and AI innovation. This recognition not only highlights the significant impact and transformative potential of my work in advancing artificial intelligence technologies but also underscores its role in shaping the future of smart automation. By pushing the boundaries of AI research, my efforts aim to drive cutting-edge solutions that address real-world challenges, foster industry innovation, and contribute to a smarter, more connected world.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2023 10:30 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>Online / Techsauce News</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/ai-research-featured-in-techsauce-news/" >
          <img src="/talk/ai-research-featured-in-techsauce-news/featured_hucaa9159efce365802350beff1492ec88_3504204_300x0_resize_lanczos_3.png" alt="AI Research Featured in Techsauce News" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/invited-to-italy-for-iciap2023/" >Invited to Italy for ICIAP2023</a>
      </div>

      
      <a href="/talk/invited-to-italy-for-iciap2023/"  class="summary-link">
        <div class="article-style">
          I was invited to Italy to present my research, &ldquo;MARS, Mask Attention Refinement with Sequential Quadtree Nodes,&rdquo; at the ICIAP 2023 Workshop. This prestigious conference, organized biennially by CVPL under the International Association for Pattern Recognition (IAPR), will unite experts to discuss advancements in car insurance and computer vision. My research addresses the challenge of accurately evaluating car damages using MARS, which enhances instance segmentation through self-attention mechanisms and quadtree transformers.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2023 10:30 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>University of Udine, Italy</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/invited-to-italy-for-iciap2023/" >
          <img src="/talk/invited-to-italy-for-iciap2023/featured_hu9964f41bcd0a09cbed35d2d2fee2bab5_5377881_300x0_resize_lanczos_3.png" alt="Invited to Italy for ICIAP2023" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  

  
  
  

    
    
      
    

    

    <div class="see-all">
      <a href="/event/">
        See all events
        <i class="fas fa-angle-right"></i>
      </a>
    </div>
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="communities" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Research Communities</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku.pdf" target="_blank" rel="noopener">Visiting Faculty - College of Computing, Khon Kaen University</a></p>
<ul>
<li>
<p>June 2023 - Present</p>
</li>
<li>
<p>Teach courses:</p>
<ul>
<li><a href="https://github.com/kaopanboonyuen/SC310005_ArtificialIntelligence_2023s1" target="_blank" rel="noopener">SC310005 Artificial Intelligence and Machine Learning Application</a>: Introduction to AI and ML concepts and their applications.</li>
<li><a href="https://github.com/kaopanboonyuen/CP020002_SmartProcessManagement_2024s1" target="_blank" rel="noopener">CP020002 Smart Process Management</a>: Techniques for optimizing and automating business processes.</li>
<li><a href="https://github.com/kaopanboonyuen/CS101" target="_blank" rel="noopener">SC320002 Business Intelligence</a>: Methods for data analysis and decision-making in business contexts.</li>
<li><a href="https://github.com/kaopanboonyuen/CP020001_ComputerProgramming_2023s1" target="_blank" rel="noopener">CP020001 Introduction to Computers and Programming</a>: Basics of computer systems and introductory programming skills.</li>
<li><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toKKU_DataEngineer_2025.pdf" target="_blank" rel="noopener">DE200001 Fundamentals of Data Engineering</a>: Introduction to data engineering concepts and fundamental tools for beginners.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku.pdf" target="_blank" rel="noopener">Ministerial Order on the Appointment of Academic Staff (Order 5907-2566)</a></p>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku_ai_2025.pdf" target="_blank" rel="noopener">Invitation Letter for a Special Lecturer Position (Order อว 660101.26/9304)</a></p>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku_ai.pdf" target="_blank" rel="noopener">Invitation Letter for a Special Lecturer Position (Order อว 660101.26/24844)</a></p>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku_spm.pdf" target="_blank" rel="noopener">Invitation Letter for a Special Lecturer Position (Order อว 660101.26/13320)</a></p>
</li>
</ul>
</li>
<li>
<p>Guest Lecturer and AI Committee Member</p>
<ul>
<li>
<p><a href="https://www.nstda.or.th/nac/2025/youth-activities/youth-activity-2/" target="_blank" rel="noopener">Smart Detective: AI in Solving Mysteries (NAC2025)</a></p>
<ul>
<li>Delivered an interactive workshop on deep learning and computer vision, introducing how Vision Transformers achieve breakthroughs in image recognition.</li>
<li>Guided hands‑on model building—training an AI system to detect Waldo.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/UAMC2025_TVIxMARSxKMITL.pdf" target="_blank" rel="noopener">Keynote Speaker (UAMC2025)</a></p>
<ul>
<li>Delivered keynote on &ldquo;Mathematical Foundations of Vision Transformers in Car Insurance AI.&rdquo;</li>
<li><a href="https://kaopanboonyuen.github.io/files/poster/UAMC2025_TVIxMARSxKMITL_Poster.png" target="_blank" rel="noopener">Poster: UAMC2025</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toDOL_2025.pdf" target="_blank" rel="noopener">AI Instructor - Department of Lands, Thailand (2025)</a></p>
<ul>
<li>Taught Large Language Models (LLMs) using land title deed data and demonstrated AI-driven automation for creating land deeds.</li>
<li><a href="https://github.com/kaopanboonyuen/ai_for_dept_of_lands" target="_blank" rel="noopener">Code and Lecture Slides</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toKMUTNB_OCSB_2025.pdf" target="_blank" rel="noopener">AI Instructor - Office of the Cane and Sugar Board (2025)</a></p>
<ul>
<li>Leveraged Vision Transformer model for accurate sugarcane area classification and boundary detection from satellite images.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/GYSS/panboonyuen_NSTDA2024_invitation_letter.pdf" target="_blank" rel="noopener">NSTDA One Day Camp at Sirindhorn Science Home (2024)</a></p>
<ul>
<li>Talking about career opportunities and becoming a research scientist in AI as part of the GYSS2025 scholarship program.</li>
<li><a href="https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/" target="_blank" rel="noopener">Full Blog and Slide: Career Paths for AI Research Scientists</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toSurvey_CU_2023.pdf" target="_blank" rel="noopener">2108421 Modern Integrated Survey Technology (MIST) - Chulalongkorn University</a></p>
<ul>
<li>Guided students in applying Machine Learning to survey engineering.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku_ai_inspiration.pdf" target="_blank" rel="noopener">CP411701 AI Inspiration Course - Khon Kaen University</a></p>
<ul>
<li>Delivered a lecture on &ldquo;Generative AI: Current Trends and Practical Applications&rdquo; at the College of Computing, Khon Kaen University.</li>
<li><a href="https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/" target="_blank" rel="noopener">Slide: Generative AI</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/KVIS_Science_Fair_2024_Panboonyuen.pdf" target="_blank" rel="noopener">The 7th KVIS Invitational Science Fair</a></p>
<ul>
<li>Served as a committee member for the AI project at Kamnoetvidya Science Academy, Rayong, Thailand (29 January - 2 February 2024).</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toKMUTNB_asIAB.pdf" target="_blank" rel="noopener">Industrial Advisory Board (IAB) - ECE KMUTNB</a></p>
<ul>
<li>Contributed to curriculum assessment and provided comments for the Department of Electrical and Computer Engineering (ECE).</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toKunnatee_AI_2021.pdf" target="_blank" rel="noopener">AI and ML Instructor - Nomklao Kunnathi Demonstration School</a></p>
<ul>
<li>Taught AI and ML under the Design Graphics Science and Technology Learning Group for high school students (Grade 10) in the Science and Mathematics Curriculum Plan.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toThammasat_AI_2023.pdf" target="_blank" rel="noopener">Deep Learning Instructor - Thammasat University</a></p>
<ul>
<li>Conducted training on satellite data processing and interpretation for advanced military and disaster missions at the Faculty of Liberal Arts.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toThammasat_AI_Projects_2022.pdf" target="_blank" rel="noopener">Senior Project Advisor - Thammasat University</a></p>
<ul>
<li>Advised students on senior projects in the Department of Geography, Faculty of Liberal Arts.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toLandDept_AI_2024.pdf" target="_blank" rel="noopener">AI Instructor - Department of Lands, Thailand</a></p>
<ul>
<li>Taught AI using land title deed data. <a href="https://github.com/kaopanboonyuen/ai_for_dept_of_lands" target="_blank" rel="noopener">Code and Lecture Slides</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- 
* [AI-Powered Image Recognition for Transportation Mobility Factors: A Quality of Life Perspective for Bangkok City](https://kaopanboonyuen.github.io/quality-of-life-ai-transportation/)
  * Urban development hinges on improving the Quality of Life (QOL) for city inhabitants. Traditionally, QOL assessments rely heavily on questionnaire surveys, which, while informative, can be costly and time-consuming.
  * [Project (GitHub Page)](https://kaopanboonyuen.github.io/quality-of-life-ai-transportation/), [Paper](https://ieeexplore.ieee.org/document/9018796), [Code](https://github.com/kaopanboonyuen/quality-of-life-ai-transportation), [Cite](https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt)

* [Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery in Thailand](https://kaopanboonyuen.github.io/MeViT/)
  * This project introduces MeViT (Medium-Resolution Vision Transformer), a novel approach tailored for Landsat satellite imagery of key economic crops in Thailand, including para rubber, corn, and pineapple.
  * [Project (GitHub Page)](https://kaopanboonyuen.github.io/MeViT/), [Paper](https://www.mdpi.com/2072-4292/15/21/5124), [Code](https://github.com/kaopanboonyuen/MeViT), [Cite](https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt)

* [Flood Risk Assessment in Ayutthaya Province](https://kaopanboonyuen.github.io/rainfall-prediction-a-machine-learning-approach)
  * This project explores a variety of models, including Random Forest, Gradient Boosting, and Neural Networks, to build a predictive model using relevant features from the dataset.
  * [Project (GitHub Page)](https://kaopanboonyuen.github.io/rainfall-prediction-a-machine-learning-approach), [Paper](https://tis.wu.ac.th/index.php/tis/article/view/2038), [Code](https://github.com/kaopanboonyuen/rainfall-prediction-a-machine-learning-approach), [Cite](https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt)

* [The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Deep Learning](https://kaopanboonyuen.github.io/bkkurbanscapes)
  * To further study self-driving cars in Thailand, we provide both the proposed methods and the proposed dataset in this project. We hope that our architecture and our dataset would help self-driving autonomous developers improve systems for driving in many cities with unique traffic and driving conditions similar to Bangkok and elsewhere in Thailand.
  * [Project (GitHub Page)](https://kaopanboonyuen.github.io/bkkurbanscapes), [Dataset](https://www.cityscapes-dataset.com/), [Paper](https://ieeexplore.ieee.org/document/9779212), [Code](https://github.com/kaopanboonyuen/bkkurbanscapes), [Cite](https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt) -->
<!-- * [Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama](https://www.mdpi.com/2078-2489/13/1/5)
  * Identifying road asset objects in Thailand highway monitoring image sequences is essential for intelligent traffic monitoring and administration of the highway. We introduce transformer-based Feature Pyramid Network (FPN) decoder designs, injecting the FPN style of decoder design into Transformer-based YOLOX reasoning.
  * [Project](https://www.mdpi.com/2078-2489/13/1/5), [PDF](https://www.mdpi.com/2078-2489/13/1/5/pdf?version=1640592615), [Cite](https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt), [Code](https://github.com/kaopanboonyuen/) -->
<!-- * [Geospatial Big Data Analytics 2023](https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2023.jpg)
  * Extract data using web scraping with python, Visualizations using Looker Studio of Google, and PySpark for Machine Learning
  * [Poster](https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2023.jpg), [Code](https://github.com/kaopanboonyuen/GISTDA2023/tree/main/code), [Slides](https://github.com/kaopanboonyuen/GISTDA2023/tree/main/lecture_slides)

* [Geospatial Big Data Analytics 2022](https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2022.jpeg)
  * Demonstrate an understanding of the breadth of methods and techniques available for handling large volumes of geospatial data; use AI/ML methods with PySpark and techniques to conduct spatial analyses of big data and apply resulting analyses to problems within the student’s own discipline.
  * [Poster](https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2022.jpeg), [Code](https://github.com/kaopanboonyuen/GISTDA2022/tree/main/code), [Slides](https://github.com/kaopanboonyuen/GISTDA2022/tree/main/lecture_slides)

* [Achieve Data Science First Meet](https://kaopanboonyuen.github.io/files/talks/panboonyuen_data_science_talk.jpeg)
  * More and more companies realize the importance of data science, AI, and machine learning. Regardless of industry or size, organizations that wish to remain competitive in the age of big data ought to efficiently originate and implement data science capabilities or risk being left behind.
  * [Poster](https://kaopanboonyuen.github.io/files/talks/panboonyuen_data_science_talk.jpeg), [Slides](https://kaopanboonyuen.github.io/files/talks/panboonyuen_talks_2020.pdf) -->

  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>



  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  

  
  <p class="powered-by">
    ©2026 Kao Panboonyuen
  </p>
  

  
  






  <p class="powered-by">
    
    Built using <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> and the <a href="https://github.com/wowchemy/starter-hugo-academic" target="_blank" rel="noopener">Wowchemy academic template</a>. View <a href="https://github.com/kaopanboonyuen/kaopanboonyuen.github.io" target="_blank" rel="noopener">source</a>.
        
  </p>
</footer>
    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/golang.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.b36873e4e886c7b03b21e4eb97d9b6d7.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
