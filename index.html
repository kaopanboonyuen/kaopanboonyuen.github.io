<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.2.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Teerapong Panboonyuen" />

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="https://kaopanboonyuen.github.io/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    
    
    
      
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.08f2e04360a1c87f5ad39547c02bf219.css" />

  



  

  

  




  
  
  
    <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
  

  
    <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Teerapong Panboonyuen" />
  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://kaopanboonyuen.github.io/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Teerapong Panboonyuen" />
  <meta property="og:url" content="https://kaopanboonyuen.github.io/" />
  <meta property="og:title" content="Teerapong Panboonyuen" />
  <meta property="og:description" content="" /><meta property="og:image" content="https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2024-12-01T13:00:00&#43;00:00" />
    
  

  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "https://kaopanboonyuen.github.io/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "https://kaopanboonyuen.github.io/"
}
</script>


  

  

  





  <title>Teerapong Panboonyuen</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper   "  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.4be02a3b391999348b0c7478778a0e4b.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#awards" data-target="#awards"><span>Awards</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#press" data-target="#press"><span>Press</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured" data-target="#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects" data-target="#projects"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#tags" data-target="#tags"><span>Topics</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks" data-target="#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#communities" data-target="#communities"><span>Communities</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/blog/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
          
          <li class="nav-item d-none d-lg-inline-flex">
            <a class="nav-link" href="https://x.com/kaopanboonyuen" data-toggle="tooltip" data-placement="bottom" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
              <i class="fab fa-twitter" aria-hidden="true"></i>
            </a>
          </li>
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    











  
<span class="js-widget-page d-none"></span>





  
  
  
  




  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="about" class="home-section wg-about  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    

      




  










<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">

      
      
      <img class="avatar avatar-circle" src="/author/teerapong-panboonyuen/avatar_hu3bf8c3b6af25e9d1c9865942b827a76a_6860848_270x270_fill_q75_lanczos_center.jpg" alt="Teerapong Panboonyuen">
      

      <div class="portrait-title">
        <h2>Teerapong Panboonyuen</h2>
        

        
        <h3>
          <a href="https://research.google/people/NatashaJaques/" target="_blank" rel="noopener">
          <span>Senior AI Research Scientist, PostDoc Fellow</span>
          </a>
        </h3>
        
        <h3>
          
          <span>MARS, Chula</span>
          
        </h3>
        
      </div>

      <ul class="network-icon" aria-hidden="true">
        
        
        
        
          
        
        
        
        
        
        <li>
          <a href="mailto:teerapong.panboonyuen@gmail.com"  aria-label="envelope">
            <i class="fas fa-envelope big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://x.com/kaopanboonyuen" target="_blank" rel="noopener" aria-label="twitter">
            <i class="fab fa-twitter big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://scholar.google.co.th/citations?user=myy0qDgAAAAJ&amp;hl=en" target="_blank" rel="noopener" aria-label="graduation-cap">
            <i class="fas fa-graduation-cap big-icon"></i>
          </a>
        </li>
        
        
        
        
        
        
        
        
          
        
        <li>
          <a href="https://orcid.org/0000-0001-8464-4476" target="_blank" rel="noopener" aria-label="orcid">
            <i class="ai ai-orcid big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener" aria-label="github">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://kaopanboonyuen.github.io/files/panboonyuen_cv.pdf" target="_blank" rel="noopener" aria-label="download">
            <i class="fas fa-download big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://www.linkedin.com/in/teerapong-panboonyuen" target="_blank" rel="noopener" aria-label="linkedin">
            <i class="fab fa-linkedin big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-12 col-lg-8">

    
    

    <div class="article-style">
      <p>My research focuses on <strong>Pattern Recognition</strong>—developing cutting-edge algorithms with <strong>Optimization Theory</strong> and <strong>Statistical Learning</strong> to push AI&rsquo;s limits. I work with advanced models like <strong>GANs</strong> and <strong>Diffusion Models</strong>, use <strong>Semi-Supervised Learning</strong>, and explore <strong>Adversarial Attacks</strong> and <strong>Large Language Models (LLMs)</strong> to revolutionize AI capabilities.</p>
<p>I am currently a <strong>Senior AI Research Scientist</strong> at <a href="https://www.marssolution.io/" target="_blank" rel="noopener"><strong>MARS (Motor AI Recognition Solution)</strong></a> and a <strong>Postdoctoral Fellow</strong> at <a href="https://www.chula.ac.th/en/" target="_blank" rel="noopener"><strong>Chulalongkorn University</strong></a>. I earned my <a href="https://www.cp.eng.chula.ac.th/en/prospective/graduate/phd-computerengineering/" target="_blank" rel="noopener"><strong>Ph.D. in Computer Engineering</strong></a> from  <a href="https://www.chula.ac.th/en/" style="color:#FF6FE7;" target="_blank"><strong>Chulalongkorn University</strong></a>, where I specialized in AI.</p>
<p>My passion lies in advancing <strong>AI Technologies</strong> to unlock human potential. I am particularly interested in <strong>Remote Sensing</strong>, where AI unveils transformative insights and redefines how we perceive and interact with our environment.</p>
<p>You can find summaries of my academic, industry, and teaching experience in my <a href="https://kaopanboonyuen.github.io/files/panboonyuen_cv.pdf" target="_blank" rel="noopener"><strong>CV</strong></a>, and explore more about my personal life on my <a href="https://kaopanboonyuen.wordpress.com/" target="_blank" rel="noopener"><strong>blog</strong></a>. Additionally, check out some of my <a href="https://soundcloud.com/kaopanboonyuen" target="_blank" rel="noopener"><strong>music on SoundCloud</strong></a>.</p>
<p>Call me <a href="https://kaopanboonyuen.wordpress.com/" target="_blank" rel="noopener"><strong>Teerapong Panboonyuen</strong></a>, or just Kao (เก้า) in Thai: <a href="https://kaopanboonyuen.wordpress.com/" target="_blank" rel="noopener"><strong>ธีรพงศ์ ปานบุญยืน</strong></a>.</p>
<p>
  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/files/panboonyuen_cv.pdf" target="_blank">Download my CV</a>.</p>
<!-- 

  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/uploads/panboonyuen_cv_Thai.pdf" target="_blank">Download my Thai CV</a>. -->

    </div>

    <div class="row">

      
      <div class="col-md-5">
        <div class="section-subheading">Interests</div>
        <ul class="ul-interests mb-0">
          
          <li>Pattern Recognition</li>
          
          <li>Computer Vision</li>
          
          <li>Deep Learning</li>
          
          <li>Machine Learning</li>
          
          <li>Reinforcement Learning</li>
          
          <li>Human-AI Interaction</li>
          
          <li>Remote Sensing</li>
          
        </ul>
      </div>
      

      
      <div class="col-md-7">
        <div class="section-subheading">Education</div>
        <ul class="ul-edu fa-ul mb-0">
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PostDoc Fellow in AI, 2025</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD in Computer Engineering, 2020</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">MEng in Computer Engineering, 2017</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">BEng in Computer Engineering, 2015</p>
              <p class="institution">KMUTNB (Merit-based Admission Quota)</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Pre-Engineering School (PET21), 2012</p>
              <p class="institution">KMUTNB (Senior High School)</p>
            </div>
          </li>
          
        </ul>
      </div>
      

    </div>
  </div>
</div>


    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="awards" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Selected Awards</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/panboonyuen_72nd_anniversary_of_HM_King_Bhumibol_scholarship_v2.jpg" target="_blank" rel="noopener">H.M. the King Bhumibhol Adulyadej’s 72nd Birthday Anniversary Scholarship</a> (Master)</li>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/Recipient_100years_2_2560.pdf" target="_blank" rel="noopener">The 100th Anniversary Chulalongkorn University Fund for Doctoral Scholarship</a> (Ph.D.)</li>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/Recipient_90yearsfund_2_2563.pdf" target="_blank" rel="noopener">The 90th Anniversary of Chulalongkorn University Scholarship</a> (Ph.D.)</li>
<li><a href="https://kaopanboonyuen.github.io/files/postdoc/Recipient_Posdoc_6_65.pdf" target="_blank" rel="noopener">Ratchadapisek Somphot Fund for Postdoctoral Fellowship</a> (Chulalongkorn University, 2021, 2022, 2023, 2024, 2025)</li>
<li>2017 <a href="https://link.springer.com/book/10.1007/978-3-319-60663-7" target="_blank" rel="noopener">Best Student Paper Award</a> in International Conference on Computing and Information Technology</li>
</ul>
<p>Reviewer for International Journals/Conferences:</p>
<ul>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/pattern-recognition" target="_blank" rel="noopener">Pattern Recognition</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/neurocomputing" target="_blank" rel="noopener">Neurocomputing</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding" target="_blank" rel="noopener">Computer Vision and Image Understanding</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/computers-and-geosciences" target="_blank" rel="noopener">Computers and Geosciences</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://journals.plos.org/plosone/" target="_blank" rel="noopener">PLoS ONE</a> (Public Library of Science)</li>
<li>Invited Reviewer of <a href="https://www.mdpi.com/journal/remotesensing" target="_blank" rel="noopener">Remote Sensing</a> (MDPI)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/toc/tgis20/current" target="_blank" rel="noopener">GIScience &amp; Remote Sensing</a></li>
<li>Invited Reviewer of <a href="https://www.mdpi.com/journal/forests" target="_blank" rel="noopener">Forests</a> (MDPI)</li>
<li>Invited Reviewer of <a href="https://www.springer.com/journal/11063" target="_blank" rel="noopener">Neural Processing Letters</a> (Springer Nature)</li>
<li>Invited Reviewer of <a href="https://www.nature.com/srep/" target="_blank" rel="noopener">Scientific Reports</a> (Nature Portfolio)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/journals/tres20" target="_blank" rel="noopener">International Journal of Remote Sensing</a> (Taylor &amp; Francis)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/toc/lijr20/current" target="_blank" rel="noopener">International Journal of Food Properties</a> (Taylor &amp; Francis)</li>
<li>Invited Reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank" rel="noopener">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://www.ieee-cis.org/pubs/taai/" target="_blank" rel="noopener">IEEE Transactions on Artificial Intelligence</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank" rel="noopener">IEEE Transactions on Image Processing</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://dl.acm.org/journal/tkdd" target="_blank" rel="noopener">Transactions on Knowledge Discovery from Data</a> (ACM)</li>
<li>Invited Reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank" rel="noopener">IEEE Transactions on Neural Networks and Learning Systems</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://www.embs.org/tmi/" target="_blank" rel="noopener">IEEE Transactions on Medical Imaging</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://www.grss-ieee.org/publications/transactions-on-geoscience-and-remote-sensing/" target="_blank" rel="noopener">IEEE Transactions on Geoscience and Remote Sensing</a> (IEEE)</li>
</ul>


  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="press" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Selected Press</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li><a href="https://www.kaohoon.com/pr/632082" target="_blank" rel="noopener">Kaohoon</a>. <em>Excellent award-winning event at the international stage ICIAP 2023.</em></li>
<li><a href="https://www.mitihoon.com/2023/10/09/411039/" target="_blank" rel="noopener">Mitistock</a>. <em>Presenting the MARS model or Mask Attention Refinement with Sequential Quadtree Nodes, an AI model for predicting car injuries in Thailand&rsquo;s car data sets, utilizing Self-Attention Mechanisms and Sequential Quadtree Nodes.</em></li>
<li><a href="https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai" target="_blank" rel="noopener">Techsauce</a>. <em>Sharing the best innovation ideas for automatic car damage assessment using AI technology, recognized for excellence at the International Conference on Image Analysis and Processing (ICIAP 2023) in Udine, Italy.</em></li>
<li><a href="https://moneychat.co.th/mars-deep-tech-startup-gets-award-from-iciap-2023/" target="_blank" rel="noopener">Moneychat</a>. <em>Shares innovative ideas for estimating car damage with AI, recognized with an award at the International Conference on Image Analysis and Processing (ICIAP 2023) in Italy.</em></li>
<li><a href="https://www.thestorythailand.com/10/10/2023/113372/" target="_blank" rel="noopener">The Story Thailand</a>. <em>Dr. Teerapong and his team present the latest AI techniques in car wound detection with the MARS model or Mask Attention Refinement with Sequential Quadtree Nodes, achieving superior detection accuracy and setting international standards of precision.</em></li>
<li><a href="https://www.car.chula.ac.th/display7.php?bib=2156287" target="_blank" rel="noopener">Chulalongkorn University</a>. <em>Published the article &lsquo;Semantic Road Segmentation on Remotely-Sensed Images Using Deep Convolutional Neural Networks and Landscape Metrics.&rsquo;</em></li>
</ul>


  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="featured" class="home-section wg-featured  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Featured Publications</h1>
            
          </div>
        
      
    

      






























  




<div class="col-12 col-lg-8">

  

  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em> <strong>Impact Factor 4.2</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/mevit-a-medium-resolution-vision-transformer/">
    <img src="/publication/mevit-a-medium-resolution-vision-transformer/featured_hubb2d2a67c4b44c04dfd372ab97ed06f7_2102839_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mevit-a-medium-resolution-vision-transformer/">MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</a>
  </div>

  
  <a href="/publication/mevit-a-medium-resolution-vision-transformer/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT&rsquo;s effectiveness in accurately segmenting Thai Landsat-8 data.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/15/21/5124" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mevit-a-medium-resolution-vision-transformer/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MeVit" target="_blank" rel="noopener">
  Code
</a>













  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Nithisopa</span>, <span >P. Pienroj</span>, <span >L. Jirachuphun</span>, <span >C. Watthanasirikrit</span>, <span >N. Pornwiriyakul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Image Analysis and Processing</em> <strong>ICIAP 2023</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/">
    <img src="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/featured_hu2872fb430e8c008b7eed1ab9972290e6_1008939_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/">MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</a>
  </div>

  
  <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" class="summary-link">
    <div class="article-style">
      <p>Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at <a href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/MARS</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>













  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/doi/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
    
    ACM
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2305.04743" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Chulalongkorn University</em> <strong>Thesis Evaluation - Very Good Score (Outstanding Achievement)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/">
    <img src="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/featured_hu52720e836b83478b302644ff7d703760_151536_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/">Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</a>
  </div>

  
  <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" class="summary-link">
    <div class="article-style">
      <p>My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener">
  Dataset
</a>







  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/picture/phd_defense_day.jpg" target="_blank" rel="noopener">
  Source Document
</a>



  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >I. Wichakam</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Udomcharoenchaikit</span>, <span >P. Vateekul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Multimedia Modeling</em> <strong>MMM 2018</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/">
    <img src="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/featured_hue5389440b0bf3553b83698c7fc10eb5d_1115912_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/">Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</a>
  </div>

  
  <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" class="summary-link">
    <div class="article-style">
      <p>Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-73603-7_32" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >P. Srestasathiern</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em> <strong>Impact Factor 4.2</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/road-segmentation-on-remote-sensing/">
    <img src="/publication/road-segmentation-on-remote-sensing/featured_hu5832d1824d617f359dcd8e3656d08a5e_2953080_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/road-segmentation-on-remote-sensing/">Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</a>
  </div>

  
  <a href="/publication/road-segmentation-on-remote-sensing/" class="summary-link">
    <div class="article-style">
      <p>Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/9/7/680" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-remote-sensing/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >P. Srestasathiern</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Computing and Information Technology (IC2IT)</em> <strong>Best Student Paper Honourable Mention (top 0.26% of submissions)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/road-segmentation-on-aerial-imagery/">
    <img src="/publication/road-segmentation-on-aerial-imagery/featured_hu11f288c1790bfded302f0a332f5e15da_536314_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/road-segmentation-on-aerial-imagery/">An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery</a>
  </div>

  
  <a href="/publication/road-segmentation-on-aerial-imagery/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we introduce an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Enhancements include the use of ELU (exponential linear unit) instead of ReLU, dataset augmentation with incrementally-rotated images to increase training data by eight times, and the use of landscape metrics to remove false road objects. Tested on the Massachusetts Roads dataset, our method outperformed the SegNet benchmark and other baselines in precision, recall, and F1 scores.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-aerial-imagery/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
    
    ICIAP 2017 Best Papers
  </a>

  </div>
  

</div>
    
  

  
  
  

</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="projects" class="home-section wg-portfolio  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  justify-content-center">
      
        
          <div class="section-heading col-12 mb-3 text-center">
            <h1 class="mb-0">Publications</h1>
            
          </div>
        
      
    

      







<div class="col-12 ">

  <p>To find relevant content, try <a href="./publication/">searching publications</a>, filtering using the buttons below, or exploring <a href="#tags">popular topics</a>. A * denotes equal contribution.</p>


  

    

    
    
    
    
      
    

    <span class="d-none default-project-filter">*</span>

    
    
    <div class="project-toolbar">
      <div class="project-filters">
        <div class="btn-toolbar">
          <div class="btn-group flex-wrap">
            
              
              
              
                
              
              <a href="#" data-filter="*" class="btn btn-primary btn-lg active">All</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Deep-Learning" class="btn btn-primary btn-lg">Deep Learning</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Semantic-Segmentation" class="btn btn-primary btn-lg">Semantic Segmentation</a>
            
              
              
              
              <a href="#" data-filter=".js-id-High-Resolution-Imagery" class="btn btn-primary btn-lg">High-Resolution Imagery</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Remote-Sensing" class="btn btn-primary btn-lg">Remote Sensing</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Multi-branch-Architectures" class="btn btn-primary btn-lg">Multi-branch Architectures</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Mixed-scale-Convolutional-Feedforward-Networks" class="btn btn-primary btn-lg">Mixed-scale Convolutional Feedforward Networks</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Encoder-Decoder-Networks" class="btn btn-primary btn-lg">Encoder-Decoder Networks</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Transformer" class="btn btn-primary btn-lg">Transformer</a>
            
          </div>
        </div>
      </div>
    </div>
    
  

  <div class="isotope projects-container row js-layout-row ">

    
    
      
    

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Landsat-8 js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Vision-Transformers js-id-Transformer js-id-Multi-branch-Architectures js-id-Mixed-scale-Convolutional-Feedforward-Networks">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/mevit-a-medium-resolution-vision-transformer/" >MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</a>
      </div>

      
      <a href="/publication/mevit-a-medium-resolution-vision-transformer/"  class="summary-link">
        <div class="article-style">
          In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT&rsquo;s effectiveness in accurately segmenting Thai Landsat-8 data.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em> <strong>Impact Factor 4.2</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/15/21/5124" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mevit-a-medium-resolution-vision-transformer/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MeVit" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/mevit-a-medium-resolution-vision-transformer/" >
        <img src="/publication/mevit-a-medium-resolution-vision-transformer/compact_hud19889f4bffd1304aea8878ea4a99c88_1404370_300x0_resize_lanczos_3.png" alt="MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Attention js-id-Self-Attention js-id-MARS js-id-Sequential-Quadtree-Nodes js-id-Mask-R-CNN js-id-PointRend js-id-Mask-Transfiner">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" >MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</a>
      </div>

      
      <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/"  class="summary-link">
        <div class="article-style">
          Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at <a href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/MARS</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Nithisopa</span>, <span >P. Pienroj</span>, <span >L. Jirachuphun</span>, <span >C. Watthanasirikrit</span>, <span >N. Pornwiriyakul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Image Analysis and Processing</em> <strong>ICIAP 2023</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>













  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/doi/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
    
    ACM
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2305.04743" target="_blank" rel="noopener">
    
    ArXiv
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" >
        <img src="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/compact_hu2872fb430e8c008b7eed1ab9972290e6_1008939_300x0_resize_lanczos_3.png" alt="MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Attention js-id-Self-Attention js-id-MARS js-id-Sequential-Quadtree-Nodes js-id-Mask-R-CNN js-id-PointRend js-id-Mask-Transfiner">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/" >Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama</a>
      </div>

      
      <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/"  class="summary-link">
        <div class="article-style">
          Detecting varying-sized objects, such as kilometer stones, is challenging and impacts accuracy. This paper enhances YOLO with two main contributions, using a pre-trained Vision Transformer (ViT) to fine-tune model weights for road asset images and incorporating Feature Pyramid Network (FPN) decoders to handle different input features. Our method, Transformer-Based YOLOX with FPN, outperforms state-of-the-art detectors, achieving 61.5% AP on the Thailand highway corpus, surpassing YOLOv5L by 2.56% AP.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >S. Thongbai</span>, <span >W. Wongweeranimit</span>, <span >P. Santitamnont</span>, <span >K. Suphan</span>, <span >C. Charoenphon</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Information</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2078-2489/13/1/5" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/object-detection-of-road-assets-using-transformer-based-yolox/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/" >
        <img src="/publication/object-detection-of-road-assets-using-transformer-based-yolox/compact_hu3b317f59a79e72de1b05205333c2d7f9_616141_300x0_resize_lanczos_3.png" alt="Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Mon-Maximum-Suspension js-id-Transfer-Learning js-id-Vision-Transformer js-id-Remote-Sensing js-id-Landsat-8 js-id-Transformer">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/enhanced-feature-pyramid-vision-transformert/" >Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus</a>
      </div>

      
      <a href="/publication/enhanced-feature-pyramid-vision-transformert/"  class="summary-link">
        <div class="article-style">
          Semantic segmentation on Landsat-8 data is crucial in the integration of diverse data, allowing researchers to achieve more productivity and lower expenses. This research aimed to improve the versatile backbone for dense prediction without convolutions—namely, using the pyramid vision transformer (PRM-VS-TM) to incorporate attention mechanisms across various feature maps. Furthermore, the PRM-VS-TM constructs an end-to-end object detection system without convolutions and uses handcrafted components, such as dense anchors and non-maximum suspension (NMS). The present study was conducted on a private dataset, i.e., the Thailand Landsat-8 challenge. There are three baselines, DeepLab, Swin Transformer (Swin TF), and PRM-VS-TM. Results indicate that the proposed model significantly outperforms all current baselines on the Thailand Landsat-8 corpus, providing F1-scores greater than 80% in almost all categories. Finally, we demonstrate that our model, without utilizing pre-trained settings or any further post-processing, can outperform current state-of-the-art (SOTA) methods for both agriculture and forest classes.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Rakwatin</span>, <span >K. Intarat</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Information</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2078-2489/13/5/259" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/enhanced-feature-pyramid-vision-transformert/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/enhanced-feature-pyramid-vision-transformert/" >
        <img src="/publication/enhanced-feature-pyramid-vision-transformert/compact_hu6fc248b430e2b5795f1f28d8ca3f99ec_800538_300x0_resize_lanczos_3.png" alt="Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-DeepLab js-id-Bangkok-Urbanscapes-Dataset js-id-Xception js-id-Cityscapes">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/the-bangkok-urbanscapes-dataset/" >The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks</a>
      </div>

      
      <a href="/publication/the-bangkok-urbanscapes-dataset/"  class="summary-link">
        <div class="article-style">
          This paper addresses semantic segmentation for autonomous driving systems, focusing on self-driving cars in Thailand. We introduce DeepLab-V3-A1 with Xception, an enhanced version of DeepLab-V3+, and present the Bangkok Urbanscapes dataset. Our method improves segmentation accuracy by refining the decoder and modifying the Xception backbone. Experiments on four datasets, including CamVid, Cityscapes, IDD, and our proposed dataset, show our approach performs comparably to baseline methods. Our dataset includes 701 annotated images of various Bangkok driving environments, covering eleven semantic classes. The architecture and dataset aim to aid developers in improving autonomous driving systems for diverse urban conditions. Implementation codes and dataset are available at <a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">https://kaopanboonyuen.github.io/bkkurbanscapes</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >K. Thitisiriwech</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Kantavat</span>, <span >Y. Iwahori</span>, <span >B. Kijsirikul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779212" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/the-bangkok-urbanscapes-dataset/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">
    
    GitHub Page
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/the-bangkok-urbanscapes-dataset/" >
        <img src="/publication/the-bangkok-urbanscapes-dataset/compact_hu979c8f0d7e1c7eb9d595e6800592be70_1249132_300x0_resize_lanczos_3.png" alt="The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Transformer js-id-Semantic-Segmentation js-id-Decoder-Design js-id-Swin-Transformer js-id-Vision-Transformer js-id-Self-Attention js-id-Global-Convolutional-Network">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/" >Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</a>
      </div>

      
      <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/"  class="summary-link">
        <div class="article-style">
          Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% 𝐹1 score), Thailand North Landsat-8 corpus (63.12% 𝐹1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >P. Srestasathiern</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sesning</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/13/24/5100" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/transformer-based-decoder-designs-for-semantic-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/" >
        <img src="/publication/transformer-based-decoder-designs-for-semantic-segmentation/compact_huff770b2ecf5c710cbddd5786ef8411d7_456055_300x0_resize_lanczos_3.png" alt="Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Convolutional-Neural-Networks js-id-Landsat-8 js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Global-Convolutional-Network js-id-Encoder-Decoder-Networks js-id-ISPRS-Vaihingen">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" >Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</a>
      </div>

      
      <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/"  class="summary-link">
        <div class="article-style">
          My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Chulalongkorn University</em> <strong>Thesis Evaluation - Very Good Score (Outstanding Achievement)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener">
  Dataset
</a>







  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/picture/phd_defense_day.jpg" target="_blank" rel="noopener">
  Source Document
</a>



      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" >
        <img src="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/compact_hu0b35526d3ee382e79939f0f986de7e9b_753535_300x0_resize_lanczos_3.png" alt="Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Feature-Fusion js-id-Transfer-Learning js-id-Remote-Sensing js-id-ISPRS-Vaihingen-Dataset">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/" >Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution</a>
      </div>

      
      <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/"  class="summary-link">
        <div class="article-style">
          This paper addresses improving semantic segmentation in remote sensing for aerial and satellite images, which is crucial for agriculture, map updates, route optimization, and navigation. We propose enhancements to the state-of-the-art Enhanced Global Convolutional Network (GCN152-TL-A) by introducing a High-Resolution Representation (HR) backbone for better feature extraction, Feature Fusion (FF) to capture low-level details, and Depthwise Atrous Convolution (DA) for refined multi-resolution features. Experiments on Landsat-8 and ISPRS Vaihingen datasets demonstrate our model&rsquo;s superior performance, achieving over 90% accuracy in F1 scores and outperforming baseline models.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >P. Srestasathiern</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/12/8/1233" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/" >
        <img src="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/compact_hu8e1f0cf6e17ba7cf8fe2dcb2d4677106_777696_300x0_resize_lanczos_3.png" alt="Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Global-Convolutional-Network js-id-Transfer-Learning js-id-Channel-Attention js-id-Remote-Sensing js-id-Discriminative-Filters">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/" >Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning</a>
      </div>

      
      <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/"  class="summary-link">
        <div class="article-style">
          In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, e.g., river, building, forest, etc., on raster images. A deep convolutional encoder–decoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second, “channel attention” is presented in our network in order to select the most discriminative filters (features). Third, “domain-specific transfer learning” is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets (i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of 𝐹1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >P. Srestasathiern</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sesning</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/11/1/83" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/" >
        <img src="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/compact_hu81303e6a0c382a231e7d16a5b20170cf_307642_300x0_resize_lanczos_3.png" alt="Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Colorectal-Cancer js-id-Fully-Convolutional-Network js-id-CVC-EndoSceneStill js-id-Colonoscopy-Video">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" >Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</a>
      </div>

      
      <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/"  class="summary-link">
        <div class="article-style">
          Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >I. Wichakam</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Udomcharoenchaikit</span>, <span >P. Vateekul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Multimedia Modeling</em> <strong>MMM 2018</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-73603-7_32" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" >
        <img src="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/compact_hu3df151b013ef19e8db16ff9485f84888_309033_300x0_resize_lanczos_3.png" alt="Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Road-Segmentation js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Exponential-Linear-Unit js-id-Conditional-Random-Fields js-id-Landscape-Metrics">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-segmentation-on-remote-sensing/" >Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</a>
      </div>

      
      <a href="/publication/road-segmentation-on-remote-sensing/"  class="summary-link">
        <div class="article-style">
          Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >P. Srestasathiern</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em> <strong>Impact Factor 4.2</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/9/7/680" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-remote-sensing/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-segmentation-on-remote-sensing/" >
        <img src="/publication/road-segmentation-on-remote-sensing/compact_hu5832d1824d617f359dcd8e3656d08a5e_2953080_300x0_resize_lanczos_3.png" alt="Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Road-Segmentation js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Exponential-Linear-Unit">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-segmentation-on-aerial-imagery/" >An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery</a>
      </div>

      
      <a href="/publication/road-segmentation-on-aerial-imagery/"  class="summary-link">
        <div class="article-style">
          In this paper, we introduce an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Enhancements include the use of ELU (exponential linear unit) instead of ReLU, dataset augmentation with incrementally-rotated images to increase training data by eight times, and the use of landscape metrics to remove false road objects. Tested on the Massachusetts Roads dataset, our method outperformed the SegNet benchmark and other baselines in precision, recall, and F1 scores.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >P. Srestasathiern</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Computing and Information Technology (IC2IT)</em> <strong>Best Student Paper Honourable Mention (top 0.26% of submissions)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-aerial-imagery/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
    
    ICIAP 2017 Best Papers
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-segmentation-on-aerial-imagery/" >
        <img src="/publication/road-segmentation-on-aerial-imagery/compact_hua498d26fbf435a648e4226cf29584fa2_591805_300x0_resize_lanczos_3.png" alt="An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Road-Segmentation js-id-Connected-Component-Analysis js-id-Image-Processing">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-map-extraction-from-satellite-imagery/" >Road map extraction from satellite imagery using connected component analysis and landscape metrics</a>
      </div>

      
      <a href="/publication/road-map-extraction-from-satellite-imagery/"  class="summary-link">
        <div class="article-style">
          Road map extraction is vital for GIS and underpins many location-based applications like GPS navigation, delivery route planning, tourist attraction locating, and location-based marketing. This research uses satellite imagery, though other remotely sensed images like aerial photographs, UAVs, or drones are also applicable. Despite various proposed methods focusing primarily on accuracy, completeness of results is equally important. We enhance accuracy by incorporating connected component analysis and improve completeness using landscape metrics, which describe spatial characteristics through shape and isolation indices. Evaluated on precision, recall, quality, and F1 scores, our method achieves over 90% performance in all criteria.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE International Conference on Big Data</em> <strong>Big Data 2017</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/8258330" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-map-extraction-from-satellite-imagery/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-map-extraction-from-satellite-imagery/" >
        <img src="/publication/road-map-extraction-from-satellite-imagery/compact_hu97a7f026cf9c527f7189f7bb72e86721_577453_300x0_resize_lanczos_3.png" alt="Road map extraction from satellite imagery using connected component analysis and landscape metrics" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    
  </div>
</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="tags" class="home-section wg-tag-cloud  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Popular Topics</h1>
            
          </div>
        
      
    

      








  








<div class="col-12 col-lg-8">
  

  

    
    
    
    
    
    

    <div class="tag-cloud">
      
        
        
        
        <a href="/tag/aerial-imagery/" style="font-size:1.0802256254687514rem">Aerial Imagery</a>
      
        
        
        
        <a href="/tag/attention/" style="font-size:0.7rem">Attention</a>
      
        
        
        
        <a href="/tag/convolutional-neural-networks/" style="font-size:1.35rem">Convolutional Neural Networks</a>
      
        
        
        
        <a href="/tag/deep-learning/" style="font-size:1.35rem">Deep Learning</a>
      
        
        
        
        <a href="/tag/encoder-decoder-networks/" style="font-size:1.35rem">Encoder-Decoder Networks</a>
      
        
        
        
        <a href="/tag/exponential-linear-unit/" style="font-size:0.7rem">Exponential Linear Unit</a>
      
        
        
        
        <a href="/tag/global-convolutional-network/" style="font-size:1.0802256254687514rem">Global Convolutional Network</a>
      
        
        
        
        <a href="/tag/high-resolution-imagery/" style="font-size:1.35rem">High-Resolution Imagery</a>
      
        
        
        
        <a href="/tag/landsat-8/" style="font-size:1.0802256254687514rem">Landsat-8</a>
      
        
        
        
        <a href="/tag/mars/" style="font-size:0.7rem">MARS</a>
      
        
        
        
        <a href="/tag/mask-r-cnn/" style="font-size:0.7rem">Mask R-CNN</a>
      
        
        
        
        <a href="/tag/mask-transfiner/" style="font-size:0.7rem">Mask Transfiner</a>
      
        
        
        
        <a href="/tag/pointrend/" style="font-size:0.7rem">PointRend</a>
      
        
        
        
        <a href="/tag/remote-sensing/" style="font-size:1.874780699337443rem">remote-sensing</a>
      
        
        
        
        <a href="/tag/road-segmentation/" style="font-size:1.0802256254687514rem">Road Segmentation</a>
      
        
        
        
        <a href="/tag/self-attention/" style="font-size:1.0802256254687514rem">Self-Attention</a>
      
        
        
        
        <a href="/tag/semantic-segmentation/" style="font-size:1.5592532616767856rem">Semantic Segmentation</a>
      
        
        
        
        <a href="/tag/sequential-quadtree-nodes/" style="font-size:0.7rem">Sequential Quadtree Nodes</a>
      
        
        
        
        <a href="/tag/transfer-learning/" style="font-size:1.0802256254687514rem">Transfer Learning</a>
      
        
        
        
        <a href="/tag/transformer/" style="font-size:1.0802256254687514rem">Transformer</a>
      
    </div>
  

</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="talks" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Featured Talks</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  

  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/inspiring-the-future-of-ai-innovations-and-mastering-llm/" >Inspiring the Future of AI Innovations and Mastering LLM</a>
      </div>

      
      <a href="/talk/inspiring-the-future-of-ai-innovations-and-mastering-llm/"  class="summary-link">
        <div class="article-style">
          I had the opportunity to give a final orientation speech to the undergraduate students of the Department of Electrical and Computer Engineering at KMUTNB. The focus of my speech was on the transformative impact of AI, particularly highlighting the advancements in Large Language Models (LLMs) like ChatGPT. I discussed how these models have revolutionized natural language processing, enabling sophisticated interactions and problem-solving capabilities. Emphasizing the importance of mastering AI tools, I encouraged students to develop strong prompting skills to effectively control and harness the potential of AI technologies. The future of AI holds immense possibilities, and by staying adept at these emerging trends, students can significantly contribute to the field and drive innovation.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2024 
          </span>
          
          <span class="middot-divider"></span>
          <span>ECE KMUTNB, BKK</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/inspiring-the-future-of-ai-innovations-and-mastering-llm/" >
          <img src="/talk/inspiring-the-future-of-ai-innovations-and-mastering-llm/featured_hu5c5d44915c3b7c0a66ab40e234b02de5_430879_300x0_resize_lanczos_3.png" alt="Inspiring the Future of AI Innovations and Mastering LLM" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/geospatial-big-data-analytics/" >Geospatial Big Data Analytics</a>
      </div>

      
      <a href="/talk/geospatial-big-data-analytics/"  class="summary-link">
        <div class="article-style">
          Geospatial Data Analytics involves analyzing spatial and geographical data to gain insights and make informed decisions. Using PySpark, this process is accelerated through distributed computing, enabling the handling of large datasets efficiently. Distributed Machine Learning models further enhance the analysis by providing scalable and robust predictions. Visualization tools like Looker Studio present the analyzed data in an interactive and comprehensible format, facilitating better decision-making and strategic planning. This combination of technologies allows for comprehensive geospatial data analysis, uncovering patterns and trends that drive actionable insights.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2023 
          </span>
          
          <span class="middot-divider"></span>
          <span>Geo-Informatics and Space Technology Development Agency (GISTDA)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/geospatial-big-data-analytics/" >
          <img src="/talk/geospatial-big-data-analytics/featured_huadd97d1e3023033bd097289582bc5705_219718_300x0_resize_lanczos_3.png" alt="Geospatial Big Data Analytics" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/distributed-machine-learning-techniques-for-geospatial-data/" >Distributed Machine Learning Techniques for Geospatial Data</a>
      </div>

      
      <a href="/talk/distributed-machine-learning-techniques-for-geospatial-data/"  class="summary-link">
        <div class="article-style">
          I was invited to teach a course on distributed machine learning to the Geo-Informatics and Space Technology Development Agency (GISTDA). The curriculum covered fundamental concepts of PySpark, basic deep learning techniques, and practical applications of distributed training using TensorFlow. I also emphasized methods for leveraging Multi-GPU setups and implementing distributed training strategies, particularly in the context of geospatial data analytics, equipping participants with the skills needed to handle large-scale machine learning tasks efficiently.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2022 
          </span>
          
          <span class="middot-divider"></span>
          <span>Geo-Informatics and Space Technology Development Agency (GISTDA)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/distributed-machine-learning-techniques-for-geospatial-data/" >
          <img src="/talk/distributed-machine-learning-techniques-for-geospatial-data/featured_hudd1a16a7aafc92c0827260627632f5e6_237927_300x0_resize_lanczos_3.png" alt="Distributed Machine Learning Techniques for Geospatial Data" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/achieve-data-science-first-meet/" >Achieve Data Science First Meet</a>
      </div>

      
      <a href="/talk/achieve-data-science-first-meet/"  class="summary-link">
        <div class="article-style">
          I was invited to speak at the &ldquo;Achieve Data Science First Meet&rdquo; for a MOOC student project event, where I highlighted the growing recognition of data science, AI, and machine learning&rsquo;s importance across various industries. I advised that organizations, regardless of their size or sector, must effectively develop and implement data science capabilities to stay competitive in the era of big data, or risk falling behind.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2020 
          </span>
          
          <span class="middot-divider"></span>
          <span>Victor Club, Samyan Mitrtown, BKK</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/achieve-data-science-first-meet/" >
          <img src="/talk/achieve-data-science-first-meet/featured_hud3fd4aee595bfd6af19daa13643bde93_188092_300x0_resize_lanczos_3.png" alt="Achieve Data Science First Meet" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/ph.d.-thesis-defense/" >Ph.D. Thesis Defense</a>
      </div>

      
      <a href="/talk/ph.d.-thesis-defense/"  class="summary-link">
        <div class="article-style">
          My thesis defense at the Faculty of Engineering, Chulalongkorn University. This dissertation introduces a new architecture for remote sensing, featuring Global Convolutional Network (GCN), channel attention, domain-specific transfer learning, Feature Fusion (FF), and Depthwise Atrous Convolution (DA). Tests on Landsat-8 and ISPRS Vaihingen datasets show that this model significantly outperforms the baseline.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2020 
          </span>
          
          <span class="middot-divider"></span>
          <span>Chulalongkorn University</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/ph.d.-thesis-defense/" >
          <img src="/talk/ph.d.-thesis-defense/featured_hu9ccf3f7a1b6dbc32658d8788de9ecd24_1799990_300x0_resize_lanczos_3.png" alt="Ph.D. Thesis Defense" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="communities" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Research Communities</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li>
<p><a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Deep Learning</a></p>
<ul>
<li>To further study self-driving cars in Thailand, we provide both the proposed methods and the proposed dataset in this project. We hope that our architecture and our dataset would help self-driving autonomous developers improve systems for driving in many cities with unique traffic and driving conditions similar to Bangkok and elsewhere in Thailand.</li>
<li><a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">Project</a>, <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener">Dataset</a>, <a href="https://ieeexplore.ieee.org/document/9779212" target="_blank" rel="noopener">PDF</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a>, <a href="https://github.com/kaopanboonyuen/bkkurbanscapes" target="_blank" rel="noopener">Code</a></li>
</ul>
</li>
<li>
<p><a href="https://www.mdpi.com/2078-2489/13/1/5" target="_blank" rel="noopener">Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama</a></p>
<ul>
<li>Identifying road asset objects in Thailand highway monitoring image sequences is essential for intelligent traffic monitoring and administration of the highway. We introduce transformer-based Feature Pyramid Network (FPN) decoder designs, injecting the FPN style of decoder design into Transformer-based YOLOX reasoning.</li>
<li><a href="https://www.mdpi.com/2078-2489/13/1/5" target="_blank" rel="noopener">Project</a>, <a href="https://www.mdpi.com/2078-2489/13/1/5/pdf?version=1640592615" target="_blank" rel="noopener">PDF</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a>, <a href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">Code</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2023.jpg" target="_blank" rel="noopener">Geospatial Big Data Analytics 2023</a></p>
<ul>
<li>Extract data using web scraping with python, Visualizations using Looker Studio of Google, and PySpark for Machine Learning</li>
<li><a href="https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2023.jpg" target="_blank" rel="noopener">Poster</a>, <a href="https://github.com/kaopanboonyuen/GISTDA2023/tree/main/code" target="_blank" rel="noopener">Code</a>, <a href="https://github.com/kaopanboonyuen/GISTDA2023/tree/main/lecture_slides" target="_blank" rel="noopener">Slides</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2022.jpeg" target="_blank" rel="noopener">Geospatial Big Data Analytics 2022</a></p>
<ul>
<li>Demonstrate an understanding of the breadth of methods and techniques available for handling large volumes of geospatial data; use AI/ML methods with PySpark and techniques to conduct spatial analyses of big data and apply resulting analyses to problems within the student’s own discipline.</li>
<li><a href="https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2022.jpeg" target="_blank" rel="noopener">Poster</a>, <a href="https://github.com/kaopanboonyuen/GISTDA2022/tree/main/code" target="_blank" rel="noopener">Code</a>, <a href="https://github.com/kaopanboonyuen/GISTDA2022/tree/main/lecture_slides" target="_blank" rel="noopener">Slides</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/talks/panboonyuen_data_science_talk.jpeg" target="_blank" rel="noopener">Achieve Data Science First Meet</a></p>
<ul>
<li>More and more companies realize the importance of data science, AI, and machine learning. Regardless of industry or size, organizations that wish to remain competitive in the age of big data ought to efficiently originate and implement data science capabilities or risk being left behind.</li>
<li><a href="https://kaopanboonyuen.github.io/files/talks/panboonyuen_data_science_talk.jpeg" target="_blank" rel="noopener">Poster</a>, <a href="https://kaopanboonyuen.github.io/files/talks/panboonyuen_talks_2020.pdf" target="_blank" rel="noopener">Slides</a></li>
</ul>
</li>
<li>
<p><a href="https://github.com/kaopanboonyuen/TransportationMobilityFactorExtraction" target="_blank" rel="noopener">Transportation Mobility Factor Extraction Using Image Recognition Techniques</a></p>
<ul>
<li>We propose a method that automatically extract mobility indicators using two image recognition techniques: Semantic Segmentation and Object Recognition.</li>
<li><a href="https://ieeexplore.ieee.org/document/9018796" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/kaopanboonyuen/TransportationMobilityFactorExtraction" target="_blank" rel="noopener">Code</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a></li>
</ul>
</li>
<li>
<p><a href="https://github.com/kaopanboonyuen/SemanticLabelingOnRemoteSensingCorpora" target="_blank" rel="noopener">Semantic Labeling on Remote Sensing Corpora using Deep Learning</a></p>
<ul>
<li>We present a novel global convolutional network for segmenting multi-objects from aerial and satellite images.</li>
<li><a href="https://www.mdpi.com/2078-2489/13/1/5" target="_blank" rel="noopener">Paper</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a></li>
</ul>
</li>
<li>
<p><a href="https://github.com/kaopanboonyuen/Rainfall-Prediction-A-Machine-Learning-Approach" target="_blank" rel="noopener">Flood Risk Assessment in Ayutthaya Province</a></p>
<ul>
<li>We described a more efficient method to create flood susceptibility map - using Thailand’s Ayutthaya Province as a case study to assess flood prone areas.</li>
<li><a href="https://tis.wu.ac.th/index.php/tis/article/view/2038" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/kaopanboonyuen/Rainfall-Prediction-A-Machine-Learning-Approach" target="_blank" rel="noopener">Code</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a></li>
</ul>
</li>
</ul>


  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>



  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  

  
  <p class="powered-by">
    ©2024 Kao Panboonyuen
  </p>
  

  
  






  <p class="powered-by">
    
    Built using <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> and the <a href="https://github.com/wowchemy/starter-hugo-academic" target="_blank" rel="noopener">Wowchemy academic template</a>. View <a href="https://github.com/kaopanboonyuen/kaopanboonyuen.github.io" target="_blank" rel="noopener">source</a>.
        
  </p>
</footer>
    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/golang.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.b36873e4e886c7b03b21e4eb97d9b6d7.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
