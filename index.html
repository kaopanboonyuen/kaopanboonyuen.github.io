<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.2.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Teerapong Panboonyuen" />

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="https://kaopanboonyuen.github.io/" />

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">

    
    
    
    
      
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.08f2e04360a1c87f5ad39547c02bf219.css" />

  



  

  

  




  
  
  
    <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
  

  
    <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Teerapong Panboonyuen" />
  

  
    <link rel="manifest" href="/index.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://kaopanboonyuen.github.io/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Teerapong Panboonyuen" />
  <meta property="og:url" content="https://kaopanboonyuen.github.io/" />
  <meta property="og:title" content="Teerapong Panboonyuen" />
  <meta property="og:description" content="" /><meta property="og:image" content="https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://kaopanboonyuen.github.io/media/icon_hueaa9297dc78a770d45cebdfb81bbca28_1203332_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2025-07-02T00:00:00&#43;00:00" />
    
  

  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "https://kaopanboonyuen.github.io/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "https://kaopanboonyuen.github.io/"
}
</script>


  

  

  





  <title>Teerapong Panboonyuen</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper   "  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.4be02a3b391999348b0c7478778a0e4b.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Teerapong Panboonyuen</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#awards" data-target="#awards"><span>Awards</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#press" data-target="#press"><span>Press</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured" data-target="#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects" data-target="#projects"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#tags" data-target="#tags"><span>Topics</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks" data-target="#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#communities" data-target="#communities"><span>Communities</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/blog/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
          
          <li class="nav-item d-none d-lg-inline-flex">
            <a class="nav-link" href="https://x.com/kaopanboonyuen" data-toggle="tooltip" data-placement="bottom" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
              <i class="fab fa-twitter" aria-hidden="true"></i>
            </a>
          </li>
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    











  
<span class="js-widget-page d-none"></span>





  
  
  
  




  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="about" class="home-section wg-about  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    

      




  










<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">

      
      
      <img class="avatar avatar-circle" src="/author/teerapong-panboonyuen/avatar_hu3c429e132ccde7f98e52ca20c1f589ef_2676345_270x270_fill_q75_lanczos_center.jpg" alt="Teerapong Panboonyuen">
      

      <div class="portrait-title">
        <h2>Teerapong Panboonyuen</h2>
        

        
        <h3>
          <a href="https://kaopanboonyuen.github.io/" target="_blank" rel="noopener">
          <span>Senior AI Research Scientist, PostDoc Fellow</span>
          </a>
        </h3>
        
        <h3>
          
          <span>MARS, Chula</span>
          
        </h3>
        
      </div>

      <ul class="network-icon" aria-hidden="true">
        
        
        
        
          
        
        
        
        
        
        <li>
          <a href="mailto:teerapong.panboonyuen@gmail.com"  aria-label="envelope">
            <i class="fas fa-envelope big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://x.com/kaopanboonyuen" target="_blank" rel="noopener" aria-label="twitter">
            <i class="fab fa-twitter big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://scholar.google.co.th/citations?user=myy0qDgAAAAJ&amp;hl=en" target="_blank" rel="noopener" aria-label="graduation-cap">
            <i class="fas fa-graduation-cap big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://www.youtube.com/@kaopanboonyuen" target="_blank" rel="noopener" aria-label="youtube">
            <i class="fab fa-youtube big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener" aria-label="github">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://www.linkedin.com/in/teerapong-panboonyuen" target="_blank" rel="noopener" aria-label="linkedin">
            <i class="fab fa-linkedin big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://kaopanboonyuen.github.io/files/panboonyuen_cv.pdf" target="_blank" rel="noopener" aria-label="download">
            <i class="fas fa-download big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-12 col-lg-8">

    
    

    <div class="article-style">
      <p>My research focuses on <strong>Learning Representations</strong>—developing cutting-edge algorithms with optimization theory to push AI&rsquo;s limits. I work with advanced models like GANs and Diffusion Models, leverage Self-Supervised Learning, explore how Adversarial Attacks on Large Language Models (LLMs) could reshape the future of AI.</p>
<!-- My research focuses on **Learning Representations**, developing advanced algorithms based on optimization theory to push the boundaries of AI. I work with models like GANs and Diffusion Models, leverage Self-Supervised Learning, and explore Adversarial Attacks on Large Language Models (LLMs) to redefine AI's potential. -->
<p>I am currently a <strong>Senior AI Research Scientist</strong> at <a href="https://www.marssolution.io/" target="_blank" rel="noopener"><strong>MARS (Motor AI Recognition Solution)</strong></a> and a <a href="https://kaopanboonyuen.github.io/files/scholarship/Website_Announcement_Postdoc_May_2025.pdf" style="color:#FF69B4;" target="_blank"><strong>C2F High-Potential Postdoc</strong></a> at <a href="https://www.chula.ac.th/en/" target="_blank" rel="noopener"><strong>Chulalongkorn University</strong></a>. I received my <a href="https://www.cp.eng.chula.ac.th/en/prospective/graduate/phd-computerengineering/" target="_blank" rel="noopener"><strong>Ph.D. in Computer Engineering</strong></a> from <a href="https://www.chula.ac.th/en/" style="color:#FF69B4;" target="_blank"><strong>Chulalongkorn University</strong></a>, where I specialized in AI.</p>
<!-- My passion is focused on **Cognitive Intelligence** to unlock human potential. I am keenly interested in **Remote Sensing**, where LLMs reveals transformative insights and redefines how we perceive and interact with our environment. -->
<p>Passionate about <strong>Cognitive Intelligence</strong> and unlocking human potential, I’m also deeply immersed in <strong>Geospatial Intelligence</strong>, where LLMs uncover groundbreaking insights that reshape how we understand and interact with our world.</p>
<p>Detailed summaries of my academic, industry, and teaching experience can be found in my <a href="https://kaopanboonyuen.github.io/files/panboonyuen_cv.pdf" target="_blank" rel="noopener"><strong>CV</strong></a> or <a href="https://kaopanboonyuen.github.io/files/IEEE/IEEE_Biography_Panboonyuen_01.pdf" target="_blank" rel="noopener"><strong>IEEE Biography</strong></a>, and get a glimpse into my personal life on my <a href="https://kaopanboonyuen.wordpress.com/" target="_blank" rel="noopener"><strong>blog</strong></a> and <a href="https://kaopanboonyuen.tumblr.com/" target="_blank" rel="noopener"><strong>tumblr</strong></a>. Also, feel free to vibe to my <a href="https://soundcloud.com/kaopanboonyuen" target="_blank" rel="noopener"><strong>music on SoundCloud</strong></a>.</p>
<p>Thai name: <a href="https://kaopanboonyuen.github.io/kaopanboonyuen/" target="_blank" rel="noopener"><strong>ธีรพงศ์ ปานบุญยืน</strong></a>, aka <a href="https://kaopanboonyuen.wordpress.com/" target="_blank" rel="noopener"><strong>Kao Panboonyuen</strong></a>, or just Kao (เก้า).</p>
<p>
  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/files/panboonyuen_cv.pdf" target="_blank">Download my CV</a>.  
  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/files/panboonyuen_cv_th.pdf" target="_blank">Download my Thai CV (Restricted to Thai government use only)</a>.</p>
<!-- 
  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/files/panboonyuen_cv_Thai.pdf" target="_blank">Download my Thai CV</a>. -->
<!-- 

  <i class="fas fa-download  pr-1 fa-fw"></i><a href="/uploads/panboonyuen_cv_Thai.pdf" target="_blank">Download my Thai CV</a>. -->

    </div>

    <div class="row">

      
      <div class="col-md-5">
        <div class="section-subheading">Interests</div>
        <ul class="ul-interests mb-0">
          
          <li>Applied Earth Observations</li>
          
          <li>Geoscience and Remote Sensing</li>
          
          <li>Computer Vision</li>
          
          <li>Semantic Distillation</li>
          
          <li>Human-AI Interaction</li>
          
          <li>Learning Representations</li>
          
        </ul>
      </div>
      

      
      <div class="col-md-7">
        <div class="section-subheading">Education</div>
        <ul class="ul-edu fa-ul mb-0">
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PostDoc Fellow in AI, 2026</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD in Computer Engineering, 2020</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">MEng in Computer Engineering, 2017</p>
              <p class="institution">Chulalongkorn University</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">BEng in Computer Engineering, 2015</p>
              <p class="institution">KMUTNB (Top 1% in University Mathematics)</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Pre-Engineering School (PET21), 2012</p>
              <p class="institution">KMUTNB (Senior High School, 10th - 12th Grade)</p>
            </div>
          </li>
          
        </ul>
      </div>
      

    </div>
  </div>
</div>


    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="awards" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Selected Awards</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/panboonyuen_72nd_anniversary_of_HM_King_Bhumibol_scholarship_v2.jpg" target="_blank" rel="noopener">H.M. the King Bhumibhol Adulyadej’s 72nd Birthday Anniversary Scholarship</a> (Master)</li>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/Recipient_100years_2_2560.pdf" target="_blank" rel="noopener">The 100th Anniversary Chulalongkorn University Fund for Doctoral Scholarship</a> (Ph.D.)</li>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/Recipient_90yearsfund_2_2563.pdf" target="_blank" rel="noopener">The 90th Anniversary of Chulalongkorn University Scholarship</a> (Ph.D.)</li>
<li><a href="https://kaopanboonyuen.github.io/files/postdoc/Recipient_Posdoc_6_65.pdf" target="_blank" rel="noopener">Postdoctoral Grant, Ratchadapisek Research Fund (RRF) (Chulalongkorn University)</a> (Postdoc, 2021-2025)</li>
<li><a href="https://kaopanboonyuen.github.io/files/scholarship/Website_Announcement_Postdoc_May_2025.pdf" target="_blank" rel="noopener">Postdoctoral Research Grant, Second Century Fund (C2F) (Chulalongkorn University)</a> (Postdoc, 2025-2026)</li>
<li>Top 1% Score in University <a href="https://en.wikipedia.org/wiki/Differential_calculus" target="_blank" rel="noopener">Differential Calculus</a> and <a href="https://en.wikipedia.org/wiki/Engineering_mathematics" target="_blank" rel="noopener">Engineering Mathematics</a></li>
<li>2017 <a href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">Best Student Paper Award</a> in International Conference on Computing and Information Technology (<a href="https://link.springer.com/conference/ic2it" target="_blank" rel="noopener">IC2IT</a>)</li>
<li>2019 <a href="https://kaopanboonyuen.github.io/quality-of-life-ai-transportation/" target="_blank" rel="noopener">Best Young Researcher Paper Award</a> in First International Conference on Smart Technology &amp; Urban Development (<a href="https://ieeexplore.ieee.org/document/9018796" target="_blank" rel="noopener">STUD</a>)</li>
<li>2022 <a href="https://kaopanboonyuen.github.io/files/BKK_MARATHON_42KM/Kao_42K_BKKMARATHON_Finisher_01.png" target="_blank" rel="noopener">Bangkok Marathon 42.195K Finisher</a> with successfully completed a full marathon run (42.195 kilometers) (<a href="https://www.bkkmarathon.com" target="_blank" rel="noopener">Bangkok Marathon</a>)</li>
<li>2024 <a href="https://kaopanboonyuen.github.io/files/IRONMAN703/Kao_IRONMAN2024_Finisher_01.png" target="_blank" rel="noopener">IRONMAN 70.3 Finisher</a> with successfully completed a challenging triathlon consisting of a 1.9K swim, 90K bike ride, and 21.1K run (<a href="https://www.ironman.com/races" target="_blank" rel="noopener">IM70.3</a>)</li>
<li>2024 <a href="https://kaopanboonyuen.github.io/files/Laguna_Phuket_Triathlon/Panboonyuen_RaceCertificate_LAGUNA_PHUKHET_TRI_2024.png" target="_blank" rel="noopener">Laguna Phuket Triathlon Finisher</a> with successfully completed a challenging triathlon consisting of a 1.8K swim, 55K bike ride, and 12K run (<a href="https://www.lagunaphukettri.com/lpt-individual/" target="_blank" rel="noopener">LPT</a>)</li>
<li>2024 <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42" target="_blank" rel="noopener">Distinguished Reviewer for the Bronze Level</a> of IEEE Transactions on Medical Imaging (<a href="https://kaopanboonyuen.github.io/files/certificate/IEEE_Transactions_on_Medical_Imaging_Distinguished_Reviewer_Certificate_2024.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>2025 <a href="https://kaopanboonyuen.github.io/files/CBM2025_MARATHON_42KM/KAO_CBM2025_CERT.png" target="_blank" rel="noopener">Chombueng Marathon 42.195K Finisher</a> with successfully completed a full marathon run (42.195 kilometers) (<a href="https://www.runningconnect.com/event/CBM2025" target="_blank" rel="noopener">Chombueng Marathon</a>)</li>
<li>2025 <a href="https://kaopanboonyuen.github.io/files/GYSS/panboonyuen_GYSS2025.jpg" target="_blank" rel="noopener">Global Young Scientists Summit (GYSS) Scholarship</a> from Her Royal Highness Princess Maha Chakri Sirindhorn (<a href="https://kaopanboonyuen.github.io/files/GYSS/panboonyuen_GYSS2025_announcement_EN.pdf" target="_blank" rel="noopener">GYSS</a>)</li>
</ul>
<p>Reviewer for International Journals/Conferences:</p>
<ul>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/pattern-recognition" target="_blank" rel="noopener">Pattern Recognition</a> (Elsevier) (<a href="https://kaopanboonyuen.github.io/files/certificate/2025/Certificate_PR_Recognised.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/neurocomputing" target="_blank" rel="noopener">Neurocomputing</a> (Elsevier) (<a href="https://kaopanboonyuen.github.io/files/certificate/2025/Certificate_NEUCOM_Recognised.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://dl.acm.org/journal/tkdd" target="_blank" rel="noopener">Transactions on Knowledge Discovery from Data</a> (ACM)</li>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding" target="_blank" rel="noopener">Computer Vision and Image Understanding</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://www.journals.elsevier.com/computers-and-geosciences" target="_blank" rel="noopener">Computers and Geosciences</a> (Elsevier)</li>
<li>Invited Reviewer of <a href="https://www.sciencedirect.com/journal/neural-networks" target="_blank" rel="noopener">Neural Networks</a> (Elsevier) (<a href="https://kaopanboonyuen.github.io/files/certificate/2025/Certificate_NN_Recognised.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://www.mdpi.com/journal/remotesensing" target="_blank" rel="noopener">Remote Sensing</a> (MDPI)</li>
<li>Invited Reviewer of <a href="https://link.springer.com/journal/10462" target="_blank" rel="noopener">Artificial Intelligence Review</a> (Nature Portfolio) (<a href="https://kaopanboonyuen.github.io/files/certificate/2025/Reviewer_Certificate_03_March_2025.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://www.nature.com/srep/" target="_blank" rel="noopener">Scientific Reports</a> (Nature Portfolio) (<a href="https://kaopanboonyuen.github.io/files/certificate/2025/Reviewer_Certificate_25_June_2025_SR.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/toc/tgis20/current" target="_blank" rel="noopener">GIScience &amp; Remote Sensing</a> (Taylor &amp; Francis)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/journals/tejr20" target="_blank" rel="noopener">European Journal of Remote Sensing</a> (Taylor &amp; Francis)</li>
<li>Invited Reviewer of <a href="https://www.tandfonline.com/journals/tres20" target="_blank" rel="noopener">International Journal of Remote Sensing</a> (Taylor &amp; Francis)</li>
<li>Invited Reviewer of <a href="https://cis.ieee.org/publications/ieee-transactions-on-artificial-intelligence" target="_blank" rel="noopener">IEEE Transactions on Artificial Intelligence</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank" rel="noopener">IEEE Transactions on Image Processing</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://www.embs.org/tmi/" target="_blank" rel="noopener">IEEE Transactions on Medical Imaging</a> (IEEE) (<a href="https://kaopanboonyuen.github.io/files/certificate/IEEE_Transactions_on_Medical_Imaging_Distinguished_Reviewer_Certificate_2024.pdf" target="_blank" rel="noopener">Certificate</a>)</li>
<li>Invited Reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36" target="_blank" rel="noopener">IEEE Transactions on Geoscience and Remote Sensing</a> (IEEE)</li>
<li>Invited Reviewer of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank" rel="noopener">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</a> (IEEE)</li>
<li>More reviews can be found under my WoS ID <a href="https://www.webofscience.com/wos/author/rid/AAO-4985-2020" target="_blank" rel="noopener"><strong>AAO-4985-2020</strong></a>.</li>
</ul>


  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="press" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Selected Press</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li><a href="https://theleaderasia.com/?p=35538" target="_blank" rel="noopener">The Leader Asia</a>: <em>Dr. Teerapong and his team introduced their advanced AI for car damage detection at ICIAP 2023 in Udine, setting new accuracy standards with their innovative MARS model.</em></li>
<li><a href="https://techsauce.co/news/mars-deep-tech-startup-thaivivat-ai" target="_blank" rel="noopener">Techsauce</a>: <em>Highlighted their AI technology for automatic car damage assessment, earning recognition for excellence at ICIAP 2023 in Italy.</em></li>
<li><a href="https://today.line.me/th/v2/article/RBJPMlY" target="_blank" rel="noopener">LINE TODAY</a>: <em>Showcased the MARS model at ICIAP 2023, noted for its high accuracy and setting new global standards in car damage detection.</em></li>
<li><a href="https://moneychat.co.th/mars-deep-tech-startup-gets-award-from-iciap-2023/" target="_blank" rel="noopener">Moneychat</a>: <em>Reported the award-winning innovation in AI for car damage estimation presented at ICIAP 2023.</em></li>
<li><a href="https://www.kaohoon.com/pr/632082" target="_blank" rel="noopener">Kaohoon</a>: <em>Celebrated the award-winning success of MARSAIL at ICIAP 2023.</em></li>
<li><a href="https://www.mitihoon.com/2023/10/09/411039/" target="_blank" rel="noopener">Mitistock</a>: <em>Introduced the MARS model, featuring advanced self-attention mechanisms for vehicle damage assessment in Thailand.</em></li>
<li><a href="https://www.thestorythailand.com/10/10/2023/113372/" target="_blank" rel="noopener">The Story Thailand</a>: <em>Presented cutting-edge AI techniques in car wound detection, achieving high accuracy and setting international benchmarks.</em></li>
<li><a href="https://www.mediaofthailand.com/2023/10/mars-deep-tech-startup-ai-iciap-2023.html" target="_blank" rel="noopener">Media of Thailand</a>: <em>Unveiled the MARS model at ICIAP 2023, recognized globally for its precision in car damage detection.</em></li>
<li><a href="https://thailandinsurancenews.com/featured/mars-deep-tech-startup-%e0%b9%82%e0%b8%8a%e0%b8%a7%e0%b9%8c%e0%b8%99%e0%b8%a7%e0%b8%b1%e0%b8%95%e0%b8%81%e0%b8%a3%e0%b8%a3%e0%b8%a1%e0%b8%aa%e0%b8%b3%e0%b8%a3%e0%b8%a7%e0%b8%88%e0%b8%84%e0%b8%a7/" target="_blank" rel="noopener">Thailand Insurance News</a>: <em>Featured Dr. Teerapong&rsquo;s MARS model at ICIAP 2023 for its groundbreaking accuracy in car damage detection.</em></li>
<li><a href="https://www.wealthplustoday.com/2023/10/09/mars-deep/" target="_blank" rel="noopener">WealthPlusToday</a>: <em>Dr. Teerapong’s MARSAIL wowed ICIAP 2023 in Italy, clinching an excellence award for next-gen car damage detection.</em></li>
<li><a href="https://www.car.chula.ac.th/display7.php?bib=2156287" target="_blank" rel="noopener">Chulalongkorn University</a>: <em>Published a study on semantic road segmentation using deep convolutional neural networks.</em></li>
<li><a href="https://www.eng.chula.ac.th/th/48902" target="_blank" rel="noopener">Chula Engineering News</a>: <em>Featured Dr. Teerapong&rsquo;s participation in the Global Young Scientists Summit (GYSS) 2025, highlighting academic leadership and global collaboration.</em></li>
<li><a href="https://careers.thaivivat.co.th/en/newsandevents/6808628ebfad6e8912fd5c57" target="_blank" rel="noopener">Thaivivat Insurance</a>: <em>Announced Dr. Teerapong’s research recognition at UAMC 2025, emphasizing advancements in AI for urban analytics and mobility challenges.</em></li>
</ul>


  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="featured" class="home-section wg-featured  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Featured Publications</h1>
            
          </div>
        
      
    

      






























  




<div class="col-12 col-lg-8">

  

  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em> <strong>Impact Factor 3.4</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/">
    <img src="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/featured_hua09e2e14fc39ba01fcd3d2647b5c4baf_4177266_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/">SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery</a>
  </div>

  
  <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/" class="summary-link">
    <div class="article-style">
      <p>Satellite image inpainting is a critical task in remote sensing, requiring accurate restoration of missing or occluded regions for reliable image analysis. In this paper, we present SatDiff, an advanced inpainting framework based on diffusion models, specifically designed to tackle the challenges posed by very high-resolution (VHR) satellite datasets such as DeepGlobe and the Massachusetts Roads Dataset. Building on insights from our previous work, SatInPaint, we enhance the approach to achieve even higher recall and overall performance. SatDiff introduces a novel Latent Space Conditioning technique that leverages a compact latent space for efficient and precise inpainting. Additionally, we integrate Explicit Propagation into the diffusion process, enabling forward-backward fusion for improved stability and accuracy. Inspired by encoder-decoder architectures like the Segment Anything Model (SAM), SatDiff is seamlessly adaptable to diverse satellite imagery scenarios. By balancing the efficiency of preconditioned models with the flexibility of postconditioned approaches, SatDiff establishes a new benchmark in VHR satellite datasets, offering a scalable and high-performance solution for satellite image restoration. The code for SatDiff is publicly available at <a href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/SatDiff</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/SatDiff" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >C. Dechsupa</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >W. Vatanawood</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em> <strong>Impact Factor 3.4</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/">
    <img src="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/featured_hu7a81832a7eeca137a3170df504e2a58c_615118_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="ENGRU: A Preliminary Investigation of AI-Augmented Formal Verification and Its Challenges" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/">ENGRU: A Preliminary Investigation of AI-Augmented Formal Verification and Its Challenges</a>
  </div>

  
  <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/" class="summary-link">
    <div class="article-style">
      <p>State-space graphs and automata are essential for modeling and analyzing computational systems. Recurrent neural networks (RNNs) underpin language models by processing sequential data and capturing contextual dependencies. Both RNNs and state-space graphs evaluate discrete-time systems, but their equivalence, especially in sentence structure modeling, remains unresolved. This paper introduces ENGRU (Enhanced Gated Recurrent Units), a deep learning approach for formal verification. ENGRU combines model checking, Colored Petri Nets (CPNs), and sequential learning to analyze systems abstractly. CPNs undergo state-space enumeration to generate graphs and automata, which are transformed into sequential representations for ENGRU to learn and predict system behaviors. ENGRU effectively predicts goal states in discrete-time models, aiding early bug detection and predictive state-space exploration. Experimental results show high accuracy and efficiency in goal state predictions. ENGRU’s source code is available at <a href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/ENGRU</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/10993355" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">
  Project
</a>










  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In 17th International Conference on Knowledge and Smart Technology (KST2025)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/sea-vit-sea-surface-currents-forecasting/">
    <img src="/publication/sea-vit-sea-surface-currents-forecasting/featured_hu149d7f089d7ef724d8c51beb2497e6de_5716842_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/sea-vit-sea-surface-currents-forecasting/">SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling</a>
  </div>

  
  <a href="/publication/sea-vit-sea-surface-currents-forecasting/" class="summary-link">
    <div class="article-style">
      <p>Forecasting sea surface currents is essential for applications such as maritime navigation, environmental monitoring, and climate analysis, particularly in regions like the Gulf of Thailand and the Andaman Sea. This paper introduces SEA-ViT, an advanced deep learning model that integrates Vision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to capture spatio-temporal covariance for predicting sea surface currents (U, V) using high-frequency radar (HF) data. The name SEA-ViT is derived from Sea Surface Currents Forecasting using Vision Transformer, highlighting the model&rsquo;s emphasis on ocean dynamics and its use of the ViT architecture to enhance forecasting capabilities. SEA-ViT is designed to unravel complex dependencies by leveraging a rich dataset spanning over 30 years and incorporating ENSO indices (El Niño, La Niña, and neutral phases) to address the intricate relationship between geographic coordinates and climatic variations. This development enhances the predictive capabilities for sea surface currents, supporting the efforts of the Geo-Informatics and Space Technology Development Agency (GISTDA) in Thailand&rsquo;s maritime regions. The code and pretrained models are available at <a href="https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2409.16313" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sea-vit-sea-surface-currents-forecasting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/gistda-ai-sea-surface-currents/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2409.16313" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-56.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Rattanachona</span>, <span >P. Thungthin</span>, <span >N. Subsompon</span>, <span >S. Thongbai</span>, <span >W. Wongweeranimit</span>, <span >R. Phukham</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>5th International Conference on Highway Engineering</em> <strong>ICHE 2024</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/">
    <img src="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/featured_hud0a2bba3e417817482b2e4c65e1f0435_5323024_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/">REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models</a>
  </div>

  
  <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" class="summary-link">
    <div class="article-style">
      <p>This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements—like pavilions, pedestrian bridges, information and warning signs, and concrete guardrails—to boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we&rsquo;ve integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model’s superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.arxiv.org/abs/2409.09877" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/REG" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/REG/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/REG/" target="_blank" rel="noopener">
  Poster
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/slides/20240906_Panboonyuen_AI_ThaiHighway.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
    
    Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.arxiv.org/abs/2409.09877" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-57.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em> <strong>Impact Factor 4.2</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/mevit-a-medium-resolution-vision-transformer/">
    <img src="/publication/mevit-a-medium-resolution-vision-transformer/featured_hubb2d2a67c4b44c04dfd372ab97ed06f7_2102839_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mevit-a-medium-resolution-vision-transformer/">MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</a>
  </div>

  
  <a href="/publication/mevit-a-medium-resolution-vision-transformer/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT&rsquo;s effectiveness in accurately segmenting Thai Landsat-8 data.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/15/21/5124" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mevit-a-medium-resolution-vision-transformer/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MeVit" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MeViT/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/GYSS/panboonyuen_MeViT_Poster_toGYSS2025.pdf" target="_blank" rel="noopener">
  Poster
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=tgcKR97Ea8I" target="_blank" rel="noopener">
  Video
</a>




  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Nithisopa</span>, <span >P. Pienroj</span>, <span >L. Jirachuphun</span>, <span >C. Watthanasirikrit</span>, <span >N. Pornwiriyakul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Image Analysis and Processing</em> <strong>ICIAP 2023</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/">
    <img src="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/featured_hu2872fb430e8c008b7eed1ab9972290e6_1008939_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/">MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</a>
  </div>

  
  <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" class="summary-link">
    <div class="article-style">
      <p>Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at <a href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/MARS</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MARS" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/MARS/panboonyuen_MARS_ICIAP2023_Poster.pdf" target="_blank" rel="noopener">
  Poster
</a>








  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/MARS/panboonyuen_MARS_ICIAP2023_Poster.pdf" target="_blank" rel="noopener">
    
    ICIAP Poster
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/doi/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
    
    ACM
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2305.04743" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >T. Vajeethaveesin</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Trends in Sciences (Trends Sci. or TiS)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/rainfall-prediction-a-machine-learning-approach/">
    <img src="/publication/rainfall-prediction-a-machine-learning-approach/featured_hu1be311b1bf32db7800481f0b35db29c4_2683455_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/rainfall-prediction-a-machine-learning-approach/">A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province</a>
  </div>

  
  <a href="/publication/rainfall-prediction-a-machine-learning-approach/" class="summary-link">
    <div class="article-style">
      <p>Flooding poses a significant challenge in Thailand due to its complex geography, traditionally addressed through GIS methods like the Flood Risk Assessment Model (FRAM) combined with the Analytical Hierarchy Process (AHP). This study assesses the efficacy of Artificial Neural Networks (ANN) in flood susceptibility mapping, using data from Ayutthaya Province and incorporating 5-fold cross-validation and Stochastic Gradient Descent (SGD) for training. ANN achieved superior performance with precision of 79.90%, recall of 79.04%, F1-score of 79.08%, and accuracy of 79.31%, outperforming the traditional FRAM approach. Notably, ANN identified that only three factors—flow accumulation, elevation, and soil types—were crucial for predicting flood-prone areas. This highlights the potential for ANN to simplify and enhance flood risk assessments. Moreover, the integration of advanced machine learning techniques underscores the evolving capability of AI in addressing complex environmental challenges.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://tis.wu.ac.th/index.php/tis/article/view/2038" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rainfall-prediction-a-machine-learning-approach/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/rainfall-prediction-a-machine-learning-approach" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/rainfall-prediction-a-machine-learning-approach" target="_blank" rel="noopener">
  Project
</a>










  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Chulalongkorn University</em> <strong>Thesis Evaluation - Very Good Score (Outstanding Achievement)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/">
    <img src="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/featured_hub0e8538b64c26dd17fceedaa9984021c_2172024_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/">Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</a>
  </div>

  
  <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" class="summary-link">
    <div class="article-style">
      <p>My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/FusionNetGeoLabel" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener">
  Dataset
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/picture/phd_defense_day.jpg" target="_blank" rel="noopener">
  Source Document
</a>



  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >I. Wichakam</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Udomcharoenchaikit</span>, <span >P. Vateekul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Multimedia Modeling</em> <strong>MMM 2018</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/">
    <img src="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/featured_hue5389440b0bf3553b83698c7fc10eb5d_1115912_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/">Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</a>
  </div>

  
  <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" class="summary-link">
    <div class="article-style">
      <p>Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-73603-7_32" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













  </div>
  

</div>
    
  
    
      







  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em> <strong>Impact Factor 4.2</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/road-segmentation-on-remote-sensing/">
    <img src="/publication/road-segmentation-on-remote-sensing/featured_hu5832d1824d617f359dcd8e3656d08a5e_2953080_808x455_fit_q90_lanczos_3.png" class="article-banner" alt="Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields" loading="lazy">
  </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/road-segmentation-on-remote-sensing/">Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</a>
  </div>

  
  <a href="/publication/road-segmentation-on-remote-sensing/" class="summary-link">
    <div class="article-style">
      <p>Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/9/7/680" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-remote-sensing/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
    
    IC2IT 2017 Best Paper Award
  </a>

  </div>
  

</div>
    
  

  
  
  

</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="projects" class="home-section wg-portfolio  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  justify-content-center">
      
        
          <div class="section-heading col-12 mb-3 text-center">
            <h1 class="mb-0">Publications</h1>
            
          </div>
        
      
    

      







<div class="col-12 ">

  <p>To find relevant content, try <a href="./publication/">searching publications</a>, filtering using the buttons below, or exploring <a href="#tags">popular topics</a>. A * denotes equal contribution.</p>


  

    

    
    
    
    
      
    

    <span class="d-none default-project-filter">*</span>

    
    
    <div class="project-toolbar">
      <div class="project-filters">
        <div class="btn-toolbar">
          <div class="btn-group flex-wrap">
            
              
              
              
                
              
              <a href="#" data-filter="*" class="btn btn-primary btn-lg active">All</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Deep-Learning" class="btn btn-primary btn-lg">Deep Learning</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Semantic-Segmentation" class="btn btn-primary btn-lg">Semantic Segmentation</a>
            
              
              
              
              <a href="#" data-filter=".js-id-High-Resolution-Imagery" class="btn btn-primary btn-lg">High-Resolution Imagery</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Remote-Sensing" class="btn btn-primary btn-lg">Remote Sensing</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Multi-branch-Architectures" class="btn btn-primary btn-lg">Multi-branch Architectures</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Mixed-scale-Convolutional-Feedforward-Networks" class="btn btn-primary btn-lg">Mixed-scale Convolutional Feedforward Networks</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Encoder-Decoder-Networks" class="btn btn-primary btn-lg">Encoder-Decoder Networks</a>
            
              
              
              
              <a href="#" data-filter=".js-id-Transformer" class="btn btn-primary btn-lg">Transformer</a>
            
          </div>
        </div>
      </div>
    </div>
    
  

  <div class="isotope projects-container row js-layout-row ">

    
    
      
    

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Satellite-Image-Inpainting js-id-Diffusion-Models js-id-Latent-Space-Conditioning js-id-VHR-Satellite-Datasets js-id-Image-Restoration">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/" >SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery</a>
      </div>

      
      <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/"  class="summary-link">
        <div class="article-style">
          Satellite image inpainting is a critical task in remote sensing, requiring accurate restoration of missing or occluded regions for reliable image analysis. In this paper, we present SatDiff, an advanced inpainting framework based on diffusion models, specifically designed to tackle the challenges posed by very high-resolution (VHR) satellite datasets such as DeepGlobe and the Massachusetts Roads Dataset. Building on insights from our previous work, SatInPaint, we enhance the approach to achieve even higher recall and overall performance. SatDiff introduces a novel Latent Space Conditioning technique that leverages a compact latent space for efficient and precise inpainting. Additionally, we integrate Explicit Propagation into the diffusion process, enabling forward-backward fusion for improved stability and accuracy. Inspired by encoder-decoder architectures like the Segment Anything Model (SAM), SatDiff is seamlessly adaptable to diverse satellite imagery scenarios. By balancing the efficiency of preconditioned models with the flexibility of postconditioned approaches, SatDiff establishes a new benchmark in VHR satellite datasets, offering a scalable and high-performance solution for satellite image restoration. The code for SatDiff is publicly available at <a href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/SatDiff</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em> <strong>Impact Factor 3.4</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/SatDiff" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/SatDiff" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/10929005/" target="_blank" rel="noopener">
    
    ArXiv
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/" >
        <img src="/publication/satdiff-a-stable-diffusion-framework-for-inpainting/compact_huaf5cbfabad837d74a6ff8a88a71377d4_1877760_300x0_resize_lanczos_3.png" alt="SatDiff: A Stable Diffusion Framework for Inpainting Very High-Resolution Satellite Imagery" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Text-Recognition js-id-Conditional-Random-Fields js-id-Vision-Transformer js-id-Optical-Character-Recognition js-id-Deformable-Convolutions">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/dota-deformable-optimized-transformer-architecture/" >DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation</a>
      </div>

      
      <a href="/publication/dota-deformable-optimized-transformer-architecture/"  class="summary-link">
        <div class="article-style">
          Text recognition in natural images remains a highly complex yet vital challenge, with wide-ranging applications in computer vision and natural language processing. In this paper, we present a novel end-to-end framework that integrates ResNet and Vision Transformer (ViT) backbones with cutting-edge techniques such as Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations work together to significantly improve feature representation and Optical Character Recognition (OCR) performance. By replacing the standard convolution layers in the third and fourth blocks with Deformable Convolutions, the framework adapts more flexibly to complex text layouts, while adaptive dropout helps prevent overfitting and enhance generalization. Moreover, incorporating CRFs refines the sequence modeling for more accurate text recognition. Extensive experiments on six benchmark datasets—IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80—demonstrate the framework’s exceptional performance, achieving remarkable accuracies of 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, yielding an average accuracy of 77.77%. These results establish a new state-of-the-art for text recognition, showing the robustness of our approach across diverse and challenging datasets. Our method represents a significant leap forward in OCR technology, addressing challenges in recognizing text with various distortions, fonts, and orientations. The framework has proven not only effective in controlled conditions but also adaptable to more complex, real-world scenarios. The code for this framework is available at <a href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/DOTA</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >N. Nithisopa</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In 17th International Conference on Knowledge and Smart Technology (KST2025)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dota-deformable-optimized-transformer-architecture/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/DOTA" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-53.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/dota-deformable-optimized-transformer-architecture/" >
        <img src="/publication/dota-deformable-optimized-transformer-architecture/compact_hu2088feb730217643235b5ad0eba36fe5_376718_300x0_resize_lanczos_3.png" alt="DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Formal-Verification js-id-Model-Checking js-id-Petri-Nets js-id-Computational-Modeling js-id-Analytical-Models">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/" >ENGRU: A Preliminary Investigation of AI-Augmented Formal Verification and Its Challenges</a>
      </div>

      
      <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/"  class="summary-link">
        <div class="article-style">
          State-space graphs and automata are essential for modeling and analyzing computational systems. Recurrent neural networks (RNNs) underpin language models by processing sequential data and capturing contextual dependencies. Both RNNs and state-space graphs evaluate discrete-time systems, but their equivalence, especially in sentence structure modeling, remains unresolved. This paper introduces ENGRU (Enhanced Gated Recurrent Units), a deep learning approach for formal verification. ENGRU combines model checking, Colored Petri Nets (CPNs), and sequential learning to analyze systems abstractly. CPNs undergo state-space enumeration to generate graphs and automata, which are transformed into sequential representations for ENGRU to learn and predict system behaviors. ENGRU effectively predicts goal states in discrete-time models, aiding early bug detection and predictive state-space exploration. Experimental results show high accuracy and efficiency in goal state predictions. ENGRU’s source code is available at <a href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/ENGRU</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >C. Dechsupa</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >W. Vatanawood</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2025
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em> <strong>Impact Factor 3.4</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/10993355" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/ENGRU" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/" >
        <img src="/publication/engru-a-preliminary-investigation-of-ai-augmented-formal-verification-and-its-challenges/compact_hueaa165650bc07e7d97d5637af97f5ff5_733744_300x0_resize_lanczos_3.png" alt="ENGRU: A Preliminary Investigation of AI-Augmented Formal Verification and Its Challenges" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Deep-Learning js-id-Sea-Surface-Currents-Forecasting js-id-Vision-Transformer js-id-GRU js-id-Spatio-Temporal-Covariance-Modeling">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/sea-vit-sea-surface-currents-forecasting/" >SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling</a>
      </div>

      
      <a href="/publication/sea-vit-sea-surface-currents-forecasting/"  class="summary-link">
        <div class="article-style">
          Forecasting sea surface currents is essential for applications such as maritime navigation, environmental monitoring, and climate analysis, particularly in regions like the Gulf of Thailand and the Andaman Sea. This paper introduces SEA-ViT, an advanced deep learning model that integrates Vision Transformer (ViT) with bidirectional Gated Recurrent Units (GRUs) to capture spatio-temporal covariance for predicting sea surface currents (U, V) using high-frequency radar (HF) data. The name SEA-ViT is derived from Sea Surface Currents Forecasting using Vision Transformer, highlighting the model&rsquo;s emphasis on ocean dynamics and its use of the ViT architecture to enhance forecasting capabilities. SEA-ViT is designed to unravel complex dependencies by leveraging a rich dataset spanning over 30 years and incorporating ENSO indices (El Niño, La Niña, and neutral phases) to address the intricate relationship between geographic coordinates and climatic variations. This development enhances the predictive capabilities for sea surface currents, supporting the efforts of the Geo-Informatics and Space Technology Development Agency (GISTDA) in Thailand&rsquo;s maritime regions. The code and pretrained models are available at <a href="https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In 17th International Conference on Knowledge and Smart Technology (KST2025)
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2409.16313" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sea-vit-sea-surface-currents-forecasting/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/gistda-ai-sea-surface-currents" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/gistda-ai-sea-surface-currents/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2409.16313" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-56.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/sea-vit-sea-surface-currents-forecasting/" >
        <img src="/publication/sea-vit-sea-surface-currents-forecasting/compact_hu6dfea2a8864ffe6d781e9905895c9bc9_1453080_300x0_resize_lanczos_3.png" alt="SEA-ViT: Sea Surface Currents Forecasting Using Vision Transformer and GRU-Based Spatio-Temporal Covariance Modeling" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Deep-Learning js-id-Semantic-Segmentation js-id-Convolutional-Neural-Networks js-id-Thai-Highway-Imagery js-id-Generalized-Focal-Loss js-id-YOLO js-id-REG">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" >REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models</a>
      </div>

      
      <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/"  class="summary-link">
        <div class="article-style">
          This paper dives into the cutting-edge world of road asset detection on Thai highways, showcasing a novel approach that combines an upgraded REG model with Generalized Focal Loss. Our focus is on identifying key road elements—like pavilions, pedestrian bridges, information and warning signs, and concrete guardrails—to boost road safety and infrastructure management. While deep learning methods have shown promise, traditional models often struggle with accuracy in tricky conditions, such as cluttered backgrounds and variable lighting. To tackle these issues, we&rsquo;ve integrated REG with Generalized Focal Loss, enhancing its ability to detect road assets with greater precision. Our results are impressive, the REGx model led the way with a mAP50 of 80.340, mAP50-95 of 60.840, precision of 79.100, recall of 76.680, and an F1-score of 77.870. These findings highlight the REGx model’s superior performance, demonstrating the power of advanced deep learning techniques to improve highway safety and infrastructure maintenance, even in challenging conditions.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Rattanachona</span>, <span >P. Thungthin</span>, <span >N. Subsompon</span>, <span >S. Thongbai</span>, <span >W. Wongweeranimit</span>, <span >R. Phukham</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>5th International Conference on Highway Engineering</em> <strong>ICHE 2024</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.arxiv.org/abs/2409.09877" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/REG" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/REG/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/REG/" target="_blank" rel="noopener">
  Poster
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/slides/20240906_Panboonyuen_AI_ThaiHighway.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
  Source Document
</a>



  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/blog/2024-09-07-refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-models/" target="_blank" rel="noopener">
    
    Blog
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.arxiv.org/abs/2409.09877" target="_blank" rel="noopener">
    
    ArXiv
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/certificate/KST2025/Panboonyuen-Certificate-of-Contributions-57.pdf" target="_blank" rel="noopener">
    
    Certificate
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/" >
        <img src="/publication/refined-generalized-focal-loss-for-road-asset-detection-on-thai-highways-using-vision-based-detection-and-segmentation-models/compact_hu36f1f18a22f4ac4ee5bd7950f6fd263e_843865_300x0_resize_lanczos_3.png" alt="REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Landsat-8 js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Vision-Transformers js-id-Transformer js-id-Multi-branch-Architectures js-id-Mixed-scale-Convolutional-Feedforward-Networks">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/mevit-a-medium-resolution-vision-transformer/" >MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand</a>
      </div>

      
      <a href="/publication/mevit-a-medium-resolution-vision-transformer/"  class="summary-link">
        <div class="article-style">
          In this paper, we present MeViT (Medium-Resolution Vision Transformer), designed for semantic segmentation of Landsat satellite imagery, focusing on key economic crops in Thailand para rubber, corn, and pineapple. MeViT enhances Vision Transformers (ViTs) by integrating medium-resolution multi-branch architectures and revising mixed-scale convolutional feedforward networks (MixCFN) to extract multi-scale local information. Extensive experiments on a public Thailand dataset demonstrate that MeViT outperforms state-of-the-art deep learning methods, achieving a precision of 92.22%, recall of 94.69%, F1 score of 93.44%, and mean IoU of 83.63%. These results highlight MeViT&rsquo;s effectiveness in accurately segmenting Thai Landsat-8 data.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Charoenphon</span>, <span >C. Satirapod</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em> <strong>Impact Factor 4.2</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/15/21/5124" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mevit-a-medium-resolution-vision-transformer/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MeVit" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MeViT/" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/GYSS/panboonyuen_MeViT_Poster_toGYSS2025.pdf" target="_blank" rel="noopener">
  Poster
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=tgcKR97Ea8I" target="_blank" rel="noopener">
  Video
</a>




      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/mevit-a-medium-resolution-vision-transformer/" >
        <img src="/publication/mevit-a-medium-resolution-vision-transformer/compact_hud19889f4bffd1304aea8878ea4a99c88_1404370_300x0_resize_lanczos_3.png" alt="MeViT: A Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery for Agriculture in Thailand" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Attention js-id-Self-Attention js-id-MARS js-id-Sequential-Quadtree-Nodes js-id-Mask-R-CNN js-id-PointRend js-id-Mask-Transfiner">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" >MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation</a>
      </div>

      
      <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/"  class="summary-link">
        <div class="article-style">
          Evaluating car damages is crucial for the car insurance industry, but current deep learning networks fall short in accuracy due to inadequacies in handling car damage images and producing fine segmentation masks. This paper introduces MARS (Mask Attention Refinement with Sequential quadtree nodes) for instance segmentation of car damages. MARS employs self-attention mechanisms to capture global dependencies within sequential quadtree nodes and a quadtree transformer to recalibrate channel weights, resulting in highly accurate instance masks. Extensive experiments show that MARS significantly outperforms state-of-the-art methods like Mask R-CNN, PointRend, and Mask Transfiner on three popular benchmarks, achieving a +1.3 maskAP improvement with the R50-FPN backbone and +2.3 maskAP with the R101-FPN backbone on the Thai car-damage dataset. Demos are available at <a href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">https://github.com/kaopanboonyuen/MARS</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >N. Nithisopa</span>, <span >P. Pienroj</span>, <span >L. Jirachuphun</span>, <span >C. Watthanasirikrit</span>, <span >N. Pornwiriyakul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Image Analysis and Processing</em> <strong>ICIAP 2023</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/MARS" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/MARS" target="_blank" rel="noopener">
  Project
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/MARS/panboonyuen_MARS_ICIAP2023_Poster.pdf" target="_blank" rel="noopener">
  Poster
</a>








  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/files/MARS/panboonyuen_MARS_ICIAP2023_Poster.pdf" target="_blank" rel="noopener">
    
    ICIAP Poster
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/doi/10.1007/978-3-031-51023-6_3" target="_blank" rel="noopener">
    
    ACM
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2305.04743" target="_blank" rel="noopener">
    
    ArXiv
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/" >
        <img src="/publication/mars-mask-attention-refinement-with-sequential-quadtree-nodes/compact_hu2872fb430e8c008b7eed1ab9972290e6_1008939_300x0_resize_lanczos_3.png" alt="MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Attention js-id-Self-Attention js-id-MARS js-id-Sequential-Quadtree-Nodes js-id-Mask-R-CNN js-id-PointRend js-id-Mask-Transfiner">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/" >Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama</a>
      </div>

      
      <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/"  class="summary-link">
        <div class="article-style">
          Detecting objects of varying sizes, like kilometer stones, remains a significant challenge and directly affects the accuracy of object counts. Transformers have shown remarkable success in natural language processing (NLP) and image processing due to their ability to model long-range dependencies. This paper proposes an enhanced YOLO (You Only Look Once) series with two key contributions, (i) We employ a pre-training objective to obtain original visual tokens from image patches of road assets, using a pre-trained Vision Transformer (ViT) backbone, which is then fine-tuned on downstream tasks with additional task layers. (ii) We incorporate Feature Pyramid Network (FPN) decoder designs into our deep learning network to learn the significance of different input features, avoiding issues like feature mismatch and performance degradation that arise from simple summation or concatenation. Our proposed method, Transformer-Based YOLOX with FPN, effectively learns general representations of objects and significantly outperforms state-of-the-art detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. It achieves a 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP on the test-dev dataset.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >S. Thongbai</span>, <span >W. Wongweeranimit</span>, <span >P. Santitamnont</span>, <span >K. Suphan</span>, <span >C. Charoenphon</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Information</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2078-2489/13/1/5" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/object-detection-of-road-assets-using-transformer-based-yolox/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/object-detection-of-road-assets-using-transformer-based-yolox/" >
        <img src="/publication/object-detection-of-road-assets-using-transformer-based-yolox/compact_hu3b317f59a79e72de1b05205333c2d7f9_616141_300x0_resize_lanczos_3.png" alt="Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-DeepLab js-id-Sustainability js-id-Quality-of-Life-(QOL) js-id-Bangkok-Urbanscapes-Dataset js-id-Xception js-id-Cityscapes">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/" >Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks</a>
      </div>

      
      <a href="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/"  class="summary-link">
        <div class="article-style">
          In today&rsquo;s world, urban design and sustainable development are crucial for megacities, impacting residents&rsquo; wellbeing. Quality of Life (QOL) is a key performance indicator (KPI) used to measure the effectiveness of city planning. Traditionally, QOL is assessed through costly and time-consuming surveys, but our AI-based approach offers a more efficient solution. Using Bangkok as a case study, we apply deep convolutional neural networks (DCNNs) for semantic segmentation and object detection to gather relevant image data. Then, we use linear regression to infer QOL scores. Our method, tested with state-of-the-art models and public datasets, proves to be a practical alternative for QOL assessment, with implementation codes and datasets available at <a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">https://kaopanboonyuen.github.io/bkkurbanscapes</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >K. Thitisiriwech</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Kantavat</span>, <span >Y. Iwahori</span>, <span >B. Kijsirikul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Sustainability</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779212" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">
    
    GitHub Page
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/" >
        <img src="/publication/quality-of-life-prediction-in-driving-scenes-on-thailand-roads/compact_hu415a1e80b87b33a556a682352986d296_897020_300x0_resize_lanczos_3.png" alt="Quality of Life Prediction in Driving Scenes on Thailand Roads Using Information Extraction from Deep Convolutional Neural Networks" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Flood-Susceptibility-Assessment js-id-Machine-Learning js-id-Artificial-Neural-Networks js-id-GIS">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/rainfall-prediction-a-machine-learning-approach/" >A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province</a>
      </div>

      
      <a href="/publication/rainfall-prediction-a-machine-learning-approach/"  class="summary-link">
        <div class="article-style">
          Flooding poses a significant challenge in Thailand due to its complex geography, traditionally addressed through GIS methods like the Flood Risk Assessment Model (FRAM) combined with the Analytical Hierarchy Process (AHP). This study assesses the efficacy of Artificial Neural Networks (ANN) in flood susceptibility mapping, using data from Ayutthaya Province and incorporating 5-fold cross-validation and Stochastic Gradient Descent (SGD) for training. ANN achieved superior performance with precision of 79.90%, recall of 79.04%, F1-score of 79.08%, and accuracy of 79.31%, outperforming the traditional FRAM approach. Notably, ANN identified that only three factors—flow accumulation, elevation, and soil types—were crucial for predicting flood-prone areas. This highlights the potential for ANN to simplify and enhance flood risk assessments. Moreover, the integration of advanced machine learning techniques underscores the evolving capability of AI in addressing complex environmental challenges.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >T. Vajeethaveesin</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Trends in Sciences (Trends Sci. or TiS)</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://tis.wu.ac.th/index.php/tis/article/view/2038" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rainfall-prediction-a-machine-learning-approach/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/rainfall-prediction-a-machine-learning-approach" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/rainfall-prediction-a-machine-learning-approach" target="_blank" rel="noopener">
  Project
</a>










      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/rainfall-prediction-a-machine-learning-approach/" >
        <img src="/publication/rainfall-prediction-a-machine-learning-approach/compact_hub06b8241b47209da7c8f2d1d7857e458_1587300_300x0_resize_lanczos_3.png" alt="A Performance Comparison between GIS-based and Neuron Network Methods for Flood Susceptibility Assessment in Ayutthaya Province" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Mon-Maximum-Suspension js-id-Transfer-Learning js-id-Vision-Transformer js-id-Remote-Sensing js-id-Landsat-8 js-id-Transformer">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/enhanced-feature-pyramid-vision-transformert/" >Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus</a>
      </div>

      
      <a href="/publication/enhanced-feature-pyramid-vision-transformert/"  class="summary-link">
        <div class="article-style">
          Semantic segmentation on Landsat-8 data is crucial in the integration of diverse data, allowing researchers to achieve more productivity and lower expenses. This research aimed to improve the versatile backbone for dense prediction without convolutions—namely, using the pyramid vision transformer (PRM-VS-TM) to incorporate attention mechanisms across various feature maps. Furthermore, the PRM-VS-TM constructs an end-to-end object detection system without convolutions and uses handcrafted components, such as dense anchors and non-maximum suspension (NMS). The present study was conducted on a private dataset, i.e., the Thailand Landsat-8 challenge. There are three baselines, DeepLab, Swin Transformer (Swin TF), and PRM-VS-TM. Results indicate that the proposed model significantly outperforms all current baselines on the Thailand Landsat-8 corpus, providing F1-scores greater than 80% in almost all categories. Finally, we demonstrate that our model, without utilizing pre-trained settings or any further post-processing, can outperform current state-of-the-art (SOTA) methods for both agriculture and forest classes.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Rakwatin</span>, <span >K. Intarat</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Information</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2078-2489/13/5/259" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/enhanced-feature-pyramid-vision-transformert/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/enhanced-feature-pyramid-vision-transformert/" >
        <img src="/publication/enhanced-feature-pyramid-vision-transformert/compact_hu6fc248b430e2b5795f1f28d8ca3f99ec_800538_300x0_resize_lanczos_3.png" alt="Enhanced Feature Pyramid Vision Transformer for Semantic Segmentation on Thailand Landsat-8 Corpus" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-DeepLab js-id-Bangkok-Urbanscapes-Dataset js-id-Xception js-id-Cityscapes">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/the-bangkok-urbanscapes-dataset/" >The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks</a>
      </div>

      
      <a href="/publication/the-bangkok-urbanscapes-dataset/"  class="summary-link">
        <div class="article-style">
          This paper addresses semantic segmentation for autonomous driving systems, focusing on self-driving cars in Thailand. We introduce DeepLab-V3-A1 with Xception, an enhanced version of DeepLab-V3+, and present the Bangkok Urbanscapes dataset. Our method improves segmentation accuracy by refining the decoder and modifying the Xception backbone. Experiments on four datasets, including CamVid, Cityscapes, IDD, and our proposed dataset, show our approach performs comparably to baseline methods. Our dataset includes 701 annotated images of various Bangkok driving environments, covering eleven semantic classes. The architecture and dataset aim to aid developers in improving autonomous driving systems for diverse urban conditions. Implementation codes and dataset are available at <a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">https://kaopanboonyuen.github.io/bkkurbanscapes</a>.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >K. Thitisiriwech</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Kantavat</span>, <span >Y. Iwahori</span>, <span >B. Kijsirikul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE Access</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779212" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/the-bangkok-urbanscapes-dataset/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">
    
    GitHub Page
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/the-bangkok-urbanscapes-dataset/" >
        <img src="/publication/the-bangkok-urbanscapes-dataset/compact_hu979c8f0d7e1c7eb9d595e6800592be70_1249132_300x0_resize_lanczos_3.png" alt="The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Enhanced Encoder-Decoder with Atrous Depthwise Separable A1 Convolutional Neural Networks" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Transformer js-id-Semantic-Segmentation js-id-Decoder-Design js-id-Swin-Transformer js-id-Vision-Transformer js-id-Self-Attention js-id-Global-Convolutional-Network">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/" >Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</a>
      </div>

      
      <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/"  class="summary-link">
        <div class="article-style">
          Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% 𝐹1 score), Thailand North Landsat-8 corpus (63.12% 𝐹1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sesning</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/13/24/5100" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/transformer-based-decoder-designs-for-semantic-segmentation/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/transformer-based-decoder-designs-for-semantic-segmentation/" >
        <img src="/publication/transformer-based-decoder-designs-for-semantic-segmentation/compact_huff770b2ecf5c710cbddd5786ef8411d7_456055_300x0_resize_lanczos_3.png" alt="Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Convolutional-Neural-Networks js-id-Landsat-8 js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Global-Convolutional-Network js-id-Encoder-Decoder-Networks js-id-ISPRS-Vaihingen js-id-Transfer-Learning js-id-Domain-Adaptation js-id-Channel-Attention js-id-Depthwise-Atrous-Convolution js-id-Feature-Fusion js-id-Remote-Sensing">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" >Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network</a>
      </div>

      
      <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/"  class="summary-link">
        <div class="article-style">
          My PhD thesis focuses on improving semantic segmentation of aerial and satellite images, a crucial task for applications like agriculture planning, map updates, route optimization, and navigation. Current models like the Deep Convolutional Encoder-Decoder (DCED) have limitations in accuracy due to their inability to recover low-level features and the scarcity of training data. To address these issues, I propose a new architecture with five key enhancements, a Global Convolutional Network (GCN) for improved feature extraction, channel attention for selecting discriminative features, domain-specific transfer learning to address data scarcity, Feature Fusion (FF) for capturing low-level details, and Depthwise Atrous Convolution (DA) for refining features. Experiments on Landsat-8 datasets and the ISPRS Vaihingen benchmark showed that my proposed architecture significantly outperforms the baseline models in remote sensing imagery.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Chulalongkorn University</em> <strong>Thesis Evaluation - Very Good Score (Outstanding Achievement)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://digital.car.chula.ac.th/chulaetd/8534/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/FusionNetGeoLabel" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener">
  Dataset
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/FusionNetGeoLabel/" target="_blank" rel="noopener">
  Project
</a>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_phd_defense_2020.pdf" target="_blank" rel="noopener">
  Slides
</a>




  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/picture/phd_defense_day.jpg" target="_blank" rel="noopener">
  Source Document
</a>



      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/" >
        <img src="/publication/phd-thesis-semantic-segmentation-on-remotely-sensed-images-using-deep-learning/compact_hucc5c721db64a119cec0d8b135db7f95b_676763_300x0_resize_lanczos_3.png" alt="Semantic Segmentation on Remotely Sensed Images Using Deep Convolutional Encoder-Decoder Neural Network" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Feature-Fusion js-id-Transfer-Learning js-id-Remote-Sensing js-id-ISPRS-Vaihingen-Dataset">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/" >Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution</a>
      </div>

      
      <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/"  class="summary-link">
        <div class="article-style">
          This paper addresses improving semantic segmentation in remote sensing for aerial and satellite images, which is crucial for agriculture, map updates, route optimization, and navigation. We propose enhancements to the state-of-the-art Enhanced Global Convolutional Network (GCN152-TL-A) by introducing a High-Resolution Representation (HR) backbone for better feature extraction, Feature Fusion (FF) to capture low-level details, and Depthwise Atrous Convolution (DA) for refined multi-resolution features. Experiments on Landsat-8 and ISPRS Vaihingen datasets demonstrate our model&rsquo;s superior performance, achieving over 90% accuracy in F1 scores and outperforming baseline models.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/12/8/1233" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/" >
        <img src="/publication/feature-fusion-based-enhanced-global-convolutional-network-with-high-resolution-representations-and-depthwise-atrous-convolution/compact_hu8e1f0cf6e17ba7cf8fe2dcb2d4677106_777696_300x0_resize_lanczos_3.png" alt="Semantic Labeling in Remote Sensing Corpora Using Feature Fusion-Based Enhanced Global Convolutional Network with High-Resolution Representations and Depthwise Atrous Convolution" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Global-Convolutional-Network js-id-Transfer-Learning js-id-Channel-Attention js-id-Remote-Sensing js-id-Discriminative-Filters">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/" >Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning</a>
      </div>

      
      <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/"  class="summary-link">
        <div class="article-style">
          In the remote sensing domain, it is crucial to complete semantic segmentation on the raster images, e.g., river, building, forest, etc., on raster images. A deep convolutional encoder–decoder (DCED) network is the state-of-the-art semantic segmentation method for remotely sensed images. However, the accuracy is still limited, since the network is not designed for remotely sensed images and the training data in this domain is deficient. In this paper, we aim to propose a novel CNN for semantic segmentation particularly for remote sensing corpora with three main contributions. First, we propose applying a recent CNN called a global convolutional network (GCN), since it can capture different resolutions by extracting multi-scale features from different stages of the network. Additionally, we further enhance the network by improving its backbone using larger numbers of layers, which is suitable for medium resolution remotely sensed images. Second, “channel attention” is presented in our network in order to select the most discriminative filters (features). Third, “domain-specific transfer learning” is introduced to alleviate the scarcity issue by utilizing other remotely sensed corpora with different resolutions as pre-trained data. The experiment was then conducted on two given datasets (i) medium resolution data collected from Landsat-8 satellite and (ii) very high resolution data called the ISPRS Vaihingen Challenge Dataset. The results show that our networks outperformed DCED in terms of 𝐹1 for 17.48% and 2.49% on medium and very high resolution corpora, respectively.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sesning</em>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/11/1/83" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/" >
        <img src="/publication/an-enhanced-global-convolutional-network-with-channel-attention-and-domain-specific-transfer-learning/compact_hu37254e66b17731df190db781954cf922_1620348_300x0_resize_lanczos_3.png" alt="Semantic Segmentation on Remotely Sensed Images Using an Enhanced Global Convolutional Network with Channel Attention and Domain Specific Transfer Learning" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-YOLOv5 js-id-Quality-of-Life-(QOL) js-id-Semantic-Segmentation js-id-Object-Detection js-id-Image-Recognition">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/" >Transportation Mobility Factor Extraction Using Image Recognition Techniques</a>
      </div>

      
      <a href="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/"  class="summary-link">
        <div class="article-style">
          For an urban development, the Quality of Life (QOL) of people in the city is a vital issue that should be considered. There are many researches in QOL topics that use questionnaire survey approach. These studies yield very useful information for city development planning. As the Artificial Intelligence technologies are developed very fast recently, they are applied to solve many transportation problems. In this paper, we propose a method that automatically extract mobility indicators using two image recognition techniques, Semantic Segmentation and Object Recognition. Because the mobility is an important factor in QOL evaluation, our work can be used to enhance a performance and reduce a data gathering cost of the QOL evaluation.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >P. Kantavat</span>, <span >Y. Hayashi</span>, <span >G. City</span>, <span >B. Kijsirikul</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >Y. Iwahori</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>First International Conference on Smart Technology &amp; Urban Development</em> <strong>Best Paper; (STUD 2019)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/9018796" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/quality-of-life-ai-transportation" target="_blank" rel="noopener">
  Code
</a>




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/quality-of-life-ai-transportation/" target="_blank" rel="noopener">
  Project
</a>










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/quality-of-life-ai-transportation/" target="_blank" rel="noopener">
    
    GitHub Page
  </a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://kaopanboonyuen.github.io/quality-of-life-ai-transportation/" target="_blank" rel="noopener">
    
    Best Young Researcher Paper Award
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/" >
        <img src="/publication/transportation-mobility-factor-extraction-using-image-recognition-techniques/compact_hu66f853e7e796cad48a347fa6c9fa4aae_566432_300x0_resize_lanczos_3.png" alt="Transportation Mobility Factor Extraction Using Image Recognition Techniques" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Colorectal-Cancer js-id-Fully-Convolutional-Network js-id-CVC-EndoSceneStill js-id-Colonoscopy-Video">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" >Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network</a>
      </div>

      
      <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/"  class="summary-link">
        <div class="article-style">
          Colorectal cancer is one of the leading causes of cancer death worldwide. As of now, colonoscopy is the most effective screening tool for diagnosing colorectal cancer by searching for polyps which can develop into colon cancer. The drawback of manual colonoscopy process is its high polyp miss rate. Therefore, polyp detection is a crucial issue in the development of colonoscopy application. Despite having high evaluation scores, the recently published methods based on fully convolutional network (FCN) require a very long inferring (testing) time that cannot be applied in a real clinical process due to a large number of parameters in the network. In this paper, we proposed a compressed fully convolutional network by modifying the FCN-8s network, so our network is able to detect and segment polyp from video images within a real-time constraint in a practical screening routine. Furthermore, our customized loss function allows our network to be more robust when compared to the traditional cross-entropy loss function. The experiment was conducted on CVC-EndoSceneStill database which consists of 912 video frames from 36 patients. Our proposed framework has obtained state-of-the-art results while running more than 7 times faster and requiring fewer weight parameters by more than 9 times. The experimental results convey that our system has the potential to support clinicians during the analysis of colonoscopy video by automatically indicating the suspicious polyps locations.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >I. Wichakam</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >C. Udomcharoenchaikit</span>, <span >P. Vateekul</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Multimedia Modeling</em> <strong>MMM 2018</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-73603-7_32" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/" >
        <img src="/publication/real-time-polyps-segmentation-for-colonoscopy-video-frames/compact_hu3df151b013ef19e8db16ff9485f84888_309033_300x0_resize_lanczos_3.png" alt="Real-Time Polyps Segmentation for Colonoscopy Video Frames Using Compressed Fully Convolutional Network" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Convolutional-Neural-Networks js-id-Semantic-Segmentation js-id-Near-Infrared-(NIR) js-id-Short-Wave-Infrared-(SWIR) js-id-Remote-Sensing">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/semantic-segmentation-on-medium-resolution-satellite-images/" >Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices</a>
      </div>

      
      <a href="/publication/semantic-segmentation-on-medium-resolution-satellite-images/"  class="summary-link">
        <div class="article-style">
          Semantic Segmentation is a fundamental task in computer vision and remote sensing imagery. Many applications, such as urban planning, change detection, and environmental monitoring, require the accurate segmentation; hence, most segmentation tasks are performed by humans. Currently, with the growth of Deep Convolutional Neural Network (DCNN), there are many works aiming to find the best network architecture fitting for this task. However, all of the studies are based on very-high resolution satellite images, and surprisingly; none of them are implemented on medium resolution satellite images. Moreover, no research has applied geoinformatics knowledge. Therefore, we purpose to compare the semantic segmentation models, which are FCN, SegNet, and GSN using medium resolution images from Landsat-8 satellite. In addition, we propose a modified SegNet model that can be used with remote sensing derived indices. The results show that the model that achieves the highest accuracy RGB bands of medium resolution aerial imagery is SegNet. The overall accuracy of the model increases when includes Near Infrared (NIR) and Short-Wave Infrared (SWIR) band. The results showed that our proposed method (our modified SegNet model, named RGB-IR-IDX-MSN method) outperforms all of the baselines in terms of mean F1 scores.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >S. Chantharaj</span>, <span >K. Pornratthanapong</span>, <span >P. Chitsinpchayakun</span>, <span class="author-highlighted">Teerapong Panboonyuen</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In *15th International Joint Conference on Computer Science and Software Engineering * <strong>JCSSE 2018</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/8457378/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/semantic-segmentation-on-medium-resolution-satellite-images/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx" target="_blank" rel="noopener">
  Dataset
</a>












      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/semantic-segmentation-on-medium-resolution-satellite-images/" >
        <img src="/publication/semantic-segmentation-on-medium-resolution-satellite-images/compact_hua87129e6e693cceff941178b50e40d9b_837150_300x0_resize_lanczos_3.png" alt="Semantic Segmentation On Medium-Resolution Satellite Images Using Deep Convolutional Networks With Remote Sensing Derived Indices" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Road-Segmentation js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Exponential-Linear-Unit js-id-Conditional-Random-Fields js-id-Landscape-Metrics">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-segmentation-on-remote-sensing/" >Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields</a>
      </div>

      
      <a href="/publication/road-segmentation-on-remote-sensing/"  class="summary-link">
        <div class="article-style">
          Semantic segmentation of remotely-sensed aerial (or very-high resolution, VHS) images and satellite (or high-resolution, HR) images has numerous application domains, particularly in road extraction, where the segmented objects serve as essential layers in geospatial databases. Despite several efforts to use deep convolutional neural networks (DCNNs) for road extraction from remote sensing images, accuracy remains a challenge. This paper introduces an enhanced DCNN framework specifically designed for road extraction from remote sensing images by incorporating landscape metrics (LMs) and conditional random fields (CRFs). Our framework employs the exponential linear unit (ELU) activation function to improve the DCNN, leading to a higher quantity and more accurate road extraction. Additionally, to minimize false classifications of road objects, we propose a solution based on the integration of LMs. To further refine the extracted roads, a CRF method is incorporated into our framework. Experiments conducted on Massachusetts road aerial imagery and Thailand Earth Observation System (THEOS) satellite imagery datasets demonstrated that our proposed framework outperforms SegNet, a state-of-the-art object segmentation technique, in most cases regarding precision, recall, and F1 score across various types of remote sensing imagery.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Remote Sensing</em> <strong>Impact Factor 4.2</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.mdpi.com/2072-4292/9/7/680" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-remote-sensing/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
    
    IC2IT 2017 Best Paper Award
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-segmentation-on-remote-sensing/" >
        <img src="/publication/road-segmentation-on-remote-sensing/compact_hu9f88805f555d8244de91ac83c5923989_1612368_300x0_resize_lanczos_3.png" alt="Road segmentation of remotely-sensed images using deep convolutional neural networks with landscape metrics and conditional random fields" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Road-Segmentation js-id-Deep-Learning js-id-Semantic-Segmentation js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-Convolutional-Neural-Networks js-id-Encoder-Decoder-Networks js-id-Exponential-Linear-Unit">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-segmentation-on-aerial-imagery/" >An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery</a>
      </div>

      
      <a href="/publication/road-segmentation-on-aerial-imagery/"  class="summary-link">
        <div class="article-style">
          In this paper, we introduce an improved deep convolutional encoder-decoder network (DCED) for segmenting road objects from aerial images. Enhancements include the use of ELU (exponential linear unit) instead of ReLU, dataset augmentation with incrementally-rotated images to increase training data by eight times, and the use of landscape metrics to remove false road objects. Tested on the Massachusetts Roads dataset, our method outperformed the SegNet benchmark and other baselines in precision, recall, and F1 scores.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>International Conference on Computing and Information Technology (IC2IT)</em> <strong>Best Student Paper Honourable Mention (top 0.26% of submissions)</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-segmentation-on-aerial-imagery/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/files/panboonyuen_IC2IT2017_BestPaperAward.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/chapter/10.1007/978-3-319-60663-7_18" target="_blank" rel="noopener">
    
    IC2IT 2017 Best Paper Award
  </a>

      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-segmentation-on-aerial-imagery/" >
        <img src="/publication/road-segmentation-on-aerial-imagery/compact_hua498d26fbf435a648e4226cf29584fa2_591805_300x0_resize_lanczos_3.png" alt="An enhanced deep convolutional encoder-decoder network for road segmentation on aerial imagery" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Road-Segmentation js-id-Connected-Component-Analysis js-id-Image-Processing">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/road-map-extraction-from-satellite-imagery/" >Road map extraction from satellite imagery using connected component analysis and landscape metrics</a>
      </div>

      
      <a href="/publication/road-map-extraction-from-satellite-imagery/"  class="summary-link">
        <div class="article-style">
          Road map extraction is vital for GIS and underpins many location-based applications like GPS navigation, delivery route planning, tourist attraction locating, and location-based marketing. This research uses satellite imagery, though other remotely sensed images like aerial photographs, UAVs, or drones are also applicable. Despite various proposed methods focusing primarily on accuracy, completeness of results is equally important. We enhance accuracy by incorporating connected component analysis and improve completeness using landscape metrics, which describe spatial characteristics through shape and isolation indices. Evaluated on precision, recall, quality, and F1 scores, our method achieves over 90% performance in all criteria.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>IEEE International Conference on Big Data</em> <strong>Big Data 2017</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/8258330" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/road-map-extraction-from-satellite-imagery/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/road-map-extraction-from-satellite-imagery/" >
        <img src="/publication/road-map-extraction-from-satellite-imagery/compact_hu97a7f026cf9c527f7189f7bb72e86721_577453_300x0_resize_lanczos_3.png" alt="Road map extraction from satellite imagery using connected component analysis and landscape metrics" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    

      
      
      

      
        


<div class="col-12 isotope-item js-id-Remote-Sensing js-id-Road-Segmentation js-id-Spatio-Temporal-Data js-id-High-Resolution-Imagery js-id-Aerial-Imagery js-id-K-Means-Clustering js-id-Ramer-Douglas-Peucker">
  







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/publication/image-vectorization-of-road-satellite-data-sets/" >Image Vectorization of Road Satellite Data Sets</a>
      </div>

      
      <a href="/publication/image-vectorization-of-road-satellite-data-sets/"  class="summary-link">
        <div class="article-style">
          Data extraction of geo-spatial objects from satellite images is a crucial step in facilitating analysis of geo-spatial or spatio-temporal data, typically involving line (road) and polygon (area) layers. This paper introduces a method for transforming satellite data (raster images) containing roads from pixel form into spatial objects comprising lines and polygons. Our algorithm consists of three primary steps. First, roads are isolated from other objects using k-means clustering. Second, line extraction is performed on the road areas by applying morphological operations to skeletonize the image, followed by enhancement using the Ramer-Douglas-Peucker algorithm. Finally, land-cover classification is applied to non-road objects to extract polygons. Experimental results demonstrate that both lines (road networks) and polygons (areas) can be accurately extracted from satellite imagery simultaneously.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        

        
          


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">Teerapong Panboonyuen</span>, <span >P. Vateekul</span>, <span >S. Lawawirojwong</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      In <em>Journal of Remote Sensing and GIS Association of Thailand</em> <strong>RESGAT</strong>
    
  </span>
  

  

  
  
  
  
  
  

  
  

</div>

        
      </div>

      
      <div class="btn-links">
        








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://learn.gistda.or.th/wp-content/uploads/2017/06/GISTDA-Research-Image-Understaning-2559-Image-Vectorization.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/image-vectorization-of-road-satellite-data-sets/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/kaopanboonyuen/" target="_blank" rel="noopener">
  Code
</a>













      </div>
      

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
      
      
      <a href="/publication/image-vectorization-of-road-satellite-data-sets/" >
        <img src="/publication/image-vectorization-of-road-satellite-data-sets/compact_hu5f735fde0cbbf0b088a30bae6537e8b5_2587210_300x0_resize_lanczos_3.png" alt="Image Vectorization of Road Satellite Data Sets" loading="lazy">
      </a>
      
    </div>
  </div>
</div>  
</div>

      

    
  </div>
</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="tags" class="home-section wg-tag-cloud  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Popular Topics</h1>
            
          </div>
        
      
    

      








  








<div class="col-12 col-lg-8">
  

  

    
    
    
    
    
    

    <div class="tag-cloud">
      
        
        
        
        <a href="/tag/aerial-imagery/" style="font-size:1.1814030808637654rem">Aerial Imagery</a>
      
        
        
        
        <a href="/tag/ai-applications/" style="font-size:0.7rem">AI-applications</a>
      
        
        
        
        <a href="/tag/attention/" style="font-size:0.7rem">Attention</a>
      
        
        
        
        <a href="/tag/automotive-ai/" style="font-size:0.7rem">automotive-ai</a>
      
        
        
        
        <a href="/tag/bangkok-urbanscapes-dataset/" style="font-size:0.7rem">Bangkok Urbanscapes Dataset</a>
      
        
        
        
        <a href="/tag/computer-vision/" style="font-size:1.3363802575591441rem">computer-vision</a>
      
        
        
        
        <a href="/tag/convolutional-neural-networks/" style="font-size:1.4630058309007024rem">Convolutional Neural Networks</a>
      
        
        
        
        <a href="/tag/deep-learning/" style="font-size:1.944408911764468rem">deep-learning</a>
      
        
        
        
        <a href="/tag/encoder-decoder-networks/" style="font-size:1.1814030808637654rem">Encoder-Decoder Networks</a>
      
        
        
        
        <a href="/tag/global-convolutional-network/" style="font-size:0.9816027500369371rem">Global Convolutional Network</a>
      
        
        
        
        <a href="/tag/high-resolution-imagery/" style="font-size:1.3363802575591441rem">High-Resolution Imagery</a>
      
        
        
        
        <a href="/tag/landsat-8/" style="font-size:0.9816027500369371rem">Landsat-8</a>
      
        
        
        
        <a href="/tag/remote-sensing/" style="font-size:1.8839779583857523rem">remote-sensing</a>
      
        
        
        
        <a href="/tag/road-segmentation/" style="font-size:1.1814030808637654rem">Road Segmentation</a>
      
        
        
        
        <a href="/tag/running/" style="font-size:0.9816027500369371rem">Running</a>
      
        
        
        
        <a href="/tag/self-attention/" style="font-size:0.9816027500369371rem">Self-Attention</a>
      
        
        
        
        <a href="/tag/semantic-segmentation/" style="font-size:1.6628061617275307rem">Semantic Segmentation</a>
      
        
        
        
        <a href="/tag/transfer-learning/" style="font-size:1.1814030808637654rem">Transfer Learning</a>
      
        
        
        
        <a href="/tag/transformer/" style="font-size:0.9816027500369371rem">Transformer</a>
      
        
        
        
        <a href="/tag/vision-transformer/" style="font-size:1.1814030808637654rem">Vision Transformer</a>
      
    </div>
  

</div>


    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="talks" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Featured Talks</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  

  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/uamc2025-undergraduate-in-applied-mathematics-conference-2025/" >UAMC2025 (Undergraduate in Applied Mathematics Conference 2025)</a>
      </div>

      
      <a href="/talk/uamc2025-undergraduate-in-applied-mathematics-conference-2025/"  class="summary-link">
        <div class="article-style">
          Discover the powerful math behind Vision Transformers (ViTs) and how they’re revolutionizing AI in the car insurance industry. We’ll dive into the self-attention mechanism that allows ViTs to process visual data with precision, explore custom loss functions to improve claim prediction accuracy, and discuss the exciting potential of integrating Large Language Models (LLMs) to elevate AI, unlocking new possibilities and driving innovation in the insurance world. By examining these cutting-edge techniques, we’ll uncover how AI is set to redefine the way insurance claims are handled, making processes smarter, faster, and more efficient.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2025 1:00 PM
          </span>
          
          <span class="middot-divider"></span>
          <span>King Mongkut&#39;s Institute of Technology Ladkrabang (KMITL)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/uamc2025-undergraduate-in-applied-mathematics-conference-2025/" >
          <img src="/talk/uamc2025-undergraduate-in-applied-mathematics-conference-2025/featured_hu1aa0874992620a4a5b125fdd892bf6ab_631062_300x0_resize_lanczos_3.png" alt="UAMC2025 (Undergraduate in Applied Mathematics Conference 2025)" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/nac2025-nstda-annual-conference-2025/" >NAC2025 (NSTDA Annual Conference 2025)</a>
      </div>

      
      <a href="/talk/nac2025-nstda-annual-conference-2025/"  class="summary-link">
        <div class="article-style">
          In this session, I’m excited to take you on a deep dive into the cutting-edge world of artificial intelligence and computer vision, as we explore the process of training an AI model to tackle the classic challenge of &ldquo;Where’s Waldo.&rdquo; This isn&rsquo;t just about spotting a red-and-white striped figure—it&rsquo;s about unlocking the power of AI to recognize and understand intricate patterns in images. As we break down the complexities of image recognition, we’ll harness the latest techniques in deep learning, including the revolutionary Vision Transformers, to train a model that can pick out Waldo from a sea of distractions. This session will equip you with hands-on experience in AI model development, giving you the tools and knowledge to apply to a wide range of real-world problems in visual recognition.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2025 9:30 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>National Science and Technology Development Agency (NSTDA)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/nac2025-nstda-annual-conference-2025/" >
          <img src="/talk/nac2025-nstda-annual-conference-2025/featured_hu886a67331a8a992673e34dee8a816b44_2380872_300x0_resize_lanczos_3.png" alt="NAC2025 (NSTDA Annual Conference 2025)" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/young-scientists-quickfire-pitch/" >Young Scientists Quickfire Pitch</a>
      </div>

      
      <a href="/talk/young-scientists-quickfire-pitch/"  class="summary-link">
        <div class="article-style">
          In this quick pitch, I’m thrilled to introduce MeViT—a medium-resolution Vision Transformer developed for high-precision semantic segmentation of Landsat satellite imagery.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2024 6:30 PM
          </span>
          
          <span class="middot-divider"></span>
          <span>National University of Singapore, Singapore</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
          <iframe width="300" src="https://www.youtube.com/embed/tgcKR97Ea8I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/exploring-careers-as-an-ai-research-scientist/" >Exploring Careers as an AI Research Scientist</a>
      </div>

      
      <a href="/talk/exploring-careers-as-an-ai-research-scientist/"  class="summary-link">
        <div class="article-style">
          I recently had the opportunity to speak with high school students about ‘Career Paths for AI Research Scientists.’ During the talk, I shared my experiences as a postdoctoral researcher in AI, diving into the exciting world of artificial intelligence. I discussed the various career opportunities in the field, from academic research to industry roles, and highlighted how my own journey has shaped my understanding of Generative AI.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2024 9:05 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>NSTDA, Pathum Thani, Thailand</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/exploring-careers-as-an-ai-research-scientist/" >
          <img src="/talk/exploring-careers-as-an-ai-research-scientist/featured_hu9f97ca678baa234138ccd8f90bbea5df_3761120_300x0_resize_lanczos_3.png" alt="Exploring Careers as an AI Research Scientist" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/inspiring-the-future-of-ai-innovations/" >Inspiring the Future of AI Innovations</a>
      </div>

      
      <a href="/talk/inspiring-the-future-of-ai-innovations/"  class="summary-link">
        <div class="article-style">
          I had the opportunity to give a final orientation speech to the undergraduate students of the Department of Electrical and Computer Engineering at KMUTNB. The focus of my speech was on the transformative impact of AI, particularly highlighting the advancements in Large Language Models (LLMs) like ChatGPT. I discussed how these models have revolutionized natural language processing, enabling sophisticated interactions and problem-solving capabilities.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2024 9:09 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>ECE KMUTNB, BKK</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/inspiring-the-future-of-ai-innovations/" >
          <img src="/talk/inspiring-the-future-of-ai-innovations/featured_hu5c5d44915c3b7c0a66ab40e234b02de5_430879_300x0_resize_lanczos_3.png" alt="Inspiring the Future of AI Innovations" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/geospatial-big-data-analytics/" >Geospatial Big Data Analytics</a>
      </div>

      
      <a href="/talk/geospatial-big-data-analytics/"  class="summary-link">
        <div class="article-style">
          Geospatial Data Analytics involves analyzing spatial and geographical data to gain insights and make informed decisions. Using PySpark, this process is accelerated through distributed computing, enabling the handling of large datasets efficiently. Distributed Machine Learning models further enhance the analysis by providing scalable and robust predictions. Visualization tools like Looker Studio present the analyzed data in an interactive and comprehensible format, facilitating better decision-making and strategic planning. This combination of technologies allows for comprehensive geospatial data analysis, uncovering patterns and trends that drive actionable insights.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2023 8:15 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>Geo-Informatics and Space Technology Development Agency (GISTDA)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/geospatial-big-data-analytics/" >
          <img src="/talk/geospatial-big-data-analytics/featured_huadd97d1e3023033bd097289582bc5705_219718_300x0_resize_lanczos_3.png" alt="Geospatial Big Data Analytics" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/invited-to-italy-for-iciap2023/" >Invited to Italy for ICIAP2023</a>
      </div>

      
      <a href="/talk/invited-to-italy-for-iciap2023/"  class="summary-link">
        <div class="article-style">
          I was invited to Italy to present my research, &ldquo;MARS, Mask Attention Refinement with Sequential Quadtree Nodes,&rdquo; at the ICIAP 2023 Workshop. This prestigious conference, organized biennially by CVPL under the International Association for Pattern Recognition (IAPR), will unite experts to discuss advancements in car insurance and computer vision. My research addresses the challenge of accurately evaluating car damages using MARS, which enhances instance segmentation through self-attention mechanisms and quadtree transformers.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2023 10:30 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>University of Udine, Italy</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/invited-to-italy-for-iciap2023/" >
          <img src="/talk/invited-to-italy-for-iciap2023/featured_hu9964f41bcd0a09cbed35d2d2fee2bab5_5377881_300x0_resize_lanczos_3.png" alt="Invited to Italy for ICIAP2023" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/distributed-machine-learning-techniques-for-geospatial-data/" >Distributed Machine Learning Techniques for Geospatial Data</a>
      </div>

      
      <a href="/talk/distributed-machine-learning-techniques-for-geospatial-data/"  class="summary-link">
        <div class="article-style">
          I was invited to teach a course on distributed machine learning to the Geo-Informatics and Space Technology Development Agency (GISTDA). The curriculum covered fundamental concepts of PySpark, basic deep learning techniques, and practical applications of distributed training using TensorFlow. I also emphasized methods for leveraging Multi-GPU setups and implementing distributed training strategies, particularly in the context of geospatial data analytics, equipping participants with the skills needed to handle large-scale machine learning tasks efficiently.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2022 8:45 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>Geo-Informatics and Space Technology Development Agency (GISTDA)</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/distributed-machine-learning-techniques-for-geospatial-data/" >
          <img src="/talk/distributed-machine-learning-techniques-for-geospatial-data/featured_hudd1a16a7aafc92c0827260627632f5e6_237927_300x0_resize_lanczos_3.png" alt="Distributed Machine Learning Techniques for Geospatial Data" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/achieve-data-science-first-meet/" >Achieve Data Science First Meet</a>
      </div>

      
      <a href="/talk/achieve-data-science-first-meet/"  class="summary-link">
        <div class="article-style">
          I was invited to speak at the &ldquo;Achieve Data Science First Meet&rdquo; for a MOOC student project event, where I highlighted the growing recognition of data science, AI, and machine learning&rsquo;s importance across various industries. I advised that organizations, regardless of their size or sector, must effectively develop and implement data science capabilities to stay competitive in the era of big data, or risk falling behind.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2020 9:30 AM
          </span>
          
          <span class="middot-divider"></span>
          <span>Victor Club, Samyan Mitrtown, BKK</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/achieve-data-science-first-meet/" >
          <img src="/talk/achieve-data-science-first-meet/featured_hud3fd4aee595bfd6af19daa13643bde93_188092_300x0_resize_lanczos_3.png" alt="Achieve Data Science First Meet" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  
    
      







  







  


<div class="container">
  <div class="row media stream-item">
    <div class="col col-sm-12 col-lg-8 col-md-7 ml-3 media-body">

      <div class="section-subheading article-title mb-0 mt-0">
        <a href="/talk/ph.d.-thesis-defense/" >Ph.D. Thesis Defense</a>
      </div>

      
      <a href="/talk/ph.d.-thesis-defense/"  class="summary-link">
        <div class="article-style">
          This dissertation introduces a new architecture for remote sensing, featuring Global Convolutional Network (GCN), channel attention, domain-specific transfer learning, Feature Fusion (FF), and Depthwise Atrous Convolution (DA). Tests on Landsat-8 and ISPRS Vaihingen datasets show that this model significantly outperforms the baseline.
        </div>
      </a>
      

      <div class="stream-meta article-metadata">

        
        <div>
          <span>
            2020 2:00 PM
          </span>
          
          <span class="middot-divider"></span>
          <span>Chulalongkorn University</span>
          
        </div>
        

        
      </div>

    </div>
    <div class="col col col-sm-12 col-lg-4 col-md-5 ml-3 mb-3">
      
        
        
        
        <a href="/talk/ph.d.-thesis-defense/" >
          <img src="/talk/ph.d.-thesis-defense/featured_hu9ccf3f7a1b6dbc32658d8788de9ecd24_1799990_300x0_resize_lanczos_3.png" alt="Ph.D. Thesis Defense" loading="lazy">
        </a>
        
      
    </div>
  </div>
</div>  
    
  

  
  
  

    
    
      
    

    

    <div class="see-all">
      <a href="/event/">
        See all events
        <i class="fas fa-angle-right"></i>
      </a>
    </div>
  

</div>

    
      </div>
    

    </div>
  </section>

  
  
  
  

  

  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="communities" class="home-section wg-pages  "  >
   <div class="home-section-bg " ></div>
    <div class="container">

    
      <div class="row  ">
      
        
          <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
            <h1 class="mb-0">Research Communities</h1>
            
          </div>
        
      
    

      








  
























  





  




<div class="col-12 col-lg-8">

  <ul>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku.pdf" target="_blank" rel="noopener">Visiting Faculty - College of Computing, Khon Kaen University</a></p>
<ul>
<li>
<p>June 2023 - Present</p>
</li>
<li>
<p>Teach courses:</p>
<ul>
<li><a href="https://github.com/kaopanboonyuen/SC310005_ArtificialIntelligence_2023s1" target="_blank" rel="noopener">SC310005 Artificial Intelligence and Machine Learning Application</a>: Introduction to AI and ML concepts and their applications.</li>
<li><a href="https://github.com/kaopanboonyuen/CP020002_SmartProcessManagement_2024s1" target="_blank" rel="noopener">CP020002 Smart Process Management</a>: Techniques for optimizing and automating business processes.</li>
<li><a href="https://github.com/kaopanboonyuen/CS101" target="_blank" rel="noopener">SC320002 Business Intelligence</a>: Methods for data analysis and decision-making in business contexts.</li>
<li><a href="https://github.com/kaopanboonyuen/CP020001_ComputerProgramming_2023s1" target="_blank" rel="noopener">CP020001 Introduction to Computers and Programming</a>: Basics of computer systems and introductory programming skills.</li>
<li><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toKKU_DataEngineer_2025.pdf" target="_blank" rel="noopener">DE200001 Fundamentals of Data Engineering</a>: Introduction to data engineering concepts and fundamental tools for beginners.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku.pdf" target="_blank" rel="noopener">Ministerial Order on the Appointment of Academic Staff (Order 5907-2566)</a></p>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku_ai_2025.pdf" target="_blank" rel="noopener">Invitation Letter for a Special Lecturer Position (Order อว 660101.26/9304)</a></p>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku_ai.pdf" target="_blank" rel="noopener">Invitation Letter for a Special Lecturer Position (Order อว 660101.26/24844)</a></p>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku_spm.pdf" target="_blank" rel="noopener">Invitation Letter for a Special Lecturer Position (Order อว 660101.26/13320)</a></p>
</li>
</ul>
</li>
<li>
<p>Guest Lecturer and AI Committee Member</p>
<ul>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/UAMC2025_TVIxMARSxKMITL.pdf" target="_blank" rel="noopener">Keynote Speaker - UAMC2025</a></p>
<ul>
<li>Delivered keynote on &ldquo;Mathematical Foundations of Vision Transformers in Car Insurance AI.&rdquo;</li>
<li><a href="https://kaopanboonyuen.github.io/files/poster/UAMC2025_TVIxMARSxKMITL_Poster.png" target="_blank" rel="noopener">Poster: UAMC2025</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toDOL_2025.pdf" target="_blank" rel="noopener">AI Instructor - Department of Lands, Thailand (2025)</a></p>
<ul>
<li>Taught Large Language Models (LLMs) using land title deed data and demonstrated AI-driven automation for creating land deeds.</li>
<li><a href="https://github.com/kaopanboonyuen/ai_for_dept_of_lands" target="_blank" rel="noopener">Code and Lecture Slides</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toKMUTNB_OCSB_2025.pdf" target="_blank" rel="noopener">AI Instructor - Office of the Cane and Sugar Board (2025)</a></p>
<ul>
<li>Leveraged Vision Transformer model for accurate sugarcane area classification and boundary detection from satellite images.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/GYSS/panboonyuen_NSTDA2024_invitation_letter.pdf" target="_blank" rel="noopener">NSTDA One Day Camp at Sirindhorn Science Home (2024)</a></p>
<ul>
<li>Talking about career opportunities and becoming a research scientist in AI as part of the GYSS2025 scholarship program.</li>
<li><a href="https://kaopanboonyuen.github.io/blog/2024-09-01-career-paths-for-ai-research-scientist/" target="_blank" rel="noopener">Full Blog and Slide: Career Paths for AI Research Scientists</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toSurvey_CU_2023.pdf" target="_blank" rel="noopener">2108421 Modern Integrated Survey Technology (MIST) - Chulalongkorn University</a></p>
<ul>
<li>Guided students in applying Machine Learning to survey engineering.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/panboonyuen_kku_ai_inspiration.pdf" target="_blank" rel="noopener">CP411701 AI Inspiration Course - Khon Kaen University</a></p>
<ul>
<li>Delivered a lecture on &ldquo;Generative AI: Current Trends and Practical Applications&rdquo; at the College of Computing, Khon Kaen University.</li>
<li><a href="https://kaopanboonyuen.github.io/blog/2024-08-02-generative-ai-current-trends-and-practical-applications/" target="_blank" rel="noopener">Slide: Generative AI</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/KVIS_Science_Fair_2024_Panboonyuen.pdf" target="_blank" rel="noopener">The 7th KVIS Invitational Science Fair</a></p>
<ul>
<li>Served as a committee member for the AI project at Kamnoetvidya Science Academy, Rayong, Thailand (29 January - 2 February 2024).</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toKMUTNB_asIAB.pdf" target="_blank" rel="noopener">Industrial Advisory Board (IAB) - ECE KMUTNB</a></p>
<ul>
<li>Contributed to curriculum assessment and provided comments for the Department of Electrical and Computer Engineering (ECE).</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toKunnatee_AI_2021.pdf" target="_blank" rel="noopener">AI and ML Instructor - Nomklao Kunnathi Demonstration School</a></p>
<ul>
<li>Taught AI and ML under the Design Graphics Science and Technology Learning Group for high school students (Grade 10) in the Science and Mathematics Curriculum Plan.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toThammasat_AI_2023.pdf" target="_blank" rel="noopener">Deep Learning Instructor - Thammasat University</a></p>
<ul>
<li>Conducted training on satellite data processing and interpretation for advanced military and disaster missions at the Faculty of Liberal Arts.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toThammasat_AI_Projects_2022.pdf" target="_blank" rel="noopener">Senior Project Advisor - Thammasat University</a></p>
<ul>
<li>Advised students on senior projects in the Department of Geography, Faculty of Liberal Arts.</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/files/invitation_letters/Invitation_toLandDept_AI_2024.pdf" target="_blank" rel="noopener">AI Instructor - Department of Lands, Thailand</a></p>
<ul>
<li>Taught AI using land title deed data. <a href="https://github.com/kaopanboonyuen/ai_for_dept_of_lands" target="_blank" rel="noopener">Code and Lecture Slides</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/quality-of-life-ai-transportation/" target="_blank" rel="noopener">AI-Powered Image Recognition for Transportation Mobility Factors: A Quality of Life Perspective for Bangkok City</a></p>
<ul>
<li>Urban development hinges on improving the Quality of Life (QOL) for city inhabitants. Traditionally, QOL assessments rely heavily on questionnaire surveys, which, while informative, can be costly and time-consuming.</li>
<li><a href="https://kaopanboonyuen.github.io/quality-of-life-ai-transportation/" target="_blank" rel="noopener">Project (GitHub Page)</a>, <a href="https://ieeexplore.ieee.org/document/9018796" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/kaopanboonyuen/quality-of-life-ai-transportation" target="_blank" rel="noopener">Code</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/MeViT/" target="_blank" rel="noopener">Medium-Resolution Vision Transformer for Semantic Segmentation on Landsat Satellite Imagery in Thailand</a></p>
<ul>
<li>This project introduces MeViT (Medium-Resolution Vision Transformer), a novel approach tailored for Landsat satellite imagery of key economic crops in Thailand, including para rubber, corn, and pineapple.</li>
<li><a href="https://kaopanboonyuen.github.io/MeViT/" target="_blank" rel="noopener">Project (GitHub Page)</a>, <a href="https://www.mdpi.com/2072-4292/15/21/5124" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/kaopanboonyuen/MeViT" target="_blank" rel="noopener">Code</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/rainfall-prediction-a-machine-learning-approach" target="_blank" rel="noopener">Flood Risk Assessment in Ayutthaya Province</a></p>
<ul>
<li>This project explores a variety of models, including Random Forest, Gradient Boosting, and Neural Networks, to build a predictive model using relevant features from the dataset.</li>
<li><a href="https://kaopanboonyuen.github.io/rainfall-prediction-a-machine-learning-approach" target="_blank" rel="noopener">Project (GitHub Page)</a>, <a href="https://tis.wu.ac.th/index.php/tis/article/view/2038" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/kaopanboonyuen/rainfall-prediction-a-machine-learning-approach" target="_blank" rel="noopener">Code</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a></li>
</ul>
</li>
<li>
<p><a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">The Bangkok Urbanscapes Dataset for Semantic Urban Scene Understanding Using Deep Learning</a></p>
<ul>
<li>To further study self-driving cars in Thailand, we provide both the proposed methods and the proposed dataset in this project. We hope that our architecture and our dataset would help self-driving autonomous developers improve systems for driving in many cities with unique traffic and driving conditions similar to Bangkok and elsewhere in Thailand.</li>
<li><a href="https://kaopanboonyuen.github.io/bkkurbanscapes" target="_blank" rel="noopener">Project (GitHub Page)</a>, <a href="https://www.cityscapes-dataset.com/" target="_blank" rel="noopener">Dataset</a>, <a href="https://ieeexplore.ieee.org/document/9779212" target="_blank" rel="noopener">Paper</a>, <a href="https://github.com/kaopanboonyuen/bkkurbanscapes" target="_blank" rel="noopener">Code</a>, <a href="https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt" target="_blank" rel="noopener">Cite</a></li>
</ul>
</li>
</ul>
<!-- * [Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama](https://www.mdpi.com/2078-2489/13/1/5)
  * Identifying road asset objects in Thailand highway monitoring image sequences is essential for intelligent traffic monitoring and administration of the highway. We introduce transformer-based Feature Pyramid Network (FPN) decoder designs, injecting the FPN style of decoder design into Transformer-based YOLOX reasoning.
  * [Project](https://www.mdpi.com/2078-2489/13/1/5), [PDF](https://www.mdpi.com/2078-2489/13/1/5/pdf?version=1640592615), [Cite](https://kaopanboonyuen.github.io/files/citation/kao_phd_chula.txt), [Code](https://github.com/kaopanboonyuen/) -->
<!-- * [Geospatial Big Data Analytics 2023](https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2023.jpg)
  * Extract data using web scraping with python, Visualizations using Looker Studio of Google, and PySpark for Machine Learning
  * [Poster](https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2023.jpg), [Code](https://github.com/kaopanboonyuen/GISTDA2023/tree/main/code), [Slides](https://github.com/kaopanboonyuen/GISTDA2023/tree/main/lecture_slides)

* [Geospatial Big Data Analytics 2022](https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2022.jpeg)
  * Demonstrate an understanding of the breadth of methods and techniques available for handling large volumes of geospatial data; use AI/ML methods with PySpark and techniques to conduct spatial analyses of big data and apply resulting analyses to problems within the student’s own discipline.
  * [Poster](https://kaopanboonyuen.github.io/files/talks/panboonyuen_Geospatial_Big_Data_Analytics_2022.jpeg), [Code](https://github.com/kaopanboonyuen/GISTDA2022/tree/main/code), [Slides](https://github.com/kaopanboonyuen/GISTDA2022/tree/main/lecture_slides)

* [Achieve Data Science First Meet](https://kaopanboonyuen.github.io/files/talks/panboonyuen_data_science_talk.jpeg)
  * More and more companies realize the importance of data science, AI, and machine learning. Regardless of industry or size, organizations that wish to remain competitive in the age of big data ought to efficiently originate and implement data science capabilities or risk being left behind.
  * [Poster](https://kaopanboonyuen.github.io/files/talks/panboonyuen_data_science_talk.jpeg), [Slides](https://kaopanboonyuen.github.io/files/talks/panboonyuen_talks_2020.pdf) -->

  

  
  
  

</div>

    
      </div>
    

    </div>
  </section>



  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  

  

  
  <p class="powered-by">
    ©2025 Kao Panboonyuen
  </p>
  

  
  






  <p class="powered-by">
    
    Built using <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> and the <a href="https://github.com/wowchemy/starter-hugo-academic" target="_blank" rel="noopener">Wowchemy academic template</a>. View <a href="https://github.com/kaopanboonyuen/kaopanboonyuen.github.io" target="_blank" rel="noopener">source</a>.
        
  </p>
</footer>
    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js" integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/golang.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    <script src="/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js"></script>

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.b36873e4e886c7b03b21e4eb97d9b6d7.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
